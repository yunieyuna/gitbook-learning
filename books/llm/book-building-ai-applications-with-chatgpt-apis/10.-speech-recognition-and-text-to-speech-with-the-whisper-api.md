# 10. Speech Recognition and Text-to-Speech with the Whisper API

## 10. Speech Recognition and Text-to-Speech with the Whisper API <a href="#_idparadest-113" id="_idparadest-113"></a>

Welcome to [_Chapter 10_](https://learning.oreilly.com/library/view/building-ai-applications/9781805127567/B21110\_10.xhtml#\_idTextAnchor120) of our journey into the world of cutting-edge AI technologies. In this chapter, we embark on an exploration of the remarkable **Whisper API**. Harnessing the power of advanced speech recognition and translation, the Whisper API opens exciting possibilities for transforming audio into text. Imagine having the ability to transcribe conversations, interviews, podcasts, or any spoken content effortlessly. Whether you aim to extract valuable insights from multilingual audio files or create accessible content for a global audience, the Whisper API has you covered.

In this chapter, we will do a deep dive into the core functionalities of the Whisper API by developing a language transcription project using Python. We’ll get acquainted with its essential endpoints, namely transcriptions and translations, which form the backbone of its speech-to-text capabilities. With its state-of-the-art open source model, Whisper equips developers with powerful tools to seamlessly transcribe audio files in multiple languages and even translate them into English. The Whisper API handles an extensive range of languages, ensuring compatibility with diverse speech requirements. It can be utilized in smart speakers, hands-free devices, and other voice-enabled technologies, enhancing user experiences by providing accurate and responsive speech synthesis capabilities.

In a world where global communication is more important than ever, the translations endpoint of the Whisper API plays a vital role. This endpoint not only transcribes audio files but also translates them into English, bridging the gap between languages and facilitating seamless understanding.

In this chapter, you will learn how to do the following:

* Code a speech-to-text conversion using the Whisper API
* Transcribe audio files from multiple languages
* Translate audio files into English
* Handle longer audio inputs and audio splitting using PyDub
* Explore the limitations and benchmarks of the Whisper model

By the end of this chapter, you will have gained a comprehensive understanding of the Whisper API and its remarkable capabilities, including speech-to-text conversion and language translation. You will also learn how to leverage the API to transcribe audio files accurately, translate them into English, handle longer inputs, and unlock the potential of spoken language in your applications.

## Technical Requirements <a href="#_idtextanchor122" id="_idtextanchor122"></a>

To successfully undertake this project of developing a desktop application for language translation, you must meet the following technical prerequisites:

* Ensure that your machine has Python 3.7 or a newer version installed
* Have a code editor such as PyCharm (recommended) set up
* Create a Python virtual environment
* Obtain an OpenAI API key
* Install PyDub in your project

The code snippets showcased in this chapter are available on the GitHub platform. You can access them by following this link: [https://github.com/PacktPublishing/Building-AI-Applications-with-ChatGPT-APIs/tree/main/Chapter10%20Whisper](https://github.com/PacktPublishing/Building-AI-Applications-with-ChatGPT-APIs/tree/main/Chapter10%20Whisper)

## Implementing Text Translation and Transcription with the Whisper API <a href="#_idtextanchor123" id="_idtextanchor123"></a>

In this section, we will explore the capabilities of the Whisper API to transcribe and translate audio files seamlessly using Python. With the advancements in speech recognition and translation technology, we now can effortlessly convert spoken language into text and bridge language barriers effectively. By following the step-by-step instructions provided, you will be equipped with the knowledge and skills necessary to integrate the Whisper API into your Python projects and unlock the potential of audio-based data.

Throughout this section, we will explore the different aspects of transcribing and translating audio files. Starting with the setup and installation requirements, we will ensure that you have the necessary tools, including Python, a code editor, a Python virtual environment, and an OpenAI API key.

To proceed with transcribing and translating audio files using the Whisper API in Python, it is recommended to create a new PyCharm project. PyCharm is a popular **integrated development environment** (**IDE**) that provides a convenient and efficient environment for Python development. Creating a new project in PyCharm will help organize your code and dependencies effectively.

To create a new PyCharm project, follow these steps:

1. Open PyCharm and select **Create New Project** from the welcome screen or go to **File** | **New Project** from the menu bar.
2. In the **New Project** dialog, keep the default location for where your project will be created.
3. Specify the project name as **WhisperAPI** and click **Create** to create the project.

Once the project is created, you will be presented with the PyCharm editor window, where you can start writing your Python code. You can create Python scripts, import libraries, and organize your project structure within the PyCharm project view.

Now that you have set up your PyCharm project, the next step is to install the necessary libraries, namely OpenAI and PyDub. To do that in PyCharm, open the terminal by going to **View** | **Tool Windows** | **Terminal**. From the terminal, type the following commands:

```
$pip install openai
$pip install pydub
$brew install ffmpeg
```

By executing these commands, PyCharm will automatically download and install the OpenAI and PyDub libraries, ensuring that they are readily available for your project.

Installing these libraries is crucial as OpenAI provides the necessary functionality to interact with the Whisper API, and PyDub allows for efficient handling of audio files, including splitting and exporting them.

Now, it’s time to create a new file called **config.py** in your PyCharm project. This file will store your API key securely, allowing your code to access the Whisper API.

To create the **config.py** file and add your API key, follow these steps:

1. In PyCharm’s project view, right-click on the root folder of your project.
2. Select **New** from the context menu and then choose **Python File**.
3. Name the file **config.py** and press _Enter_.

Now, you have created the **config.py** file. Open the file in the PyCharm editor and add the following line of code:

```
API_KEY = "YOUR_API_KEY"
```

To keep your audio files organized and easily accessible throughout this chapter, it is recommended to create a folder called **audio\_files** within your PyCharm project. This folder will serve as a centralized location to store all the audio files that will be used in the various examples and demonstrations covered in the chapter.

To create the **audio\_files** folder, follow these steps:

1. In PyCharm’s project view, right-click on the root folder of your project.
2. Select **New** from the context menu and then choose **Directory**.
3. Name the directory as **audio\_files** and press _Enter_.

Now that you have set up the necessary configurations and folder structure, it’s time to create a new Python file called **transcribe.py**. This file will serve as the starting point for testing the transcription capabilities of the Whisper API. Open the file in the PyCharm editor, and this will be the space where you’ll write the code to interact with the Whisper API and transcribe audio files.

Before writing any code, you can place an English speech audio file of your choice in our **audio\_files** folder. For this example, I will use an audio file called **apple.mp3**. The file consists of a few sentences about Apple computers, and you can download it from our Git repository: [https://github.com/PacktPublishing/Building-AI-Applications-with-ChatGPT-APIs/blob/main/Chapter10%20Whisper/audio\_files/apple.mp3](https://github.com/PacktPublishing/Building-AI-Applications-with-ChatGPT-APIs/blob/main/Chapter10%20Whisper/audio\_files/apple.mp3).

Important Note

When using the Whisper API for audio processing and transcription tasks, it is crucial to ensure that the audio files provided as input are in one of the supported file formats. The Whisper API currently accepts the following file types: **mp3**, **mp4**, **mpeg**, **mpga**, **m4a**, **wav**, and **webm**.

Once your audio file is in place, you can transcribe it by adding the following code to the **transcribe.py**:

```
import openai
import config
# API Token
openai.api_key = config.API_KEY
file= open("audio_files/apple.mp3", "rb")
result = openai.Audio.transcribe("whisper-1", file)
print(result)
```

Here, the **openai** library provides access to the OpenAI API, which allows developers to interact with various language models and AI capabilities. The API key is assigned to the **openai.api\_key** variable, using the value stored in the API key.

Then, we open a file named **apple.mp3** located at the local file path **audio\_files/apple.mp3** in read-binary mode, **rb**. This suggests that the code intends to read an audio file.

After that, the **openai.Audio.transcribe()** function is called with two arguments: **whisper-1** and the opened audio file. This function is part of the OpenAI library and is used to transcribe the audio file. The **whisper-1** parameter is the specific transcription model within the OpenAI system.

Finally, we see the result of the transcription in the console:

```
{
  "text": "Macbook laptops are known for their modern design and high-quality build, offering seamless experience. They are favored by many professionals and creators for their powerful performance and integration with Apple's ecosystem."
}
```

The result of the transcription, printed in the console, is a JSON object containing a single key-value pair. The key is **text**, and the corresponding value is the transcribed text.

This suggests that the transcription process has successfully converted the audio file into text, capturing the content related to the features and reputation of MacBook laptops.

If your audio file is in a language other than English, Whisper will handle that scenario too, by automatically transcribing and then translating the file in the background. Now, we will guide you on how to translate the transcribed text, using the Whisper API. For this example, we will use a German speech file called **german.mp3**. You can also use an **mp3** file recorded in any language.

To utilize the Whisper API for translating audio files, you can create a new Python file called **translate.py** and add the following code:

```
import openai
import config
# API Token
openai.api_key = config.API_KEY
whisper_file= open("audio_files/german.mp3", "rb")
result = openai.Audio.translate("whisper-1", whisper_file)
print(result)
```

Once the API key and the path to the audio file have been correctly set, the code calls the **openai.Audio.translate()** function, providing the model name or **whisper-1** configuration and the opened audio file, **whisper\_file**. This function performs the translation using the Whisper API.

The translated result will be stored in the **result** variable, and it can be printed using **print(result)** to display the translated text in the console:

```
{
  "text": "The Germans are known for not liking to do small talk. Today I'm going to try here in Berlin to see if I can do a little small talk with the people here. Let's go!"
}
```

As you can see, the text was successfully transcribed and translated from German to English using the Whisper API in combination with less than 10 lines of Python code.

In this section, you learned about the capabilities of the Whisper API for transcribing and translating audio files seamlessly using Python. The step-by-step instructions guided you through the setup process to transcribe and translate audio files, along with the supported file formats. The section showcased successful transcription and translation examples, demonstrating the effectiveness of the Whisper API in converting audio to text and bridging language barriers with minimal code.

In the next section, you will learn how to integrate Tkinter and the Whisper API to develop a user-friendly language transcription application that can convert spoken language into text in real time.

## Building a Voice Transcriber Application <a href="#_idtextanchor126" id="_idtextanchor126"></a>

In this section, we will explore the development of a language transcription application by integrating Tkinter, a popular Python GUI toolkit, with the powerful Whisper API. This integration will allow us to create a user-friendly interface that enables the real-time transcription of spoken language. By following the step-by-step instructions and harnessing the capabilities of Tkinter and the Whisper API, you will be empowered to develop your own GUI application, opening a myriad of possibilities in speech recognition and language processing.

Whether you aspire to create a tool for transcribing interviews, generating subtitles for videos, or simply exploring the potential of speech-to-text technology, this section will equip you with the knowledge and skills to bring your ideas to life. So, let’s dive in and embark on this exciting journey of building a language transcription app with Tkinter and the Whisper API.

To continue with the language transcription application project, you can create a new Python file called **app.py** within the same project. This file will serve as the main code base for developing the application.

By creating the **app.py** file, you will have a dedicated space to write the necessary code to integrate Tkinter and the Whisper API, enabling real-time transcription functionality in your application.

In our pursuit of simplicity, our application will adopt a minimalist design featuring a text field and a button. The text field will serve as the dedicated space for displaying the transcribed text, while the button will provide the functionality to effortlessly browse through our file system and locate the desired audio file for transcription.

To convert our language translation code into an actual application, we will need to create the **transcribe\_audio()** function and use it from the Tkinter graphics:

```
import tkinter as tk
from tkinter import filedialog
import openai
import config
# API Token
openai.api_key = config.API_KEY
def transcribe_audio():
    file_path = filedialog.askopenfilename(
        filetypes=[("Audio Files", "*.mp3")])
    if file_path:
        try:
            audio_file = open(file_path, "rb")
            transcript = openai.Audio.transcribe(
                "whisper-1", audio_file)
            text_window.insert(tk.END, transcript.text)
        except Exception as e:
            text_window.insert(tk.END, f"Error: {str(e)}")
    else:
        text_window.insert(tk.END, "No file selected.")
# Create the Tkinter window
window = tk.Tk()
window.title("Whisper Transcription App")
# Create a text window
text_window = tk.Text(window, height=50, width=200)
text_window.pack()
# Create a button to select the audio file
button = tk.Button(window, text="Select Audio File", command=transcribe_audio)
button.pack()
# Start the Tkinter event loop
window.mainloop()
```

Upon execution of the function, it first opens a file dialog window using the **filedialog.askopenfilename()** method, allowing the user to select an audio file for transcription. The file dialog is restricted to display only files with the **.mp3** extension, as specified by the **filetypes** parameter.

If a valid file path is obtained from the file dialog, indicating that the user has selected an audio file, the code proceeds to transcribe the audio. Inside a **try** block, the selected audio file is opened in read-binary mode using **open()** and assigned to the **audio\_file** variable.

Using the **openai.Audio.transcribe()** function, the audio file is passed along with the specific transcription model to initiate the transcription process by the Whisper API. The resulting transcription is stored in the **transcript** variable.

Finally, the transcribed text is inserted into a text window. The **tk.END** argument ensures that the text is inserted at the end of the text window. In case of any exceptions or errors during the process, an error message is displayed in the text window.

If no file path is obtained from the file dialog, indicating that the user did not select an audio file, a **No file selected** message is inserted into the text window.

Under the **transcribe\_audio()** function, we have created the Tkinter app that is using it. First, a Tkinter window is created using **tk.Tk()**. The window’s title is set as **Whisper Transcription App**.

Next, a text window is created using **tk.Text()**, where the Whisper API transcription will be displayed. A button is also added to the Tkinter window using **tk.Button()**. The button’s label is set as **Select Audio File**, and the **command** parameter is set to **transcribe\_audio**. This means that when the button is clicked, the **transcribe\_audio()** function (previously defined) will be executed.

To run the Whisper transcription app from PyCharm, follow these steps:

1. In PyCharm, locate the **app.py** file within your project structure.
2. Right-click on the **app.py** file and select **Run app**.
3. Once the application is running, a Tkinter window titled **Whisper Transcription App** will appear.
4. The window will display a text area where the transcribed text will be shown.
5. Click on the **Select Audio File** button. A file dialog window will open, allowing you to navigate your file system.
6. Find and select an MP3 audio file that you want to transcribe.

After selecting the file, the code will attempt to transcribe the audio. If successful, the transcribed text will be displayed in the text area within the Tkinter window (see _Figure 10.1_). If there are any errors during the transcription process, an error message will be displayed instead.

![Figure 10.1 – Whisper transcription application](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781805127567/files/image/B21110\_10\_01.png)

Figure 10.1 – Whisper transcription application

Important Note

Please be aware that in certain screen resolutions, the **Select Audio File** button might be concealed. To ensure its visibility, maximize the application window.

By running the app and selecting an MP3 file, you will be able to witness the Whisper API’s transcription capabilities in action. The app will leverage the selected audio file, initiate the transcription process using the Whisper API, and display the transcribed text within the Tkinter window. This allows you to convert spoken language into written text, opening possibilities for various applications and use cases.

In this section, we embarked on the development of a language transcription application, integrating Tkinter and the Whisper API. With a minimalist design featuring a text field and a button, the application allows users to select an MP3 audio file for transcription. Upon selecting the file, the code initiates the transcription process using the Whisper API and displays the transcribed text in the Tkinter window.

In the next section, you will learn how to integrate PyDub with the Whisper API to overcome the file size limitation and efficiently split large audio files for seamless transcription.

## Using PyDub for Longer Audio Inputs <a href="#_idtextanchor127" id="_idtextanchor127"></a>

In this section, we will explore the integration of **PyDub**, a powerful audio processing library for Python, with the Whisper API to overcome the file size limitation of 25 MB imposed by the API. With PyDub, we can efficiently split large audio files into smaller segments, enabling the seamless transcription of lengthy recordings. By following the instructions and leveraging PyDub’s capabilities, you will be able to harness the full potential of the Whisper API for transcribing audio files of any size.

Leveraging the power of PyDub to enhance your language transcription workflow is a straightforward process. By utilizing this library, you can effortlessly divide lengthy audio files into smaller segments. For instance, if you have a 10-minute audio file, you can easily split it into two separate files, each with a duration of 5 minutes. These smaller files can then be submitted to the Whisper API for transcription, ensuring that your files are not rejected due to their size limitations. With PyDub, you can seamlessly overcome the file size constraint and streamline your transcription process.

In this exercise, we utilized an approximately eleven-minute-long audio file from an iPhone review. Now, you can create a new Python file within your project, specifically named **longer\_files.py**. This file will serve as a dedicated space to implement the necessary code for handling larger audio files in your language transcription workflow:

```
import openai
from pydub import AudioSegment
import config
# API Token
openai.api_key = config.API_KEY
song = AudioSegment.from_mp3("audio_files/phone.mp3")
# 5 minute portion
five_minutes = 5 * 60 * 1000
first_min_5 = song[:five_minutes]
first_min_5.export("audio_files/phone_first_5.mp3", format="mp3")
last_min_5 = song[five_minutes:]
last_min_5.export("audio_files/phone_last_5.mp3", format="mp3")
file= open("audio_files/phone_first_5.mp3", "rb")
result = openai.Audio.transcribe("whisper-1", file)
print(result)
file= open("audio_files/phone_last_5.mp3", "rb")
result = openai.Audio.transcribe("whisper-1", file)
print(result)
```

The provided code demonstrates the usage of the **PyDub** library and the Whisper API for processing larger audio files in the language transcription workflow.

You can download the **phone.mp3** file from here: [https://github.com/PacktPublishing/Building-AI-Applications-with-ChatGPT-APIs/blob/main/Chapter10%20Whisper/audio\_files/phone.mp3](https://github.com/PacktPublishing/Building-AI-Applications-with-ChatGPT-APIs/blob/main/Chapter10%20Whisper/audio\_files/phone.mp3)

First, the necessary imports are made, including the **openai** and **AudioSegment** modules from the respective libraries. We load an audio file named **phone.mp3** using PyDub’s **AudioSegment.from\_mp3()** method and assign it to the **song** variable.

To break down the audio file into smaller segments, the code defines a five-minute portion using the **five\_minutes** variable, which represents the desired duration in milliseconds. The **song** variable is then sliced using the specified duration to obtain the first five minutes of the audio, which is stored in the **first\_min\_5** variable. This segment is then exported as a separate MP3 file named **phone\_first\_5.mp3** using the **export()** method.

Similarly, the code obtains the remaining portion of the audio file, starting from the five-minute mark until the end, using the slicing operation, and assigns it to the **last\_min\_5** variable. This segment is also exported as a separate MP3 file named **phone\_last\_5.mp3**.

The code proceeds to open the **phone\_first\_5.mp3** file with the **openai.Audio.transcribe()** method invoked. The resulting transcription is stored in the **result** variable, which is then printed to the console. Following the same procedure, the code repeats the transcription process for the **phone\_last\_5.mp3** file, opening it, transcribing it, and printing the result.

You can see the unique transcriptions for each file printed in the console as follows:

```
{
  "text": "It's finally here, the most wanted phone this year, the most amazing camera set ever built in a phone. Here is the iPhone 13 Pro.…….reduced the front camera module, now the phone will look good."
}
{
  "text": "weights about 10 grams more which is something you can't really feel. However, while everybody is trying to reduce the weight on the phones, Apple actually increased it……... Now thanks for watching and I will see you in the next one."
}
```

By breaking down the larger audio file into smaller segments using PyDub, the code enables the processing of these segments within the size limits of the Whisper API. This approach allows for the efficient handling of larger audio files while leveraging the transcription capabilities provided by the Whisper API.

This is how we can integrate PyDub with the Whisper API to overcome the file size limitation, enabling the seamless transcription of longer audio files.

## Summary <a href="#_idtextanchor129" id="_idtextanchor129"></a>

In this chapter, we explored the Whisper API, a powerful tool for converting audio into text through advanced speech recognition and translation. The chapter provided step-by-step instructions on developing a language transcription project using Python, covering essential aspects such as handling audio files, installing necessary libraries, and setting up the API key. You learned how to transcribe and translate audio files using the Whisper API. The chapter also introduced a voice transcription application, integrating Tkinter and the Whisper API for real-time transcription.

You also learned how to use PyDub, a powerful audio processing library for Python, with the Whisper API to overcome the file size limitation of 25 MB. By leveraging PyDub’s capabilities, we can efficiently split large audio files into smaller segments, enabling the seamless transcription of lengthy recordings. You saw how to use PyDub and the Whisper API to process larger audio files in the language transcription workflow. By breaking down the audio file into smaller segments and transcribing each segment individually, we can handle larger audio files while benefiting from the transcription capabilities of the Whisper API.

In [_Chapter 11_](https://learning.oreilly.com/library/view/building-ai-applications/9781805127567/B21110\_11.xhtml#\_idTextAnchor131), you will learn about the different API models available in the ChatGPT API and gain insights into how to choose the most suitable model for your specific projects. We will explore the various parameters that can be utilized in API requests to achieve more efficient, improved prompt completions. You will also understand the limitations associated with different AI models.
