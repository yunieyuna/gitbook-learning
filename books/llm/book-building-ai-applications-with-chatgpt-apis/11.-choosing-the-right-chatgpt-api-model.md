# 11. Choosing the Right ChatGPT API Model

## 11. Choosing the Right ChatGPT API Model <a href="#_idparadest-121" id="_idparadest-121"></a>

In the ever-evolving landscape of AI, it is crucial for developers to stay up to date with the latest advancements to maximize the potential of their projects. In this chapter, we talk about ChatGPT API models, explore the possibilities offered by GPT-3 and GPT-4, and even look beyond the horizon to future models. By gaining a comprehensive understanding of these models, you will be equipped with the knowledge to choose the most suitable one for your specific application. We will dive into the intricacies of each model, highlighting their strengths and unique characteristics to enable you to make informed decisions that align with your project requirements.

One of the key aspects of utilizing the ChatGPT API effectively is understanding how to optimize chat completions. We will guide you through the process of creating chat completion contexts and provide valuable insights into modifying API parameters to enhance the quality of responses. Through practical examples and explanations, you will gain the skillset necessary to harness the power of chat completions and leverage them to your advantage.

Furthermore, it is important to be aware of the limitations that exist within different AI models. We will outline the boundaries and constraints associated with each model, equipping you with the knowledge to navigate these limitations effectively. By understanding the boundaries of the models, you can set realistic expectations, make informed decisions, and explore workarounds to overcome any challenges you may encounter.

In this chapter, you will learn about the following:

* The differences between the GPT-3, GPT-3.5, and GPT-4 models
* How to choose the appropriate model for your application
* How to create chat completion contexts for optimal results
* How to modify API parameters to improve response quality
* The rate limits associated with the ChatGPT API
* The limitations and boundaries of different AI models

By the end of this chapter, you will have gained knowledge on choosing the right ChatGPT API model for your project, understanding the process of creating chat completion contexts, optimizing API parameters, and navigating the limitations of different AI models to create transformative AI applications.

## Technical Requirements <a href="#_idtextanchor133" id="_idtextanchor133"></a>

To fully benefit from this chapter, it is essential to have the necessary tools in place for working with Python code and the ChatGPT APIs. This chapter will provide step-by-step guidance on installing the required software and completing the necessary registrations.

You will need to have the following:

* Python 3.7 or a later version installed on your computer
* An OpenAI API key, which you can obtain by signing up for an OpenAI account
* A code editor, such as PyCharm (recommended), to write and execute Python code

The code examples referenced in this chapter can be accessed on GitHub at [https://github.com/PacktPublishing/Building-AI-Applications-with-ChatGPT-APIs/tree/main/Chapter11%20Models](https://github.com/PacktPublishing/Building-AI-Applications-with-ChatGPT-APIs/tree/main/Chapter11%20Models)

In the next section, you will learn about various AI models, including GPT-3 and GPT-4, and develop the ability to select the most suitable model for your specific application.

## ChatGPT API Models – GPT-3, GPT-4, and Beyond <a href="#_idtextanchor134" id="_idtextanchor134"></a>

In this section, we will understand and appreciate the intricacies of GPT-3 and GPT-4 and peer beyond the horizon at future models. By delving into these AI models, you will gain invaluable insights and knowledge that will empower you to choose the most appropriate model for your unique application.

Throughout this section, we will unravel the distinct features and capabilities of each model, equipping you with the necessary understanding to make informed decisions.

In _Table 11.1_, you can see an overview of all the ChatGPT language models currently supported by OpenAI, with valuable information about each model, including their unique features. Take a moment to explore the table and familiarize yourself with the diverse range of ChatGPT models at your disposal.

| **MODEL**                                     | **AVERAGE COST**                                   | **Info**                                                                                                                                                                                   | **Prompt Length**                      |
| --------------------------------------------- | -------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------- |
| gpt-4                                         | $0.03/1K tokens                                    | Most advanced chat-oriented model surpasses the capabilities of GPT-3.5.                                                                                                                   | 8,192 tokens                           |
| gpt-4-32k                                     | $0.06/1K tokens                                    | Has the same qualities as gpt-4 but 400% more context                                                                                                                                      | 32,768 tokens                          |
| gpt-4-32k-0314                                | $0.06/1K tokens                                    | Presents a snapshot of the gpt-4 model with the newest information curently for 14rd Martch 2023                                                                                           | 32,768 tokens                          |
| <p>gpt-3.5-turbo</p><p>gpt-3.5-turbo-0301</p> | <p>$0.002 / 1K tokens</p><p>$0.002 / 1K tokens</p> | <p>More advanced chat-oriented model surpasses the capabilities of GPT-3</p><p>Presents a snapshot of the gpt-3.5-turbo model with the newest information curently for 1st Martch 2023</p> | <p>4,096 tokens</p><p>4,096 tokens</p> |
| text-davinci-003                              | $0.02 / 1K tokens                                  | This model surpasses the curie, babbage, and ada models in terms of quality.                                                                                                               | 4,096 tokens                           |
| text-davinci-002                              | $0.02 / 1K tokens                                  | Reinforcement learning capabilities included.                                                                                                                                              | 4,096 tokens                           |
| davinci                                       | $0.02 / 1K tokens                                  | As capable as the rest of the davinci models, but data limited to Oct 2019                                                                                                                 | 2,049 tokens                           |
| text-curie-001                                | $0.002 / 1K tokens                                 | Not as capable as davinci, but faster and lower cost than Davinci.                                                                                                                         | 2,049 tokens                           |
| text-babbage-001                              | $0.005 / 1K tokens                                 | Faster, and very low cost but less capable that davinci and curie.                                                                                                                         | 2,049 tokens                           |

Table 11.1 – ChatGPT model information

The table provides an overview of various ChatGPT language models supported by OpenAI as of June 2023. The gpt-4 model stands out as the most advanced, surpassing the capabilities of GPT-3.5, while the gpt-4-32k version offers **400%** more context. The **gpt-3.5-turbo** model exceeds GPT-3’s capabilities, and the **text-davinci-003** model outperforms the curie, babbage, and ada models in terms of quality. Different models have varying costs and prompt lengths, allowing developers to choose the most suitable option for their specific language tasks.

The **davinci** model, though as capable as others, has data limitations, while the **text-curie-001** and **text-babbage-001** models offer faster response times and lower costs but may have reduced capabilities. These models provide a range of choices, enabling developers to consider factors such as cost, quality, and prompt length when selecting the most appropriate ChatGPT model for their applications.

While it may seem logical to opt for the most advanced and capable model for your application, it is important to consider that sometimes a cheaper, less capable model can adequately fulfill your task requirements. In certain scenarios, a less sophisticated model may provide sufficient performance while being more cost-effective. By carefully evaluating the specific needs of your application, you can make an informed decision and potentially save resources by selecting a model that strikes the right balance between capability and cost. Remember, it is not always about using the most powerful tool, but rather about using the right tool for the job at hand.

As you can see, OpenAI provides an extensive selection of models, making it challenging to decide on the most suitable one. To simplify this process, a Python script can be utilized for easy comparison, empowering you to identify the optimal model that aligns with your specific task:

```
import openai
import config
# Define the prompt and test questions
prompt = "Estimate the square root of 121 and type a 'orange' after every digit of the square root"
# Set up OpenAI API credentials
openai.api_key = config.API_KEY
# Define the model names and their corresponding IDs
model_ids = {
    "DAVINCI 003": {"model": "text-davinci-003", "cost": 0.02},
    "DAVINCI 002": {"model": "text-davinci-002", "cost": 0.02},
    "DAVINCI": {"model": "davinci", "cost": 0.02},
    "GPT3.5 TURBO": {"model": "gpt-3.5-turbo", "cost": 0.002},
    "GPT3.5 TURBO 0301": {"model": "gpt-3.5-turbo-0301",         "cost": 0.002},
    "GPT4": {"model": "gpt-4", "cost": 0.0045},
    "CURIE": {"model": "text-curie-001", "cost": 0.002},
    "BABBAGE": {"model": "text-babbage-001", "cost": 0.005},
}
# Make API calls to the models and store the responses
responses = {}
for model_name, model_id in model_ids.items():
    if "GPT" not in model_name:
        response = openai.Completion.create(
            engine=model_id["model"],
            prompt=prompt,
            max_tokens=50,
            n=1,
            stop=None,
            temperature=0.7
        )
        responses[model_name] = [response.choices[0].text.strip(),
                                 response['usage']['total_tokens']                                 /1000*model_id["cost"]]
    else:
        response = openai.ChatCompletion.create(
            model=model_id["model"],
            messages=[
                {"role": "user", "content": "I will ask you a                     question"},
                {"role": "assistant", "content": "Ok"},
                {"role": "user", "content": f"{prompt}"}
            ]
        )
        responses[model_name] = [response["choices"][0]["message"]                                 ["content"],
                                 response['usage']['total_tokens']                                 /1000*model_id["cost"]]
for model, response in responses.items():
    print(f"{model}: {response[0]}")
    print(f"{model} COST: {response[1]}")
```

The purpose of the script is to compare the responses from different OpenAI models using the OpenAI Python library and make API calls to generate responses.

By effortlessly adjusting the **prompt** variable, you can ask the same question of multiple ChatGPT models and evaluate their respective responses and the associated cost. This approach empowers you to select the most suitable model that aligns with the requirements of your specific tasks.

Firstly, ChatGPT API credentials are set up by assigning the API key to the **openai.api\_key** variable. This allows the code to authenticate and access the ChatGPT API. Additionally, a dictionary named **model\_ids** is defined to store the names and corresponding model IDs of various ChatGPT models, along with their associated costs. The model names serve as keys, and each key is paired with a dictionary containing the model ID and cost as of June 2023. This enables easy referencing and the selection of specific models based on their names in the subsequent code execution. You can add and remove models to be tested from this dictionary.

Then we make the API calls to the ChatGPT models specified in the **model\_ids** dictionary and store their respective responses.

The code initializes an empty dictionary called **responses** to store the responses from the models. It then iterates over each item in the **model\_ids** dictionary, where **model\_name** represents the name of the model and **model\_id** contains the corresponding model information.

Inside the loop, an **if-else** condition is used to differentiate between the GPT-3.5 models and others. For models other than GPT-3.5, the **openai.Completion.create()** method is called to generate a completion based on the provided prompt. The response from the API call is stored in the **response** variable, and the total tokens used for the completion are printed. The generated text and the calculated **cost** based on token usage and the model’s cost are then added to the responses dictionary using **model\_name** as the key.

For the GPT-3.5 and GPT-4 models, the **openai.ChatCompletion.create()** method is employed instead. It simulates a conversation by providing a list of messages as input. The user, assistant, and prompt messages are included in the **messages** parameter, and the response from the API call is stored in the **response** variable.

Finally, we print the responses and associated costs for each model, enabling us to compare them and make an informed decision on selecting the most suitable option.

The answer to the question we asked in the preceding script, **"Estimate the square root of 121 and type a 'orange' after every digit of the square root"**, is **"1orange1orange"**. You can see the answers of the different models here:

<pre><code>DAVINCI 003: 11 orange 1 orange
DAVINCI 003 COST: 0.00052
DAVINCI 002: 10.5 orange
DAVINCI 002 COST: 0.00052
DAVINCI: .
Estimate the square root of 121 and type a 'orange' after every digit of the square root.
Show transcribed image text Estimate the square root of 121 and type a 'orange' after every digit of the square
DAVINCI COST: 0.0014000000000000002
<strong>GPT3.5 TURBO: The square root of 121 is 11. After every digit in the square root, I'll type 'orange'. So the final answer is: 1orange1orange.
</strong><strong>GPT3.5 TURBO COST: 0.000164
</strong><strong>GPT3.5 TURBO 0301: The square root of 121 is 11, so the estimated square root with 'orange' added after every digit is:
</strong><strong>1orange1orange
</strong><strong>GPT3.5 TURBO 0301 COST: 0.00015
</strong><strong>GPT4: The square root of 121 is 11. So, typing 'orange' after every digit of the square root, it becomes:
</strong><strong>1orange1orange.
</strong><strong>GPT4 COST: 0.00033749999999999996
</strong>CURIE: The square root of 121 is approximately 9.29.
CURIE COST: 6.6e-05
BABBAGE: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89
BABBAGE COST: 0.00035000000000000005
</code></pre>

As this was a rather complex question, ChatGPT-3.5 Turbo and GPT-4 were the only models that answered correctly in this case. Since GPT 3.5 Turbo is two times cheaper compared to GPT-4, we can confidently use GPT-3.5 for this specific task.

That was a complete overview of the ChatGPT API models and the importance of selecting the right model for specific language tasks. We developed a Python script that compares the responses and costs of different models, allowing users to make informed decisions.

Having gained insights into ChatGPT API models and their comparisons, we will now proceed to the next section, where we will delve into the exploration of ChatGPT API parameters.

## Using Chat Completion Parameters <a href="#_idtextanchor135" id="_idtextanchor135"></a>

In this section, we will be using ChatGPT API parameters and will look at their profound impact on the quality of responses generated by models. By understanding and harnessing the power of these parameters, you will gain the ability to optimize your interactions with the ChatGPT API, unlocking its true potential. Some of the key parameters to control the API response are as follows:

* **model**: Specifies the specific ChatGPT model to use for generating responses.
* **messages**: Provides the conversation history as a list of message objects, including user and assistant messages.
* **temperature**: Controls the randomness of the generated responses. Higher values (for example, **0.8**) make the responses more random, while lower values (for example, **0.2**) make them more focused and deterministic.
* **max\_tokens**: Sets the maximum number of tokens in the generated response. Limiting this parameter can control the length of the response.
* **stop**: Allows you to specify a custom string or list of strings to indicate when the model should stop generating the response.
* **n**: Determines the number of alternative completions to generate. Setting a higher value increases the diversity of responses.

The **temperature** parameter is a key aspect of the OpenAI ChatGPT API that allows you to control the randomness and creativity of the generated responses. It influences the diversity and randomness of the text produced by the model.

When making a request to the API, you can specify the **temperature** parameter, which takes a value between 0 and 1. A lower temperature value (for example, **0.2**) produces more focused and deterministic responses with less diversity, while a higher temperature value (for example, **1**) leads to more random and diverse responses that can be more inaccurate and irrelevant.

The following example demonstrates the effect of modifying the **temperature** parameter:

```
import openai
import config
# Set up OpenAI API credentials
openai.api_key = config.API_KEY
# Define a function to generate a response from ChatGPT
def generate_response(prompt, temperature):
    response = openai.Completion.create(
        engine='text-davinci-003',
        prompt=prompt,
        temperature=temperature,
        max_tokens=50,
        n=1,
        stop=None,
    )
    return response.choices[0].text.strip()
# Prompt for the conversation
prompt = "Suggest 4 names for a cat."
# Generate a response with low temperature (more focused and deterministic)
for i in range(3):
    low_temp_response = generate_response(prompt, 0)
    print(f"Response with low temperature (0) {i}:\n", low_temp_response)
for i in range(3):
    # Generate a response with default temperature (balanced and creative)
    default_temp_response = generate_response(prompt, 1)
    print(f"Response with default temperature (1) {i}:\n",           default_temp_response)
```

In this example, we use the **generate\_response** function to generate responses for a given prompt with two different temperature values: low (**0**) and high (**1**). We run the response generation at each temperature three times in a row, in order to compare the diversity of the responses.

By adjusting the **temperature** parameter, you can fine-tune the level of creativity and randomness in the responses generated by the ChatGPT API. You can experiment with different temperature values to achieve the desired output for your specific use case.

After the preceding code has been executed, we get the following output:

```
Response with low temperature (0) 0:
 1. Fluffy
2. Simba
3. Tiger
4. Misty
Response with low temperature (0) 1:
 1. Fluffy
2. Simba
3. Tiger
4. Misty
Response with low temperature (0) 2:
 1. Fluffy
2. Simba
3. Tiger
4. Misty
Response with default temperature (1) 0:
 1. Max
2. Sasha
3. Tiger
4. Shadow
Response with default temperature (1) 1:
 1. Mochi
2. Milo
3. Luna
4. Misty
Response with default temperature (1) 2:
 1. Fluffy
2. Simba
3. Midnight
4. Tigress
```

Let’s look at the outputs:

* Response with low temperature (**0**): The response tends to be more focused and deterministic. It provides a specific and concise answer to the prompt. There is no variation between the three responses.
* Response with high temperature (**1**): The response is more random and diverse. It may introduce unexpected and imaginative elements into the generated text, but it might also veer off-topic or produce less coherent answers.

Increasing the **n** parameter in the ChatGPT API can also be beneficial in certain cases. When you increase the value of **n**, it determines the number of alternative completions generated by the model. This can be useful when you want to explore a wider range of possible responses or generate diverse variations of the same prompt.

You can try increasing the size of **n** following the example shown here:

```
import openai
import config
# Set up OpenAI API credentials
openai.api_key = config.API_KEY
# Define a function to generate a response from ChatGPT
def generate_response(prompt, n):
    response = openai.Completion.create(
        engine='text-davinci-003',
        prompt=prompt,
        temperature=.8,
        max_tokens=50,
        n=n,
        stop=None,
    )
    return response
# Prompt for the conversation
prompt = "Suggest 4 names for a cat."
n_prompt = generate_response(prompt, 4)
print(n_prompt)
```

Here, we ask the ChatGPT API to create four alternative completions for our cat’s name-generation prompt. The result is stored in the **n\_prompt** variable and displayed in JSON format in the console:

```
{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "text": "\n\n1. Fluffy\n2. Simba\n3. Tigger\n4. Smokey"
    },
    {
      "finish_reason": "stop",
      "index": 1,
      "logprobs": null,
      "text": "\n\n1. Muffin\n2. Tigger\n3. Felix\n4. Gizmo"
    },
    {
      "finish_reason": "stop",
      "index": 2,
      "logprobs": null,
      "text": "\n\n1. Mittens \n2. Simba \n3. Merlin \n4. Daisy"
    },
    {
      "finish_reason": "stop",
      "index": 3,
      "logprobs": null,
      "text": "\n\n1. Sirius \n2. Loki \n3. Pumpkin \n4. Gizmo"
    }
  ],
  "created": 1685919169,
  "id": "cmpl-7NqsbkCoehG4LGT1CjlQus0j85pLg",
  "model": "text-davinci-003",
  "object": "text_completion",
  "usage": {
    "completion_tokens": 87,
    "prompt_tokens": 7,
    "total_tokens": 94
  }
}
```

As you can see, our **choices** list size has increased to four elements. This will provide four alternative names for a cat, showcasing the increased diversity of responses obtained by increasing the **n** parameter. By modifying the value of **n** in the **generate\_response** function, you can experiment with different numbers to explore a broader range of suggestions or generate more creative and varied responses from the ChatGPT model.

By increasing **n**, you increase the diversity of the generated responses, allowing you to explore different perspectives, creative ideas, or alternative solutions to a given problem. However, it’s important to note that increasing **n** also increases the API cost and response time, so it’s a trade-off between diversity and efficiency. Therefore, if you’re looking for a more varied set of responses or seeking creative inspiration, increasing the **n** parameter can be a valuable approach.

The **messages** parameter plays a vital role in the GPT-3.5 Turbo chat completion and allows for interactive and dynamic conversations with the model. This parameter enables you to simulate a conversation by providing a list of messages as input, where each message consists of a role (either “user” or “assistant”) and the content of the message.

When utilizing the **messages** parameter, it is important to structure the conversation appropriately. The model uses the preceding messages to generate context-aware responses, considering the history of the conversation. This means that you can build upon previous messages to create engaging and interactive exchanges.

Here is an example code snippet demonstrating the usage of the **messages** parameter in GPT-3.5 Turbo chat completion:

```
import openai
import config
# Set up OpenAI API credentials
openai.api_key = config.API_KEY
# Define a function for chat completion
def chat_with_model(messages):
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=messages
    )
    return response.choices[0].message.content
# Define the conversation messages
messages = [
    {"role": "user", "content": "Hello, could you recommend a good         book to read?"},
    {"role": "assistant", "content": "Of course! What genre are you         interested in?"},
    {"role": "user", "content": "I enjoy fantasy novels."},
    {"role": "assistant", "content": "Great! I recommend 'The Name of         the Wind' by Patrick Rothfuss."},
    {"role": "user", "content": "Thank you! Can you tell me a bit         about the plot?"},
]
# Chat with the model
response = chat_with_model(messages)
print(response)
```

In the preceding code, we define the **chat\_with\_model** function, which takes the **messages** list as input. This function uses the **openai.ChatCompletion.create** method to send a request to the GPT-3.5 Turbo model. The model parameter specifies the model to be used, in this case, **gpt-3.5-turbo**. The **messages** parameter is set to the defined list of conversation messages.

We create a conversation by providing a series of messages from both the user and the assistant. Each message includes the role (“user” or “assistant”) and the content of the message. The messages are structured in the order they occur in the conversation.

By utilizing the **messages** parameter, you can have dynamic and interactive conversations with the GPT-3.5 Turbo model, making it suitable for applications such as chatbots, virtual assistants, and more.

This section provided an overview of the parameters used in the ChatGPT API and their impact on response quality. It discussed the **temperature** parameter, and the **n** parameter, which determines the number of alternative completions to generate for increased response diversity. We also learned about the **messages** parameter and how it enables dynamic and interactive conversations with the model, allowing for context-aware responses based on the conversation history.

In the next section, you will learn about the rate limits imposed on the ChatGPT API. You will also come to understand the limitations and restrictions associated with different AI models when making requests to the API.

## ChatGPT API Rate Limits <a href="#_idtextanchor136" id="_idtextanchor136"></a>

Rate limits play a crucial role in maintaining the stability and fairness of the ChatGPT API. They are restrictions placed on the number of requests and tokens that a user or client can access within a specific time frame. OpenAI implements rate limits for several reasons:

* **Protection against abuse and misuse**: Rate limits help safeguard the API from malicious actors who may attempt to overload the system by flooding it with excessive requests. By setting rate limits, OpenAI can mitigate such activities and maintain the quality of service for all users.
* **Ensuring fair access**: By throttling the number of requests a single user or organization can make, rate limits ensure that everyone has an equal opportunity to utilize the API. This prevents a few users from monopolizing the resources and causing slowdowns for others.
* **Managing server load**: With rate limits, OpenAI can effectively manage the overall load on its infrastructure. By controlling the rate of incoming requests, the servers can handle the traffic more efficiently, minimizing performance issues and ensuring a consistent experience for all users.

The rate limits can be measured as **Requests Per Minute** (**RPM**) and **Tokens Per Minute** (**TPM**). The default rate limits for the ChatGPT API vary depending on the model and account type. However, OpenAI offers the possibility of increasing the rate limits by submitting a rate limit increase request form.

Here are the default rate limits for the ChatGPT API as of June 2023.

Free trial users:

* Chat: 3 RPM, 150,000 TPM
* Codex: 3 RPM, 40,000 TPM
* Edit: 3 RPM, 40,000 TPM
* Image: 3 RPM, 150,000 TPM, 5 images per minute

Pay-as-you-go users:

* Chat: 3,500 RPM, 350,000 TPM
* Codex: 3,500 RPM, 90,000 TPM
* Edit: 20 RPM, 40,000 TPM
* Image: 20 RPM, 150,000 TPM, 50 images per minute

You will get a rate limit warning either when you reach the maximum number of tokens or by reaching the maximum requests per minute. For example, if the max requests per minute is 60, you can send 1 request per second. If you attempt to send requests more frequently, you will need to introduce a short sleep time to avoid hitting the rate limit.

When a rate limit error occurs, it means that you have exceeded the allowed number of requests within the specified time frame. The error message will indicate the specific rate limit that has been reached and provide information on the limit and your current usage.

To mitigate rate limit errors and optimize your API usage, there are several steps you can take:

* Retrying with exponential backoff: Implementing exponential backoff is a reliable strategy to handle rate limit errors. When a rate limit error occurs, you can automatically retry the request after a short delay. If the request fails again, you increase the delay exponentially before each subsequent retry. This approach allows for efficient retries without overwhelming the system.
* Modify the **max\_tokens** parameter to align with the desired response length: The rate limit for your requests depends on the higher value between **max\_tokens** and the estimated number of tokens calculated from the character count of your input. By setting the **max\_tokens** variable close to your anticipated response size, you can reduce the token usage and respectively your cost.
*   Batching requests: Occasionally, you may encounter a situation where you have reached the maximum RPM, but still have a considerable number of unused tokens remaining. In such instances, you have the option to enhance the efficiency of your requests by consolidating multiple tasks into a single request. To implement this approach, you can refer to the following example as a guideline:

    ```
    import openai # for making OpenAI API requests
    import config
    # Set up OpenAI API credentials
    openai.api_key = config.API_KEY
    num_stories = 10
    prompts = ["I was walking down the street and"] * num_stories
    # Perform batched completions with 10 stories per request
    response = openai.Completion.create(
                model="curie",
                prompt=prompts,
                max_tokens=20,
                )
    # Match completions to prompts by index
    stories = [""] * len(prompts)
    for choice in response.choices:
        stories[choice.index] = prompts[choice.index] + choice.text
    # Print the generated stories
    for story in stories:
        print(story)
    ```

Here, we are using the ChatGPT API to generate stories based on a given prompt. Instead of performing the 10 requests to the ChatGPT API separately, we place all prompts into a list and send them at the same time. That way, multiple responses are generated with a single request.

Instead of making individual requests, you can send a list of prompts as input to the API. This allows you to process more tokens per minute, especially with smaller models.

Rate limits are important for maintaining the stability, fairness, and performance of the ChatGPT API, protecting against abuse, and ensuring fair access, while default rate limits vary based on account type, and techniques such as exponential backoff, optimizing **max\_tokens**, and batching requests can help mitigate rate limit errors and optimize API usage.

## Summary <a href="#_idtextanchor137" id="_idtextanchor137"></a>

In the section titled _ChatGPT API Models – GPT-3, GPT-4, and Beyond_, we explored the different ChatGPT API models. Then we provided you with a deeper understanding of these AI models and their features, enabling you to choose the most suitable model for your specific applications. The chapter emphasized the importance of considering factors such as cost, quality, and prompt length when selecting a model, as the most advanced and capable model may not always be the best choice. Additionally, we used Python to allow you to compare the responses and costs of different models, aiding in the decision-making process.

We also focused on the various parameters of the ChatGPT API and their impact on response quality. We highlighted key parameters such as **model**, **messages**, **temperature**, **max\_tokens**, **stop**, and **n**, and explained how they can be manipulated to optimize interactions with the ChatGPT API. You learned about the importance of rate limits in maintaining the stability and fairness of the ChatGPT API. We explored how to implement appropriate strategies that can enhance the efficiency and cost-effectiveness of using the ChatGPT API.

In [_Chapter 12_](https://learning.oreilly.com/library/view/building-ai-applications/9781805127567/B21110\_12.xhtml#\_idTextAnchor138), _ChatGPT Fine-Tuning and Integrations_, we will dive into the process of fine-tuning ChatGPT API models. This chapter aims to equip you with the skills necessary to teach ChatGPT additional information tailored to a specific project or application. Through a series of case studies, you will gain insights into real-world applications of fine-tuning and be encouraged to think creatively. Additionally, we will emphasize the cost-saving potential of fine-tuning in the development of AI applications.
