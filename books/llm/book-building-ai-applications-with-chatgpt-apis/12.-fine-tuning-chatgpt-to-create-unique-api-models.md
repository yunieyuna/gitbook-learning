# 12. Fine-Tuning ChatGPT to Create Unique API Models

## 12. Fine-Tuning ChatGPT to Create Unique API Models <a href="#_idparadest-128" id="_idparadest-128"></a>

In this chapter, we will explore fine-tuning the ChatGPT API. This transformative process grants developers the power to mold the behavior of ChatGPT API responses to align with their distinct needs. We will immerse ourselves in the art of fine-tuning, exploring the intricate techniques and critical factors at play. By mastering the craft of building and deploying our very own fine-tuned model, we unlock the potential to enhance our AI applications in unprecedented ways.

ChatGPT fine-tuning is a process that involves training a pre-trained language model, such as **davinci**, on a specific dataset to improve its performance and adapt it to a particular task or domain. The fine-tuning process typically begins with a dataset that is carefully curated and labeled, and it involves training the model on this dataset using techniques such as transfer learning. The model’s parameters are adjusted during fine-tuning to make it more accurate and contextually appropriate to generate responses in the target domain. By fine-tuning, the model can acquire domain-specific knowledge, language patterns, and nuances, enabling it to generate more relevant and coherent responses for specific applications or use cases.

We will demonstrate how to fine-tune ChatGPT to generate concise book summaries using JSON prompts and completions data.

In this chapter, we will cover the following topics:

* The API models that can be fine-tuned
* The cost involved in fine-tuning an AI model
* Using JSON to prepare the training data
* Creating a fine-tuned model with the OpenAI **command-line interface** (**CLI**)
* Listing all available fine-tuned models and their information
* Using fine-tuned models with the ChatGPT API completions
* Deleting a fine-tuned model

## Technical Requirements <a href="#_idtextanchor140" id="_idtextanchor140"></a>

The technical requirements for this chapter are listed as follows:

* Python 3.7 or later installed on your machine
* An OpenAI API key
* The OpenAI Python library installed

In the forthcoming section, we shall commence our journey into ChatGPT fine-tuning by examining the array of available models, establishing our development environment, and acquainting ourselves with the book summary fine-tuned model that we are about to construct.

You can find all code examples from this chapter on GitHub at [https://github.com/PacktPublishing/Building-AI-Applications-with-ChatGPT-APIs/tree/main/Chapter12%20FineTuning](https://github.com/PacktPublishing/Building-AI-Applications-with-ChatGPT-APIs/tree/main/Chapter12%20FineTuning)

## Fine-Tuning ChatGPT <a href="#_idtextanchor141" id="_idtextanchor141"></a>

In this section, you will learn about the process of fine-tuning ChatGPT models. We will talk about the ChatGPT models available for fine-tuning and provide information on their training and usage costs. We will also cover the installation of the **openai** library and set up the API key as an environmental variable in the terminal session. This section will serve as an overview of fine-tuning, its benefits, and the necessary setup to train a fine-tuned model.

Fine-tuning enhances the capabilities of API models in several ways. Firstly, it yields higher-quality outcomes compared to designing prompts alone. By incorporating more training examples than can be accommodated in a prompt, fine-tuning enables models to grasp a wider range of patterns and nuances. Secondly, it reduces token usage by utilizing shorter prompts, resulting in more efficient processing. Additionally, fine-tuning facilitates lower-latency requests, enabling faster and more responsive interactions.

GPT-3 has undergone extensive pre-training on a vast corpus of text sourced from the internet. When given a prompt with limited examples, it often demonstrates the ability to understand the intended task and generate plausible completions – a concept referred to as **few-shot learning**. However, fine-tuning takes few-shot learning a step further by utilizing a larger set of examples, surpassing the prompt’s capacity. This comprehensive training enables superior performance across a wide array of tasks.

If a model has undergone fine-tuning, we can use the resultant model without the need to pass any further training data. On _Table 12.1_, taken from the official OpenAI pricing page at [https://openai.com/pricing](https://openai.com/pricing), you can see all the ChatGPT models that are available for fine-tuning, each accompanied by detailed information about their training and usage costs per 1,000 tokens, as of June 2023.

![Table 12.1: The Pricing for ChatGPT Model Fine-Tuning](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781805127567/files/image/Image96403.png)

Table 12.1: The Pricing for ChatGPT Model Fine-Tuning

During the fine-tuning process, it is necessary to provide the training data in the form of a JSON file, and the training token count specifically refers to those used within the prompt and completion fields contained within that file.

Each line of the JSON document should consist of a **prompt** and **completion** field that corresponds to our desired training data, as shown here:

```
{"prompt": "Book Summary: The Adventure Begins", "completion": "A thrilling tale of courage and discovery."}
{"prompt": "Book Summary: Secrets Unveiled", "completion": "An intriguing mystery that will keep you guessing until the end."}
```

This snippet is an example of how two of the training lines should be displayed in the JSON file. In the next sections, we will build the complete JSON file, using a considerably larger dataset and with the help of the **OpenAI CLI data preparations tool**.

Our objective is to develop a fine-tuned model that specializes in generating concise, one-sentence summaries for books provided by the user. To accomplish this, we will construct a new JSON training file that includes the book titles in the prompt field and their corresponding summaries in the completion field. This training process will equip our model with the necessary skills to effectively generate book summaries based on user-provided input, once it has completed its training phase.

Upon the completion of the training phase, OpenAI will provide us with a distinct and exclusive name for our fine-tuned model. This unique name can then be utilized within the ChatGPT completion prompt to effectively engage with the model.

Before jumping into the preparation of our dataset, we need to ensure that we have the **openai** library installed on our device. To do that, open a new terminal or Command Prompt and type the following:

```
$ pip install --upgrade openai
```

This line will ensure that the last version of the **openai** package has been installed on your device.

To fine-tune a model from your local terminal, you will need to also provide the ChatGPT API key as an environmental variable:

*   Mac users:

    ```
    $ export OPENAI_API_KEY="<OPENAI_API_KEY>"
    ```
*   Windows users:

    ```
    set OPENAI_API_KEY=<your_api_key>
    ```

In this line, the **export** command is used to create an environment variable named **OPENAI\_API\_KEY**. The **\<OPENAI\_API\_KEY>** value is a placeholder that you should replace with the API key obtained from your OpenAI account. By setting this environment variable, we ensure that the API key is securely stored and can be accessed by the rest of the commands we execute in the terminal session.

This section provided an overview of fine-tuning ChatGPT models. It discussed how fine-tuning enhances few-shot learning and the necessity of providing training data in a JSON file. We discussed the fine-tuned model that we will build to generate concise book summaries based on user-provided input. We also covered the environment setup, which included the installation of the **openai** library and setting the API key as an environmental variable in our terminal session.

## Fine-Tuned Model Dataset Preparation <a href="#_idtextanchor142" id="_idtextanchor142"></a>

To effectively fine-tune our model, we need to prepare the training data in a specific format. In this section, we will walk you through the process of data preparation using a JSON file and the OpenAI CLI data preparations tool.

When preparing data for a fine-tuned model such as OpenAI’s, it’s essential to follow a structured process to ensure optimal performance and accurate results. The first step is to gather the relevant data that will be used to train the model. This data can come from a variety of sources, such as books, articles, or even specialized datasets.

To begin, create a new folder called **Fine\_Tune\_Data** on your desktop, and inside the folder, create a new file called **train\_data.json**. For our book summary fine-tuned model, we will use one-sentence summaries for 30 different books. Those summaries will be written inside the file we just created in a JSON format:

```
[
{"prompt": "Book Summary: The Adventure Begins", "completion": "A thrilling tale of courage and discovery."},
{"prompt": "Book Summary: Secrets Unveiled", "completion": "An intriguing mystery that will keep you guessing until the end."},
{"prompt": "Book Summary: Love and Betrayal", "completion": "A heart-wrenching story of love, trust, and deception."},
{"prompt": "Book Summary: The Quest for Freedom", "completion": "A captivating journey of self-discovery and liberation."},
{"prompt": "Book Summary: Beyond the Stars", "completion": "An epic science fiction adventure that pushes the boundaries of imagination."},
{"prompt": "Book Summary: The Power Within", "completion": "A gripping exploration of inner strength and resilience."},
{"prompt": "Book Summary: Shadows of the Past", "completion": "A haunting tale of redemption and confronting the ghosts of the past."},
{"prompt": "Book Summary: A World Apart", "completion": "A mesmerizing exploration of parallel universes and the power of choices."},
{"prompt": "Book Summary: Into the Unknown", "completion": "A suspenseful journey into uncharted territories and the pursuit of truth."},
{"prompt": "Book Summary: The Lost Artifact", "completion": "A thrilling archaeological adventure that unravels ancient mysteries."},
{"prompt": "Book Summary: Broken Pieces", "completion": "A poignant story of healing, forgiveness, and the beauty of imperfection."},
{"prompt": "Book Summary: Echoes of Silence", "completion": "A tale of loss and resilience, where words hold the power to heal."},
{"prompt": "Book Summary: The Forgotten Realm", "completion": "An enchanting fantasy world brimming with magic and destiny."},
{"prompt": "Book Summary: Life's Unexpected Turns", "completion": "A captivating narrative of life's twists and the resilience to overcome."},
{"prompt": "Book Summary: From Ashes to Glory", "completion": "A triumphant story of rising from adversity and finding one's purpose."},
{"prompt": "Book Summary: The Last Stand", "completion": "A gripping account of courage, sacrifice, and the fight for justice."},
{"prompt": "Book Summary: In the Shadows of Time", "completion": "A mesmerizing blend of past and present, weaving a tale of love and destiny."},
{"prompt": "Book Summary: A Journey of Discovery", "completion": "An introspective expedition that uncovers the mysteries of the self."},
{"prompt": "Book Summary: Torn Between Worlds", "completion": "A captivating exploration of identity and the search for belonging."},
{"prompt": "Book Summary: The Art of Letting Go", "completion": "A transformative journey of releasing the past and embracing the future."},
{"prompt": "Book Summary: Whispers in the Wind", "completion": "A lyrical narrative that captures the ephemeral beauty of fleeting moments."},
{"prompt": "Book Summary: The Hidden Truths", "completion": "A riveting exposé of secrets, lies, and the quest for truth."},
{"prompt": "Book Summary: Beyond the Veil", "completion": "A spellbinding adventure that blurs the boundaries between reality and the supernatural."},
{"prompt": "Book Summary: Unbreakable Bonds", "completion": "A tale of friendship, loyalty, and the unyielding strength of human connections."},
{"prompt": "Book Summary: The Songbird's Melody", "completion": "A melodic tale of passion, dreams, and the pursuit of artistic expression."},
{"prompt": "Book Summary: Shattered Reflections", "completion": "A psychological thriller that delves into the darkest corners of the human mind."},
{"prompt": "Book Summary: A Patchwork of Memories", "completion": "A nostalgic journey through fragments of the past, weaving a tapestry of remembrance."},
{"prompt": "Book Summary: Embers of Hope", "completion": "A tale of resilience and hope, where even the smallest spark can ignite change."},
{"prompt": "Book Summary: Beneath the Surface", "completion": "A suspenseful exploration of hidden truths lurking beneath seemingly ordinary lives."},
{"prompt": "Book Summary: The Road Less Traveled", "completion": "A transformative odyssey that challenges conventions and embraces individuality."}
]
```

The given JSON file contains a collection of book summaries represented as prompt-completion pairs. Each entry consists of a **prompt** field that introduces the book with a summary title, followed by a **completion** field that provides a brief description of the book. The prompts highlight different genres, themes, and emotions associated with the books, while the completions capture the essence of each story. The summaries encompass a variety of genres, including adventure, mystery, romance, science fiction, and fantasy. This JSON file data will serve as a dataset to train a fine-tuned model to generate concise book summaries, based on user-provided input.

Once the JSON file has been created, we can now prepare the data using the default **prepare\_data** function part of the **openai** library. This versatile tool can accept various file formats if they have a **prompt** and **completion** column or key. Whether you have a JSON, JSONL, CSV, TSV, or XLSX file, this tool can handle it. Once you provide the input file, the tool will guide you through any necessary adjustments and save the output in the JSONL format, which is specifically designed for fine-tuning purposes.

Unlike using base models where prompts can include detailed instructions or multiple examples, for fine-tuning, each prompt should conclude with a specific separator (such as **\n\n###\n**) to signal to the model when the prompt ends and the completion begins. Those separators can be added either by you or the ChatGPT data preparation tool.

To achieve superior performance through fine-tuning, it is advisable to include a substantial number of high-quality examples. Even though our project includes only 30 examples, for optimal results surpassing base models, you should aim to provide a few hundred or more high-quality examples, preferably vetted by human experts. Typically, as the number of examples doubles, performance shows a linear increase. Increasing the number of examples is usually the most effective and reliable approach to enhance performance.

To activate the data preparation tool for our project type, execute the following commands by using our **train\_data.json**:

```
$ cd Fine_Tune_Data
$ openai tools fine_tunes.prepare_data -f train_data.json
```

After executing the command, the data preparation tool will meticulously examine the contents of our JSON file. Considering that our JSON data may not be flawlessly formatted and lacks appropriate separators, with each example beginning with the prefix **Book Summary:**, the data preparation tool will generate a helpful suggestion to rectify the training data, which will be displayed in the terminal output:

```
Analyzing...
- Your file contains 30 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples
- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty
- All prompts start with prefix `Book Summary: `. Fine-tuning doesn't require the instruction specifying the task, or a few-shot example scenario. Most of the time you should only add the input data into the prompt, and the desired output into the completion
- All completions end with suffix `.`
- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details
```

Furthermore, as part of the data preparation process, the tool will inquire whether you would like to incorporate the suggested modifications into your data before proceeding with the creation of the JSONL output file. **JSONL** stands for the **JSON Lines** format, which is a text-based data interchange format where each line in the file represents a single JSON object. This format is commonly used to store and exchange structured data, making it easy to read, write, and process data in a streaming fashion. This interactive feature ensures that you can review and consider the proposed adjustments, enabling you to refine your dataset and generate a more refined JSONL file:

```
Based on the analysis we will perform the following actions:
- [Recommended] Add a suffix separator ` ->` to all prompts [Y/n]: Y
- [Recommended] Remove prefix `Book Summary: ` from all prompts [Y/n]: Y
- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y
Your data will be written to a new JSONL file. Proceed [Y/n]: Y
Wrote modified file to `train_data_prepared.jsonl`
Feel free to take a look!
Now use that file when fine-tuning:
> openai api fine_tunes.create -t "train_data_prepared.jsonl"
```

By offering this option, the tool empowers you to exercise greater control and make informed decisions regarding the preparation of your data to fine-tune the model.

Once the task has been completed, you will find a new file, **train\_data\_prepared.jsonl**, inside the **Fine\_Tune\_Data** directory. If you accepted all recommendations from the data preparation tool, your JSONL file should look as follows:

```
{"prompt":"The Adventure Begins ->","completion":" A thrilling tale of courage and discovery."}
{"prompt":"Secrets Unveiled ->","completion":" An intriguing mystery that will keep you guessing until the end."}
{"prompt":"Love and Betrayal ->","completion":" A heart-wrenching story of love, trust, and deception."}
{"prompt":"The Quest for Freedom ->","completion":" A captivating journey of self-discovery and liberation."}
{"prompt":"Beyond the Stars ->","completion":" An epic science fiction adventure that pushes the boundaries of imagination."}
……
```

You can find the full JSOL file in our Git repository: [https://github.com/PacktPublishing/Building-AI-Applications-with-ChatGPT-APIs/blob/main/Chapter12%20FineTuning/train\_data\_prepared.jsonl](https://github.com/PacktPublishing/Building-AI-Applications-with-ChatGPT-APIs/blob/main/Chapter12%20FineTuning/train\_data\_prepared.jsonl).

To prepare the dataset to fine-tune the model, the JSON file was modified by adding a separator at the end of each prompt, removing the prefix **Book Summary:** from the prompts, starting the completions with a whitespace character, and saving the data in the JSONL format. These modifications enhance the model’s understanding and generate concise book summaries based on user input.

With our dataset prepared and optimized, we have laid the foundation to construct the fine-tuned model, a crucial step that will be explored in detail in the next section.

## Building and Using the Fine-Tuned Model <a href="#_idtextanchor143" id="_idtextanchor143"></a>

In this section, we will explore the process of creating and utilizing a fine-tuned model using OpenAI’s CLI. OpenAI offers newcomers the opportunity to avail of a $5 credit to access the ChatGPT API and its fine-tuning services.

Fine-tuning involves building a specialized model based on an existing base model, and in our example, we will use the most advanced ChatGPT model available for fine-tuning called **davinci**. We will improve the performance of that model for book summarization tasks.

We will learn how to start a fine-tuning job, which uploads and processes the training data, and then we’ll monitor its progress until completion. Once the fine-tuning job is done, we will use the newly created fine-tuned model to generate text. We’ll learn how to make requests to the fine-tuned model using the completions API, and we’ll cover how to manage and delete fine-tuned models if needed.

We will begin by using our newly created JSONL file to create the fine-tuned model with the following command:

```
openai api fine_tunes.create -t train_data_prepared.jsonl  -m davinci
```

The preceding command is used to create a fine-tuning job for a text dataset. The command starts the process of fine-tuning a model by specifying the training data file and the base model to start from. In this case, we will use our training data file called **train\_data\_prepared.jsonl**. By executing this command, the training data will be uploaded and processed, and a fine-tuning job will be created using the specified base model:

```
Upload progress: 100%|██| 3.68k/3.68k [00:00<00:00, 2.95Mit/s]
Uploaded file from train_data_prepared.jsonl: file-QzFonh5QTZKu2IVc4lBKdwp1
Created fine-tune: ft-GrpW4DOXtWkXIgznJrSB9kOD
Streaming events until fine-tuning is complete...
(Ctrl-C will interrupt the stream, but not cancel the fine-tune)
[2023-06-26 18:01:23] Created fine-tune: ft-GrpW4DOXtWkXIgznJrSB9kOD
```

In the provided output, you can find the ID of your fine-tuned model, **ft-GrpW4DOXtWkXIgznJrSB9kOD**. It’s important to note that the process of creating the model may vary in duration, taking anywhere from a few minutes to hours. If you accidentally interrupt the data stream by pressing _Ctrl_ + _C_, rest assured that your job will not be canceled. The OpenAI servers will continue processing your fine-tuned model until it reaches completion. To ensure you stay informed about the progress of the fine-tuning job, OpenAI offers you the option to monitor its status using the provided model ID. This way, you can keep track of the creation process and be notified once your fine-tuned model is ready.

To find that information, you can type the following:

```
$ openai api fine_tunes.follow -i ft-GrpW4DOXtWkXIgznJrSB9kOD
```

To consistently monitor our fine-tuning job, we will utilize the **follow** function. It is important to note that your specific ID will be unique and distinct from the one demonstrated in the previous example. If you happen to misplace your ID, there’s no need to worry, as you can effortlessly retrieve the details of all completed and pending fine-tuned models associated with your account by entering the following command:

```
$ openai api fine_tunes.list
```

You can also find the information about a specific model by referring to its ID:

```
$ openai api fine_tunes.get -i ft-GrpW4DOXtWkXIgznJrSB9kOD
```

As there is currently only one fine-tuning job in progress, the output from both prompts will yield identical results:

```
{
  "object": "fine-tune",
  "id": "ft-GrpW4DOXtWkXIgznJrSB9kOD",
  "hyperparams": {
    "n_epochs": 4,
    "batch_size": null,
    "prompt_loss_weight": 0.01,
    "learning_rate_multiplier": null
  },
  "organization_id": "org-yBpXCfCEnXErZ6PUZZE3VKZx",
  "model": "davinci",
  "training_files": [
    {
      "object": "file",
      "id": "file-QzFonh5QTZKu2IVc4lBKdwp1",
      "purpose": "fine-tune",
      "filename": "train_data_prepared.jsonl",
      "bytes": 3679,
      "created_at": 1687816883,
      "status": "processed",
      "status_details": null
    }
  ],
  "validation_files": [],
  "result_files": [],
  "created_at": 1687816883,
  "updated_at": 1687816883,
  "status": "pending",
  "fine_tuned_model": null,
  "events": [
    {
      "object": "fine-tune-event",
      "level": "info",
      "message": "Created fine-tune: ft-GrpW4DOXtWkXIgznJrSB9kOD",
      "created_at": 1687816883
    }
  ]
}
```

The provided output represents the details of a fine-tuning job. It includes information such as the object type, job ID, hyperparameters, organization ID, base model, training files, validation files, result files, timestamps of creation and update, current status, fine-tuned model, and a list of events. The **hyperparameters** section specifies the settings for the fine-tuning process. The **training\_files** section contains details about the training data file used for the fine-tuning job. The most important parameter to consider is **status** of the fine-tuning job. It is currently **pending**, indicating that the fine-tuning process is still in progress.

By utilizing the provided commands, you can continually track and monitor the progress of your fine-tuned model until its creation is finalized. Throughout this period, you can regularly check the status field to remain informed about the current state of the job.

If you wish to cancel the job for any reason, you can initiate the cancellation process by inputting the following command:

```
$ openai api fine_tunes.cancel -i ft-GrpW4DOXtWkXIgznJrSB9kOD
```

Once the fine-tuning process is successfully completed, the status field returned from the **.get** or **.list** functions will be updated, and it will also reveal the name of the newly generated model:

```
      "status": "succeeded",
      "fine_tuned_model": "davinci:ft-personal-2023-06-26-06-55-07"
```

The **fine\_tuned\_model** field corresponds to the distinctive name assigned to the recently generated model. This name holds significant value, as it can be seamlessly incorporated into any ChatGPT API **completion** function by assigning it to the **model** parameter. By doing so, you can leverage the capabilities of the fine-tuned model and harness its enhanced text generation abilities.

After the successful creation of your fine-tuned model, you can conduct easy and quick tests directly from the terminal using the **curl** command. This allows you to seamlessly interact with your fine-tuned model and assess its performance without any complications:

```
$ curl https://api.openai.com/v1/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "prompt": "Secrets Unveiled ->",
    "max_tokens": 30,
    "model": "davinci:ft-personal-2023-06-26-01-42-07"}'
```

The given example includes several parameters and headers to specify the details of the request.

The URL specified in the command is [https://api.openai.com/v1/completions](https://api.openai.com/v1/completions), indicating the endpoint for the ChatGPT completion API.

The **-H** flag is used to set headers for the request. In this case, two headers are included:

* The **Content-Type** header: **application/json** specifies that the content type of the request is JSON
* The **Authorization** header: **Bearer $OPENAI\_API\_KEY** is an authentication header, where the **$OPENAI\_API\_KEY** variable should be replaced with the actual API key if we previously set it as an environmental variable

The **-d** flag is used to pass data or payload in the request. In this example, the data being passed is a JSON object enclosed in single quotes. The JSON object contains the following fields:

* **prompt**: This is the text prompt used to start the completion. In this case, it is set to **Secrets Unveiled ->**, which is the header and the separator of one of the books with which we trained our book summary fine-tuned model.
* **max\_tokens**: This is the maximum number of tokens expected in the completion response. Here, it is set to **30**.
* **model**: This is the name of the model to be used for completion. The example shows a fine-tuned model named **davinci:ft-personal-2023-06-26-01-42-07** being used. Here, you should place the unique name of your ChatGPT fine-tuned model.

Once the **curl** command is executed, you will receive the following response in the JSON format.

<pre><code>{
  "id": "cmpl-7Vpp9P3vv6JKZwjYbrmjSPeasxiDH",
  "object": "text_completion",
  "created": 1687821735,
  "model": "davinci:ft-personal-2023-06-26-23-07-29",
  "choices": [
    {
<strong>      "text": " An intriguing mystery that will keep you guessing        until the end. A hidden truth, an unsolved puzzle, and the        pursuit of truth. A gripping tale of",
</strong>      "index": 0,
      "logprobs": null,
      "finish_reason": "length"
    }
  ],
  "usage": {
    "prompt_tokens": 6,
    "completion_tokens": 30,
    "total_tokens": 36
  }
}
</code></pre>

The preceding output is a standard ChatGPT response, except that here we used our fine-tuned book summary model. To verify that the model worked properly, you can compare the **text** field with the training data provided in our JSONL file for the corresponding book header prompt:

<pre><code><strong>{"prompt":"Secrets Unveiled ->","completion":" An intriguing mystery that will keep you guessing until the end."}
</strong></code></pre>

Comparing the two preceding code snippers, you can clearly see that our fine-tuned model has worked properly and the **completion** field from our training data is part of our ChatGPT API response in its entirety. This means that the trained model shows an improved performance incorporating the training data.

Finally, if you decide to remove or delete a fine-tuned model that you created, OpenAI provides a straightforward command that you can use. Execute the following command in your command-line interface:

```
$ openai api models.delete -i davinci:ft-personal-2023-06-26-06-55-07
```

It is important to note that deleting a fine-tuned model should be done judiciously, as it permanently removes the model and its associated data. Therefore, it is advisable to carefully consider your decision and ensure that you no longer need the fine-tuned model before proceeding with the deletion command.

In this section, we have explored the entire life cycle of fine-tuning models. Armed with this knowledge, you are now equipped to continue your journey of experimentation by constructing fine-tuned models that specialize in various tasks and incorporate a larger volume of training data entries. This opens a realm of possibilities to refine and enhance the performance of your models, enabling you to achieve even more impressive results.

## Summary <a href="#_idtextanchor144" id="_idtextanchor144"></a>

In this chapter, we discussed the concept of fine-tuning within the ChatGPT API, exploring how it can help us to tailor ChatGPT API responses to our specific needs. By training a pre-existing language model on a diverse dataset, we enhanced the **davinci** model performance and adapted it to a particular task and domain. Fine-tuning enriched the model’s capacity to generate accurate and contextually fitting responses by incorporating domain-specific knowledge and language patterns. Throughout the chapter, we covered several key aspects of fine-tuning, including the available models for customization, the associated costs, data preparation using JSON files, the creation of fine-tuned models via the OpenAI CLI, and the utilization of these models with the ChatGPT API. We underscored the significance of fine-tuning to achieve superior outcomes, reduce token consumption, and enable faster and more responsive interactions.

Additionally, the chapter offered a comprehensive step-by-step guide on dataset preparation for fine-tuning. It dived into the process of data organization using a JSON file and the OpenAI CLI data preparation tool. The dataset comprised prompt-completion pairs representing book summaries. The tool analyzed the data and proposed modifications, such as adding separators to prompts, eliminating unnecessary prefixes, and initiating completions with whitespace characters. These modifications enhanced the model’s comprehension. Once the dataset was prepared and optimized, the chapter detailed the process of constructing and utilizing the fine-tuned model. It provided insights on initiating a fine-tuning job, monitoring its progress, and harnessing the fine-tuned model for text generation through the ChatGPT API. We emphasized the importance of fine-tuning in improving model performance and learned how to manage and delete fine-tuned models as necessary.

In this book, you embarked on a comprehensive journey into the world of ChatGPT and its API, gaining knowledge and skills to build a variety of powerful applications. You gradually built a ChatGPT clone, developed a SaaS application, integrated payment services, and explored frameworks such as Django and PyQt for web and desktop app development. You also discovered the possibilities of integration with Microsoft Word, Outlook, and PowerPoint, as well as transcribing audio files. You gained insights into selecting the right model and optimizing completions, and you even explored the realm of ChatGPT fine-tuning. With these skills, you are now well equipped to create innovative AI applications and leverage the full potential of the ChatGPT API.
