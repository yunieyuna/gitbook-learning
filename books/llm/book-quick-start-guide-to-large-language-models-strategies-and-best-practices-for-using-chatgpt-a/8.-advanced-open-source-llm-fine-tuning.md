# 8. Advanced Open-Source LLM Fine-Tuning

### 8. Advanced Open-Source LLM Fine-Tuning <a href="#ch08" id="ch08"></a>

#### Introduction <a href="#ch08lev1sec1" id="ch08lev1sec1"></a>

If I were to admit an ulterior motive for writing this book besides helping you understand and use LLMs, it would be to convince you that with the proper data and fine-tuning, smaller open-source models can be as amazing as huge closed-source models like GPT-4, especially for hyper-specific tasks. By now, I hope you understand the advantages of fine-tuning models over using closed-source models via an API. These closed-source models are truly powerful, but they don’t always generalize to what we need—which is why we need to fine-tune them with our own data.

This chapter aims to help you harness the maximum potential of open-source models to deliver results that rival those possible with their larger, closed-source counterparts. By adopting the techniques and strategies outlined in this chapter, you will be able to mold and shape these models to your specific requirements.

As an ML engineer, I’d argue that the beauty of fine-tuning lies in its flexibility and adaptability, which allows us to tailor the models to our unique needs. Whether you’re aiming to develop a sophisticated chatbot, a simple classifier, or a tool that can generate creative content, the fine-tuning process ensures that the model aligns with your objectives.

This journey will demand rigor, creativity, problem-solving skills, and a thorough understanding of the underlying principles of machine learning. But rest assured, the reward (pun intended for the final example) is worth the effort. Let’s get started, shall we?

#### Example: Anime Genre Multilabel Classification with BERT <a href="#ch08lev1sec2" id="ch08lev1sec2"></a>

You thought I was done talking about anime? Nope, sorry. For our first example, we’ll use the anime dataset from [Chapter 6](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06) to build a genre prediction engine. Recall that in [Chapter 6](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06), we built a recommendation engine using a generated description as the base feature of an anime title; in doing so, one of the features we used was the genre list of the anime. Let’s assume that our new goal is to assist people in tagging an anime’s genre list given the other features. There are 42 unique genres, as shown in [Figure 8.1](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig01).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig01.jpg" alt="A chart represents the Distribution of Genres." height="392" width="775"><figcaption><p>Figure 8.1 We have 42 genres to categorize from in our multilabel anime genre classification task.</p></figcaption></figure>

**Using the Jaccard Score to Measure Performance for Multilabel Genre Prediction of Anime Titles**

To evaluate the performance of our genre prediction model, we will use the Jaccard score, a metric that measures the similarity between sets of items. This score is appropriate for our multilabel (we are able to predict multiple labels per item) genre prediction task, as it will enable us to assess the accuracy of our model in predicting the correct genres for each anime title.

[Listing 8.1](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#list8\_1) shows how we can define custom metrics in our `Trainer`. In this case, we will define four metrics:

* **Jaccard score:** Similar to how we used the Jaccard score in [Chapter 6](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06), it will help us gauge the similarity and diversity of sample sets in this example. In the context of evaluating model performance, a higher Jaccard score indicates that the model’s predictions are more similar to the actual labels.
* **F1 score:** The F1 score is a measure of a model’s accuracy on a dataset. It is used to evaluate binary classification systems, which classify examples as either “positive” or “negative.” The F1 score is the harmonic mean of the precision and recall; it reaches its best value at 1 (perfect precision and recall) and its worst at 0.
* **ROC/AUC:** The receiver operating characteristic (ROC) is a probability curve; the area under the curve (AUC) represents the degree or measure of separability. The AUC indicates how well a model distinguishes between classes: The higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s.
* **Accuracy:** As you might expect, accuracy quantifies how often the predicted label matches the true label exactly. While it’s easy to interpret, this metric can be misleading for imbalanced datasets, where the model can achieve a high accuracy by merely predicting the majority class.

Listing 8.1 Defining custom metrics for our multilabel genre prediction

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08\_images.xhtml#f0177-01a)

```
# Define a function to compute several multilabel metrics
def multi_label_metrics(predictions, labels, threshold=0.5):
    # Initialize the sigmoid function, which we'll use to transform our raw prediction
values
    sigmoid = torch.nn.Sigmoid()

    # Apply sigmoid function to our predictions
    probs = sigmoid(torch.Tensor(predictions))

    # Create a binary prediction array based on our threshold
    y_pred = np.zeros(probs.shape)
    y_pred[np.where(probs >= threshold)] = 1

    # Use actual labels as y_true
    y_true = labels

    # Compute F1 score, ROC/AUC score, accuracy, and Jaccard score
    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')
    roc_auc = roc_auc_score(y_true, y_pred, average='micro')
    accuracy = accuracy_score(y_true, y_pred)
    jaccard = jaccard_score(y_true, y_pred, average='micro')

    # Package the scores into a dictionary and return it
    metrics = {'f1': f1_micro_average,
               'roc_auc': roc_auc,
               'accuracy': accuracy,
               'jaccard': jaccard}
    return metrics
# Define a function to compute metrics for predictions
def compute_metrics(p: EvalPrediction):
    # Extract the prediction values from the EvalPrediction object
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions

    # Compute the multilabel metrics for the predictions and actual labels
    result = multi_label_metrics(predictions=preds, labels=p.label_ids)

    # Return the results
    return result
```

**A Simple Fine-Tuning Loop**

To fine-tune our model, we will set up the following components, each of which plays a crucial role in the customization process:

* **Dataset:** We will use our previously prepared training and testing sets from the MyAnimeList dataset. The dataset serves as the foundation for the entire fine-tuning process, as it contains the input data (synopses) and target labels (genres) that the model will learn to predict. Properly splitting the dataset into training and testing sets is vital for evaluating the performance of our customized model on unseen data.
* **Data collator:** The data collator is responsible for processing and preparing the input data for our model. It takes raw input data, such as text, and transforms it into a format that the model can understand, typically involving tokenization, padding, and batching. By using a data collator, we ensure that our input data is correctly formatted and efficiently fed into the model during training.
* **`TrainingArguments`:** `TrainingArguments` is a configuration object provided by the Hugging Face library that allows us to specify various hyperparameters and options for the training process. These can include learning rate, batch size, number of training epochs, and more. By setting up `TrainingArguments`, we can fine-tune the training process to achieve optimal performance for our specific task.
* **Weights & Biases and `Trainer`:** Weights & Biases (WandB) is a library that facilitates tracking and visualizing the progress of the training process. By integrating WandB, we can monitor key metrics, such as loss and accuracy, and gain insights into how well our model is performing over time. `Trainer` is a utility provided by the Hugging Face library that manages the fine-tuning process. It handles tasks such as loading data, updating model weights, and evaluating the model’s performance. By setting up a `Trainer`, we can streamline the fine-tuning process and ensure that our model is effectively trained on the task at hand.

[Figure 8.2](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig02) visualizes the basic deep learning training loop using Hugging Face’s built-in fine-tuning components.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig02.jpg" alt="A diagram represents the basic deep learning training loop." height="687" width="775"><figcaption><p>Figure 8.2 We will rely on the benevolence of Hugging Face’s built-in training components to fine-tune our models in this chapter.</p></figcaption></figure>

**General Tips for Fine-Tuning Open-Source LLMs**

In this section, I’ll highlight a few tips and tricks for fine-tuning LLMs, regardless of the task you are performing.

**Data Preparation + Feature Engineering**

I’m pretty vocal when it comes to the importance of data preparation and feature engineering in machine learning. In fact, I wrote two whole books about it (so far). In terms of LLM fine-tuning, one of the easiest things we can do is to construct new composite features from raw features. For instance, we created a “Generated Description” feature in [Chapter 6](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06) that included the synopsis of the anime, the genres, the producers, and more in hopes of giving ample context to the model. In this example, we will create the same exact description except without the genres—because, well, it would be cheating to include the genres in the input and have genre prediction be the task.

Recall the discussion of the importance of de-duplicating our data in [Chapter 4](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#ch04). Although there are no duplicate animes in our example dataset, we can still think about deduping at a semantic level. There are likely some animes that are based on the same source material or perhaps multiple movies based on the same plot that might confuse the model. [Listing 8.2](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#list8\_2) defines a simple function that uses a bi-encoder to encode our descriptions and remove animes that are too semantically similar (via cosine similarity) to other animes.

Listing 8.2 Semantically deduping a corpus using a bi-encoder

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08\_images.xhtml#f0180-01aa)

<pre><code># Import necessary libraries
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Initialize our model that encodes semantically similar texts to be near each other
# 'paraphrase-distilroberta-base-v1' is a pre-trained model for semantic textual
similarity
downsample_model = SentenceTransformer('paraphrase-distilroberta-base-v1')

def filter_semantically_similar_texts(texts, similarity_threshold=0.8):
    # Generate embeddings for all texts. These embeddings are numerical
representations of the text that encode meaning to a high-dimensional space
    embeddings = downsample_model.encode(texts)

    # Cosine similarity between all pairs of text embeddings. The
    # result is a matrix where the cell at row i and column j
    # is the cosine similarity between the embeddings of texts [i] and [j]
    similarity_matrix = cosine_similarity(embeddings)

    # Set the diagonal elements of the similarity matrix to 0, because they represent
    # the similarity of each text with itself, which is always 1.
    np.fill_diagonal(similarity_matrix, 0)

    # Initialize an empty list to store the texts that are not too similar
    filtered_texts = []

    # A set to store the indices of the texts that are too similar
    excluded_indices = set()

    for i, text in enumerate(texts):
        # If the current text is not too similar to any other text
        if i not in excluded_indices:
            # Add it to the list of nonsimilar texts
            filtered_texts.append(text)
            # Find the indices of the texts that are too similar to the current text
            similar_texts_indices = np.where(similarity_matrix[i] > similarity_
threshold)[0]

            # Exclude these texts from further consideration
            excluded_indices.update(similar_texts_indices)

    return filtered_texts

# List of sample texts for testing the function
texts = [
    "This is a sample text.",
    "This is another sample text.",
    "This is a similar text.",
    "This is a completely different text.",
    "This text is quite alike.",
]

# Use the function to filter semantically similar texts
filtered_texts = filter_semantically_similar_texts(texts, similarity_threshold=0.9)
# Print the texts that passed the semantic similarity filter

<strong>filtered_texts == [
</strong><strong>  'This is a sample text.',
</strong><strong>  'This is a similar text.',
</strong><strong>  'This is a completely different text.',
</strong><strong>  'This text is quite alike.'
</strong>]
</code></pre>

Note that we run the risk of losing valuable information through this process. Just because an anime is semantically similar to another anime, it doesn’t mean that they will have the same genres. This issue is not something that will halt us in our tracks but it is worth mentioning. The process employed here—often referred to as **semantic similarity deduping**—can be thought of as part of our pipeline, and the threshold that we use for removing similar documents (the `similarity_threshold` variable in [Listing 8.2](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#list8\_2)) can be thought of as just another hyperparameter, like the number of training epochs or the learning rate.

**Adjusting Batch Sizes and Gradient Accumulation**

Finding an optimal batch size is an essential fine-tuning method to balance the trade-off between memory and stability of the model. A larger batch size means more data points processed by the model during a particular training run and can provide a more accurate estimate of the gradient, but it also requires more computational resources.

If memory limitations are an issue, gradient accumulation can be an excellent solution. Gradient accumulation allows you to effectively train with a larger batch size by splitting it over several backward passes, reducing the memory required for each pass. As a result, you can train with a more stable gradient with less memory.

**Dynamic Padding**

Dynamic padding (visualized in [Figure 8.3](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig03)) is a technique that can greatly reduce wasted computational resources when you’re dealing with large numbers of variable-length sequences, such as text data. Traditional uniform-length padding techniques often pad each sequence to the length of the longest sequence in the entire dataset, which can lead to a lot of wasted computations if the lengths of sequences vary widely. Dynamic padding adjusts the amount of padding for each batch separately, meaning that less padding is used on average, making computations more efficient.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig03.jpg" alt="A diagram illustrates uniform and dynamic padding." height="636" width="775"><figcaption><p>Figure 8.3 Orange: actual tokens; blue: padding tokens. Uniform padding (top) pads all sequences in the dataset to be of equal length, usually to the longest sequence in the entire dataset. This is extremely computationally inefficient. Dynamic padding (bottom) pads sequences in each batch to be of equal length, usually to the longest sequence in the batch.</p></figcaption></figure>

Performing dynamic padding can be as simple as using the DataCollatorWithPadding object from the Transformers package. [Listing 8.3](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#list8\_3) shows a quick example of altering code to use DataCollatorWithPadding. As always, full examples are available on the book’s code repository.

Listing 8.3 Using DataCollatorWithPadding for dynamic padding

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08\_images.xhtml#f0183-01a)

```
# Import DataCollatorWithPadding
from transformers import DataCollatorWithPadding

model = AutoModelForSequenceClassification.from_pretrained(
    … # instantiate some model, like BERT for GPT-2
)
# Define our collator with tokenizer and how we want to pad as input.
# "longest" is the default and pads every sequence in a batch to the longest length of
that batch.

# Tokenizing (but NOT PADDING) text in a dataset so that our collator can dynamically
pad during training/testing
# assuming we have some "raw_train" and "raw_test" datasets at our disposal.
train = raw_train.map(lambda x: tokenizer(x["text"], truncation=True), batched=True)
test = raw_test.map(lambda x: tokenizer(x["text"], truncation=True), batched=True)

collate_fn = DataCollatorWithPadding(tokenizer=tokenizer, padding="longest")

trainer = Trainer(
    model=model,
    train_dataset=train,
    eval_dataset=test,
    tokenizer=tokenizer,
    args=training_args,
    data_collator=collate_fn,  # Setting our collator (by default, this uses a
standard non-padding data collator
)
… # the rest of our training code
```

Dynamic padding is one of the simplest things we can add to most training pipelines to achieve an immediate reduction in memory usage and training time.

**Mixed-Precision Training**

Mixed-precision training is a method that can significantly enhance the efficiency of your model training process, especially when training on GPUs. GPUs, particularly the latest generations, are designed to perform certain operations faster in lower precision (i.e., 16-bit floating-point format, also known as FP16) compared to the standard 32-bit format (FP32).

The concept behind mixed-precision training is to use a mix of FP32 and FP16 to exploit the faster speed of FP16 operations while maintaining the numerical stability provided by FP32. Generally, forward and backward propagations are done in FP16 for speed, while weights are stored in FP32 to preserve precision and avoid numerical issues like underflow and overflow.

Not all operations are performed faster in FP16 on all GPUs. Given that reality, this method is particularly suited to certain GPUs that have tensor cores designed to perform these operations faster in FP16.

**Incorporating PyTorch 2.0**

A recent update of PyTorch introduced more built-in optimizations for training models and compiling them for production use. One of these optimizations is the one-line ability to compile models by calling `torch.compile(model)`. To see examples of this ability, check out the book’s code repository, which includes a definition of a separate environment for using Torch 2.0’s `compile` feature.

I didn’t include results from Torch 2.0 in this session because it’s still a bit limited in terms of the environments supported. I was running this code on my own personal Windows machine, which has multiple GPUs using Python 3.11. However, Torch 2.0’s `compile` function doesn’t work for Windows, nor does it work for Python 3.11 as yet.

**Summary of Results**

Even without Torch 2.0, we should step back and take a look at how these training pipeline changes are affecting our training times and memory usage. [Figure 8.4](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig04) shows a chart of training/memory trade-offs for these tricks when training a simple classification task using BERT (base-cased) as the foundation model.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig04.jpg" alt="A vertical bar chart represents the open-source training hyperparameters." height="529" width="776"><figcaption><p>Figure 8.4 Finding the optimal combinations of training parameters is almost never easy. It will take a few iterations and probably a few training failures to figure out what works best for your system. Note that the final set of bars represents trying four techniques at once; it produces the most dramatic reduction in speed and a decent reduction in memory used. Often, a combination of parameters will work best.</p></figcaption></figure>

Let’s talk about one more technique that is widely used to help speed up training—model freezing.

**Model Freezing**

A common approach to fine-tuning pre-trained models involves the freezing of model weights. In this process, the pre-trained model’s parameters or weights are kept constant (frozen) during training, preventing them from being updated. This is done to retain the pre-learned features that the model has gained from its previous training.

The rationale behind freezing is rooted in the way deep learning models learn representations. Lower layers (closer to the initial embeddings at the beginning) of a deep learning model typically learn general features (e.g., edges or contours in image classification tasks, or low-level word semantics in natural language processing), whereas higher layers (toward the end of the attention calculations) learn more complex, task-specific features. By freezing the weights of the lower layers, we ensure that these general features are preserved. Only the higher layers, which are responsible for task-specific features, are fine-tuned on the new task.

When using a model like BERT for a downstream task (as we are about to do), we can freeze some or all of BERT’s layers to retain the general language understanding the model has already learned. Then, we can train only the few layers that will be specialized for our task.

For instance, you might freeze all the weights up to the last three layers of BERT. Then, during the training phase of your downstream task, only the last three layers of the BERT model will be updated (and any other additional layers, such as our classification layer), while the weights of the other layers will remain the same. This technique is particularly useful if you’re dealing with a smaller dataset for your task, as it reduces the risk of overfitting. Also, it can reduce the computational requirements, making the model faster to train.

In practice, freezing layers in BERT would look like [Listing 8.4](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#list8\_4). A few options for freezing are also visualized in [Figure 8.5](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig05).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig05.jpg" alt="A diagram represents three freezing model weights." height="1128" width="765"><figcaption><p>Figure 8.5 When freezing model weights, it’s generally better to freeze lower weights near the beginning of the model, as seen here. The model shown here has only six encoding layers. Option 1 (top) doesn’t freeze anything, option 2 (middle) partially freezes some lower weights, and option 3 (bottom) freezes the entire model except for any additional layers we add.</p></figcaption></figure>

Listing 8.4 Freezing all but the last three layers + CLF layers in BERT

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08\_images.xhtml#f0185-01a)

```
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL,
    problem_type="multi_label_classification",
    num_labels=len(unique_labels)
)

# Freeze everything up until the final 3 encoder layers
for name, param in model.named_parameters():
    if 'distilbert.transformer.layer.4' in name:
        break
    param.requires_grad = False
```

I will try to train the model totally unfrozen (option 1) and with only some of the layers frozen (option 2), and summarize our results in the next section.

**Summary of Results**

Both training procedures (fine-tuning BERT with no freezing of layers and freezing everything up until the last three encoding layers) start from the same place, with the model essentially making random guesses, as indicated by the F1, ROC/AUC, accuracy, and Jaccard metrics.

However, the training trajectories begin to diverge as training progresses. By the final epoch, here is how these metrics stood:

* **Training loss:** Both models show a decline in training loss over time, indicating that the models are successfully learning and improving their fit to the training data. However, the model without any layer freezing demonstrates a marginally lower training loss (0.1147 versus 0.1452), indicating a better grasp of the training data.
* **Validation loss:** The validation loss for both models also decreases over time, suggesting an improved generalization to unseen data. The model without any layer freezing attains a marginally lower validation loss (0.1452 versus 0.1481), implying a better choice if minimizing validation loss is the goal.
* **F1 score:** The F1 score, a balanced metric of precision and recall, is higher for the model without any layer freezing (0.5380 versus 0.4886), indicating superior precision and recall for this model.
* **ROC/AUC:** The ROC/AUC also stands higher for the model without any layer freezing (0.7085 versus 0.6768), indicating an overall superior classification performance.
* **Accuracy:** The model without layer freezing also achieves a marginally higher accuracy score (0.1533 versus 0.1264), suggesting more frequent accurate predictions.
* **Jaccard score:** The Jaccard score, which measures the similarity between predicted and actual labels, is higher for the model without any layer freezing (0.3680 versus 0.3233), indicating it predicts labels more akin to the actual labels.

The unfrozen model appears to have better performance than the model in which the last three layers were frozen. It could be the case that, by allowing all layers to be fine-tuned, the model was better able to adapt to the specifics of the task. However, this might not always be the case depending on the task and the specific dataset. In some scenarios, freezing initial layers can prevent overfitting and lead to better generalization. The choice between these strategies often involves a trade-off that must be considered in the context of the specific task and data.

It’s also worth noting that while the unfrozen model performs better, it does so at the cost of more extensive computational resources and time. The partially frozen model was _30% faster_ to train than its unfrozen counterpart. Depending on the specific use-case, the trade-off between performance and computational efficiency needs to be considered. Sometimes, a slight decrease in performance might be acceptable for significant savings in computational time and resources, especially with larger datasets or more complex models. [Figure 8.6](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig06) highlights these differences.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig06.jpg" alt="A vertical bar chart represents the frozen and unfrozen models." height="490" width="775"><figcaption><p>Figure 8.6 Our unfrozen model outperforms the partially frozen model in every metric (recall that a lower loss is better). This advantage is apparent even though the partially frozen model was 30% faster to train.</p></figcaption></figure>

To use our new model, we can use the pipeline object as we have done in previous chapters. [Listing 8.5](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#list8\_5) provides the relevant code.

Listing 8.5 Using our genre predictor

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08\_images.xhtml#f0188-01a)

```
# Import necessary classes from the transformers library
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer

# Load the tokenizer associated with the model
tokenizer = AutoTokenizer.from_pretrained(MODEL)

# Load the pre-trained model for sequence classification, setting the problem type as
'multi_label_classification'.
# The '.eval()' method is used to set the model to evaluation mode.
# This deactivates the Dropout layers in the model, which randomly exclude neurons
during training to prevent overfitting.
# In evaluation mode, all neurons are used, ensuring consistent output.
trained_model = AutoModelForSequenceClassification.from_pretrained(
    f"genre-prediction", problem_type="multi_label_classification",
).eval()

# Create a pipeline for text classification. This pipeline will use the loaded model
and tokenizer.
# The parameter 'return_all_scores=True' ensures that the pipeline returns scores for
all labels, not just the highest one.
classifier = pipeline(
    "text-classification",model=trained_model, tokenizer=tokenizer,
    return_all_scores=True
)

# Use the classifier pipeline to make predictions for the given texts
prediction = classifier(texts)

# Set a threshold for label scores. Only labels with scores above this threshold will
be considered as predicted labels.
THRESHOLD = 0.5

# Filter out labels whose score is less than the threshold
prediction = [[label for label in p if label['score'] > THRESHOLD] for p in
prediction]

# Print each text, the scores of the predicted labels, and the actual labels.
# The predicted labels are sorted in descending order of score.
for _text, scores, label in zip(texts, prediction, labels):
    print(_text)
    print('------------')
    for _score in sorted(scores, key=lambda x: x['score'], reverse=True):
        print(f'{_score["label"]}: {_score["score"]*100:.2f}%')

    print('actual labels: ', label)
    print('------------')
```

Our model is generally good at getting at least a few of the correct tags, and it rarely mispredicts something severely.

#### Example: LaTeX Generation with GPT2 <a href="#ch08lev1sec3" id="ch08lev1sec3"></a>

Our first generative fine-tuning example in this chapter pertains to a translation task. When choosing the language for this experiment, I wanted to select one with which GPT-2 might not be intimately familiar. It needed to be a language that is not frequently encountered during the model’s pre-training phase, which is based on data from WebCrawl (a large corpus derived from links on Reddit). Consequently, I chose LaTeX.

**LaTeX** is a typesetting system with features designed for the production of technical and scientific documentation. LaTeX is not only a markup language but also a programming language that’s used to typeset complex mathematical formulae and manage high-quality typesetting of text. It is widely used for the communication and publication of scientific documents in many fields, including mathematics, physics, computer science, statistics, economics, and political science. I used LaTeX frequently in graduate school when I was studying theoretical mathematics.

The challenge is twofold. First, we have to get GPT-2 to understand LaTeX, which is quite different from the natural languages like English on which GPT-2 was initially trained. Second, we have to teach GPT-2 to translate text from English to LaTeX, a task that not only involves language translation but also requires an understanding of the context and semantics of the text. [Figure 8.7](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig07) outlines this task at a high level.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig07.jpg" alt="A figure shows two examples of English to LaTeX translations." height="115" width="451"><figcaption><p>Figure 8.7 Our dataset is 50 examples of English to LaTeX translation written by yours truly. With the help of GPT-2 pre-training and transfer learning, these should be enough to give GPT-2 a sense of the task.</p></figcaption></figure>

Our data? This might come as a shock, but I could not find a dataset for this specific task anywhere online. So, I took it upon myself to write 50 simple examples of English to LaTeX translation. This is by far the smallest dataset used in this book, but it will be a great aid in exploring just how much transfer learning will help us here. With only 50 examples, we will need to rely on GPT-2 recognition of a translation task and its ability to transfer that knowledge to this task.

**Prompt Engineering for Open-Source Models**

Thinking back to [Chapters 3](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03) and [5](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch05.xhtml#ch05) on prompt engineering, we need to define a prompt that we will feed into our model that clearly outlines the task and gives clear directions on what to do, just as we would for an already aligned model like ChatGPT or Cohere. [Figure 8.8](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig08) shows the final prompt I settled on, which includes a clear instruction and clear prefixes to delineate where the model is meant to read/write the response.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig08.jpg" alt="A figure illustrates the instruction and response to G P T 2." height="226" width="650"><figcaption><p>Figure 8.8 We put our prompt-engineering skills to work by defining a prompt for the LaTeX conversion task with a clear instruction and prefixes to help guide the model, and by keeping things succinct.</p></figcaption></figure>

The basic idea is to take the 50 examples of English to LaTeX translation in our engineered prompt format and let our GPT-2 model read them over and over again (multiple epochs) with the standard defined loss for autoregressive language modeling—that is, cross-entropy on next token prediction. Basically, this is a classification task in which the labels are tokens selected from the vocabulary. [Listing 8.6](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#list8\_6) shows a snippet of the code to generate our dataset.

Listing 8.6 Setting up our custom dataset for LaTeX generation

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08\_images.xhtml#f0191-01a)

```
data = pd.read_csv('../data/english_to_latex.csv')

# Add our singular prompt
CONVERSION_PROMPT = 'Convert English to LaTeX\n'
CONVERSION_TOKEN = 'LaTeX:'

# This is our "training prompt" that we want GPT-2 to recognize and learn
training_examples = f'{CONVERSION_PROMPT}English: ' + data['English'] + '\n' +
CONVERSION_TOKEN + ' ' + data['LaTeX'].astype(str)

task_df = pd.DataFrame({'text': training_examples})

# We convert our pandas DataFrame containing the LaTeX data into a Hugging Face
dataset
latex_data = Dataset.from_pandas(task_df)

def preprocess(examples):
    # Here we tokenize our text, truncating where necessary. Padding is not
performed here
    # because our collator will handle it dynamically at a later stage.
    return tokenizer(examples['text'], truncation=True)

# We apply our preprocessing function to our LaTeX dataset. The map function applies
the
# preprocessing function to all the examples in the dataset. The option batched=True
allows
# the function to operate on batches of examples for efficiency.
latex_data = latex_data.map(preprocess, batched=True)

# We split our preprocessed dataset into training and testing sets. The train_test_
split
# function randomly splits the examples, allocating 80% of them for training and the
rest for testing.
latex_data = latex_data.train_test_split(train_size=.8)
```

Once we have our dataset defined, we can define our model and our training set. Instead of the `AutoModelForSequenceClassification` class we used for genre prediction, we will instead use `AutoModelForCausalLM` to represent the new task of autoregressive language modeling. [Listing 8.7](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#list8\_7) shows how we set up our training loop.

Listing 8.7 Autoregressive language modeling with GPT-2

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08\_images.xhtml#f0192-01a)

```
# We start by converting our pandas DataFrame containing the LaTeX data into
a Hug

# DataCollatorForLanguageModeling is used to collate our examples into batches.
# This is a dynamic process that is handled during training.
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# We initialize our GPT-2 model using the pre-trained version.
latex_gpt2 = AutoModelForCausalLM.from_pretrained(MODEL)

# We define our training arguments. These include directory for output, number of
training epochs,
# batch sizes for training and evaluation, log level, evaluation strategy, and saving
strategy.
training_args = TrainingArguments(
    output_dir="./english_to_latex",
    overwrite_output_dir=True,
    num_train_epochs=5,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=20,
    load_best_model_at_end=True,
    log_level='info',
    evaluation_strategy='epoch',
    save_strategy='epoch'
)

# We initialize our Trainer, passing in the GPT-2 model, training arguments, datasets,
and data collator.
trainer = Trainer(
    model=latex_gpt2,
    args=training_args,
    train_dataset=latex_data["train"],
    eval_dataset=latex_data["test"],
    data_collator=data_collator,
)

# Finally, we evaluate our model using the test dataset.
trainer.evaluate()
```

**Summary of Results**

Our validation loss dropped by quite a lot, though our model is certainly not the greatest LaTeX converter in the world. [Listing 8.8](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#list8\_8) shows an example of using our LaTeX converter.

Listing 8.8 Autoregressive language modeling with GPT-2

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08\_images.xhtml#f0193-01a)

<pre><code>loaded_model = AutoModelForCausalLM.from_pretrained('./math_english_to_
latex')
latex_generator = pipeline('text-generation', model=loaded_model, tokenizer=tokenizer)

text_sample = 'g of x equals integral from 0 to 1 of x squared'
conversion_text_sample = f'{CONVERSION_PROMPT}English: {text_sample}\n{CONVERSION_
TOKEN}'

print(latex_generator(
    conversion_text_sample, num_beams=2, early_stopping=True, temperature=0.7,
    max_new_tokens=24
)[0]['generated_text'])
<strong>----
</strong><strong>Convert English to LaTeX
</strong><strong>English: g of x equals integral from 0 to 1 of x squared
</strong><strong>LaTeX: g(x) = \int_{0}^{1} x^2 \,dx
</strong></code></pre>

With only 50 examples of a task, GPT-2 was able to pick it up surprisingly quickly. Hmm, what if we took that concept a bit further in our final example?

#### Sinan’s Attempt at Wise Yet Engaging Responses: SAWYER <a href="#ch08lev1sec4" id="ch08lev1sec4"></a>

It’s not too far-fetched to say that a lot of this book has been leading up to this point. We know open-source models have a lot of power locked inside their pre-trained parameters but often need a bit of fine-tuning to become truly useful to us. We’ve seen how pre-trained models like GPT-2 can be adapted for various tasks and how fine-tuning can help us squeeze out additional performance from these models, just as OpenAI did when it instruction-fine-tuned the GPT-3 model in 2022 to kick off a new wave of interest in AI.

Now, it’s time for us to embark on an exciting journey of our own. We will take the once-mighty GPT-2, a model with “only” approximately 120 million parameters, and see how far we can push it. If you’re wondering why we’re focusing on GPT-2 rather than its bigger sibling GPT-3, remember that bigger isn’t always better. Plus, GPT-3 isn’t an open-source model, and working with GPT-2 allows us to get our hands dirty without getting too overwhelmed with GPUs and such.

We will attempt a feat similar to what OpenAI accomplished with GPT-3, ChatGPT, and other models. Our plan is to fine-tune GPT-2 with a specific focus on instruction, defining a reward model to simulate human feedback (giving human feedback directly can be time-consuming and impractical at scale) and using that reward model to perform reinforcement learning (RL) to guide the model to improve over time, nudging it toward generating responses that are closer to what a human would prefer.

This plan involves three steps, as shown in [Figure 8.9](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig09):

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig09.jpg" alt="A diagram illustrates a plan for SAWYER." height="537" width="775"><figcaption><p>Figure 8.9 The plan to make SAWYER a reality has three steps: (1) make GPT-2 understand the concept of answering a question, (2) define a reward model that rates human-preferred responses to questions highly, and (3) set up a reinforcement learning loop to nudge GPT-2 to give human-preferred responses.</p></figcaption></figure>

1. **Take a pre-trained GPT-2 and make it understand the concept of answering a question:** Our first goal is to ensure that the GPT-2 model has a firm grasp of the task at hand. This involves making it understand that it needs to provide responses to specific questions or prompts.
2. **Define a reward model that rates human-preferred responses to questions highly:** Once GPT-2 is clear about its task, we need to set up a system that can assess its performance. This is where the reward model comes into play. It’s designed to rate responses that align with human preferences more favorably.
3. **Implement a reinforcement learning loop to nudge GPT-2 to give human-preferred responses:** The final step is to create a feedback mechanism that helps GPT-2 improve over time. We’ll use reinforcement learning to provide this feedback. By nudging the model toward giving more human-preferred responses, we hope to continually refine and enhance GPT-2’s performance.

It’s a challenging task, no doubt, but one that’s packed with learning opportunities. By the end of this experiment, our objective is to push GPT-2’s limits and see how much it can improve given the constraints. After all, this is what data science is all about—learning, experimenting, and pushing the boundaries of what’s possible. So, let’s roll up our sleeves and get to work!

**Step 1: Supervised Instruction Fine-Tuning**

Our first step is virtually identical to that in our LaTeX example, in that we will fine-tune an open-source causal model (GPT-2, in this case) on a set of new documents. In the LaTeX example, we were fine-tuning the model to solve a particular task, and that focus doesn’t change here. The difference is that instead of defining a single task to solve (English → LaTeX, for example), we will feed GPT-2 with a corpus of general single-shot question/answer examples from a subset of the Open Instruction Generalist (OIG) dataset. OIG is a large open-source instruction dataset that currently contains approximately 43 million instructions. We will use a bit more than 100,000 of these examples. One of these examples is shown in [Figure 8.10](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig10).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig10.jpg" alt="A figure represents the question and response pairs." height="648" width="775"><figcaption><p>Figure 8.10 A sample of the more than 100,000 examples of instruction/response pairs we use to fine-tune GPT-2 to recognize the pattern of “a question comes in and a response comes out.”</p></figcaption></figure>

[Listing 8.9](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#list8\_9) has a snippet of this code. It should look very familiar because it’s similar to our LaTeX fine-tuning code

Listing 8.9 Supervised instruction fine-tuning

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08\_images.xhtml#f0196-01a)

```
from transformers import TrainingArguments, Trainer

# We initialize the TrainingArguments object provided by Hugging Face
training_args = TrainingArguments(
    output_dir="./sawyer_supervised_instruction",  # The directory where the outputs
(checkpoints, logs etc.) will be stored
    overwrite_output_dir=True,  # This flag allows overwriting the content of the
output directory if it exists (useful during development)
    num_train_epochs=1,  # Specifies the number of training epochs
    per_device_train_batch_size=2,  # Batch size for training per device
    per_device_eval_batch_size=4,  # Batch size for evaluation per device
    gradient_accumulation_steps=16,  # Number of steps for which gradients will be
accumulated before performing an update. This can be useful when dealing with memory
limitations
    load_best_model_at_end=True,  # Whether to load the best model found at each
evaluation
    evaluation_strategy='epoch',  # Defines when evaluation is carried out: after each
epoch
    save_strategy='epoch',  # Defines when checkpoints are saved: after each epoch
    report_to="all",  # Where to send the training metrics: "all" refers to all
available tracking systems (TensorBoard, WandB, etc.)
    seed=seed,  # Seed for random number generation to ensure reproducibility
    fp16=True,  # Enable mixed-precision training; beneficial for GPUs with tensor
cores like the NVIDIA Volta and newer
)

# We initialize the Trainer object provided by Hugging Face
trainer = Trainer(
    model=model,  # The model to be trained
    args=training_args,  # Training configuration
    train_dataset=chip2_dataset['train'],  # Training dataset
    eval_dataset=chip2_dataset['test'],  # Evaluation dataset
    data_collator=data_collator  # The function to be used to collate data samples
into batches during training and evaluation
)

# Evaluate the model on the evaluation dataset
trainer.evaluate()
```

Once we have a model that understands the basic task, we need to define a model that can assess its performance.

**Step 2: Reward Model Training**

Having fine-tuned a model that can grasp the basic task of processing instructions and generating responses, the next challenge is to define a model that can effectively evaluate its performance. In machine learning parlance, this is referred to as a reward model. In the following section, we will discuss the process of training such a reward model.

For this step, we will utilize a new dataset of response comparisons, in which a single query has multiple responses attached to it, all given by various LLMs. Humans then grade each response from 1 to 10, where 1 is an awful response and 10 is a spectacular response. [Figure 8.11](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig11) shows an example of one of these comparisons.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig11.jpg" alt="A screenshot represents the responses to queries given by L L Ms." height="264" width="700"><figcaption><p>Figure 8.11 Our reward data is, at its core, simple: It compares responses to queries given by LLMs to quantify how helpful LLMs are at responding to queries.</p></figcaption></figure>

With this human-labeled data, we can move on to defining a reward model architecture. The basic idea (visualized in [Figure 8.12](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig12)) is to take the human-preferred responses to questions and the nonpreferred responses, give them both to our reward model LLM (we will use BERT), and let it learn to distinguish between what is preferred and what is not preferred as a response to an instruction. Note that we are not using the same queries as we employed in fine-tuning. The idea is that if we use the same data here, the system will have seen data from only a single dataset. Our intention is to make the system more diverse in terms of data seen to promote its ability to answer unseen queries.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig12.jpg" alt="A figure represents the response taken in the reward model." height="293" width="650"><figcaption><p>Figure 8.12 Our reward model will take in responses to queries from various LLMs that were scored by humans and learn to distinguish between what is preferred and what is not preferred in a response to a query.</p></figcaption></figure>

This could be considered a simple classification task: Given two responses and a question, classify which one is preferred. However, standard classification metrics merely reward a system for picking the right choice, whereas here we are more interested in a continuous reward scale. For this reason, we will learn from OpenAI’s experience and define a custom loss function for these labeled responses.

**Defining a Custom Loss Function**

There’s often a need to develop custom loss functions when we are fine-tuning models. As a rule of thumb, the choice of loss function is determined by the problem at hand, not by the model used. It is, after all, the guiding light for the model during training. This function quantifies the difference between the model’s predictions and the actual data, steering the model’s learning toward the desired outcome. Therefore, when the task-specific nuances aren’t effectively captured by the available loss functions, creating a custom loss function becomes necessary.

The process of defining a custom loss function calls for a clear understanding of the objective of your task and the nature of your data. This requires understanding how your model learns and how its predictions can be compared to the actual targets in a meaningful and helpful way. Additionally, it’s crucial to consider the balance between complexity and interpretability of your loss function. While complex functions might capture the task’s intricacies better, they might also make training more challenging and results harder to interpret.

At a lower level, we also have to make sure that a custom loss function is differentiable—that is, it must have a derivative everywhere. This requirement arises because learning in these models is accomplished through gradient descent, which requires computing the derivative of the loss function.

For our reward model, we will define a custom loss function based on **negative log-likelihood loss**. This particular loss function is particularly relevant for tasks involving probabilities and ranking. In such cases, we’re interested in not just whether our model makes the right prediction, but also how confident it is in its predictions. Negative log-likelihood serves as a way to penalize models that are overconfident in incorrect predictions or underconfident in correct ones.

Negative log-likelihood, therefore, encapsulates the model’s confidence in its predictions, driving it to learn a more nuanced understanding of the data. It encourages the model to assign higher probabilities to preferred outcomes and lower probabilities to less preferred ones. This mechanism makes it particularly effective in training a model to rank responses or any other scenario where relative preference matters.

We will define a pairwise log-likelihood loss as visualized in [Figure 8.13](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig13). This function will take in a question and a pair of responses with scores from a human and train the model to prefer the response with the higher score.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig13.jpg" alt="A figure represents the steps of the custom loss function." height="1091" width="575"><figcaption><p>Figure 8.13 Our custom loss function is doing a lot but at its core, it takes in two responses and the score differential between them and rewards the model if the reward differential for the preferred response and the nonpreferred response is correlated to the human score differential.</p></figcaption></figure>

This function is similar to the original InstructGPT loss function defined by OpenAI in a paper from March 2022 ([https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)), but I added the step of multiplying by the square of score differential in an effort to learn more from less data. [Listing 8.10](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#list8\_10) shows the custom loss function in Python that we define for our Trainer class.

Listing 8.10 Custom reward pairwise log loss

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08\_images.xhtml#f0200-01a)

```
# We are subclassing the Hugging Face Trainer class to customize the loss
computation
class RewardTrainer(Trainer):
    # Overriding the compute_loss function to define how to compute the loss for our
specific task
    def compute_loss(self, model, inputs, return_outputs=False):
        # Calculate the reward for a preferred response y_j using the model. The input
IDs and attention masks for y_j are provided in inputs.
        rewards_j = model(input_ids=inputs["input_ids_j"], attention_
mask=inputs["attention_mask_j"])[0]

        # Similarly, calculate the reward for a less preferred response y_k.
        rewards_k = model(input_ids=inputs["input_ids_k"], attention_
mask=inputs["attention_mask_k"])[0]

        # Calculate the loss using the negative log-likelihood function.
        # We take the difference of rewards (rewards_j - rewards_k) and multiply it by
the squared score difference provided in the inputs.
        # Then, we apply the sigmoid function (via torch.nn.functional.logsigmoid) and
negate the result.
        # The mean loss is calculated across all examples in the batch.
        loss = -nn.functional.logsigmoid((rewards_j - rewards_k) * torch.pow(torch.
tensor(inputs['score_diff'], device=rewards_j.device), 2)).mean()

        # If we also want to return the outputs (rewards for y_j and y_k) along with
the loss, we do so.
        if return_outputs:
            return loss, {"rewards_j": rewards_j, "rewards_k": rewards_k}

        # Otherwise, we simply return the computed loss.
        return loss
```

The reward model’s ability to accurately assign rewards to preferred responses will be critical to the next step in reinforcement learning. At this point, we have a model that understands the concept of responding to a query and a model that knows how to reward and punish responses that are preferred and nonpreferred, respectively. We can now define our reinforcement learning loop, just as we did in [Chapter 7](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch07.xhtml#ch07).

**Step 3: Reinforcement Learning from (Estimated) Human Feedback**

We started to explore the topic of reinforcement learning from feedback in [Chapter 7](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch07.xhtml#ch07) when we attempted to have a FLAN-T5 model create more grammatically correct and neutral summaries. For our current example, we won’t diverge from that structure too much. Technically, our loop this time around is a bit simpler. Instead of combining two reward models as we did in [Chapter 7](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch07.xhtml#ch07), we’ll just use our custom reward model. [Figure 8.14](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig14) outlines the process for our reinforcement learning loop.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig14.jpg" alt="A diagram represents the loop of human-preferred responses in four steps." height="519" width="772"><figcaption><p>Figure 8.14 Our reinforcement learning loop to nudge SAWYER to have more human-preferred responses.</p></figcaption></figure>

As always, for the full code, check out the book’s code repository. Given that it is nearly identical to the RL code from [Chapter 7](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch07.xhtml#ch07), we’ll skip the repetition here.

**Summary of Results**

There’s a reason I didn’t show you the progress made by the model at every step of the way. It’s important to understand the process before examining how well each step went because in reality, before we can look at results, we need to define our pipeline. Here, I defined my process in such a way that if every individual component was performing well, it _should_ yield the result I’m after: a relatively competent instruction-fine-tuned model. [Figure 8.15](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig15) outlines quantitatively how well each component of our system was able to learn its part.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig15.jpg" alt="A figure illustrates the three steps of learning in the graphs." height="843" width="775"><figcaption><p>Figure 8.15 By the numbers, our three steps seemed to perform (relatively) as expected.</p></figcaption></figure>

In general, given our tasks, custom losses, and custom RLF loops, it _seems_ that SAWYER may be ready to answer some questions, so let’s give it some to try it out. [Figure 8.16](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig16) showcases a few runs of the model.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig16.jpg" alt="A figure illustrates the SAWYER to showcase a few runs." height="792" width="775"><figcaption><p>Figure 8.16 SAWYER is doing well. Here, I’ve asked it to write a backstory for a fictional character (top) and to rewrite the sentence “The job search was a slow and tedious process” (bottom). SAWYER (Supervised + RL) did pretty well compared to Vanilla GPT-2 and GPT-2 + Supervised but without the RL.</p></figcaption></figure>

When trying out SAWYER, it was also relatively easy to find instances where the reward model was clearly not doing as well as we’d expect. [Figure 8.17](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig17) highlights a few cases.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig17.jpg" alt="A figure illustrates the SAWYER to showcase a few cases." height="983" width="775"><figcaption><p>Figure 8.17 When I asked what the opposite of “above” is, SAWYER did get the answer right, but the more succinct answer was given a negative reward (top). When I asked what Google is (bottom), a seemingly fine answer given by the RL-less version was given a very negative reward for some reason.</p></figcaption></figure>

Is SAWYER ready to take on GPT-4? _NO._ Is SAWYER ready to be put into production as a general question-answering AI? _NO._ Is it possible to take small open-source models and be creative with what we can make them do for us? _YES._ [Figure 8.18](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08fig18) shows some notable failures of SAWYER.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/08fig18.jpg" alt="A figure illustrates the failures of SAWYER." height="987" width="775"><figcaption><p>Figure 8.18 SAWYER couldn’t tell me where Princeton University is located, even though the version without RL could (top). It also said some crazy stuff when I asked who the current chancellor of Germany is (bottom). Note that the rewards given to both of the actual correct answers were negative, which is another ding to our reward model.</p></figcaption></figure>

I’ll address two points about the “who is the current Chancellor of Germany” question. The smaller point is, did the AI get the answer ... At the time of writing, Olaf Scholz is the current Chancellor, putting the spotlight on how a knowledge cutoff presents itself in a dated LLM. To address the larger “AI is talking about Hitler” elephant in the room, I’m not totally surprised that his name came up so quickly in the model’s response. This is a glaring example of the unexpected outputs that we are warned might arise from an LLM. The underlying issue could stem from GPT-2’s pre-training data, which includes vast quantities of information scraped from various sources, including Reddit. Reddit, while being a rich and diverse source of information, also contains—to put it mildly—misleading and false information. This data could have become embedded into the model’s understanding of the world during pre-training, causing it to generate the disconcerting response.

These kinds of aberrations highlight the need for rigorous model training and validation. They underline the importance of monitoring the quality of the input data used for pre-training and the need for continuous validation and testing of the model’s output.

Wrapping up, the goal with this example was never to usurp the big dogs with our model. In all honesty, I am surprised with SAWYER’s ability to handle basic tasks despite having only approximately 120 million parameters. Color me (mostly) proud.

#### The Ever-Changing World of Fine-Tuning <a href="#ch08lev1sec5" id="ch08lev1sec5"></a>

As we continue to navigate the world of fine-tuning LLMs, remember that innovation will never stop. New fine-tuning methods continue to surface, each presenting unique opportunities to refine and optimize our models and our training pipelines.

For example, one fascinating technique that’s captured the attention of LLM engineers in recent years is **PEFT LoRA**. This method is a clever marriage of two strategies:

* **Parameter-efficient fine-tuning (PEFT)** greatly shrinks the number of adjustable parameters within an LLM by freezing the majority of pre-trained weights in place and adding only a few additional weights on the side.
* **Low-rank adaptation (LoRA)** further slims down the supplemental weights from PEFT by decomposing them into compact, lower-rank matrices.

The combined strength of PEFT and LoRA offers an impressive reduction in training time and memory requirements, allowing for more flexible and optimal LLM fine-tuning without sacrificing much (if any) performance.

This chapter is already fairly long, so we’ll save a PEFT LoRA example for the book’s GitHub repository, and maybe even its next edition. With any new technique, however, it’s essential to remember that our fundamental principles hold strong. Novel strategies usually just optimize an existing process with relatively few adjustments, making the most of what we’ve discussed in the preceding chapters. In essence, while PEFT LoRA offers a path to greater efficiency, the core tenets of fine-tuning LLMs remain largely unchanged.

#### Summary <a href="#ch08lev1sec6" id="ch08lev1sec6"></a>

We’ve examined numerous applications and modifications of open-source LLMs, dived deep into their strengths and weaknesses, and highlighted areas for potential improvement. Our discussion spanned from fine-tuning to real-world applications, showcasing the versatility and scalability of LLMs in an array of contexts.

Our focus on fine-tuning BERT for classification highlighted that even simple tasks can be greatly optimized with techniques such as freezing, gradient accumulation, and semantic downsampling. Careful balancing of these elements can lead to improved performance. The depth of control and customization available when we fine-tune these models are vast and permit us to adapt them to a wide array of tasks and domains.

Our LaTeX equation generation experiment reiterated that LLMs, when well tuned, can generate meaningful and contextually appropriate outputs, even in specialized domains like mathematical notation.

With SAWYER, we saw that even with a relatively modest parameter count of approximately 120 million, an LLM can deliver impressive results, albeit with quirks. This system’s surprising proficiency on several tasks is a testament to the vast potential of LLMs and the value of fine-tuning strategies. However, the unexpected and somewhat erroneous outputs also serve as a stark reminder of the challenges involved in refining these models and the importance of thorough validation and testing.

In essence, this chapter has been a deep dive into the intricacies of open-source LLMs, showcasing their incredible flexibility, their wide-ranging applications, and the numerous considerations that go into fine-tuning and deploying these models. The journey, though riddled with challenges, has offered immense learning opportunities, opened up avenues for improvement, and left us with an overwhelming sense of optimism about the future of LLMs. In the final chapter, we will explore how to share our great work with the world, so that it’s not just us who benefit from what we build. See you there!
