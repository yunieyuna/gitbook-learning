# Appendices: LLM Application Archetyps

### LLM Application Archetypes <a href="#appc" id="appc"></a>

In this appendix, you’ll find a comprehensive table showcasing different archetypes of LLM applications and the related factors you should consider for each. The table serves as a concise guide to the myriad ways we can apply and manipulate these models, along with their potential pitfalls and mitigation strategies.

#### Chatbots/Virtual Assistants <a href="#appclev1sec1" id="appclev1sec1"></a>

| Applications                                                                      | Data                                                | Potential Pitfalls                                                                                                       | Strategies for Implementing                                                                                                 |
| --------------------------------------------------------------------------------- | --------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------- |
| Customer service, personal assistance, entertainment, healthcare, education, etc. | Dialogue datasets, domain-specific knowledge bases. | The bot may not reflect the intended persona, risk of semantic misunderstanding, incorrect responses to complex queries. | Defining and grounding the bot’s persona during the design phase, using semantic search for accurate information retrieval. |

#### Fine-Tuning a Closed-Source LLM <a href="#appclev1sec2" id="appclev1sec2"></a>

| Applications                                                                                                  | Data                                                                                   | Potential Pitfalls                                                                                                                                            | Strategies for Implementing                                                                                                                                                                                                           |
| ------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Customization of language models for specific tasks such as text generation, summarization, translation, etc. | Domain-specific datasets, fine-tuning guidelines, and target task evaluation datasets. | Overfitting to specific data, loss of generalization ability, possibility of unexpected outputs or behaviors. Inability to inspect the underlying base model. | Careful selection of fine-tuning datasets, regular validation and testing of model outputs, applying techniques such as differential privacy to improve robustness, and adding postprocessing steps to filter out unexpected outputs. |

#### Fine-Tuning an Open-Source LLM <a href="#appclev1sec3" id="appclev1sec3"></a>

| Applications                                                                                | Data                                                       | Potential Pitfalls                                                                                 | Strategies for Implementing                                                                                                                                                                                                                          |
| ------------------------------------------------------------------------------------------- | ---------------------------------------------------------- | -------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Text classification, named entity recognition, sentiment analysis, question answering, etc. | Domain-specific datasets, target task evaluation datasets. | Overfitting on specific data, potential loss of generalization, compute resources can be limiting. | Selection of appropriate datasets, using early stopping and regularization techniques to avoid overfitting, distributed training for dealing with compute resource constraints. Experimenting with various model architectures for best performance. |

#### Fine-Tuning a Bi-encoder to Learn New Embeddings <a href="#appclev1sec4" id="appclev1sec4"></a>

| Applications                                                                               | Data                                                                           | Potential Pitfalls                                                                                                          | Strategies for Implementing                                                                                                                                                                                                     |
| ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Semantic similarity, sentence similarity, information retrieval, document clustering, etc. | Pairs or sets of texts with similarity scores or other relational information. | The embeddings might not capture the nuances of certain terms or contexts. Difficulty in tuning due to high dimensionality. | Proper choice of similarity measure (e.g., cosine similarity or Euclidean distance). Utilization of annotated datasets for specific tasks. Applying dimensionality reduction techniques to facilitate tuning and visualization. |

#### Fine-Tuning an LLM for Following Instructions Using Both LM Training and Reinforcement Learning from Human / AI Feedback (RLHF & RLAIF) <a href="#appclev1sec5" id="appclev1sec5"></a>

| Applications                                                                           | Data                                                                                                           | Potential Pitfalls                                                                                                  | Strategies for Implementing                                                                                                                                                                                    |
| -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Task-oriented dialogue systems, gaming bots, guided automation, procedural tasks, etc. | Datasets with instructions and corresponding correct actions or outcomes, human feedback on model performance. | Misinterpretation of instructions, overfitting to the training set, sparse reward signal in reinforcement learning. | Leveraging diverse training sets to capture the variety of instruction formats, fine-tuning with feedback loops to improve instruction following, devising robust reward functions for reinforcement learning. |

#### Open-Book Question-Answering <a href="#appclev1sec6" id="appclev1sec6"></a>

| Applications                                                                                                | Data                                                                                        | Potential Pitfalls                                                                                                                                                                                      | Strategies for Implementing                                                               |
| ----------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |
| <p>Question-<br>answering systems, educational tools, knowledge extraction, information retrieval, etc.</p> | Datasets containing questions, answers, and associated reference documents or “open books.” | Disconnection from the “open book” during question-answering, difficulty in aligning and integrating external knowledge with internal representations, potential for irrelevant or erroneous responses. | Grounding the model in the provided “open book,” implementing chain-of-thought prompting. |
