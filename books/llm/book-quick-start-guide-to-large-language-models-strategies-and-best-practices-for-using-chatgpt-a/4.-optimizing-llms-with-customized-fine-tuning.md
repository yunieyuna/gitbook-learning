# 4. Optimizing LLMs with Customized Fine-Tuning

### 4. Optimizing LLMs with Customized Fine-Tuning <a href="#ch04" id="ch04"></a>

#### Introduction <a href="#ch04lev1sec1" id="ch04lev1sec1"></a>

So far, we’ve exclusively used LLMs, both open- and closed-source, just as they are off the shelf. We were relying on the power of the Transformer’s attention mechanisms and their speed of computation to perform some pretty complex problems with relative ease. As you can probably guess, that isn’t always enough.

In this chapter, we will delve into the world of fine-tuning large language models (LLMs) to unlock their full potential. Fine-tuning updates off-the-shelf models and empowers them to achieve higher-quality results; it can lead to token savings, and often lower-latency requests. While GPT-like LLMs’ pre-training on extensive text data enables impressive few-shot learning capabilities, fine-tuning takes matters a step further by refining the model on a multitude of examples, resulting in superior performance across various tasks.

Running inference with fine-tuned models can be extremely cost-effective in the long run, particularly when working with smaller models. For instance, a fine-tuned Ada model from OpenAI (only 350 million parameters) costs only $0.0016 per 1000 tokens, while ChatGPT (1.5 billion parameters) costs $0.002, and DaVinci (175 billion parameters) costs $0.002. Over time, the cost of using a fine-tuned model is much more attractive, as shown in [Figure 4.1](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#ch04fig01).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/04fig01.jpg" alt="A graph of Finetuned A D A slash DaVinci slash Chat G P T prices." height="482" width="770"><figcaption><p>Figure 4.1 Assuming only 1000 classifications a day and a relatively liberal prompt ratio (150 tokens [for few-shot examples, instructions, and other items] for DaVinci or ChatGPT for every 40 tokens), the cost of a fine-tuned model, even with an up-front cost, almost always wins the day overall cost-wise. Note that this does not take into account the cost of fine-tuning a model, which we will explore later in this chapter.</p></figcaption></figure>

My goal in this chapter is to guide you through the fine-tuning process, beginning with the preparation of training data, strategies for training a new or existing fine-tuned model, and a discussion of how to incorporate your fine-tuned model into real-world applications. This is a big topic, so we will have to assume some big pieces are being handled behind the scenes, such as data labeling. Labeling data can be a huge expense in many cases of complex and specific tasks, but for now we’ll assume we can rely on the labels in our data for the most part. For more information on how to handle cases like these, feel free to check out some of my other content on feature engineering and label cleaning.

By understanding the nuances of fine-tuning and mastering its techniques, you will be well equipped to harness the power of LLMs and create tailored solutions for your specific needs.

#### Transfer Learning and Fine-Tuning: A Primer <a href="#ch04lev1sec2" id="ch04lev1sec2"></a>

Fine-tuning hinges on the idea of transfer learning. **Transfer learning** is a technique that leverages pre-trained models to build upon existing knowledge for new tasks or domains. In the case of LLMs, this involves utilizing the pre-training to transfer general language understanding, including grammar and general knowledge, to particular domain-specific tasks. However, the pre-training may not be sufficient to understand the nuances of certain closed or specialized topics, such as a company’s legal structure or guidelines.

**Fine-tuning** is a specific form of transfer learning that adjusts the parameters of a pre-trained model to better suit a “downstream” target task. Through fine-tuning, LLMs can learn from custom examples and become more effective at generating relevant and accurate responses.

**The Fine-Tuning Process Explained**

Fine-tuning a deep learning model involves updating the model’s parameters to improve its performance on a specific task or dataset.

* **Training set:** A collection of labeled examples used to train the model. The model learns to recognize patterns and relationships in the data by adjusting its parameters based on the training examples.
* **Validation set:** A separate collection of labeled examples used to evaluate the model’s performance during training.
* **Test set:** A third collection of labeled examples that is separate from both the training and validation sets. It is used to evaluate the final performance of the model after the training and fine-tuning processes are complete. The test set provides a final, unbiased estimate of the model’s ability to generalize to new, unseen data.
* **Loss function:** A function that quantifies the difference between the model’s predictions and the actual target values. It serves as a metric of error to evaluate the model’s performance and guide the optimization process. During training, the goal is to minimize the loss function to achieve better predictions.

The process of fine-tuning can be broken down into a few steps:

1. **Collecting labeled data:** The first step in fine-tuning is to gather our training, validation, and testing datasets of labeled examples relevant to the target task or domain. Labeled data serves as a guide for the model to learn the task-specific patterns and relationships. For example, if the goal is to fine-tune a model for sentiment classification (our first example), the dataset should contain text examples along with their respective sentiment labels, such as positive, negative, or neutral.
2. **Hyperparameter selection:** Fine-tuning involves adjusting hyperparameters that influence the learning process—for example, the learning rate, batch size, and number of epochs. The learning rate determines the step size of the model’s weight updates, while the batch size refers to the number of training examples used in a single update. The number of epochs denotes how many times the model will iterate over the entire training dataset. Properly setting these hyperparameters can significantly impact the model’s performance and help prevent issues such as overfitting (i.e., when a model learns the noise in the training data more than the signals) and underfitting (i.e., when a model fails to capture the underlying structure of the data).
3. **Model adaptation:** Once the labeled data and hyperparameters are set, the model may have to be adapted to the target task. This involves modifying the model’s architecture, such as adding custom layers or changing the output structure, to better suit the target task. For example, BERT’s architecture cannot perform sequence classification as is, but it can be modified very slightly to carry out this task. In our case study, we will not need to deal with that modification because OpenAI will handle it for us. We will, however, have to deal with this issue in a later chapter.
4. **Evaluation and iteration:** After the fine-tuning process is complete, we have to evaluate the model’s performance on a separate holdout validation set to ensure that it generalizes well to unseen data. Performance metrics such as accuracy, F1 score, or mean absolute error (MAE) can be used for this purpose, depending on the task. If the performance is not satisfactory, adjustments to the hyperparameters or dataset may be necessary, followed by retraining the model.
5. **Model implementation and further training:** Once the model is fine-tuned and we are happy with its performance, we need to integrate it with existing infrastructures in a way that can handle any errors and collect feedback from users. Doing so will enable us to add to our total dataset and rerun the process in the future.

This process is outlined in [Figure 4.2](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#ch04fig02). Note that the process may require several iterations and careful consideration of hyperparameters, data quality, and model architecture to achieve the desired results.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/04fig02.jpg" alt="A figure illustrates the fine hyphen tuning process." height="801" width="700"><figcaption><p>Figure 4.2 The fine-tuning process visualized. A dataset is broken up into training, validation, and testing tests. The training set is used to update the model’s weights and evaluate the model, whereas the validation set is used to evaluate the model during training. The final model is then tested against the testing set and evaluated against a set of criteria. If the model passes all of these tests, it is used in production and monitored for further iterations.</p></figcaption></figure>

**Closed-Source Pre-trained Models as a Foundation**

Pre-trained LLMs play a vital role in transfer learning and fine-tuning, providing a foundation of general language understanding and knowledge. This foundation allows for efficient adaptation of the models to specific tasks and domains, reducing the need for extensive training resources and data.

This chapter focuses on fine-tuning LLMs using OpenAI’s infrastructure, which has been specifically designed to facilitate this process. OpenAI has developed tools and resources to make it easier for researchers and developers to fine-tune smaller models, such as Ada and Babbage, for their specific needs. The infrastructure offers a streamlined approach to fine-tuning, allowing users to efficiently adapt pre-trained models to a wide variety of tasks and domains.

**Benefits of Using OpenAI’s Fine-Tuning Infrastructure**

Leveraging OpenAI’s infrastructure for fine-tuning offers several advantages:

* Access to powerful pre-trained models, such as GPT-3, which have been trained on extensive and diverse datasets
* A relatively user-friendly interface that simplifies the fine-tuning process for people with varying levels of expertise
* A range of tools and resources that help users optimize their fine-tuning process, such as guidelines for selecting hyperparameters, tips on preparing custom examples, and advice on model evaluation

This streamlined process saves time and resources while ensuring the development of high-quality models capable of generating accurate and relevant responses in a wide array of applications. We will dive deep into open-source fine-tuning and the benefits and drawbacks it offers in [Chapters 6](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06) through [9](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch09.xhtml#ch09).

#### A Look at the OpenAI Fine-Tuning API <a href="#ch04lev1sec3" id="ch04lev1sec3"></a>

The GPT-3 API offers developers access to one of the most advanced LLMs available. This API provides a range of fine-tuning capabilities, allowing users to adapt the model to specific tasks, languages, and domains. This section discusses the key features of the GPT-3 fine-tuning API, the supported methods, and best practices for successfully fine-tuning models.

**The GPT-3 Fine-Tuning API**

The GPT-3 fine-tuning API is like a treasure chest, brimming with powerful features that make customizing the model a breeze. From supporting various fine-tuning capabilities to offering a range of methods, it’s a one-stop shop for tailoring the model to your specific tasks, languages, or domains. This section aims to unravel the secrets of the GPT-3 fine-tuning API, highlighting the tools and techniques that make it such an invaluable resource.

**Case Study: Amazon Review Sentiment Classification**

Let’s introduce our first case study. We will be working with the **`amazon_reviews_multi`** dataset (previewed in [Figure 4.3](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#ch04fig03)). This dataset is a collection of product reviews from Amazon, spanning multiple product categories and languages (English, Japanese, German, French, Chinese, and Spanish). Each review in the dataset is accompanied by a rating on a scale of 1 to 5 stars, with 1 star being the lowest rating and 5 stars being the highest. Our goal in this case study is to fine-tune a pre-trained model from OpenAI to perform sentiment classification on these reviews, enabling it to predict the number of stars given in a review. Taking a page out of my own book (albeit one from just a few pages ago), let’s start looking at the data.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/04fig03.jpg" alt="A screenshot illustrates the input context and response." height="350" width="650"><figcaption><p>Figure 4.3 A snippet of the <code>amazon_reviews_multi</code> dataset shows our input context (review titles and bodies) and our response (the thing we are trying to predict—the number of stars given out by the reviewer).</p></figcaption></figure>

We will care about three columns in the dataset for this round of fine-tuning:

* `review_title`: The text title of the review
* `review_body`: The text body of the review
* `stars`: An integer between 1 and 5 indicating the number of stars

Our goal will be to use the context of the title and body of the review and predict the rating that was given.

**Guidelines and Best Practices for Data**

In general, there are a few items to consider when selecting data for fine-tuning:

* **Data quality:** Ensure that the data used for fine-tuning is of high quality, is free from noise, and accurately represents the target domain or task. This will enable the model to learn effectively from the training examples.
* **Data diversity:** Make sure the dataset is diverse, covering a broad range of scenarios to help the model generalize well across different situations.
* **Data balancing:** Maintaining a balanced distribution of examples across different tasks and domains helps prevent overfitting and biases in the model’s performance. This can be achieved with unbalanced datasets by undersampling majority classes, oversampling minority classes, or adding synthetic data. Our sentiment is perfectly balanced due to the fact that this dataset was curated—but check out an even harder example in our code base, where we attempt to classify the very unbalanced category classification task.
* **Data quantity:** Determine the total amount of data needed to fine-tune the model. Generally, larger language models like LLMs require more extensive data to capture and learn various patterns effectively, but smaller datasets if the LLM was pre-trained on similar enough data. The exact quantity of data needed can vary based on the complexity of the task at hand. Any dataset should be not only extensive, but also diverse and representative of the problem space to avoid potential biases and ensure robust performance across a wide range of inputs. While using a large quantity of training data can help to improve model performance, it also increases the computational resources required for model training and fine-tuning. This trade-off needs to be considered in the context of the specific project requirements and resources.

#### Preparing Custom Examples with the OpenAI CLI <a href="#ch04lev1sec4" id="ch04lev1sec4"></a>

Before diving into fine-tuning, we need to prepare the data by cleaning and formatting it according to the API’s requirements. This includes the following steps:

* **Removing duplicates:** To ensure the highest data quality, start by removing any duplicate reviews from the dataset. This will prevent the model from overfitting to certain examples and improve its ability to generalize to new data.
* **Splitting the data:** Divide the dataset into training, validation, and test sets, maintaining a random distribution of examples across each set. If necessary, consider using stratified sampling to ensure that each set contains a representative proportion of the different sentiment labels, thereby preserving the overall distribution of the dataset.
*   **Shuffling the training data:** Shuffling training data before fine-tuning helps to avoid biases in the learning process by ensuring that the model encounters examples in a random order, reducing the risk of learning unintended patterns based on the order of the examples. It also improves model generalization by exposing the model to a more diverse range of instances at each stage of training, which also helps to prevent overfitting, as the model is less likely to memorize the training examples and instead will focus on learning the underlying patterns. [Figure 4.4](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#ch04fig04) shows the benefits of shuffling training data. Ideally, the data will be shuffled before every single epoch to reduce the chance of the model overfitting on the data as much as possible.

    <figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/04fig04.jpg" alt="A figure shows four graphs of unshuffled training and shuffled training data." height="729" width="825"><figcaption><p>Figure 4.4 Unshuffled data makes for bad training data! It gives the model room to overfit on specific batches of data and lowers the overall quality of the responses. The top two graphs represent a model trained on unshuffled training data and the accuracy is horrible compared to a model trained on shuffled data, seen in the bottom two graphs.</p></figcaption></figure>
* **Creating the OpenAI JSONL format:** OpenAI’s API expects the training data to be in JSONL (newline-delimited JSON) format. For each example in the training and validation sets, create a JSON object with two fields: “prompt” (the input) and “completion” (the target class). The “prompt” field should contain the review text, and the “completion” field should store the corresponding sentiment label (stars). Save these JSON objects as newline-delimited records in separate files for the training and validation sets.

For completion tokens in our dataset, we should ensure a leading space appears before the class label, as this enables the model to understand that it should generate a new token. Additionally, when preparing the prompts for the fine-tuning process, there’s no need to include few-shot examples, as the model has already been fine-tuned on the task-specific data. Instead, provide a prompt that includes the review text and any necessary context, followed by a suffix (e.g., “Sentiment:” with no trailing space or “\n\n###\n\n” as in [Figure 4.5](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#ch04fig05)) that indicates the desired output format. [Figure 4.5](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#ch04fig05) shows an example of a single line of our JSONL file.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/04fig05.jpg" alt="A screenshot depicts a simple J S O N L example." height="585" width="525"><figcaption><p>Figure 4.5 A single JSONL example for our training data that we will feed to OpenAI. Every JSON has a prompt key, denoting the input to the model sans any few-shot examples, instructions, or other data, and a completion key, denoting what we want the model to output—a single classification token, in this case. In this example, the user is rating the product with one star.</p></figcaption></figure>

For our input data, I have concatenated the title and the body of the review as the singular input. This was a personal choice, reflecting my belief that the title can have more direct language to indicate general sentiment while the body likely has more nuanced language to pinpoint the exact number of stars the reviewer will give. Feel free to explore different ways of combining text fields together! We will explore this topic further in later case studies, along with other ways of formatting fields for a single text input.

[Listing 4.1](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#list4\_1) loads the Amazon Reviews dataset and converts the `train` subset into a pandas DataFrame. Then, it preprocesses the DataFrame using the custom `prepare_df_for_openai` function, which combines the review title and review body into a prompt, creates a new completion column, and filters the DataFrame to include only English-language reviews. Finally, it removes duplicate rows based on the “prompt” column and returns a DataFrame with only the “prompt” and “completion” columns.

Listing 4.1 Generating a JSONL file for our sentiment training data

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04\_images.xhtml#f0086-01a)

```
from datasets import load_dataset
import pandas as pd

# Load the Amazon Reviews Multi-Languages dataset
dataset = load_dataset("amazon_reviews_multi", "all_languages")
# Convert the 'train' subset of the dataset to a pandas DataFrame
training_df = pd.DataFrame(dataset['train'])
def prepare_df_for_openai(df):
    # Combine 'review_title' and 'review_body' columns, and add a custom suffix
'\n\n###\n\n' at the end to create the 'prompt' column
    df['prompt'] = df['review_title'] + '\n\n' + df['review_body'] + '\n\n###\n\n'
    # Create a new 'completion' column by adding a space before the 'stars' values
    df['completion'] = ' ' + df[stars]
    # Filter the DataFrame to include only rows with 'language' equal to 'en'
(English)
    english_df = df[df['language'] == 'en']
    # Remove duplicate rows based on the 'prompt' column
    english_df.drop_duplicates(subset=['prompt'], inplace=True)
    # Return the shuffled and filtered DataFrame with only the 'prompt' and
'completion' columns
    return english_df[['prompt', 'completion']].sample(len(english_df))

english_training_df = prepare_df_for_openai(training_df)
# export the prompts and completions to a JSONL file
english_training_df.to_json("amazon-english-full-train-sentiment.jsonl",
  orient='records', lines=True)
```

We would follow a similar process with the `validation` subset of the dataset and the holdout `test` subset for a final test of the fine-tuned model. A quick note: We are filtering for English only in this case, but you are free to train your model by mixing in more languages. In this case, I simply wanted to get some quick results at an efficient price.

#### Setting Up the OpenAI CLI <a href="#ch04lev1sec5" id="ch04lev1sec5"></a>

The OpenAI command line interface (CLI) simplifies the process of fine-tuning and interacting with the API. The CLI allows you to submit fine-tuning requests, monitor training progress, and manage your models, all from your command line. Ensure that you have the OpenAI CLI installed and configured with your API key before proceeding with the fine-tuning process.

To install the OpenAI CLI, you can use pip, the Python package manager. First, make sure you have Python 3.6 or later installed on your system. Then, follow these steps:

1. Open a terminal (on macOS or Linux) or a command prompt (on Windows).
2. Run the following command to install the openai package: **`pip install openai`**
   1. This command installs the OpenAI Python package, which includes the CLI.
3. To verify that the installation was successful, run the following command: **`openai --version`**
   1. This command should display the version number of the installed OpenAI CLI.

Before you can use the OpenAI CLI, you need to configure it with your API key. To do this, set the `OPENAI_API_KEY` environment variable to your API key value. You can find your API key in your OpenAI account dashboard.

**Hyperparameter Selection and Optimization**

With our JSONL document created and OpenAI CLI installed, we are ready to select our hyperparameters. Here’s a list of key hyperparameters and their definitions:

* **Learning rate:** The learning rate determines the size of the steps the model takes during optimization. A smaller learning rate leads to slower convergence but potentially better accuracy, while a larger learning rate speeds up training but may cause the model to overshoot the optimal solution.
* **Batch size:** Batch size refers to the number of training examples used in a single iteration of model updates. A larger batch size can lead to more stable gradients and faster training, while a smaller batch size may result in a more accurate model but slower convergence.
* **Training epochs:** An epoch is a complete pass through the entire training dataset. The number of training epochs determines how many times the model will iterate over the data, allowing it to learn and refine its parameters.

OpenAI has done a lot of work to find optimal settings for most cases, so we will lean on its recommendations for our first attempt. The only thing we will change is to train for one epoch instead of the default four epochs. We’re doing this because we want to see how the performance looks before investing too much time and money. Experimenting with different values and using techniques like grid search will help you find the optimal hyperparameter settings for your task and dataset, but be mindful that this process can be time-consuming and costly.

#### Our First Fine-Tuned LLM <a href="#ch04lev1sec6" id="ch04lev1sec6"></a>

Let’s kick off our first fine-tuning. [Listing 4.2](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#list4\_2) makes a call to OpenAI to train an Ada model (fastest, cheapest, weakest) for one epoch on our training and validation data.

Listing 4.2 Making our first fine-tuning call

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04\_images.xhtml#f0088-01a)

```
# Execute the 'fine_tunes.create' command using the OpenAI API
!openai api fine_tunes.create \
  # Specify the training dataset file in JSONL format
  -t "amazon-english-full-train-sentiment.jsonl" \
  # Specify the validation dataset file in JSONL format
  -v "amazon-english-full-val-sentiment.jsonl" \
  # Enable computation of classification metrics after fine-tuning
  --compute_classification_metrics \
  # Set the number of classes for classification (5 in this case)
  --classification_n_classes 5 \
  # Specify the base model to be fine-tuned (using the smallest model, ada)
  -m ada \
  # Set the number of epochs for training (1 in this case)
  --n_epochs 1
```

**Evaluating Fine-Tuned Models with Quantitative Metrics**

Measuring the performance of fine-tuned models is essential for understanding their effectiveness and identifying areas for improvement. Utilizing metrics and benchmarks, such as accuracy, F1 score, or perplexity, will provide quantitative measures of the model’s performance. In addition to quantitative metrics, qualitative evaluation techniques, such as human evaluation and analyzing example outputs, can offer valuable insights into the model’s strengths and weaknesses, helping identify areas ripe for further fine-tuning.

After one epoch (further metrics shown in [Figure 4.6](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#ch04fig06)), our classifier has over 63% accuracy on the holdout testing dataset. Recall that the testing subset was not given to OpenAI; instead, we held it out for final model comparisons.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/04fig06.jpg" alt="A figure shows two graphs." height="306" width="761"><figcaption><p>Figure 4.6 Our model is performing pretty well after only one epoch on de-duplicated shuffled training data.</p></figcaption></figure>

A 63% accuracy rate might sound low to you, but hear me out: Predicting the _exact_ number of stars is tricky because people aren’t always consistent in what they write and how they finally review the product. So, I’ll offer two more metrics:

* Relaxing our accuracy calculation to be binary (did the model predict three or fewer stars and was the review actually three or fewer stars) is equivalent to an accuracy rate of **92%**, meaning the model can distinguish between “good” and “bad.”
* Relaxing the calculation to be “one-off” so that, for example, the model predicting two stars would count as correct if the actual rating was one, two, or three stars, is equivalent to an accuracy rate of **93%**.

So you know what? Not bad. Our classifier is definitely learning the difference between good and bad. The next logical thought might be, “Let’s keep the training going!” We trained for only a single epoch, so more epochs must be better, right?

This process of taking smaller steps in training and updating already fine-tuned models for more training steps/epochs with new labeled datapoints is called **incremental learning**, also known as continuous learning or online learning. Incremental learning often results in more controlled learning, which can be ideal when working with smaller datasets or when you want to preserve some of the model’s general knowledge. Let’s try some incremental learning! We’ll take our already fine-tuned Ada model and let it run for three more epochs on the same data. The results are shown in [Figure 4.7](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#ch04fig07).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/04fig07.jpg" alt="A figure shows four graphs." height="728" width="775"><figcaption><p>Figure 4.7 The model’s performance seems to barely move during a further three epochs of incremental learning after a successful single epoch. Four times the cost for 1.02 times the performance? No, thank you.</p></figcaption></figure>

Uh oh, more epochs didn’t seem to really do anything. But nothing is set in stone until we test on our holdout `test` data subset and compare it to our first model. [Table 4.1](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#ch04tab01) shows the results.

| Quantitative Metric (on Test Set If Applicable) | 1 Epoch Sentiment Classifier: Unshuffled Data | 1 Epoch Sentiment Classifier: Shuffled Data | 4 Epochs Sentiment Classifier: Shuffled Data |
| ----------------------------------------------- | --------------------------------------------- | ------------------------------------------- | -------------------------------------------- |
| _Accuracy_                                      | 32%                                           | 63%                                         | **64%**                                      |
| _“Good” versus “bad”_                           | 70%                                           | **92%**                                     | **92%**                                      |
| _One-off accuracy_                              | 71%                                           | **93%**                                     | **93%**                                      |
| _Cost to fine-tune (overall in USD)_            | **$4.42**                                     | **$4.42**                                   | $17.68                                       |

So for 4 times the price, we get a single percentage point increase in accuracy? That’s not worth the effort in my book, but maybe it is for you. Some industries demand near-perfection in their models and single percentage points matter. I’ll leave that decision up to you, while noting that in general more epochs will not always lead to better results. Incremental/online learning can help you find the right stopping point at the cost of more up-front effort, which will be well worth it in the long run.

**Qualitative Evaluation Techniques**

When carried out alongside quantitative metrics, qualitative evaluation techniques offer valuable insights into the strengths and weaknesses of our fine-tuned model. Examining generated outputs and employing human evaluators can help identify areas where the model excels or falls short, guiding our future fine-tuning efforts.

For example, we can get the probability for our classification by looking at the probabilities of predicting the first token either in the playground (as seen in [Figure 4.8](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#ch04fig08)) or via the API’s `logprobs` value (as seen in [Listing 4.3](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#list4\_3)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/04fig08.jpg" alt="A screenshot depicts the probability of classification." height="461" width="700"><figcaption><p>Figure 4.8 The playground and the API for GPT-3-like models (including our fine-tuned Ada model, as seen in this figure) offer token probabilities that we can use to check the model’s confidence on a particular classification. Note that the main option is “ 1” with a leading space, just as in our training data, but one of the tokens on the top of the list is “1” with no leading space. These are two separate tokens according to many LLMs—which is why I am calling this distinction out so often. It can be easy to forget and mix them up.</p></figcaption></figure>

Listing 4.3 Getting token probabilities from the OpenAI API

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04\_images.xhtml#f0092-01a)

```
import math
# Select a random prompt from the test dataset

prompt = english_test_df[‘prompt’].sample(1).iloc[0]

# Generate a completion using the fine-tuned model
res = openai.Completion.create(
    model=’ada:ft-personal-2023-03-31-05-30-46’,
    prompt=prompt,
    max_tokens=1,
    temperature=0,
    logprobs=5,
)

# Initialize an empty list to store probabilities
probs = []
# Extract logprobs from the API response
logprobs = res[‘choices’][0][‘logprobs’][‘top_logprobs’]
# Convert logprobs to probabilities and store them in the ‘probs’ list
for logprob in logprobs:
    _probs = {}
    for key, value in logprob.items():
        _probs[key] = math.exp(value)
    probs.append(_probs)
# Extract the predicted category (star) from the API response
pred = res[‘choices’][0].text.strip()
# Nicely print the prompt, predicted category, and probabilities
print(“Prompt: \n”, prompt[:200], “...\n”)
print(“Predicted Star:”, pred)
print(“Probabilities:”)
for prob in probs:
    for key, value in sorted(prob.items(), key=lambda x: x[1], reverse=True):
        print(f”{key}: {value:.4f}”)
    print()
```

Output:

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04\_images.xhtml#f0092-02a)

```
Prompt:
 Great pieces of jewelry for the price

Great pieces of jewelry for the price. The 6mm is perfect for my tragus piercing. I
gave four stars because I already lost one because it fell out! Other than that I am
very happy with the purchase!



Predicted Star: 4

Probabilities:
 4: 0.9831
 5: 0.0165
 3: 0.0002
 2: 0.0001
 1: 0.0001
```

Between quantitative and qualitative measures, let’s assume we believe our model is ready to go into production—or at least a development or staging environment for further testing. Let’s take a minute to consider how we can incorporate our new model into our applications.

**Integrating Fine-Tuned GPT-3 Models into Applications**

Integrating a fine-tuned GPT-3 model into your application is identical to using a base model provided by OpenAI. The primary difference is that you’ll need to reference your fine-tuned model’s unique identifier when making API calls. Here are the key steps to follow:

1. **Identify your fine-tuned model:** After completing the fine-tuning process, you will receive a unique identifier for your fine-tuned model—something like `'ada:ft-personal-2023-03-31-05-30-46'`. Make sure to note this identifier, as it will be required for API calls.
2. **Use the OpenAI API normally:** Use your OpenAI API to make requests to your fine-tuned model. When making requests, replace the base model’s name with your fine-tuned model’s unique identifier. [Listing 4.3](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#list4\_3) offers an example of doing this.
3. **Adapt any application logic:** Since fine-tuned models may require different prompt structures or generate different output formats, you may need to update your application’s logic to handle these variations. For example, in our prompts, we concatenated the review title with the body and added a custom suffix “\n\n###\n\n”.
4. **Monitor and evaluate performance:** Continuously monitor your fine-tuned model’s performance and collect user feedback. You may need to iteratively fine-tune your model with even more data to improve its accuracy and effectiveness.

#### Case Study 2: Amazon Review Category Classification <a href="#ch04lev1sec7" id="ch04lev1sec7"></a>

Now that we have a successfully fine-tuned Ada model for a relatively simple example like sentiment classification, let’s up the stakes and tackle a more challenging task. In a second case study, we will explore how fine-tuning a GPT-3 model can improve its performance on the task of Amazon review category classification from the same dataset. This task involves classifying Amazon product reviews into their respective product categories based on the review title and body, just as we did for sentiment. We no longer have 5 classes, for example, but instead have 31 unbalanced classes ([Figure 4.9](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#ch04fig09)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/04fig09.jpg" alt="A bar chart illustrates the Product Categories Distribution." height="600" width="775"><figcaption><p>Figure 4.9 The category classification task has 31 unique categories to choose from and a very unbalanced class distribution. That’s a perfect storm that creates a difficult classification task.</p></figcaption></figure>

The much harder category classification task reveals a lot of hidden difficulties associated with machine learning, such as dealing with unbalanced data and **ill-defined data**—where the distinction between categories is subtle or ambiguous. In these cases, the model may struggle to discern the correct category. To improve performance, consider refining the problem definition, deleting redundant or confusing training examples, merging similar categories, or providing additional context to the model through prompts. You can check out all of that work in this book’s code repository.

#### Summary <a href="#ch04lev1sec8" id="ch04lev1sec8"></a>

Fine-tuning LLMs like GPT-3 is an effective way to enhance their performance on specific tasks or domains. By integrating a fine-tuned model into your application and following best practices for deployment, you can create a more efficient, accurate, and cost-effective language processing solution. Continuously monitor and evaluate your model’s performance, and iterate on its fine-tuning to ensure it meets the evolving needs of your application and users.

We will revisit the idea of fine-tuning in later chapters with some more complicated examples while also exploring the fine-tuning strategies for open-source models to achieve even further cost reductions.
