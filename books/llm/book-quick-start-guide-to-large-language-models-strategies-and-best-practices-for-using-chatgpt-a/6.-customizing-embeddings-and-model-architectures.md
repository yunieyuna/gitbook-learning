# 6. Customizing Embeddings and Model Architectures

### 6. Customizing Embeddings and Model Architectures <a href="#ch06" id="ch06"></a>

#### Introduction <a href="#ch06lev1sec1" id="ch06lev1sec1"></a>

Two full chapters of prompt engineering equipped us with the knowledge of how to effectively interact with (prompt) LLMs, acknowledging their immense potential as well as their limitations and biases. We have also fine-tuned models, both open and closed source, to expand on an LLM’s pre-training to better solve our own specific tasks. We have even seen a full case study of how semantic search and embedding spaces can help us retrieve relevant information from a dataset with speed and ease.

To further broaden our horizons, we will utilize lessons learned from earlier chapters and dive into the world of fine-tuning embedding models and customizing pre-trained LLM architectures to unlock even greater potential in our LLM implementations. By refining the very foundations of these models, we can cater to specific business use-cases and foster improved performance.

Foundation models, while impressive on their own, can be adapted and optimized to suit a variety of tasks through minor to major tweaks in their architectures. This customization enables us to address unique challenges and tailor LLMs to specific business requirements. The underlying embeddings form the basis for these customizations, as they are responsible for capturing the semantic relationships between data points and can significantly impact the success of various tasks.

Recalling our semantic search example, we identified that the original embeddings from OpenAI were designed to preserve semantic similarity, but the bi-encoder was further tuned to cater to asymmetric semantic search, matching short queries with longer passages. In this chapter, we will expand upon this concept, exploring techniques to train a bi-encoder that can effectively capture other business use-cases. By doing so, we will uncover the potential of customizing embeddings and model architectures to create even more powerful and versatile LLM applications.

#### Case Study: Building a Recommendation System <a href="#ch06lev1sec2" id="ch06lev1sec2"></a>

The majority of this chapter will explore the role of embeddings and model architectures in designing a recommendation engine while using a real-world dataset as our case study. Our objective is to highlight the importance of customizing embeddings and model architectures in achieving better performance and results tailored to specific use-cases.

**Setting Up the Problem and the Data**

To demonstrate the power of customized embeddings, we will be using the MyAnimeList 2020 dataset, which can be accessed on Kaggle. This dataset contains information about anime titles, ratings (from 1 to 10), and user preferences, offering a rich source of data to build a recommendation engine. [Figure 6.1](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06fig01) shows a snippet of the dataset on the Kaggle page.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/06fig01.jpg" alt="A figure illustrates a table of the My Anime list database." height="588" width="777"><figcaption><p>Figure 6.1 The MyAnimeList database is one of the largest datasets we have worked with to date. Found on Kaggle, it has tens of millions of rows of ratings and thousands of anime titles, including dense text features describing each anime title.</p></figcaption></figure>

To ensure a fair evaluation of our recommendation engine, we will divide the dataset into separate training and testing sets. This process allows us to train our model on one portion of the data and evaluate its performance on a separate, unseen portion, thereby providing an unbiased assessment of its effectiveness. [Listing 6.1](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#list6\_1) shows a snippet of our code to load the anime titles and split them into a train and test split.

Listing 6.1 Loading and splitting our anime data

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06\_images.xhtml#f0127-01a)

```
# Load the anime titles with genres, synopsis, producers, etc.
# There are 16,206 titles
pre_merged_anime = pd.read_csv('../data/anime/pre_merged_anime.csv')

# Load the ratings given by users who have **completed** an anime
# There are 57,633,278 ratings!
rating_complete = pd.read_csv('../data/anime/rating_complete.csv')

import numpy as np

# Split the ratings into a 90/10 train/test split
rating_complete_train, rating_complete_test = \
              np.split(rating_complete.sample(frac=1, random_state=42),
                       [int(.9*len(rating_complete))])
```

With our data loaded up and split, let’s take some time to better define what we are actually trying to solve.

**Defining the Problem of Recommendation**

Developing an effective recommendation system is, to put it mildly, a complex task. Human behavior and preferences can be intricate and difficult to predict (the understatement of the millennium). The challenge lies in understanding and predicting what users will find appealing or interesting, which is influenced by a multitude of factors.

Recommendation systems need to take into account both user features and item features to generate personalized suggestions. User features can include demographic information such as age, browsing history, and past item interactions (which will be the focus of our work in this chapter), whereas item features can encompass characteristics such as genre, price, and popularity. However, these factors alone may not paint the complete picture, as human mood and context also play a significant role in shaping preferences. For instance, a user’s interest in a particular item might change depending on their current emotional state or the time of day.

Striking the right balance between exploration and pattern exploitation is also important in recommendation systems. **Pattern exploitation** refers to a system recommending items that it is confident the user will like based on their past preferences or are just simply similar to things they have interacted with before. In contrast, we can define **exploration** to mean suggesting items that the user might not have considered before, especially if the recommendation is not exactly similar to what they have liked in the past. Striking this balance ensures that users continue to discover new content while still receiving recommendations that align with their interests. We will consider both of these factors.

Defining the problem of recommendation is a multifaceted challenge that requires considering various factors, such as user and item features, human mood, the number of recommendations to optimize, and the balance between exploration and exploitation. Given all of this, let’s dive in!

**Content Versus Collaborative Recommendations**

Recommendation engines can be broadly categorized into two main approaches: content-based and collaborative filtering. **Content-based recommendations** focus on the attributes of the items being recommended, utilizing item features to suggest similar content to users based on their past interactions. In contrast, **collaborative filtering** capitalizes on the preferences and behavior of users, generating recommendations by identifying patterns among users with similar interests or tastes.

On the one hand, in content-based recommendations, the system extracts relevant features from items, such as genre, keywords, or themes, to build a profile for each user. This profile helps the system understand the user’s preferences and suggest items with similar characteristics. For instance, if a user has previously enjoyed action-packed anime titles, the content-based recommendation engine would suggest other anime series with similar action elements.

On the other hand, collaborative filtering can be further divided into user-based and item-based approaches. User-based collaborative filtering finds users with similar preferences and recommends items that those users have liked or interacted with. Item-based collaborative filtering focuses on finding items that are similar to those the user has previously liked, based on the interactions of other users. In both cases, the underlying principle is to leverage the wisdom of the crowd to make personalized recommendations.

In our case study, we will fine-tune a bi-encoder (like the one we saw in [Chapter 2](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch02.xhtml#ch02)) to generate embeddings for anime features. Our goal is to minimize the cosine similarity loss in such a way that the similarity between embeddings reflects how common it is for users to like both animes.

In fine-tuning a bi-encoder, our goal is to create a recommendation system that can effectively identify similar anime titles based on the preferences of promoters and _not_ just because they are semantically similar. [Figure 6.2](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06fig02) shows what this approach might look like. The resulting embeddings will enable our model to make recommendations that are more likely to align with the tastes of users who are enthusiastic about the content.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/06fig02.jpg" alt="A figure illustrates the placement of embedders." height="453" width="525"><figcaption><p>Figure 6.2 Embedders are generally pre-trained to place pieces of embedded data near each other if they are semantically similar. In our case, we want an embedder that places pieces of embedded data near each other if they are similar in terms of <strong>user preferences</strong>.</p></figcaption></figure>

In terms of recommendation techniques, our approach combines elements of both content-based and collaborative recommendations. We leverage content-based aspects by using the features of each anime as input to the bi-encoder. At the same time, we incorporate collaborative filtering by considering the Jaccard score of promoters, which is based on the preferences and behavior of users. This hybrid approach allows us to take advantage of the strengths of both techniques to create a more effective recommendation system.

Explaining how we will construct this embedder, and how it will combine collaborative filtering and semantic similarity, might be helpful for envisioning the solution. In essence, we’re basing this model on the collaborative filtering as a label.

To summarize, our plan involves four steps:

1. Define/construct a series of text embedding models, either using them as is or fine-tuning them on user-preference data.
2. Define a hybrid approach of collaborative filtering (using the Jaccard score to define user/anime similarities) and content filtering (semantic similarity of anime titles by way of descriptions or other characteristics) that will influence our user-preference data structure as well as how we score recommendations given to us by the pipeline.
3. Fine-tune open-source LLMs on a training set of user-preference data.
4. Run our system on a testing set of user preference data to decide which embedder was responsible for the best anime title recommendations.

**A 10,000-Foot View of Our Recommendation System**

Our recommendation process will generate personalized anime recommendations for a given user based on their past ratings. Here’s an explanation of the steps in our recommendation engine:

1. **Input:** The input for the recommendation engine is a user ID and an integer _k_ (example 3).
2.  **Identify highly rated animes:** For each anime title that the user has rated as a 9 or 10 (a promoting score on the NPS scale), identify _k_ other relevant animes by finding nearest matches in the anime’s embedding space. From these, we consider both how often an anime was recommended and how high the resulting cosine score was in the embedding space, and take the top _k_ results for the user. [Figure 6.3](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06fig03) outlines this process. The pseudocode would look like this:

    <figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/06fig03.jpg" alt="A diagram illustrates the user-promoted anime." height="451" width="774"><figcaption><p>Figure 6.3 Step 2 takes in the user and finds k animes <strong>for each</strong> user-promoted (gave a score of 9 or 10) anime. For example, if the user promoted 4 animes (6345, 4245, 249, and 120) and we set k = 3, the system will first retrieve 12 semantically similar animes (3 per promoted animes with duplicates allowed) and then de-duplicate any animes that came up multiple times by weighing that anime slightly more than the original cosine scores. We then take the top k unique recommended anime titles considering both cosine scores for promoted animes and how often occurred in the original list of 12.</p></figcaption></figure>

    [Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06\_images.xhtml#f0130-01a)

    ```
    given: user, k=3
    promoted_animes = all anime titles that the user gave a score of 9 or a 10

    relevant_animes = []
    for each promoted_anime in promoted_animes:
        add k animes to relevant_animes with the highest cosine similarity to
    promoted_anime along with the cosine score

    # Relevant_animes should now have k * (however many animes were in promoted_
    animes)

    # Calculate a weighted score of each unique relevant anime given how many times
    it appears in the list and its similarity to promoted animes

    final_relevant_animes = the top k animes with the highest weighted cosine/occur-
    rence score
    ```

    GitHub has the full code to run this step—with examples, too. For example, given _k_ = 3 and user ID `205282`, step 2 would result in the following dictionary, where each key represents a different embedding model used and the values are anime title IDs and corresponding cosine similarity scores to promoted titles the user liked:

    [Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06\_images.xhtml#f0130-02a)

    ```
    final_relevant_animes = {
      'text-embedding-ada-002': { '6351': 0.921, '1723': 0.908, '2167': 0.905 },
      'paraphrase-distilroberta-base-v1': { '17835': 0.594, '33970': 0.589,  '1723':
    0.586 }
    }
    ```
3. **Score relevant animes:** For each of the relevant animes identified in step 2, if the anime is not present in the testing set for that user, ignore it. If we have a user rating for the anime in the testing set, we assign a score to the recommended anime given the NPS-inspired rules:
   * If the rating in the testing set for the user and the recommended anime was 9 or 10, the anime is considered a “promoter” and the system receives +1 points.
   * If the rating is 7 or 8, the anime is considered “passive” and receives 0 points.
   * If the rating is between 1 and 6, the anime is considered a “detractor” and receives –1 point.

The final output of this recommendation engine is a ranked list of the top _N_ (depending on how many we wish to show the user) animes that are most likely to be enjoyed by the user and a score of how well the system did given a testing ground truth set. [Figure 6.4](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06fig04) shows this entire process at a high level.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/06fig04.jpg" alt="A diagram illustrates finding the top K relevant recommendations given a user I D." height="807" width="768"><figcaption><p>Figure 6.4 The overall recommendation process involves using an embedder to retrieve similar animes from a user’s already promoted titles. It then assigns a score to the recommendations given if they were present in the testing set of ratings.</p></figcaption></figure>

**Generating a Custom Description Field to Compare Items**

To compare different anime titles and generate recommendations more effectively, we will create our own custom generated description field that incorporates several relevant features from the dataset (shown in [Figure 6.5](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06fig05)). This approach offers several advantages and enables us to capture a more comprehensive context of each anime title, resulting in a richer and more nuanced representation of the content.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/06fig05.jpg" alt="A screenshot represents the custom-generated description of each anime." height="712" width="525"><figcaption><p>Figure 6.5 Our custom-generated description of each anime combines many raw features, including the title, genre list, synopsis, producers, and more. This approach can be contrary to how many developers think because instead of generating a structured, tabular dataset, we are deliberately creating natural text representation of our anime titles, which we will let our LLM-based embedders capture in a vector (tabular) form.</p></figcaption></figure>

By combining multiple features, such as plot summaries, character descriptions, and genres, we can create a multidimensional representation of each anime title that allows our model to consider a broader range of information when comparing titles and identifying similarities, leading to more accurate and meaningful recommendations. Incorporating various features from the dataset into a single description field can also aid in overcoming potential limitations in the dataset, such as missing or incomplete data. By leveraging the collective strength of multiple features, we ensure that our model has access to a more robust and diverse set of information and mitigates the effect of individual titles missing pieces of information.

In addition, using a custom-generated description field enables our model to adapt to different user preferences more effectively. Some users may prioritize plot elements, whereas others may be more interested in certain genres or media (TV series versus movies). By capturing a wide range of features in our description field, we can cater to a diverse set of user preferences and deliver personalized recommendations that align with users’ individual tastes.

Overall, this approach of creating our own custom description field from several individual fields ultimately should result in a recommendation engine that delivers more accurate and relevant content suggestions. [Listing 6.2](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#list6\_2) provides a snippet of the code used to generate these descriptions.

Listing 6.2 Generating custom descriptions from multiple anime fields

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06\_images.xhtml#f0134-01a)

```
def clean_text(text):
    # Remove nonprintable characters
    text = ''.join(filter(lambda x: x in string.printable, text))
    # Replace multiple whitespace characters with a single space
    text = re.sub(r'\s{2,}', ' ', text).strip()
    return text.strip()

def get_anime_description(anime_row):
    """
    Generates a custom description for an anime title based on various features from
the input data.

    :param anime_row: A row from the MyAnimeList dataset containing relevant anime
information.
    :return: A formatted string containing a custom description of the anime.
    """

...
    description = (
        f"{anime_row['Name']} is a {anime_type}.\n"
... #  Note that I omitted over a dozen other rows here for brevity
        f"Its genres are {anime_row['Genres']}\n"
    )
    return clean_text(description)

# Create a new column in our merged anime dataframe for our new descriptions
pre_merged_anime['generated_description'] = pre_merged_anime.apply(get_anime_
  description, axis=1)
```

**Setting a Baseline with Foundation Embedders**

Before customizing our embeddings, we will establish a baseline performance using two foundation embedders: OpenAI’s powerful Ada-002 embedder and a small open-source bi-encoder based on a distilled RoBERTa model. These pre-trained models offer a starting point for comparison, helping us to quantify the improvements achieved through customization. We will start with these two models and eventually work our way up to comparing four different embedders: one closed-source embedder and three open-source embedders.

**Preparing Our Fine-Tuning Data**

As part of our quest to create a robust recommendation engine, we will fine-tune open-source embedders using the Sentence Transformers library. We will begin by calculating the Jaccard similarity between promoted animes from the training set.

**Jaccard similarity** is a simple method to measure the similarity between two sets of data based on the number of elements they share. It is calculated by dividing the number of elements that both groups have in common by the total number of distinct elements in both groups combined.

Let’s say we have two anime shows, Anime A and Anime B. Suppose we have the following people who like these shows:

* People who like Anime A: Alice, Bob, Carol, David
* People who like Anime B: Bob, Carol, Ethan, Frank

To calculate the Jaccard similarity, we first find the people who like both Anime A and Anime B. In this case, it’s Bob and Carol.

Next, we find the total number of distinct people who like either Anime A or Anime B. Here, we have Alice, Bob, Carol, David, Ethan, and Frank.

Now, we can calculate the Jaccard similarity by dividing the number of common elements (2, as Bob and Carol like both shows) by the total number of distinct elements (6, as there are 6 unique people in total):

Jaccard similarity(Anime A, Anime B) = 2/6 = 1/3 ≈ 0.33

So, the Jaccard similarity between Anime A and Anime B, based on the people who like them, is about 0.33 or 33%. In other words, 33% of the distinct people who like either show have similar tastes in anime, as they enjoy both Anime A and Anime B. [Figure 6.6](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06fig06) shows another example.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/06fig06.jpg" alt="A diagram illustrates the conversion of raw data to a pair of animes." height="633" width="413"><figcaption><p>Figure 6.6 To convert our raw ratings into pairs of animes with associated scores, we will consider every pair of anime titles and compute the Jaccard similarity score between promoting users.</p></figcaption></figure>

We will apply this logic to calculate the Jaccard similarity for every pair of animes using a training set of the ratings DataFrame. We will keep only scores above a certain threshold as “positive examples” (label of 1); the rest will be considered “negative” (label of 0).

Important note: We are free to assign any anime pairs a label between –1 and 1—but I’m using only 0 and 1 here because I’m just using _promoting_ scores to create my data. In this case, it’s not fair to say that if the Jaccard score between animes is low, then the users totally disagree on the anime. That’s not necessarily true! If I expanded this case study, I would want to explicitly label animes as –1 if and only if users were genuinely rating them in an opposite manner (i.e., if most users who promote one anime are detractors of the other).

Once we have Jaccard scores for the anime IDs, we need to convert them into tuples of anime descriptions and the cosine label (in our case, either 0 or 1). Then we can update our open-source embedders and experiment with different token windows (shown in [Figure 6.7](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06fig07)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/06fig07.jpg" alt="A diagram illustrates the conversion of the Jaccard Score to cosine labels." height="1027" width="500"><figcaption><p>Figure 6.7 Jaccard scores are converted into cosine labels and then fed into our bi-encoder, enabling the bi-encoder to attempt to learn patterns between the generated anime descriptions and how users co-like the titles.</p></figcaption></figure>

Once we have Jaccard similarities between anime pairs, we can convert these scores to labels for our bi-encoder by applying a simple rule. In our case, if the score is greater than 0.3, then we label the pair as “positive” (label 1), and if the label is less than 0.1, we label it as “negative” (label 0).

**Adjusting Model Architectures**

When working with open-source embedders, we have much more flexibility to change things around if necessary. For example, the open-source model we’ll use in this case study was pre-trained with the ability to take in only 128 tokens at a time and truncate anything longer than that. [Figure 6.8](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06fig08) shows the histogram of the token lengths for our generated anime descriptions. Clearly, we have many descriptions that are more than 128 tokens—some in the 600-token range!

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/06fig08.jpg" alt="A histogram represents the token length." height="490" width="775"><figcaption><p>Figure 6.8 We have several animes that, after tokenizing, are hundreds of tokens long. Some have more than 600 tokens.</p></figcaption></figure>

In [Listing 6.3](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#list6\_3), we change the input sequence length to be 384 instead of 128.

Listing 6.3 Modifying an open-source bi-encoder’s max sequence length

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06\_images.xhtml#f0138-01a)

```
from sentence_transformers import SentenceTransformer

# Load a pre-trained SBERT model
model = SentenceTransformer('paraphrase-distilroberta-base-v1')
model.max_seq_length = 384     # Truncate long documents to 384 tokens
model
```

Why 384?

* The histogram of token lengths ([Figure 6.8](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06fig08)) shows that 384 would capture most of our animes in their entirety and would truncate the rest.
* 384 = 256 + 128, the sum of two binary numbers, and we like binary numbers. Modern hardware components, especially graphics processing units (GPUs), are designed to perform optimally with binary numbers so they can split up workloads evenly.
* Why not 512, then, to capture more training data? We still want to be conservative here. The more we increase the maximum token window size, the more data we will need to train the system, because we are adding parameters to our model and therefore there is more to learn. It will also take more time and compute resources to load, run, and update the larger model.
* For what it’s worth, I did initially try this process with an embedding size of 512. I got worse results and the process took approximately 20% longer on my machine.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/06fig09.jpg" alt="A graph represents the performance of embedders and k values." height="415" width="771"><figcaption><p>Figure 6.9 Our larger open-source model (<code>anime_encoder_bigger</code>) consistently outperforms OpenAI’s embedder in recommending anime titles to our users based on historical preferences.</p></figcaption></figure>

To be explicit, whenever we alter an original pre-trained foundation model in any capacity, the model must learn something from scratch. In this case, the model will learn, from scratch, how text longer than 128 tokens can be formatted and how to assign attention scores across a longer text span. It can be difficult to make these model architecture adjustments, but it is often well worth the effort in terms of performance. In our case, changing the maximum input length to 384 is only the starting line because this model now has to learn about text longer than 128 tokens.

With modified bi-encoder architectures, data prepped and ready to go, we are ready to fine-tune!

**Fine-Tuning Open-Source Embedders Using Sentence Transformers**

It’s time to fine-tune our open-source embedders using Sentence Transformers. A reminder: Sentence Transformers is a library built on top of the Hugging Face Transformers library.

First, we create a custom training loop using the Sentence Transformers library shown in [Listing 6.4](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#list6\_4). We use the provided training and evaluation functionalities of the library, such as the `fit()` method for training and the `evaluate()` method for validation.

Listing 6.4 Fine-tuning a bi-encoder

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06\_images.xhtml#f0139-01a)

<pre><code># Create a DataLoader for the examples
train_dataloader = DataLoader(
    train_examples,
    batch_size=16,
    shuffle=True
)

...

# Create a DataLoader for the validation examples
val_dataloader = DataLoader(
    all_examples_val,
    batch_size=16,
    shuffle=True
)

# Use the CosineSimilarityLoss from Sentence Transformers
loss = losses.CosineSimilarityLoss(model=model)

# Set the number of epochs for training
num_epochs = 5

# Calculate warmup steps using 10% of the training data
warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)

# Create the evaluator using validation data
evaluator = evaluation.EmbeddingSimilarityEvaluator(
    val_sentences1,  # List of first anime descriptions in each pair from
validation data
    val_sentences2,  # List of second anime descriptions in each pair from
validation data
    val_scores       # List of corresponding cosine similarity labels for validation
data
)

# Get initial metrics
<strong>model.evaluate(evaluator)  # Initial embedding similarity score: 0.0202
</strong>
# Configure the training process
model.fit(
    # Set the training objective with the train dataloader and loss function
    train_objectives=[(train_dataloader, loss)],
    epochs=num_epochs,  # Set the number of epochs
    warmup_steps=warmup_steps,  # Set the warmup steps
    evaluator=evaluator,  # Set the evaluator for validation during training
    output_path="anime_encoder"  # Set the output path for saving the fine-tuned model
)

# Get final metrics
<strong>model.evaluate(evaluator)  # Final embedding similarity score:   0.8628
</strong></code></pre>

Before we begin the fine-tuning process, we need to decide on several hyperparameters, such as learning rate, batch size, and number of training epochs. I have experimented with various hyperparameter settings to find a good combination that leads to optimal model performance. I will dedicate all of [Chapter 8](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08) to discussing dozens of open-source fine-tuning hyperparameters—so if you are looking for a deeper discussion of how I came to these numbers, please refer to [Chapter 8](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch08.xhtml#ch08).

We gauge how well the model learned by checking the change in the cosine similarity. It jumped up to the high 0.8 and 0.9s! That’s great.

With our fine-tuned bi-encoder, we can generate embeddings for new anime descriptions and compare them with the embeddings of our existing anime database. By calculating the cosine similarity between the embeddings, we can recommend animes that are most similar to the user’s preferences.

Once we go through the process of fine-tuning a single custom embedder using our user preference data, we can then relatively easily swap out different models with similar architectures and run the same code, rapidly expanding our universe of embedder options. For this case study, I also fine-tuned another LLM called `all-mpnet-base-v2`, which (at the time of writing) is regarded as a very good open-source embedder for semantic search and clustering purposes. It is a bi-encoder as well, so we can simply swap out references to our RoBERTa model with mpnet and change virtually no code (see GitHub for the complete case study).

**Summary of Results**

In the course of this case study, we performed the following tasks:

* Generated a custom anime description field using several raw fields from the original dataset
* Created training data for a bi-encoder from users’ anime ratings using a combination of NPS/Jaccard scoring and our generated descriptions
* Modified an open-source architecture model to accept a larger token window to account for our longer description field
* Fine-tuned two bi-encoders with our training data to create a model that mapped our descriptions to an embedding space more aligned to our users’ preferences
* Defined an evaluation system using NPS scoring to reward a promoted recommendation (i.e., users giving an anime a score of 9 or 10 in the testing set) and punishing detracted titles (i.e., users giving it a 1–6 score in the testing set)

We had four candidates for our embedders:

* **`text-embedding-002`:** OpenAI’s recommended embedder for all use-cases, mostly optimized for semantic similarity
* **`paraphrase-distilroberta-base-v1`:** An open-source model pre-trained to summarize short pieces of text with no fine-tuning
* **`anime_encoder`:** The same `paraphrase-distilroberta-base-v1` model with a modified 384-token window and fine-tuned on our user preference data
* **`anime_encoder_bigger`:** A larger open-source model (`all-mpnet-base-v2`) pre-trained with a token window size of 512, which I further fine-tuned on our user preference data, in the same way and using the same data as for `anime_encoder`

[Figure 6.9](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06fig09) shows the final results for our four embedder candidates across lengthening recommendation windows (how many recommendations we show the user).

Each tick on the _x_-axis in [Figure 6.9](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06fig09) represents showing the user a list of that many anime titles. The _y_-axis is the aggregated score for the embedder using the scoring system outlined earlier, where we also further reward the model if a correct recommendation is placed closer to the front of the list and punish it if something that the user is a detractor for is placed closer to the beginning of the list.

Some interesting takeaways:

* The best-performing model is our larger fine-tuned model. It consistently outperforms OpenAI’s embedder in delivering recommendations to users that they would have loved!
* The fine-tuned `distilroberta` model (`anime_encoder`) has poorer performance than its pre-trained cousin (base `distilroberta` with no fine-tuning), which can take in only 128 tokens at a time. This outcome most likely occurs because:
  * The model doesn’t have enough parameters in its attention layers to capture the recommendation problem well, and its non-fine-tuned cousin is simply relying on recommending semantically similar titles.
  * The model might require more than 384 tokens to capture all possible relationships.
* All models start to degrade in performance when expected to recommend more and more titles, which is fair. The more titles any model recommends, the less confident it will be as it goes down the list.

**Exploring Exploration**

Earlier I mentioned that a recommendation system’s level of “exploration” can be defined as how often it recommends something that the user may not have watched yet. We didn’t take any explicit measures to encourage exploration in our embedders, but it is still worth seeing how they stack up. [Figure 6.10](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch06.xhtml#ch06fig10) shows a graph of the raw number of animes recommended to all of the users in our test dataset.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/06fig10.jpg" alt="A chart illustrates the unique animes recommended during the course of the testing process." height="502" width="776"><figcaption><p>Figure 6.10 Comparing how many unique animes were recommended during the course of the testing process.</p></figcaption></figure>

OpenAI’s Ada and our bigger encoder produced more recommendations than the two other options, but OpenAI clearly seems to be in the lead in terms of the diversity of unique animes recommended. This could be a sign (not proof) that our users are not especially explorative and tend to gravitate toward the same animes, and that our fine-tuned bi-encoder is picking up on this behavior and delivering fewer unique results. It could also be that the OpenAI Ada embedder was trained on such a diverse set of data and is so large in terms of parameters that it is simply better than our fine-tuned model at delivering consistently favored animes at scale.

To answer these questions and more, we would want to continue our research. For example, we could:

* Try new open-source models and closed-source models.
* Design new metrics for quality assurance to test our embedders on a more holistic scale.
* Calculate new training datasets that use other metrics like correlation coefficients instead of Jaccard similarity scores.
* Toggle the recommendation system hyperparameters, such as _k_. We only considered grabbing the first _k_ = 3 animes for each promoted anime—what if we let that number vary as well?
* Run some pre-training on blogs and wikis about anime recommendations and theory so the model has some latent access to information about how to consider recommendations.

That last idea is a bit “pie in the sky” and would work best if we could also combine it with some chain-of-thought prompting on a different LLM. Even so, this is a big question, and sometimes that means we need big ideas and big answers. So I leave it to you now—go have big ideas!

#### Summary <a href="#ch06lev1sec3" id="ch06lev1sec3"></a>

This chapter walked through the process of fine-tuning open-source embedding models for a specific use-case—generating high-quality anime recommendations based on users’ historical preferences. Comparing the performance of our customized models with that of OpenAI’s embedder, we observed that a fine-tuned model could consistently outperform OpenAI’s embedder.

Customizing embedding models and their architectures for specialized tasks can lead to improved performance and provide a viable alternative to closed-source models, especially when access to labeled data and resources for experimentation is available. I hope that the success of our fine-tuned model in recommending anime titles serves as a testament to the power and flexibility that open-source models offer, paving the way for further exploration, experimentation, and application in whatever tasks you might have.
