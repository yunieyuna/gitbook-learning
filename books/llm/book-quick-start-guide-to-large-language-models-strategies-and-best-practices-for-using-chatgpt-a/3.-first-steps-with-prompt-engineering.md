# 3. First Steps with Prompt Engineering

### 3. First Steps with Prompt Engineering <a href="#ch03" id="ch03"></a>

#### Introduction <a href="#ch03lev1sec1" id="ch03lev1sec1"></a>

In [Chapter 2](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch02.xhtml#ch02), we built an asymmetric semantic search system that leveraged the power of large language models (LLMs) to quickly and efficiently find relevant documents based on natural language queries using LLM-based embedding engines. The system was able to understand the meaning behind the queries and retrieve accurate results, thanks to the pre-training of the LLMs on vast amounts of text.

However, building an effective LLM-based application can require more than just plugging in a pre-trained model and retrieving results—what if we want to parse them for a better user experience? We might also want to lean on the learnings of massively large language models to help complete the loop and create a useful end-to-end LLM-based application. This is where prompt engineering comes into the picture.

#### Prompt Engineering <a href="#ch03lev1sec2" id="ch03lev1sec2"></a>

**Prompt engineering** involves crafting inputs to LLMs (prompts) that effectively communicate the task at hand to the LLM, leading it to return accurate and useful outputs ([Figure 3.1](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig01)). Prompt engineering is a skill that requires an understanding of the nuances of language, the specific domain being worked on, and the capabilities and limitations of the LLM being used.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/03fig01.jpg" alt="A figure illustrates the construction of inputs to L L M&#x27;s." height="407" width="575"><figcaption><p>Figure 3.1 Prompt engineering is how we construct inputs to LLMs to get the desired output.</p></figcaption></figure>

In this chapter, we will begin to discover the art of prompt engineering, exploring techniques and best practices for crafting effective prompts that lead to accurate and relevant outputs. We will cover topics such as structuring prompts for different types of tasks, fine-tuning models for specific domains, and evaluating the quality of LLM outputs. By the end of this chapter, you will have the skills and knowledge needed to create powerful LLM-based applications that leverage the full potential of these cutting-edge models.

**Alignment in Language Models**

To understand why prompt engineering is crucial to LLM-application development, we first have to understand not only how LLMs are trained, but how they are aligned to human input. **Alignment** in language models refers to how the model understands and responds to input prompts that are “in line with” (at least according to the people in charge of aligning the LLM) what the user expected. In standard language modeling, a model is trained to predict the next word or sequence of words based on the context of the preceding words. However, this approach alone does not allow for specific instructions or prompts to be answered by the model, which can limit its usefulness for certain applications.

Prompt engineering can be challenging if the language model has not been aligned with the prompts, as it may generate irrelevant or incorrect responses. However, some language models have been developed with extra alignment features, such as Constitutional AI-driven Reinforcement Learning from AI Feedback (RLAIF) from Anthropic or Reinforcement Learning from Human Feedback (RLHF) in OpenAI’s GPT series, which can incorporate explicit instructions and feedback into the model’s training. These alignment techniques can improve the model’s ability to understand and respond to specific prompts, making them more useful for applications such as question-answering or language translation ([Figure 3.2](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig02)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/03fig02.jpg" alt="A figure illustrates the alignment in G P T - 3." height="440" width="650"><figcaption><p>Figure 3.2 Even modern LLMs like GPT-3 need alignment to behave how we want them to. The original GPT-3 model, which was released in 2020, is a pure autoregressive language model; it tries to “complete the thought” and gives misinformation quite freely. In January 2022, GPT-3’s first aligned version was released (InstructGPT) and was able to answer questions in a more succinct and accurate manner.</p></figcaption></figure>

This chapter focuses on language models that have not only been trained with an autoregressive language modeling task, but also been aligned to answer instructional prompts. These models have been developed with the goal of improving their ability to understand and respond to specific instructions or tasks. They include GPT-3 and ChatGPT (closed-source models from OpenAI), FLAN-T5 (an open-source model from Google), and Cohere’s command series (another closed-source model), which have been trained using large amounts of data and techniques such as transfer learning and fine-tuning to be more effective at generating responses to instructional prompts. Through this exploration, we will see the beginnings of fully working NLP products and features that utilize these models, and gain a deeper understanding of how to leverage aligned language models’ full capabilities.

**Just Ask**

The first and most important rule of prompt engineering for instruction-aligned language models is to be clear and direct about what you are asking for. When we give an LLM a task to complete, we want to ensure that we are communicating that task as clearly as possible. This is especially true for simple tasks that are straightforward for the LLM to accomplish.

In the case of asking GPT-3 to correct the grammar of a sentence, a direct instruction of “Correct the grammar of this sentence” is all you need to get a clear and accurate response. The prompt should also clearly indicate the phrase to be corrected ([Figure 3.3](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig03)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/03fig03.jpg" alt="A screenshot shows a direct question and L L M&#x27;s response to the question." height="210" width="700"><figcaption><p>Figure 3.3 The best way to get started with an LLM aligned to answer queries from humans is to simply ask.</p></figcaption></figure>

Note

Many figures in this chapter are screenshots of an LLM’s playground. Experimenting with prompt formats in the playground or via an online interface can help identify effective approaches, which can then be tested more rigorously using larger data batches and the code/API for optimal output.

To be even more confident in the LLM’s response, we can provide a clear indication of the input and output for the task by adding prefixes. Let’s consider another simple example—asking GPT-3 to translate a sentence from English to Turkish.

A simple “just ask” prompt will consist of three elements:

* A direct instruction: “Translate from English to Turkish.” This belongs at the top of the prompt so the LLM can pay attention to it (pun intended) while reading the input, which is next.
* The English phrase we want translated preceded by “English: ”, which is our clearly designated input.
* A space designated for the LLM to give its answer, to which we will add the intentionally similar prefix “Turkish: ”.

These three elements are all part of a direct set of instructions with an organized answer area. If we give GPT-3 this clearly constructed prompt, it will be able to recognize the task being asked of it and fill in the answer correctly ([Figure 3.4](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig04)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/03fig04.jpg" alt="A screenshot shows a jusk ask prompt." height="195" width="775"><figcaption><p>Figure 3.4 This more fleshed-out version of our “just ask” prompt has three components: a clear and concise set of instructions, our input prefixed by an explanatory label, and a prefix for our output followed by a colon and no further whitespace.</p></figcaption></figure>

We can expand on this even further by asking GPT-3 to output multiple options for our corrected grammar, with the results being formatted as a numbered list ([Figure 3.5](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig05)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/03fig05.jpg" alt="A screenshot shows the input and output in G P T 3." height="321" width="650"><figcaption><p>Figure 3.5 Part of giving clear and direct instructions is telling the LLM how to structure the output. In this example, we ask GPT-3 to give grammatically correct versions as a numbered list.</p></figcaption></figure>

When it comes to prompt engineering, the rule of thumb is simple: When in doubt, just ask. Providing clear and direct instructions is crucial to getting the most accurate and useful outputs from an LLM.

**Few-Shot Learning**

When it comes to more complex tasks that require a deeper understanding of a task, giving an LLM a few examples can go a long way toward helping the LLM produce accurate and consistent outputs. Few-shot learning is a powerful technique that involves providing an LLM with a few examples of a task to help it understand the context and nuances of the problem.

Few-shot learning has been a major focus of research in the field of LLMs. The creators of GPT-3 even recognized the potential of this technique, which is evident from the fact that the original GPT-3 research paper was titled “Language Models Are Few-Shot Learners.”

Few-shot learning is particularly useful for tasks that require a certain tone, syntax, or style, and for fields where the language used is specific to a particular domain. [Figure 3.6](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig06) shows an example of asking GPT-3 to classify a review as being subjective or not; basically, this is a binary classification task. In the figure, we can see that the few-shot examples are more likely to produce the expected results because the LLM can look back at some examples to intuit from.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/03fig06.jpg" alt="A figure illustrates the simple binary classification." height="467" width="775"><figcaption><p>Figure 3.6 A simple binary classification for whether a given review is subjective or not. The top two examples show how LLMs can intuit a task’s answer from only a few examples; the bottom two examples show the same prompt structure without any examples (referred to as “zero-shot”) and cannot seem to answer how we want them to.</p></figcaption></figure>

Few-shot learning opens up new possibilities for how we can interact with LLMs. With this technique, we can provide an LLM with an understanding of a task without explicitly providing instructions, making it more intuitive and user-friendly. This breakthrough capability has paved the way for the development of a wide range of LLM-based applications, from chatbots to language translation tools.

**Output Structuring**

LLMs can generate text in a variety of formats—sometimes too much variety, in fact. It can be helpful to structure the output in a specific way to make it easier to work with and integrate into other systems. We saw this kind of structuring at work earlier in this chapter when we asked GPT-3 to give us an answer in a numbered list. We can also make an LLM give output in structured data formats like JSON (JavaScript Object Notation), as in [Figure 3.7](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig07).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/03fig07.jpg" alt="A screenshot depicts the response of G P T 3." height="554" width="775"><figcaption><p>Figure 3.7 Simply asking GPT-3 to give a response back as a JSON (top) does generate a valid JSON, but the keys are also in Turkish, which may not be what we want. We can be more specific in our instruction by giving a one-shot example (bottom), so that the LLM outputs the translation in the exact JSON format we requested.</p></figcaption></figure>

By generating LLM output in structured formats, developers can more easily extract specific information and pass it on to other services. Additionally, using a structured format can help ensure consistency in the output and reduce the risk of errors or inconsistencies when working with the model.

**Prompting Personas**

Specific word choices in our prompts can greatly influence the output of the model. Even small changes to the prompt can lead to vastly different results. For example, adding or removing a single word can cause the LLM to shift its focus or change its interpretation of the task. In some cases, this may result in incorrect or irrelevant responses; in other cases, it may produce the exact output desired.

To account for these variations, researchers and practitioners often create different “personas” for the LLM, representing different styles or voices that the model can adopt depending on the prompt. These personas can be based on specific topics, genres, or even fictional characters, and are designed to elicit specific types of responses from the LLM ([Figure 3.8](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig08)). By taking advantage of personas, LLM developers can better control the output of the model and end users of the system can get a more unique and tailored experience.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/03fig08.jpg" alt="A screenshot depicts the response of G P T 3." height="812" width="775"><figcaption><p>Figure 3.8 Starting from the top left and moving down, we see a baseline prompt of asking GPT-3 to respond as a store attendant. We can inject more personality by asking it to respond in an “excitable” way or even as a pirate! We can also abuse this system by asking the LLM to respond in a rude manner or even horribly as an anti-Semite. Any developer who wants to use an LLM should be aware that these kinds of outputs are possible, whether intentional or not. In <a href="https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch05.xhtml#ch05">Chapter 5</a>, we will explore advanced output validation techniques that can help mitigate this behavior.</p></figcaption></figure>

Personas may not always be used for positive purposes. Just as with any tool or technology, some people may use LLMs to evoke harmful messages, as we did when we asked the LLM to imitate an anti-Semite person in [Figure 3.8](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig08). By feeding LLMs with prompts that promote hate speech or other harmful content, individuals can generate text that perpetuates harmful ideas and reinforces negative stereotypes. Creators of LLMs tend to take steps to mitigate this potential misuse, such as implementing content filters and working with human moderators to review the output of the model. Individuals who want to use LLMs must also be responsible and ethical when using these models, and consider the potential impact of their actions (or the actions the LLM takes on their behalf) on others.

#### Working with Prompts Across Models <a href="#ch03lev1sec3" id="ch03lev1sec3"></a>

Prompts are highly dependent on the architecture and training of the language model, meaning that what works for one model may not work for another. For example, ChatGPT, GPT-3 (which is different from ChatGPT), T5, and models in the Cohere command series all have different underlying architectures, pre-training data sources, and training approaches, which in turn impact the effectiveness of prompts when working with them. While some prompts may transfer between models, others may need to be adapted or reengineered to work with a specific model.

In this section, we will explore how to work with prompts across models, taking into account the unique features and limitations of each model as we seek to develop effective prompts that can guide the language models to generate the desired output.

**ChatGPT**

Some LLMs can take in more than just a single “prompt.” Models that are aligned to conversational dialogue (e.g., ChatGPT) can take in a **system prompt** and multiple “user” and “assistant” prompts ([Figure 3.9](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig09)). The system prompt is meant to be a general directive for the conversation and will generally include overarching rules and personas to follow. The user and assistant prompts are messages between the user and the LLM, respectively. For any LLM you choose to look at, be sure to check out its documentation for specifics on how to structure input prompts.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/03fig09.jpg" alt="A diagram illustrates a simple translation example." height="536" width="525"><figcaption><p>Figure 3.9 ChatGPT takes in an overall system prompt as well as any number of user and assistant prompts that simulate an ongoing conversation.</p></figcaption></figure>

**Cohere**

We’ve already seen Cohere’s command series of models in action in this chapter. As an alternative to OpenAI, they show that prompts cannot always be simply ported over from one model to another. Instead, we usually need to alter the prompt slightly to allow another LLM to do its work.

Let’s return to our simple translation example. Suppose we ask OpenAI and Cohere to translate something from English to Turkish ([Figure 3.10](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig10)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/03fig10.jpg" alt="A screenshot depicts the translation done from English to Turkish." height="730" width="525"><figcaption><p>Figure 3.10 OpenAI’s GPT-3 can take a translation instruction without much hand-holding, whereas the Cohere model seems to require a bit more structure.</p></figcaption></figure>

It seems that the Cohere model in [Figure 3.10](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig10) required a bit more structuring than the OpenAI version. That doesn’t mean that the Cohere is worse than GPT-3; it just means that we need to think about how our prompt is structured for a given LLM.

**Open-Source Prompt Engineering**

It wouldn’t be fair to discuss prompt engineering and not mention open-source models like GPT-J and FLAN-T5. When working with them, prompt engineering is a critical step to get the most out of their pre-training and fine-tuning (a topic that we will start to cover in [Chapter 4](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch04.xhtml#ch04)). These models can generate high-quality text output just like their closed-source counterparts. However, unlike closed-source models, open-source models offer greater flexibility and control over prompt engineering, enabling developers to customize prompts and tailor output to specific use-cases during fine-tuning.

For example, a developer working on a medical chatbot may want to create prompts that focus on medical terminology and concepts, whereas a developer working on a language translation model may want to create prompts that emphasize grammar and syntax. With open-source models, developers have the flexibility to fine-tune prompts to their specific use-cases, resulting in more accurate and relevant text output.

Another advantage of prompt engineering in open-source models is the ability to collaborate with other developers and researchers. Open-source models have a large and active community of users and contributors, which allows developers to share their prompt engineering strategies, receive feedback, and collaborate on improving the overall performance of the model. This collaborative approach to prompt engineering can lead to faster progress and more significant breakthroughs in natural language processing research.

It pays to remember how open-source models were pre-trained and fine-tuned (if they were at all). For example, GPT-J is an autoregressive language model, so we’d expect techniques like few-shot prompting to work better than simply asking a direct instructional prompt. In contrast, FLAN-T5 was specifically fine-tuned with instructional prompting in mind, so while few-shot learning will still be on the table, we can also rely on the simplicity of just asking ([Figure 3.11](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig11)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/03fig11.jpg" alt="A screenshot depicts two open source models." height="926" width="775"><figcaption><p>Figure 3.11 Open-source models can vary dramatically in how they were trained and how they expect prompts. GPT-J, which is not instruction aligned, has a hard time answering a direct instruction (bottom left). In contrast, FLAN-T5, which was aligned to instructions, does know how to accept instructions (bottom right). Both models are able to intuit from few-shot learning, but FLAN-T5 seems to be having trouble with our subjective task. Perhaps it’s a great candidate for some fine-tuning—coming soon to a chapter near you.</p></figcaption></figure>

#### Building a Q/A Bot with ChatGPT <a href="#ch03lev1sec4" id="ch03lev1sec4"></a>

Let’s build a very simple Q/A bot using ChatGPT and the semantic retrieval system we built in [Chapter 2](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch02.xhtml#ch02). Recall that one of our API endpoints is used to retrieve documents from the BoolQ dataset given a natural query.

Note

Both ChatGPT (GPT 3.5) and GPT-4 are conversational LLMs and take in the same kind of system prompt as well as user prompts and assistant prompts. When I say, “we are using ChatGPT,” we could be using either GPT 3.5 or GPT-4. Our repository uses the most up-to-date model (which at the time of writing was GPT-4).

Here’s what we need to do to get off the ground:

1. Design a system prompt for ChatGPT.
2. Search for context in our knowledge with every new user message.
3. Inject any context we find from our database directly into ChatGPT’s system prompt.
4. Let ChatGPT do its job and answer the question.

[Figure 3.12](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig12) outlines these high-level steps.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/03fig12.jpg" alt="A figure outlines the high level steps of Chat G P T." height="373" width="775"><figcaption><p>Figure 3.12 A 10,000-foot view of our chatbot, which uses ChatGPT to provide a conversational interface in front of our semantic search API.</p></figcaption></figure>

To dig into this process a bit deeper, [Figure 3.13](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig13) shows how this will work at the prompt level, step by step.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/03fig13.jpg" alt="A screenshot depicts the architecture of bot." height="560" width="773"><figcaption><p>Figure 3.13 Starting from the top left and reading left to right, these four states represent how our bot is architected. Every time a user says something that surfaces a confident document from our knowledge base, that document is inserted directly into the system prompt, where we tell ChatGPT to use only documents from our knowledge base.</p></figcaption></figure>

Let’s wrap all of this logic into a Python class, which will have a skeleton like that shown in [Listing 3.1](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#list3\_1).

Listing 3.1 A ChatGPT Q/A bot

[Click here to view code image](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03\_images.xhtml#f0070-01a)

```
# Define a system prompt that gives the bot context throughout the
conversation and will be amended with content from our knowledge base.
SYSTEM_PROMPT = '''You are a helpful Q/A bot that can only reference material
from a knowledge base.
All context was pulled from a knowledge base.
If a user asks anything that is not "from the knowledge base," say that you cannot
answer.
'''

# Define the ChatbotGPT class
class ChatbotGPT():

    # Define the constructor method for the class
    def __init__(self, system_prompt, threshold=.8):
        # Initialize the conversation list with the system prompt as the first turn
        # Set a threshold for the similarity score between the user's input and the
knowledge base
        pass

    # Define a method to display the conversation in a readable format
    def display_conversation(self):
        # Iterate through each turn in the conversation
        # Get the role and content of the turn
        # Print out the role and content in a readable format
        pass

    # Define a method to handle the user's input
    def user_turn(self, message):
        # Add the user's input as a turn in the conversation
        # Get the best matching result from the knowledge base using Pinecone
        # Check if the confidence score between the user's input and the document
meets the threshold
        # Add the context from the knowledge base to the system prompt if we meet the
threshold
        # Generate a response from the ChatGPT model using OpenAI's API
        # Add the GPT-3.5 response as a turn in the conversation
        # Return the assistant's response
        pass
```

A full implementation of this code using GPT-4 can be found in the book’s code repository. [Figure 3.14](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig14) presents a sample conversation we can have with it.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/03fig14.jpg" alt="A screenshot depicts the conversational answers by Chat G P T." height="962" width="575"><figcaption><p>Figure 3.14 Asking our bot about information from the BoolQ dataset yields cohesive and conversational answers. Asking about Barack Obama’s age (which is information not present in the knowledge base) causes the AI to politely decline to answer, even though that is general knowledge it would try to use otherwise.</p></figcaption></figure>

As a part of testing, I decided to try something out of the box and built a new namespace in the same vector database (thank you, Pinecone). I then chunked documents out of a PDF of a _Star Wars_–themed card game I like. I wanted to use the chatbot to ask basic questions about the game and let ChatGPT retrieve portions of the manual to answer my questions. [Figure 3.15](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch03.xhtml#ch03fig15) was the result.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780138199425/files/graphics/03fig15.jpg" alt="A screenshot depicts the conversational answers by Chat G P T." height="907" width="575"><figcaption><p>Figure 3.15 The same architecture and system prompt against a new knowledge base of a card game manual. Now I can ask questions in the manual but my questions from BoolQ are no longer in scope.</p></figcaption></figure>

Not bad at all, if I do say so myself.

#### Summary <a href="#ch03lev1sec5" id="ch03lev1sec5"></a>

Prompt engineering—the process of designing and optimizing prompts to improve the performance of language models—can be fun, iterative, and sometimes tricky. We saw many tips and tricks for how to get started, such as understanding alignment, just asking, few-shot learning, output structuring, prompting personas, and working with prompts across models. We also built our own chatbot using ChatGPT’s prompt interface, which was able to tie into the API we built in the last chapter.

There is a strong correlation between proficient prompt engineering and effective writing. A well-crafted prompt provides the model with clear instructions, resulting in an output that closely aligns with the desired response. When a human can comprehend and create the expected output from a given prompt, that outcome is indicative of a well-structured and useful prompt for the LLM. However, if a prompt allows for multiple responses or is in general vague, then it is likely too ambiguous for an LLM. This parallel between prompt engineering and writing highlights that the art of writing effective prompts is more like crafting data annotation guidelines or engaging in skillful writing than it is similar to traditional engineering practices.

Prompt engineering is an important process for improving the performance of language models. By designing and optimizing prompts, you can ensure that your language models will better understand and respond to user inputs. In [Chapter 5](https://learning.oreilly.com/library/view/quick-start-guide/9780138199425/ch05.xhtml#ch05), we will revisit prompt engineering with some more advanced topics like LLM output validation, chain-of-thought prompting to force an LLM to think aloud, and chaining multiple prompts together into larger workflows.
