# 7. Project 3: Fact Service With Spring Boot, Cloud Run, And Cloud SQL

## Chapter 7. Project 3: Fact Service with Spring Boot, Cloud Run, and Cloud SQL

In previous chapters, you embraced cloud native development. You made full use of the convenience of serverless Cloud Functions and then Cloud Run. You used the Go programming language, which, due to its relatively light footprint and fast startup time, is ideal for Cloud Run and autoscaling in particular. However, this chapter includes a more traditional use case: a long-running REST API service that persists data to a relational database. If you are coming to the cloud from an enterprise environment, this type of service is likely your bread and butter.

Often, cloud native assumes you are starting from a blank slate. However, even if you are working on a green field project, you will typically inherit constraints such as the languages and frameworks your team is used to. In this case, you are going to use Java and Spring Boot, but you will see how you still have options for making this a cloud native application.

On the first cloud native project I worked on, we were using Amazon Web Services. One team dove straight in, using the recently released serverless AWS Lambda and switching to Node.js as their programming language. All of this was completely new to them, and they were learning on the job. They soon ran into trouble. If you don’t have to change everything at once, often that is a better strategy.

For the next project, you are going to build the core of the Skills Mapper system, a REST API that allows users to add and remove skills use to build a profile. I will refer to pieces of information (e.g., “Bob is learning Java”) as facts, hence the name, facts service. However, we are going to constrain ourselves to Java, Spring, and PostgreSQL to show that even when there are constraints, you can still make use of cloud native principles and services.

**NOTE**

The code for this chapter is in the [`fact-service` folder of the GitHub repository](https://oreil.ly/Eaw6u).

## Requirements

Let’s dive into the requirements for this project.

### User Story

The user story for this piece of functionality is shown in [Figure 7-1](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch07.html#p3-postit).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_0701.png" alt="Project 3 User Story" height="324" width="828"><figcaption></figcaption></figure>

**Figure 7-1. Project 3 user story**

### Elaborated Requirements

This project also has the following specific requirements:

* A user should be able to add “facts,” the level of interest or proficiency they have in a specified skill.
* A user should be able to retrieve all the facts they have added.
* The service must provide a REST API.
* The service must have a secure connection to a database.
* Credentials used to connect to the database must be stored securely.
* The service must securely and uniquely identify a user.
* The API should be highly available and be able to handle multiple simultaneous requests.
* The API should respond to requests within 500 ms, 95% of the time.
* Due to support requirements, the implementation should be in Java and Spring Boot, and the database should be a relational database that can be queried using SQL.

## Solution

When choosing how to implement this requirement, you need to answer several questions:

Where to run the compute?

For this service, you have a long-running process, so it is not suitable for Cloud Functions. You will be using Java and Spring Boot in a container, so the startup time will be several seconds when using Cloud Run. This means it will not be able to cope well with on-demand requests within the required 500 ms. However, the autoscaling of Cloud Run will still be able to cope with unexpected demand and to be highly available. You will see how to make use of Cloud Run in a slightly different way to get the best of both worlds.

What type of database to use?

This service requires the use of a traditional relational database, or at least a service that appears as such to the application code. You will use Cloud SQL, as it is the simplest solution. Specifically, use Cloud SQL for PostgreSQL, as PostgreSQL is a de facto standard supported by several other database services on Google Cloud and other clouds. It gives you the option to switch to more powerful options later that also support PostgreSQL, if you need to. When architecting applications, keeping your options open like this is a good idea.

How to connect to the database?

You’ll want to follow the 12-factor principles of externalizing configuration and storing any secrets securely. You will make use of Cloud Secret Manager for storing secrets and explore secure options for connecting to the chosen database.

How to identify a user?

You want to be able to authenticate users and link them uniquely to the facts they provide. Storing sensitive information like email addresses and passwords, creating a registration form, and changing and resetting passwords is a big headache you want to avoid if at all possible. You will use Identity Platform, a service that does the authentication and user management for you.

## Summary of Services

Here is a summary of the Google Cloud services you will be using in this solution.

### Spring Boot with Spring Cloud GCP

Spring is a popular Java framework for building web applications. It is a mature framework that has been around for a long time and has a large community. With the release of Spring Boot in 2012, it became even easier to get started with Spring, and it is now the most popular way to build Spring applications. I started using Spring in the mid-2000s, and I found it a very steep learning curve on par with the difficulty I had when learning Kubernetes for the first time. Spring Boot was a godsend in making Spring easy to adopt and be productive with.

One of the key features of Spring Boot is that it is opinionated. It makes a lot of decisions for you and provides a lot of defaults. However, it also provides abstractions over specific technologies, keeping options open, which is a great architectural principle that enables us to remain open to change. For example, Spring Boot provides abstractions over databases, messaging systems, caching, and security. Switching between different implementations of these technologies is often as simple as changing the configuration. A bit later you will see how this has been extended to the services provided by Google Cloud with [Spring Cloud GCP](https://oreil.ly/MkFwE).

### Identity Platform

In this service, a fact is made up by relating a person to a skill via a level of interest or proficiency. A skill is represented by a Stack Overflow tag you retrieved in [Chapter 5](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch05.html#chapter\_05) and made searchable in [Chapter 6](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch06.html#chapter\_06). However, you need a way of capturing the person it is associated with, and for that, you can use [Identity Platform](https://oreil.ly/X1UYp). Identity Platform provides backend service, SDKs, and UI libraries that allow you to authenticate users. It is part of the [Firebase app development platform](https://oreil.ly/jR\_6x).

**TIP**

Identity Platform is a fully managed service that handles user authentication and identity management on behalf of the application developer. Be careful not to confuse it with Google Cloud Identity which is a different service that enables you to use Google accounts for single sign-on, third-party applications.

Firebase began as a separate application development platform and was acquired by Google in 2014, with Firebase Authentication being one of the features. Firebase targets web and mobile development, and even now it feels one step removed from Google Cloud and not as strongly integrated as other services. Identity Platform supports a range of identity providers, including Google, GitHub, Twitter, and Microsoft accounts. This is great, as it offloads all the responsibility for account creation, storing sensitive information and managing accounts to these trusted providers. However, to get up and running and make testing straightforward, you will start with a simple username and password.

### Cloud SQL

[Cloud SQL](https://oreil.ly/Hzpih) is a Google service for providing managed relational databases in the cloud. It supports PostgreSQL, MySQL, and SQL Server. All three databases are SQL-compliant and are managed similarly. The main difference is the underlying technology used to provide the database.

I would not classify Cloud SQL as a cloud native database; rather than a database that has been designed specifically for the cloud, it is a traditional database hosted in the cloud. Cloud SQL is a fully managed service, so you don’t need to worry about provisioning, patching, backups, high availability, or disaster recovery, but there are some limitations associated with the traditional databases it supports.

The most significant of these limitations is that you need to plan capacity up-front, and apart from automatically increasing storage, it struggles to scale up and down as scaling is only vertical, in that the machine hosting the database can be given more CPU and memory. Horizontal scaling, creating multiple instances, is limited to read replicas. The database also requires downtime for maintenance and restarts for many of the configuration changes. These are limitations that should be left behind in a true cloud native solution. However, the databases supported by Cloud SQL, especially PostgreSQL, are widely used technologies, and Cloud SQL can be low cost while providing reasonable resilience with a 99.95% SLA, and some configurations recently offering a 99.99% SLA. It is often going to be worth considering.

When using PostgreSQL, you can use the PostgreSQL protocol that is also supported fully or partially by Google’s cloud native relational databases, AlloyDB and Spanner. This makes the transition straightforward when the limitations of Cloud SQL have been reached and 99.99%, or even 99.999%, availability and global scale are required. For that reason, you are going to choose Cloud SQL for PostgreSQL for this project.

### Cloud Run

As you have seen in [Chapter 6](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch06.html#chapter\_06), Cloud Run is a flexible solution for running containers, and you will use it here again. My approach is to work my way down from the highest abstraction until I find a service that can work within the constraints I have identified, a process I call _progressive decomposition_, if anyone asks. This is not suitable for Cloud Functions, but Cloud Run seems like a good candidate. You will see in [Chapter 14](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch14.html#chapter\_14), where services like GKE Autopilot (Kubernetes) would be a better choice, but my principle is to start with the simplest solution and only introduce something more complicated when I reach a limitation. To quote the originator of the theory of constraints:

> Technology can bring benefits if, and only if, it diminishes a limitation.
>
> Dr. Eliyahu M. Goldratt

In this case, you have not reached the limitation of Cloud Run, as it can be configured in a slightly different way to support the requirements of a highly available Java and Spring Boot service with fast response times.

### Spring Cloud GCP

When working with Spring Boot on Google Cloud, you can use the Spring Cloud GCP libraries to make it even easier to consume Google Cloud services. You will use it to connect to Cloud SQL. Spring Cloud GCP provides familiar Spring abstractions over Google Cloud services. Although it forms part of the Spring Cloud, you can use just the GCP libraries in your application without having to use any of the other Spring Cloud features.

For example, you can use the Spring Data JPA library to access Cloud SQL; that is the approach you are going to take in this chapter. In this project, we will make use of Google Cloud APIs wherever possible. The code is going to “know” it is running Google Cloud. You will use Google APIs for logging and connecting to the database.

[Table 7-1](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch07.html#spring-cloud-gcp-services) includes a breakdown of the Google Cloud services supported by Spring Cloud GCP for Spring Boot applications. The services supported by Spring Cloud GCP nicely align with the “toolkit” of services used in this book. There are Spring starters for each service, a collection of managed libraries that provide all the required dependencies in your Maven or Gradle configuration.

| Category            | GCP service                      | Spring abstraction              | Spring starter                                      |   |
| ------------------- | -------------------------------- | ------------------------------- | --------------------------------------------------- | - |
| Databases           | Cloud SQL (MySQL)                | <p>Spring JDBC<br>template</p>  | `spring-cloud-gcp-starter-sql-mysql`                |   |
|                     |                                  | Spring Data JPA                 | `spring-cloud-gcp-starter-sql-mysql`                |   |
| Databases           | Cloud SQL (PostgreSQL)           | <p>Spring JDBC<br>template</p>  | `spring-cloud-gcp-starter-sql-postgres`             |   |
|                     |                                  | Spring Data JPA                 | `spring-cloud-gcp-starter-sql-postgres`             |   |
|                     | Cloud Spanner                    | Spring Data Spanner             | `spring-cloud-gcp-data-spanner`                     |   |
|                     |                                  | Spring Data JPA with Hibernate  | `spring-cloud-gcp-data-spanner`                     |   |
|                     | Cloud Firestore (Datastore mode) | <p>Spring Data<br>Datastore</p> | `spring-cloud-gcp-data-datastore`                   |   |
|                     | Cloud Firestore (Firestore mode) | Spring Reactive Data Firestore  | `spring-cloud-gcp-data-firestore`                   |   |
| Messaging           | Cloud Pub/Sub                    | Spring Integration              | `spring-cloud-gcp-starter-pubsub`                   |   |
|                     |                                  | Spring Cloud Stream             | `spring-cloud-gcp-pubsub-stream-binder`             |   |
|                     |                                  | Spring Cloud Bus                | `spring-cloud-gcp-starter-bus-pubsub`               |   |
| Configuration       | Cloud Runtime Configuration      | Spring Cloud Config             | `spring-cloud-gcp-starter-config`                   |   |
|                     | Cloud Secret Manager             | Spring Cloud Config             | `spring-cloud-gcp-starter-secretmanager`            |   |
| Storage             | Cloud Storage                    | Spring Resource                 | `spring-cloud-gcp-starter-storage`                  |   |
| Cache               | Cloud Memorystore                | Spring Data Redis               | `spring-boot-starter-data-redis`                    |   |
| Distributed tracing | Cloud Trace                      | Zipkin/Brave                    | `spring-cloud-gcp-starter-trace`                    |   |
| Centralized logging | Cloud Logging                    | SLF4J/Logback                   | `com.google.cloud:spring-cloud-gcp-starter-logging` |   |
| Monitoring metrics  | Cloud Monitoring                 | Micrometer/Prometheus           | `spring-cloud-gcp-starter-metrics`                  |   |
| Security            | Cloud Identity-Aware Proxy       | Spring Security                 | `spring-cloud-gcp-starter-security-iap`             |   |

In this case, you just want to use Cloud SQL and Cloud Logging within Maven, so you need to include three dependencies: `spring-cloud-gcp-starter`, which is required to use any Spring Cloud GCP functionality, `spring-cloud-gcp-starter-sql-postgres` for Cloud SQL for PostgreSQL, and `spring-cloud-gcp-starter-logging` for Cloud Logging. These are added to the _pom.xml_ file.

With the services and libraries you require in place, you can start implementation now.

## Implementation

Let’s get hands-on and implement this project.

### Creating a New Spring Boot Project

To create a new Spring Boot project, you can use the [Spring Initializr](https://oreil.ly/lTwbz). This is a web application that allows you to create a new Spring Boot project with several initial dependencies. You will use the following dependencies:

* Spring Cloud GCP
* Spring Web
* Spring Data JPA

You can use this `cURL` command to generate a template project for Maven. Download the generated zip, extract it, and then import it into your integrated development environment (IDE):

```
curl https://start.spring.io/starter.zip \
  -d type=maven-project \
  -d language=java \
  -d platformVersion=2.7.9 \
  -d name=fact-service \
  -d groupId=org.skillsmapper \
  -d dependencies=cloud-gcp,web,data-jpa \
  -o fact-service.zip
```

This is a great way of using Spring Boot and Spring Cloud GCP together to kickstart a new project and get you up and running quickly. In the example code, I have added standard Spring Controllers and a JPA repository to access the database. The code is available in [the GitHub repository](https://oreil.ly/P1kx-).

### Configuring Identity Platform

To authenticate users, you can use [Identity Platform](https://oreil.ly/lQ0VJ).

This is one of the few services on Google Cloud that cannot be set up completely using the command line. So far in this book, you have used the command line wherever possible, as it easily automates the toil of typing commands later. For Identity Platform, however, the only option is to use the [Google Cloud Web Console](https://oreil.ly/Hcp6M).

To begin, you will need to go to the Identity Provider page in the cloud console’s [marketplace](https://oreil.ly/ZK\_G2) and click on the “Enable identity platform” button to enable the service.

This would be equivalent to the `gcloud services enable` command you have used for other services. Once the service is enabled, you will be taken to the Identity Platform onboarding page.

On the left-hand menu, select Providers, as shown in [Figure 7-2](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch07.html#cloud-identity-provider), and then click on the Add a Provider button in the toolbar at the top. This will display a list of available providers; you can add many providers to your application, but for now, you will just add a simple one: `Email / password`, which you can select near the bottom of the list. Toggle the enable control, accept the default settings, and click the Save button.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_0702.png" alt="Providers in Identity Platform" height="608" width="860"><figcaption></figcaption></figure>

**Figure 7-2. Providers in Identity Platform**

Create environment variables to store the username and password for the test user, as you will use that later:

```
export TEST_EMAIL=[EMAIL]
export TEST_PASSWORD=[PASSWORD]
```

**TIP**

Even for test situations like this, it is a good idea to use a good password. You can use a password manager like 1Password or LastPass to generate a secure password for you and then copy and paste it into the console. For the email address, you can use the domain `example.com`, which is reserved for testing purposes (e.g., `user@example.com`).

Now, on the left-hand menu, select Users and then click on the Add User button. This will allow you to create a new user. Enter an email address and that password as you provided to the environment variables and click on the Save button. This will create a new user in the Identity Platform.

Now that you have authentication configured, you can start getting the application ready for deployment.

### Building the Container

With the Spring Boot application written, you need to package it into a container. In [Chapter 10](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch10.html#chapter\_10), I will cover more options for automating this process; however, for now, I will introduce [Jib](https://oreil.ly/Kb3Xs), a Google open source project that allows you to build containers for a Java application without a Dockerfile.

Jib uses a plugin with either Maven or Gradle to build the container without needing Docker installed. Ultimately, a container is just a layered archive file, and all Jib does is insert a layer containing the application code and dependencies. In this case, Jib is enabled by adding the following to the plugins section of the _pom.xml_ file:

```
<plugin>
    <groupId>com.google.cloud.tools</groupId>
    <artifactId>jib-maven-plugin</artifactId>
    <version>3.3.1</version>
</plugin>
```

Adding this plugin to the _pom.xml_ file will cause Jib to build a container when you run the `mvn package` command. The container will be tagged with the name `fact-service` and the version number from the _pom.xml_ file. You could then push the container to the Google Container Registry (GCR) using the gcloud command-line tool:

```
gcloud container images push gcr.io/${PROJECT_ID}/${SERVICE_NAME}:${VERSION}
```

However, it is easier to just let Cloud Run do all this work; you will see how to do that later. For now, you need to set up a database for the service to connect to.

### Creating a Cloud SQL Instance

You can use a Cloud SQL instance to host the database. First, you need to enable the `sqlamin` service in the project using:

```
gcloud services enable sqladmin.googleapis.com
```

First, there are some environment variables to set:

```
export INSTANCE_NAME='facts-instance'
export DATABASE_TIER='db-f1-micro'
export DISK_SIZE=10
```

Here is a command to create a new instance of PostgreSQL on Cloud SQL. One instance can host multiple databases:

```
gcloud sql instances create $INSTANCE_NAME \
    --database-version=POSTGRES_14 \
    --tier=$DATABASE_TIER \
    --region=$REGION \
    --availability-type=REGIONAL \
    --storage-size=$DISK_SIZE
```

Here, you have specified the instance name, the database version, the tier, the region, the availability type, and the disk size.

The tier is the machine type and the disk size is the amount of storage. Setting the tier to `db-f1-micro` defines the amount of CPU and memory of the machine that will be used to host the instance. This is a very small machine intended only for development.

You can also set the disk size to `10` (GB), the smallest allowed. This keeps costs to around 1–2 cents per hour. You can increase these values later if needed.

Using small machines has implications for the number of concurrent connections that can be made to the database. For example, this minimum configuration supports a maximum of 25 connections, which is less than the default of 100 normal for PostgreSQL. Higher tiers intended for nondevelopment use provide at least 100 connections by default.

The database version is the version of PostgreSQL that will be used. The region is the region where the database is deployed. Make sure this is the same region as the Cloud Run service to get the best performance.

The _region_ denotes the geographical location in which the database is deployed. It’s crucial to ensure that your database and Cloud Run service are in the same region to optimize performance due to reduced network latency.

Regarding availability, opt for a regional database, which provides replication across multiple zones within the chosen region. This enhances the database’s availability, making it resistant to zone-specific failures. By default, regional databases are selected because they typically suit most use cases. They come with a 99.95% service-level agreement (SLA), which translates to expected downtime of fewer than five hours per year.

**TIP**

It may surprise you to learn that the size of your disk space can directly affect the number of input/output operations per second (IOPS) your system can handle. IOPS is a common performance measurement used to benchmark computer storage devices like hard disk drives (HDD), solid-state drives (SSD), and storage area networks. The more IOPS your system can handle, the faster it can read and write data, which can significantly improve the performance of data-heavy applications.

This relationship might seem counterintuitive at first, but here’s how it works: a larger disk size allows for more potential input and output operations. For instance, a minimum disk size of 10 GB provides 300 IOPS. This capacity scales linearly, so with each additional 10 GB of disk space, you gain another 300 IOPS. Therefore, a 100 GB disk size would grant you 3,000 IOPS.

This understanding is critical when configuring your systems because it means that, even if your data storage needs are relatively small, you might still need to opt for a larger disk size to achieve the performance level you need. This often overlooked aspect can have significant implications for the speed and efficiency of your applications, so it’s essential to take it into account when planning your system architecture.

On the other hand, you can choose a zonal database that operates within a single zone. Though this option is more affordable (half the price), it lacks the redundancy provided by multizone replication in a regional database. It’s important to note that while a single-zone deployment still carries a 99.95% SLA, the practical reliability of a regional database—with its three replicas—is likely superior. However, Google’s financial compensation only comes into play if availability falls short of the 99.95% SLA, regardless of the database type.

Behind the scenes of creating a PostgreSQL instance, a process akin to setting up a virtual machine is underway. This process involves installing and configuring PostgreSQL on this virtual infrastructure. While this is a highly automated process, it’s a bit more complex and time-consuming than deploying a service designed from the ground up to be cloud native. This explains why the setup process takes several minutes, as the system needs to establish and configure a virtual environment capable of running a full-fledged PostgreSQL database server instance.

### Creating a Database and User

Let’s establish a new database within the instance. Set a variable for database name (e.g., facts):

```
export DATABASE_NAME='facts'
```

Then use the following command to create the database, referencing the instance you previously created:

```
gcloud sql databases create $DATABASE_NAME \
    --instance=$INSTANCE_NAME
```

Set environment variables for username (e.g., fact-user) and a randomly generated password that the fact service will use to authenticate with the database:

```
export FACT_SERVICE_DB_USER='[FACT_SERVICE_DB_USER]'
export FACT_SERVICE_DB_PASSWORD='[FACT_SERVICE_DB_PASSWORD']
```

Now create a user for the fact service to use with the command:

```
gcloud sql users create $FACT_SERVICE_DB_USER \
    --instance=$INSTANCE_NAME \
    --password=$FACT_SERVICE_DB_PASSWORD
```

Your PostgreSQL database is now configured and ready for use.

### Test Deploying to Cloud Run

Just like with the Go service in [Chapter 6](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch06.html#chapter\_06), you can indirectly use Cloud Build to build the container, push it to Google Container Registry, and then deploy it to Cloud Run. However, this time Cloud Build will use Jib to build the container, as it will detect the Jib Maven plugin:

```
gcloud run deploy ${FACT_SERVICE_NAME} --source . \
--set-env-vars PROJECT_ID=$PROJECT_ID,SERVICE_NAME=$FACT_SERVICE_NAME,SPRING_PROFILES_ACTIVE=h2 \
--allow-unauthenticated
```

Here, environment variables are being passed to the container using `--set-env-vars` instead of an _env.yaml_ file.

This will deploy the service in a test mode by using the h2 in-memory database. The service will start in response to requests but will take a few seconds to start up, and when it is not being used, it will be shut down. This is a great way to test the service and make sure it is working as expected.

**TIP**

Having `spring-cloud-gcp-starter-sql-postgresql` in the classpath Spring will automatically enable Cloud SQL, and you will likely get the error `A database name must be provided`. This is avoided by putting `spring.cloud.gcp.sql.enabled=false` in the _application.properties_ file to disable it by default and enabling it with `spring.cloud.gcp.sql.enabled=true` in the _application-gcp.properties_ file.

You can get the URL of the service using:

```
export FACT_SERVICE_URL=$(gcloud run services describe $FACT_SERVICE_NAME --format='value(status.url)')
```

Then use a `cURL` command to test the service:

```
curl -X GET ${FACT_SERVICE_URL}/api/facts
```

At the moment, you will get a 401 Unauthorized error, as you have not configured authentication, but it does prove the service is running.

### Creating a Secret in Secret Manager

Before establishing a connection between the Cloud Run service and the Cloud SQL database, it’s necessary to create a secret in Secret Manager to safely store the database user’s password.

Secret Manager is a secure and convenient solution provided by Google Cloud for managing sensitive information like passwords, API keys, and other secrets. It ensures that these secrets are accessible only to authenticated services and users, thereby helping to keep your application secure. It also offers versioning and audit logging, enabling you to keep track of who accessed what secret and when.

By using Secret Manager to store the database password, you are making sure that it’s kept safe and can be accessed securely by your Cloud Run service when it needs to connect to the database.

As with any Google Cloud service, you first need to enable the Secret Manager service for your project using the following command:

```
gcloud services enable secretmanager.googleapis.com
```

Once the service is enabled, create an environment variable to hold the secret name (e.g., `fact_service_db_password_secret`):

```
export FACT_SERVICE_DB_PASSWORD_SECRET_NAME=[FACT_SERVICE_DB_PASSWORD_SECRET_NAME]
```

You can then create the secret using:

```
gcloud secrets create $FACT_SERVICE_DB_PASSWORD_SECRET_NAME \
    --replication-policy=automatic \
    --data-file=<(echo -n $FACT_SERVICE_DB_PASSWORD)
```

Here, you are taking the database password stored in the environment variable and passing it to the `gcloud` command using the `<(echo -n $DATABASE_PASSWORD)` syntax.

**TIP**

It is important to use the `-n` option to `echo` to avoid storing a newline character at the end of the password, making it invalid.

You now have a secret to which you can give a service account access, rather than sitting unencrypted in an environment variable. You will then use that service account with the Cloud Run service.

### Creating a Service Account

In line with the principle of least privilege, you can now create a dedicated service account that has only the necessary permissions to perform its tasks.

Create an environment variable for the service account name (e.g., `fact-service-sa`):

```
export FACT_SERVICE_SA=[FACT_SERVICE_SA]
```

Then create the service account using:

```
gcloud iam service-accounts create $FACT_SERVICE_SA \
    --description="${FACT_SERVICE_NAME} service account"
```

To enable your service account with the access it needs, you need to assign it two specific roles.

The first role is the Cloud SQL Client. This role grants the service account the necessary permissions to connect and interact with the Cloud SQL database. It’s like giving the service account the keys to the database room.

The second role is the Secret Manager Secret Accessor. This role allows your service account to access and retrieve the secret you stored in the Secret Manager. Think of this as giving your service account the combination to the safe where you keep your most valuable secrets.

By assigning these roles, you empower your service account to perform its tasks, while still adhering to the principle of least privilege.

Use this command to add the Cloud SQL Client role:

```
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member=serviceAccount:${FACT_SERVICE_SA}@${PROJECT_ID}.iam.gserviceaccount.com \
  --role=roles/cloudsql.client
```

Then use this command to give the service account access to the secret in Secret Manager:

```
gcloud secrets add-iam-policy-binding $FACT_SERVICE_DB_PASSWORD_SECRET_NAME \
  --member=serviceAccount:${FACT_SERVICE_SA}@${PROJECT_ID}.iam.gserviceaccount.com \
  --role=roles/secretmanager.secretAccessor
```

You now have a service account with the permissions the Cloud Run service needs and no more.

### Deploying to Cloud Run Connecting to Cloud SQL

To deploy for real, the fact service needs to connect to the Cloud SQL PostgreSQL database. There are several options for connecting to a Cloud SQL database from Cloud Run. In this case, as you are using the Spring Cloud GCP starter for Cloud SQL to connect to a Cloud SQL database using the Cloud SQL Proxy, as shown in [Figure 7-3](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch07.html#cloud-sql-proxy).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_0703.png" alt="Cloud SQL Proxy" height="626" width="1348"><figcaption></figcaption></figure>

**Figure 7-3. Cloud SQL Proxy**

The Cloud SQL Proxy is a binary that normally runs as a sidecar container to the main application container. This is a proxy client that creates a connection via a secure tunnel to a corresponding proxy server on the machine the Cloud SQL database is running on, and then forwards the connection to the application container. If you were using Kubernetes, you could use a sidecar container to run the Cloud SQL Proxy, but as you are using Cloud Run, you need to use a different approach.

Fortunately, there is a mechanism in Cloud Run to provide the Cloud Proxy for you using the Cloud SQL Proxy as a Cloud Run service and then using the `--add-cloudsql-instances` option to connect to the Cloud SQL Proxy service, specifying the connection name of the Cloud SQL instance.

You can update the Cloud Run service to switch to using the Cloud SQL database by adding the `--add-cloudsql-instances` option and using the service account to run the Cloud Run container, so it has the correct permissions.

At this point, you also switch to passing environment variables in a file using `env-vars-file` and pass the secret containing the database password using `--update-secrets`.

First generate the _env.yaml_ file using:

```
envsubst < env.yaml.template > env.yaml
```

Then run this command to update the fact service:

```
gcloud run services update $FACT_SERVICE_NAME \
    --service-account ${FACT_SERVICE_SA}@${PROJECT_ID}.iam.gserviceaccount.com \
    --add-cloudsql-instances ${PROJECT_ID}:${REGION}:${INSTANCE_NAME} \
    --env-vars-file=env.yaml \
    --update-secrets=DATABASE_PASSWORD=${FACT_SERVICE_DB_PASSWORD_SECRET_NAME}:latest
```

This will take a few seconds to redeploy the service. Once it is deployed, you can test it using the same `cURL` command as before:

```
curl -X GET ${FACT_SERVICE_URL}/api/facts
```

This time, it will be using the Cloud SQL database instead of h2, but you should still see the 401 Unauthorized error.

### Authentication with Identity Platform

The service uses authentication so that only authenticated users can add facts. If you try to add a fact using the `cURL` command, you will get a 401 Unauthorized error:

```
curl -X POST \
  -H 'Content-Type: application/json' \
  -d '{ "skill": "java", "level": "learning" }`' \
  ${FACT_SERVICE_URL}/api/facts
```

You will get a 401 Unauthorized error like this:

```
{"timestamp":"2023-07-24T17:27:07.872+00:00","status":401,"error":"Unauthorized","path":"/api/facts"}
```

This is because the service is expecting a valid token in the `Authorization` header. This token would normally be provided by the user’s browser when they log in to the application using a UI. However, for testing, you can use the command line to log in and retrieve the token. To do this, you need the API key from Identity Platform. This API key would also be used when calling the Identity Platform API from the client application.

Go to the [Identity Platform providers page in the Cloud console](https://oreil.ly/lVu05) and click the link for `APPLICATION SETUP DETAILS`.

First, copy the value from the apiKey field and store it in an environment variable using the following command:

```
export API_KEY=[apiKey]
```

Next, use this API key along with the test username and password to retrieve an ID token. This ID token serves as a credential that allows the user to access resources:

```
export ID_TOKEN= \
$(curl "https://www.googleapis.com/identitytoolkit/v3/relyingparty/verifyPassword?key=${API_KEY}" \
-H "Content-Type: application/json" --data-binary \
"{\"email\":\"${TEST_EMAIL}\",\"password\":\"${TEST_PASSWORD}\",\"returnSecureToken\":true}" \
| jq -r '.idToken')
```

Now, with the ID token, you can make authenticated requests to your service. In this case, use the token in the Authorization header of the `cURL` command to add a fact:

```
curl -X POST \
  -H "Authorization: Bearer ${ID_TOKEN}" \
  -H 'Content-Type: application/json' \
  -d '{ "skill": "java", "level": "learning" }`' \
  ${FACT_SERVICE_URL}/api/facts
```

This will return a 201 Created response like this:

```
{"id":3,"timestamp":"2023-03-15T19:59:09.82879",
"userUID":"v9qeph7MLBf5EgFYlUCKMTYwQ6i1","level":"learning","skill":"java"}
```

You can also use the `cURL` command to retrieve the facts to see that the fact has been added again, providing the token in the `Authorization` header:

```
curl -X GET ${FACT_SERVICE_URL}/api/facts \
  -H "Authorization: Bearer ${ID_TOKEN}"
```

This will return a list of all facts added by the user to which the token belongs.

In the same way, you can use the token to authorize deleting the fact again:

```
curl -X DELETE \
  -H "Authorization: Bearer ${ID_TOKEN}" \
  ${FACT_SERVICE_URL}/api/facts/1
```

You now have a secured service that can create, retrieve, and delete from the database using the API.

### Improving the Startup Time

At the moment, Cloud Run will shut down running containers when there are no requests. This means when a new request comes in, it needs to be started up again.

You can try this by running Apache Bench to send three requests to the service after not accessing it for a while:

```
ab -n 3 -H "Authorization: Bearer ${ID_TOKEN}" ${FACT_SERVICE_URL}/api/facts
```

You should see that the first request takes a few seconds to return (in my case 25 seconds) but the subsequent requests are much faster (259 ms). This is because the service has been started up to serve the first request but is still running for subsequent requests.

This is known as the _cold start problem_ and is a common issue with serverless platforms, especially when using a language like Java where startup times are relatively slow. In a language like Go, it would be less of a problem, as Go starts relatively quickly compared to Java; however, the first request would still be slower than subsequent requests.

The way to avoid this is to keep at least one instance of the service running all the time. You can do this by setting the `--min-instances` option to 1 to ensure that at least one instance is always running.

You can also set the `--max-instances` to allow the service to scale up to handle more requests. However, you don’t want it to scale to the default of 100 replicas. Remember, you only have a limited number of database connections, so you need to be careful not to scale up too much, as each instance is configured to keep five connections in its connection pool. This is one limitation of using a noncloud native database like PostgreSQL with a serverless application. It is manageable, but it is something to be aware of.

This is the command to update the service with minimum and maximum instances specified:

```
gcloud run services update $FACT_SERVICE_NAME \
    --service-account ${FACT_SERVICE_SA}@${PROJECT_ID}.iam.gserviceaccount.com \
    --add-cloudsql-instances ${PROJECT_ID}:${REGION}:${INSTANCE_NAME} \
    --env-vars-file=env.yaml \
    --update-secrets=DATABASE_PASSWORD=${FACT_SERVICE_DB_PASSWORD_SECRET_NAME}:latest \
    --min-instances=1 \
    --max-instances=3
```

If you think of Cloud Run as a taxi, transport on demand, this is a bit like always having a taxi waiting outside your house ready to take you to the airport. It is more expensive, but it is much more convenient.

If you try the Apache Bench command again, you should see that the first request is much faster:

```
ab -n 3 -H "Authorization: Bearer ${ID_TOKEN}" ${FACT_SERVICE_URL}/api/facts
```

In my test, the fastest response was 240 ms and the slowest 235 ms, a great improvement in consistency and below the 500 ms that you require.

## Evaluation

Now let’s look at how the solution will scale and how much it will cost.

### How Will This Solution Scale?

This solution is designed with scalability in mind.

First, the service itself has been designed to handle vertical scaling. This means that each instance’s capacity can be increased by augmenting memory and CPU resources. This is done by adjusting the `--memory` and `--cpu` parameters during service deployment. Changing these parameters allows the service to handle a larger number of requests.

The solution also supports horizontal scaling. It can dynamically adjust—scale up or down—to handle fluctuations in the volume of requests. Even though it currently maintains a minimum of one running instance, this baseline could be raised to swiftly respond to traffic surges.

On the other hand, the database presents a slightly different story. It primarily supports vertical scaling, which means its capacity can be increased by adjusting the `--tier`, or the machine’s specification on which the database operates. However, this process isn’t automated and necessitates a database restart. This can result in a brief downtime period, which isn’t ideal.

Moreover, there’s another crucial aspect to bear in mind. The database can handle only a limited number of connections, dependent on the machine’s size. Therefore, it’s important to ensure that the total number of connections, which is the number of instances multiplied by the connections in each pool, doesn’t exceed the available connections. This careful management of resources contributes to the overall robustness and efficiency of the solution.

### How Much Will This Solution Cost?

Cloud Run is billed based on the number of requests and the amount of memory used by the service. The first two million requests are free and after that, it is $0.0002 per request. The memory is billed at $0.00001667 per GB per hour at the time of writing.

As the service needs to always be responsive, you will keep at least one instance running all the time. This is going to be the most significant cost. However, you can minimize this by setting the CPU and memory to the minimum required to run the service and allowing more instances to be scaled up to handle more requests.

Another significant cost is going to be the database, as it will be running all the time. The database is billed based on the amount of storage used and the tier the machine is running on. At the moment, you are using the smallest available database and storage amount, which keeps costs to a minimum. In [Chapter 14](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch14.html#chapter\_14), you will see how using other more cost-effective services where possible can help us keep the cost of this database down.

**TIP**

Be aware that there is a charge per request; even though it may not seem significant when services are receiving millions of requests per month, it may start to become significant. At that point, GKE Autopilot, covered in [Chapter 14](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch14.html#chapter\_14), might be worth considering from a cost point of view alone.

The database is billed based on the amount of storage used and the tier of the machine it is running on. With the minimal configuration currently in use, the cost should be less than $10 per month.

## Summary

You have created a REST service using Spring Boot and deployed it to Cloud Run. You have also added authentication using Identity Platform and a database backend using Cloud SQL. You have followed cloud native principles and made good use of the features of the Google Cloud Platform without getting too tied to it.

For this project, you used the following services directly:

* Cloud Run is used as the container runtime to run the container
* Cloud SQL is used as the database backend for the application
* Cloud Secrets Manager is used to securely store the database password
* Identity Platform is used to authenticate users

In [Chapter 8](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch08.html#chapter\_08), you will create a service to respond asynchronously when the facts change.
