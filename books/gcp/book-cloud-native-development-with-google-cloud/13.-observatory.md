# 13. Observatory

## Chapter 13. Observatory

Every Saturday morning, I am in the habit of collecting metrics on a spreadsheet. I weigh myself, take my blood pressure, check my resting heart rate, and record how long I slept each night and how much exercise I have done. I also go for a 5 km run and record the time and how I feel. This acts as a system check. I have been doing this for several years and now I have a lot of data.

This data provides insights into what constitutes my “normal,” allowing me to perceive shifts over time and detect any abnormalities. For instance, a gradual increase in my weight could prompt me to reevaluate my diet, while an elevated blood pressure might lead me to seek medical advice. The modest effort I invest each Saturday morning offers an enlightening view of my health and fuels my motivation to continually enhance my fitness.

## Monitoring the System

With the factory and citadel in place, you have an automated system for safely deploying the application and an environment in which to securely run it. However, there are a lot of moving parts, and it is going to be tough to understand what is happening. It will be difficult to notice, let alone fix problems. This is the reason to start to collect metrics and logs and make them available in a central place, as I do with my spreadsheet. By understanding what is normal and what is changing over time and noticing any anomalies, you will understand the health of the system and identify opportunities for improvement.

In an on-premises environment, you have the advantage of physical access to the hardware for metric collection. However, this is not an option in Google Cloud or any other cloud environment, as the hardware is owned by the service provider. Fortunately, Google Cloud is engineered with built-in metrics, logging, and tracing from the ground up, and these signals are centrally aggregated. The platform automatically collects thousands of metrics, which you can supplement with custom metrics from your applications for a full picture.

The crux is, while most of the data is readily available, you need an _observatory_, a centralized point to monitor this vast data universe. This chapter will guide you in building that observatory.

**NOTE**

The code for this chapter is in the [`observatory` folder of the GitHub repository](https://oreil.ly/VFkXV).

## Site Reliability Engineering

Operating an application in a Cloud environment is a discipline in its own right. Site reliability engineering (SRE) is Google’s preferred approach, and the tools supplied by Google Cloud (as you would expect) support SRE. There are three excellent free books on the subject available at the [Google SRE website](https://sre.google/). These O’Reilly books are also highly recommended: [_Building Secure and Reliable Systems_](https://www.oreilly.com/library/view/building-secure-and/9781492083115/) by Heather Adkins et al. and [_Observability Engineering_](https://www.oreilly.com/library/view/observability-engineering/9781492076438/) by Charity Majors et al.

This chapter will not delve into the mechanics of SRE per se. Instead, it will introduce you to a collection of tools specifically designed to monitor applications operating on Google Cloud. Gaining insights into your application’s behavior is critical for identifying, debugging, and rectifying issues.

## Golden Signals

That said, one of the SRE principles worth noting is the _golden signals_. These are four metrics that are the most important to monitor for any application:

Traffic

The amount of demand the system is responding to, normally measured as the number of requests per second. Think of this as your heartbeat or pulse rate. Just as a heart rate measures the number of times your heart beats in a minute, traffic indicates how many requests your system is handling. A suddenly elevated heart rate might indicate stress or excitement, much like a surge in traffic might hint at increased user activity or a possible DoS attack.

Saturation

How much of the available capacity the system is using. This could be the percentage of CPU, memory, disk, and network capacity in use, for example. This can be likened to lung capacity when you’re exercising. When you’re at rest, you’re using a small portion of your lung capacity; when you’re running, you’re pushing your lungs to use as much of their capacity as possible. Similarly, if your system’s resources are being fully utilized, your system is “breathing heavily,” potentially leading to exhaustion or slowdown.

Errors

The proportion of requests that fail or return an unexpected result in comparison to the total number of requests. This is a good indicator of the reliability and stability of a system. Imagine going for a health checkup and receiving some abnormal test results. These anomalies, like unusual blood work, might point toward specific health issues. Similarly, a higher rate of errors in a system could indicate underlying problems that need addressing.

Latency

The time to process and respond to a particular request. This is a good indicator of the performance of a system. This is akin to the reflex time of the human body. For instance, the time it takes for your hand to pull away from something hot. In an optimal state, you’d have a quick reflex, just as an efficient system would have low latency. Delays in reflex might suggest neurological concerns, just as high latency could point toward performance bottlenecks.

These are the metrics I will concentrate on in the system. The idea is that if you can monitor these four metrics, you will have a good understanding of the health of the system, much like I attempt to understand the health of my body.

## Implementing Observability

Observability on Google Cloud is organized with Workspaces within the Cloud Monitoring service. These are dashboards that can be used to provide a “single pane of glass” to show what is happening in the whole system and give secure access to the information, some of which may be sensitive. Let’s take the skill service from the Skills Mapper project and show how you can add useful metrics about its resources using a Workspace.

### Monitoring Workspaces

In Google Cloud, a Workspace resides within a project and inherits its name. While it’s tethered to one project, it can oversee resources from up to 100 different projects. For this chapter, you’ll set up a Workspace in the management project, but it will monitor resources from the citadel project.

You might wonder, “Why not create the workspace directly in the citadel project?” There’s a reason: if you were to ever remove the citadel project, especially if it’s a temporary setup, the Workspace would vanish with it, so this is a better option.

By placing your Workspace in a distinct project, you also gain flexibility. Multiple environments—be it dev, test, qa, or prod—can all be under the watchful eyes of a single Workspace. It aligns with the vision of an observatory: a unified vantage point to gaze across your entire digital cosmos.

### Configuring Cloud Monitoring

The project that you are in—in this case, the management project—is monitored automatically. You can expand the metrics scope by adding additional projects. To add the Citadel project to the current project, set environment variables for the management and monitored project ID (e.g., `skillsmapper-management` and `skillsmapper-development`, respectively):

```
export MANAGEMENT_PROJECT_ID=[MANAGEMENT_PROJECT_ID]
export MONITORED_PROJECT_ID=[MONITORED_PROJECT_ID]
```

Make sure that your current project is the management project:

```
gcloud config set project $MANAGEMENT_PROJECT_ID
```

Just as Firebase emerged as a standalone product before its integration with Google Cloud, Google Cloud Monitoring similarly began its journey under a different identity: Stackdriver. Originally developed as an independent monitoring solution, Stackdriver was acquired by Google and subsequently integrated into the Google Cloud suite.

Because of this historical evolution, certain aspects of Google Cloud Monitoring still bear remnants of its Stackdriver roots, especially in the tools provided for users to interact with it. Notably, the gcloud CLI, which is the primary tool for managing Google Cloud resources, doesn’t fully encapsulate all the features of the monitoring service. Some of these commands, due to their transitional nature or a recent addition, are labeled as “beta,” indicating that they might not be as stable as other well-established commands or could undergo further changes in subsequent releases.

Use this command to add the monitored project to the scope of the management project:

```
gcloud beta monitoring metrics-scopes create $MONITORED_PROJECT_ID
```

You will now be able to collect metrics from the monitored project as well as the management project.

### Metrics

There are hundreds, if not thousands, of built-in metrics available for services in Google Cloud. These are collected automatically and can be viewed in the Google Cloud console. For example, you can view the metrics for a Cloud Run service by selecting the service and then selecting the Metrics tab. All these metrics are also available programmatically.

Google Cloud offers a vast array of built-in metrics across its myriad of services, with the numbers potentially reaching into the thousands. These metrics are seamlessly and automatically gathered, allowing users to gain insights into various aspects of their cloud infrastructure and applications.

To access these metrics via the Google Cloud console, it’s quite intuitive:

* Navigate to the specific service section, such as Cloud Run.
* Choose the service instance of interest.
* Click on the Metrics tab to get a comprehensive view of its performance indicators.

Moreover, the flexibility of Google Cloud extends beyond just manual viewing. These metrics are also programmatically accessible, paving the way for automated analytics, integrations with other systems, or custom monitoring dashboards tailored to unique operational needs.

### Dashboards

Dashboards are a way to visualize what is important and are formed from one or more charts. Each chart visualizes one metric of the monitored projects.

Google Cloud provides several built-in dashboards for specific services (e.g., Cloud Run, Cloud Storage, Cloud SQL, etc.), and you can view these at [Google Cloud Dashboards](https://oreil.ly/N3leT).

As an example, [Figure 13-1](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch13.html#cloud-run-dashboard) includes the default Cloud Run dashboard for a running skill service.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_1301.png" alt="Cloud Run Dashboard" height="1460" width="2876"><figcaption></figcaption></figure>

**Figure 13-1. Cloud Run dashboard**

### Creating a Custom Dashboard

You can also assemble your custom dashboards from these metrics, such as the amount of memory a specific Cloud Run service is using or the number of Cloud Run instances that are running. You can build dashboards from the Google Cloud console, but to define dashboards programmatically, you can use YAML.

As you want to automate the creation of the observatory and maintain the configuration in version control, this is a more sustainable way of doing it. For example, this YAML will create a dashboard named `Example Dashboard` with a single widget that contains `Hello World`:

```
displayName: Example Dashboard
gridLayout:
  widgets:
  - text:
      content: Hello World
```

However, you will want something more substantial. The file _templates/service-dashboard.yaml.template_ contains widgets for monitoring a Cloud Run service. Rather than just showing rudimentary metrics, it displays widgets that capture the essence of the golden signals. Let’s take a look at the widgets and their significance.

The dashboard is made up of four widgets that map to the golden signals:

Container instance count

At its core, this widget provides insights into the number of service instances that are currently active. Monitoring this count is crucial because it indicates the service’s _saturation_ level. If the number of instances suddenly spikes, it might suggest a surge in demand or potential inefficiencies that warrant attention.

Request latencies

This widget delves deep into the _latency_ of service requests. By showing P50, P90, and P99 latencies, it offers a granular view of the service’s responsiveness. Tracking these percentiles helps ensure that the vast majority of users experience optimal performance, and it also shines a light on outliers that might require further investigation.

Request Count

Keeping a tab on the _traffic_ is fundamental. This widget counts the number of incoming requests, allowing teams to understand usage patterns, anticipate scaling needs, and even gauge the success of feature launches.

Percentage unsuccessful

Not all requests will be successful, and this widget is dedicated to shedding light on the _error_ rate. It’s paramount to understand why requests fail, as these errors can diminish user trust, affect revenue, and even signal deeper technical issues.

The dashboard also contains the logs from the service. This is useful for debugging and troubleshooting. The idea is that you have a single view of everything important about the service.

Use `envsubst` to populate it with the variables applicable to your project, specifically the service name (e.g., `skill-service`) and the ID of the monitored project (e.g., `skillsmapper-management`). Set an environment `SERVICE_NAME` environment variable to the value of `$SKILL_SERVICE_NAME`:

```
export SERVICE_NAME=$SKILL_SERVICE_NAME
```

```
envsubst < templates/service-dashboard.yaml.template > service-dashboard.yaml
```

Then ensure you are in the management project:

```
gcloud config set project $MANAGEMENT_PROJECT_ID
```

Now create a dashboard from the configuration file:

```
gcloud monitoring dashboards create --config-from-file=service-dashboard.yaml
```

You can then view the dashboard by selecting it in the Google Cloud console:

```
open 'https://console.cloud.google.com/monitoring/dashboards?project='$MANAGEMENT_PROJECT_ID
```

There is also a trick you can use in the Google Cloud console to create a dashboard and then export it to YAML.

In the Google Cloud console, note the dashboard ID in the URL and save it in an environment together with the number of the management project. For example, in the URL `https://console.cloud.google.com/monitoring/dashboards/builder/7fdd430f-4fa1-4f43-aea6-5f8bd9b865df` the dashboard ID will be `7fdd430f-​4fa1-4f43-aea6-5f8bd9b865df`:

```
export DASHBOARD_ID=[DASHBOARD_ID]
```

Also, capture the project number of the management project:

```
export MANAGEMENT_PROJECT_NUMBER \
=$(gcloud projects describe $MANAGEMENT_PROJECT_ID --format='value(projectNumber)')
```

Then use the following command to export the dashboard to YAML:

```
gcloud alpha monitoring dashboards describe \
projects/$MANAGEMENT_PROJECT_NUMBER/dashboards/$DASHBOARD_ID > dashboard.yaml
```

You will then have a _dashboard.yaml_ file to customize as you like. This gives you a head start in creating your own dashboards.

### Logging

Google Cloud Logging is a centralized repository where logs and events from your applications and services are automatically collected, stored, and made available for analysis. As discussed in previous chapters, when you access the logs, you’ll see a vast collection of logs from every service within the project by default. Given the massive scale of logs that could be generated, especially in large projects, filtering becomes an essential tool.

Log filters are powerful tools that help you focus on the specific logs you’re interested in. Think of them as search criteria that can be customized to your needs.

The [Google Cloud Logging query language (LQL)](https://oreil.ly/Tn9Jy) is the language you’d use to define these filters. LQL provides a versatile syntax that can be used to write expressions that match specific log entries. The documentation provides more details, and you can dive deeper into the LQL by following the link.

An essential concept when dealing with filters is labels. Labels are like tags, key-value pairs that can be associated with various resources to categorize or mark them. These labels are immensely beneficial when you want to create specific filters. For instance, Cloud Run, the serverless compute platform by Google Cloud, automatically labels each log with the associated service’s name.

So, if you’re trying to focus solely on logs from the skill service, you can utilize this automatic labeling. The filter to accomplish this would look something like:

```
export FILTER=resource.labels.service_name=skill-service
```

Set your project back to the project your application is running in:

```
gcloud config set project $MONITORED_PROJECT_ID
```

You can then use the filter with the `gcloud logging read` command to view the logs for the skill service. The limit is set to 10 to limit the number of log entries returned:

```
gcloud logging read $FILTER --limit=10
```

This will show just the last 10 logs for the skill service.

Then narrow it down further by matching the log entry that contains the text `loaded` and `tags`:

```
export FILTER='resource.labels.service_name=skill-service AND jsonPayload.message=~"loaded.*tags"'
```

The logs returned are structured logs, so you will see the full log messages as YAML. The actual log messages in this case are in `jsonPayload.message`. You can use the `--format` option to extract just the log messages:

```
gcloud logging read $FILTER --limit=10 --format='value(jsonPayload.message)'
```

For example, with the filter above, you will see the following if the service had started twice:

```
loaded 63653 tags
loaded 63653 tags
```

Google Cloud Logging is a comprehensive system that automatically consolidates, stores, and allows analysis of logs from all your applications and services. However, given the potentially massive scale of logs produced, particularly in larger projects, filtering these logs becomes paramount.

Creating filters through Google Cloud LQL empowers you to concentrate on specific logs of interest. These filters act as tailored search parameters, taking into account labels—key-value pairs tagged to different resources. This becomes particularly helpful in scenarios like focusing solely on logs from the skill service in a Cloud Run setup.

### Log-Based Metrics

While Google Cloud offers many preconfigured metrics to monitor various aspects of your services, there will be times that you need more granularity or a focus on specific events. In such instances, Google Cloud provides the flexibility to define custom metrics. These can be broadly categorized into two types: log-based metrics and user-defined metrics.

Log-based metrics are derived directly from the logs your application generates. They allow you to quantify and monitor particular events or patterns that appear in your logs. For instance, as you just saw, each time the skill service initializes, it logs the number of tags loaded from the file stored in Google Cloud Storage. This action is logged, noting the exact count of tags loaded. With log-based metrics, you can create a custom metric that counts the number of times the tags are retrieved. You can also create another that monitors the number of tags loading, allowing you to notice if this number changes. If this number is significantly lower than the average, it is likely there is an issue that needs to be investigated.

#### Creating a counter metric from logs

As you just saw, there is a log entry created each time the skill service starts, logging the number of tags loaded from Cloud Storage. Similarly, there is a log entry created each time a request for a tag suggestion has been processed. Here is an example:

```
autocomplete for java took 84.714804ms
```

A custom counter metric will enable you to record the number of suggestions by all instances of the skill service. You cannot create this type of metric with a gcloud command alone; you need to define it in a file and then use the file to create the metric.

In the provided sample code, there’s a template file named _templates/tag\_suggestion\_count.json.template_ that serves as the blueprint for this metric. Use `envsubst` to create a JSON file from it, substituting in the monitored project ID:

```
envsubst < templates/tag_suggestion_count.json.template > tag_suggestion_count.json
```

Make sure that your current project is the monitored project:

```
gcloud config set project $MONITORED_PROJECT_ID
```

Then you can create the metric using this command:

```
gcloud logging metrics create tag_suggestion_count \
  --config-from-file=tag_suggestion_count.json
```

You have now created a user-defined, log-based counter metric. However, you will need to add it to a dashboard to see it. Before that, create another type of metric to show with it.

#### Creating a distribution metric from logs

It would also be useful to extract the time it took to make the suggestion from the log entry itself. This is possible with a distribution metric.

Instead of just counting the number of logs, you can extract the value from the log entry with a regular expression. Again, you will need to define it in a file and then use the file to create the metric with gcloud.

The file _templates/tag\_suggestion\_times.json.template_ defines the metric. It is similar to the file used to create the counter metric. The main difference is the use of a regular expression to match the log entry and extract the time it took for the suggestion:

```
"REGEXP_EXTRACT(jsonPayload.message, \"took\\\\s(\\\\d+\\\\.\\\\d+)\")"
```

Again, use `envsubst` to create a JSON file from it, substituting in the monitored project ID.

```
envsubst < templates/tag_suggestion_times.json.template > tag_suggestion_times.json
```

You can then create the metric with the following command:

```
gcloud logging metrics create tag_suggestion_times \
--config-from-file=tag_suggestion_times.json
```

To list the metrics, use this command:

```
gcloud logging metrics list
```

Note that metric data will only be collected from the point the metric was created. You will not see metrics from past logs. Therefore, at this point, generate some queries by calling the skill service using Apache Bench:

```
ab -n 100 -c 1 -rk "https://${DOMAIN}/api/skills/autocomplete?prefix=java"
```

These metrics will then be available to add to a dashboard, and there will be some data to display.

#### Creating a dashboard with user-defined metrics

To add the metrics to a dashboard, you will need to create a configuration file. The file _templates/tag-dashboard.yaml.template_ is a template for the dashboard configuration file. It contains placeholders for the project ID and service name.

Make sure that your current project is the management project:

```
gcloud config set project $MANAGEMENT_PROJECT_ID
```

```
envsubst < templates/tag-dashboard.yaml.template > tag-dashboard.yaml
```

Now create a dashboard from the configuration file:

```
gcloud monitoring dashboards create --config-from-file=tag-dashboard.yaml
```

Go to the Google Cloud console using this URL:

```
open 'https://console.cloud.google.com/monitoring/dashboards?project='$MANAGEMENT_PROJECT_ID
```

You will see a new dashboard named “skills-service performance.” Open this and you will see two charts, one visualizing the number of suggestions made per minute and the other showing the time taken to respond to the 50% percentile of requests.

### Alerts

With metrics in place and an understanding of what is normal, you can create alerts to notify you when something appears wrong. With my personal metrics, if my heart rate suddenly goes to 200 bpm, that is not normal for me and I would want to know. Similarly, if the number of tags loaded is less than 60,000, instead of the normal number of around 63,000, that is not normal for the skill service. If 90% of responses to requests are taking over 1 second, that is also not normal, and it does not quickly resolve itself; it is a problem that needs investigation. This is what alerts are for.

#### Create the metric for an alert

For example, you could create an alert that triggers when the number of tags loaded is less than 60,000, as the skill service would be working, but since the number of tags is normally around 63,000, it would indicate a problem. The log message looks like this:

```
"loaded 63653 tags"
```

Again, in the provided sample code, there’s a template file named _templates/⁠tags​\_loaded\_number.json.template_ that extracts the number from the log message. Create a JSON file from it, substituting in the monitored project ID.

```
envsubst < templates/tags_loaded_number.json.template > tags_loaded_number.json
```

Make sure that your current project is the monitored project:

```
gcloud config set project $MONITORED_PROJECT_ID
```

Then you can create the metric using this command:

```
gcloud logging metrics create tags_loaded_number \
  --config-from-file=tags_loaded_number.json
```

#### Create a notification channel for alerts

Before you can create an alert, you need a channel to be notified on. Google Cloud supports many channel types, but in this case, you will be creating a channel for notification by email. Again, the configuration is defined in YAML and then applied with gcloud. In the provided example code, there is a file _templates/email\_notifica⁠tion​\_channel.yaml.template_. This contains a placeholder for an email address.

Set the environment variable `NOTIFICATION_EMAIL_ADDRESS` to an email address where you would like to receive notifications:

```
export NOTIFICATION_EMAIL_ADDRESS=['NOTIFICATION_EMAIL_ADDRESS']
```

Now use `envsubst` to create a YAML file from the template:

```
envsubst < templates/email_notification_channel.yaml.template > email_notification_channel.yaml
```

Make sure you are in the management project:

```
gcloud config set project $MANAGEMENT_PROJECT_ID
```

Then create the notification channel from the configuration using gcloud:

```
gcloud alpha monitoring channels create --channel-content-from-file=email_notification_channel.yaml
```

This will create a channel with a unique ID. You will see this ID returned from the command. You can also you this command to set it to an environment variable, as long as there are not any other channels created already:

```
export NOTIFICATION_CHANNEL_ID=$(gcloud alpha monitoring channels list \
--format="value(name.split('/').slice(-1))")
```

You are now ready to create the alert policy.

#### Create an alerting policy

Once again, creating an alert policy is a case of defining the configuration in YAML and then applying it with gcloud. In the provided example code, there is a file _templates/tags\_loaded\_number\_alert.yaml.template_. This contains placeholders for the project ID, metric name, and notification channel ID. The alert specifies that if the number of tags loaded is less than 60,000 then an alert should be triggered:

Use `envsubst` to create a YAML file from the template:

```
envsubst < templates/tags_loaded_number_alert.json.template > tags_loaded_number_alert.json
```

Create the alerting policy using the following command:

```
gcloud alpha monitoring policies create --policy-from-file=tags_loaded_number_alert.json
```

This will create the policy. Now if the number of tags loaded is ever below 60,000, you will receive a notification at the email you provided. You could test an alert by replacing the _tags.csv_ file the skill service uses with one with less than 60,000 tags.

When the skill service is next loaded, the alert will be triggered, raise an incident, and send an email that will look like [Figure 13-2](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch13.html#alert-email). It also contains a link to the incident on Google Cloud and a description provided in the configuration to explain the alert and provide troubleshooting tips.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_1302.png" alt="Alert Email" height="1308" width="2492"><figcaption></figcaption></figure>

**Figure 13-2. Alert email**

In this case, the next time the tag updater runs on its weekly schedule, the _tags.csv_ file will be updated. When the skill service next starts, it will load over 60,000 tags again, and the incident will be resolved.

### User-Defined Metrics and Tracing

Rather than generating metrics by scraping logs, you can also create a custom metric from the application. Google provides a client library for many languages that you can use to create metrics from your application: [OpenTelemetry](https://oreil.ly/TEoFI).

OpenTelemetry is a set of open source tools for collecting telemetry data from your application and sending it to a monitoring system. At the time of writing, it is a CNCF incubating project and was formed by merging two earlier projects: OpenTracing for tracing and OpenCensus for metrics.

Once you use a solution like OpenTelemetry, you are treating traces, metrics, and logs as first-class citizens and designing them as part of your application rather than retrospectively by an operations team. This is a very cloud native approach and fits in well with SRE practices. As developers are thinking about the data, they will need to resolve problems, debug issues, and be able to design better applications.

As OpenTelemetry is a CNCF project, it does not lock you into Google Cloud. OpenTelemetry is supported by all the major cloud providers and many other products, so the work you do to instrument your application will be portable across providers. It supports many languages and frameworks, so you can use it with your existing applications.

At the moment, however, it is still in beta, and the documentation is not complete. The following sections are based on my experience of using it with the skill service. By the time you read this book, the official Google Cloud documentation is likely to have improved.

### Tracing

The skill service receives a request, performs a lookup, and returns the results. From the outside, this is the latency of a single request, but internally it is made up of several steps. You can use tracing to see the latency of each step and to understand where the time is spent.

A trace describes how the system responds to a request. A trace is made up of spans, with a span representing a step to responding to the request. These concepts were first proposed in a paper on [Dapper](https://oreil.ly/bdIZu), Google’s internal distributed tracing system. By adding trace and span IDs to log messages, Google Cloud Trace and Google Cloud Logging will then be able to interpret the traces and display them in a dashboard. Traces are also useful for requests that span multiple services, as you can see the latency of each service as well as the individual step.

In the skill service, the trace will be at the request level where the prefix is submitted in the HTTP GET request, and the result will be returned.

While Google supplied a client library for tracing, it is now deprecated, and it is recommended to use OpenTelemetry instead.

### Adding Trace and Span IDs to Logs

By default, all requests that are made to a Cloud Run HTTP endpoint already have tracing enabled, so you should already see traces in the Google Cloud Trace console.

For example, [Figure 13-3](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch13.html#cloud-trace-1) is a trace to the skill service. You can see there is a single span for the request, as no span has been specified. The overall request takes 4.6 seconds, but the actual processing time in the skill service is only 1.6 seconds. Here, the latency is due to the time taken to start the container and load the tags from Google Cloud Storage, as the skill service has not been used for a while and needs to cold start.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_1303.png" alt="Cloud Trace" height="104" width="713"><figcaption></figcaption></figure>

**Figure 13-3. Google Cloud Trace**

A trace ID is set in the `X-Cloud-Trace-Context` header for each request to enable tracking.

However, the default trace could be more useful, as currently it only shows the request to the skill service and not the internal steps. The skill service is already using the OpenTelemetry client library to generate traces. However, the service account needs to be granted the `cloudtrace.agent` role to be able to write traces to Google Cloud Trace.

In the skill service project, add the `cloudtrace.agent` role to the service account:

```
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member=serviceAccount:$SKILL_SERVICE_SA@$PROJECT_ID.iam.gserviceaccount.com \
  --role=roles/cloudtrace.agent
```

You can now see the traces in the Google Cloud Trace console in [Figure 13-4](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch13.html#cloud-trace-2). This gives an example of how traces can drill down into the code to show you where the time is spent. However, nearly all the time spent on the `autocomplete` function is in the `search` span, and this makes sense. You can see that the surrounding code that is not in the `search` span is not taking much time.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_1304.png" alt="Cloud Trace" height="74" width="714"><figcaption></figcaption></figure>

**Figure 13-4. Google Cloud Trace with spans**

The beauty of traces is that you can keep adding spans to drill down into the code to see where the time is spent. However, there is a lot more in the OpenTelemetry client documentation about how to build on this.

## How Much Does It Cost?

The ingestion of the first 50 GiB of logs per month for a project is free; after that, it is $0.50/GiB, which includes 30 days of storage, (the default). You can increase the period for logs, but this will incur a storage cost.

There is also a free allotment of monitoring data per month, but charges apply after the first 150 MiB is ingested per billing account. Log-based metrics are also chargeable. This is all as of the time of writing. It is a good idea to keep an eye on the [pricing page](https://oreil.ly/-8RTP) for changes, as when working at scale, these costs can add up once you have a lot of services and therefore a lot of data.

## Summary

You have just dipped your toe into observability. As you may appreciate by now, it is a huge topic, and there is a lot more to learn. However, you have now seen how you can use the Cloud Logging and Cloud Monitoring services to capture logs and metrics and display them in dashboards. You have also seen how you can use alerts to notify you when something goes wrong.

To create this facility, you used the following services directly:

* [Cloud Logging](https://oreil.ly/Zllr1) is used for capturing and aggregating logs in one place and making them searchable.
* [Cloud Monitoring](https://oreil.ly/U-f43) is used for collecting and displaying metrics and creating alerts.
* [OpenTelemetry](https://oreil.ly/gSjsX) is used for creating metrics and traces from your application to use in Cloud Monitoring and Cloud Trace.

It has been said, “You cannot improve what you cannot measure,” a foundational principle that informed decisions rely on tangible data and observations. Without a clear understanding of a situation, making effective improvements becomes a shot in the dark. In [Chapter 14](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch14.html#chapter\_14), you’ll look at the scalability of the system. Equipped with the right metrics, you can pinpoint inefficiencies, make informed choices, and optimize performance for better outcomes. You will learn about some of the options you have when scalability becomes an issue.
