# 12. Factory

## Chapter 12. Factory

In previous chapters, you have been taking a largely manual approach to deploying both infrastructure and services. To be fair, leveraging the automation in the gcloud CLI and Cloud Run, in particular, has helped a lot, but it is still a manual process.

In [Chapter 11](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch11.html#chapter\_11), where you built the citadel (in particular, when configuring the API Gateway and Global HTTP Load Balancer), there was a lot of manual configuration and typing of commands. It was repetitive and error-prone, and if you wanted to deploy the application again, you would have to repeat the process.

## Automating Build, Deployment, and Provisioning

This is a waste of time when you could be getting on with developing more features rather than fiddling with the command line. This type of work is known as “toil.” Toil is bad; toil should be eliminated when you see it. Toil is a sign that you need to automate something. When I first learned about software development at school, I was taught that when you have to do something more than twice, you should write a program to do it for you. This is one of those times that advice applies.

In this chapter, you will look at how you can use Google Cloud to automate away the toil you have been increasingly experiencing in previous chapters. You will also learn how this can be done for provisioning the infrastructure too.

**NOTE**

The code for this chapter is in the [`factory` folder of the GitHub repository](https://oreil.ly/fbB1L).

### Requirements

You can think of what you are doing in this chapter as building a washing machine factory, the sort of place Sir John Harvey-Jones from [Chapter 1](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch01.html#chapter\_01) would visit. There are two things the factory produces:

Applications

The containerized services themselves. These can just be deployed in the citadel with everything in place they need. These are like the washing machine; you will create a pipeline that builds them and gets them ready to be delivered and plumbed in.

Infrastructure

Washing machines need infrastructure to run. Connections to electricity and water and a space under the worktop in a kitchen or utility room (if you are British like me). With the infrastructure in place, the washing machine can just be delivered, connected to water, plugged into electricity, and away you go. Not only can the factory make the washing machine, it can also make the pipes and cables it needs too.

When you are building a factory, you want it to provide the following, as shown in [Figure 12-1](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch12.html#factory):

* Somewhere to store source code like a silo for raw materials
* An automated build pipeline that acts like a robotic assembly line
* Somewhere to store the finished products; in this case, containers, like a warehouse
* An automated deployment mechanism like a truck with an installation engineer

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_1201.png" alt="The factory" height="373" width="1436"><figcaption></figcaption></figure>

**Figure 12-1. The factory**

### The Outer Loop

In [Chapter 10](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch10.html#chapter\_10), you learned about the laboratory and how you could automate the process of build, deploy, and test for applications on your local machine to provide rapid feedback. This is known as the inner loop.

The _factory_ is the automated process that builds and deploys the application to the cloud, as shown in [Figure 12-1](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch12.html#factory). The factory is the outer loop (see [Figure 12-2](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch12.html#outer-loop)). It is made up of two parts, the build pipeline (the assembly line) and the deployment pipeline (the delivery truck and installation engineer).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_1202.png" alt="Outer Loop" height="1172" width="1424"><figcaption></figcaption></figure>

**Figure 12-2. Outer loop**

The build pipeline takes the raw materials and the source code and turns it into a containerized service. This means that whenever a developer commits a change to the source code repository, the build pipeline will automatically build, test, and store the containerized service.

The test suite will typically include all unit tests and, usually, integration tests. This is known as continuous integration ([Figure 12-3](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch12.html#continuous-integration)). Building applications in this way means that you can get rapid feedback on the quality of the code and the impact of any changes; however, hopefully, you would have picked up most issues earlier by testing in the inner loop in the laboratory.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_1203.png" alt="Continuous Integration" height="294" width="1128"><figcaption></figcaption></figure>

**Figure 12-3. Continuous integration**

By building and testing in a consistent and automated way, you can be confident that the application will work as expected when deployed. By adding security controls and checks on dependencies and the build process, you can also be confident that the application will be secure and reliable. This is known as securing the software supply chain and is discussed in depth at Supply-Chain Levels for Software Artifacts, or [SLSA](https://slsa.dev/).

This is similar to the goal of a real factory—to reliably produce a product of consistent quality.

The deployment pipeline takes the containerized service and deploys it to the cloud. This is known as continuous deployment if deployment to a production deployment is fully automated, as shown in [Figure 12-4](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch12.html#continuous-deployment). If deployment is optional or deployment is only automatic to test environments, it is known as continuous delivery.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_1204.png" alt="continuous deployment" height="294" width="1427"><figcaption></figcaption></figure>

**Figure 12-4. Continuous integration and deployment (CI/CD)**

**TIP**

In reality, continuous deployment to production is not always possible. For example, if you are deploying to a regulated environment, you may need to have a manual approval step. This is still continuous delivery but not continuous deployment. However, if the automated tests in your pipeline are comprehensive enough, you can be confident that the application will work as expected when deployed; therefore, continuous deployment is a useful tool.

Think of this again like buying a new washing machine. The machine is built and tested at a factory and stored in a warehouse. It is then delivered, and you have the choice to have it installed (deployed) at the same time or do it yourself later.

### DORA Metrics

DevOps Research and Assessment (DORA) is an organization that researches the benefits organizations get from DevOps principles and practices like the ones discussed here.

From their research, DORA has identified four metrics that high-performers score well against for delivery and operational performance. They suggest using these metrics as goals for an aspirational organization. These metrics are:

Lead time for changes

The amount of time it takes a commit to get into production; that is, how long it takes to go through the inner and outer loops. This includes both the development time (how long it takes to code the feature) and the wait time in various stages of the CI/CD pipeline.

Deployment frequency

How often an organization successfully releases to production. This could be multiple deployments per day for high-performing organizations.

Change failure rate

The percentage of deployments causing a failure in production. This could be defined as a service impairment or service outage. The key is to track when a failure requires remediation (like a hotfix, rollback, fix forward, or patch).

Time to restore serice

How long it takes an organization to recover from a failure in production. This could be the time it takes to correct a defect or to restore service during a catastrophic event (like a site outage).

These four metrics provide a balanced view of not only how fast and frequently an organization delivers software efficiently but also how effectively they respond to problems in production. They are based on years of research and have been shown to be predictive of software delivery performance and organizational performance. The type of automation provided by the factory is the enabler of these metrics. The message is that just because automation is in place does not mean an organization will perform well, but organizations that do perform well are likely to have a factory in place.

For more information on DORA metrics, you can refer to the book _Accelerate: The Science of Lean Software and DevOps_ (IT Revolution Press) by Nicole Forsgren, Jez Humble, and Gene Kim, which discusses these topics in detail.

### Canary Releases

When deploying to production, you may want to do a canary release. This is where you deploy the new version of the application alongside the old version and gradually increase the traffic to the new version. This allows you to test the new version in production and roll back if there are any issues.

For example, you could choose to deploy the new version of a service to only 5% of users initially and then monitor the error rate and performance. If everything is working as expected, you can gradually increase the percentage of users until all users are using the new version. In [Chapter 13](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch13.html#chapter\_13) on observability and monitoring, you will look at how you can monitor the performance of your application and use this to make decisions about when to roll out new versions.

### Summary of Services

To build this facility, you will use the following services:

Source repository—GitHub

The source code is the raw materials and blueprints for building containerized services. As you would expect, these are stored in a source code repository. Google Cloud has its own source code repository, [Google Source Repositories](https://oreil.ly/cRKc7), which is Git-based. But for many people, GitHub will be the first choice. Fortunately, there are mechanisms for connecting to other Git providers in the Google Cloud ecosystem.

These include mirroring the GitHub repositories in Google Source Repositories, but you can also connect the build pipeline directly to several Git service providers, including GitHub and BitBucket. These connections can only be created via the console on Cloud Build (1st gen) but can be connected programmatically with the 2nd gen. Therefore, you will be using 2nd gen.

Build automation—Cloud Build

Cloud Build is the mechanism for creating a continuous integration pipeline, the automated assembly line for building and testing containerized services to store, ready for deployment. You have come across Cloud Build before, but this time you will be automating how it is triggered rather than using Cloud Run.

Container storage—Artifact Registry

When containers are assembled, they will automatically be stored in Artifact Repository until the automated deployment is ready for them. Think of Artifact Repository as the warehouse storing the containers ready for delivery. At this point, you can also publish a message saying a new container is available.

Deployment automation—Cloud Build or Cloud Deploy

When a container is ready for deployment, you need something to pick it up and deploy it into the citadel. In this chapter you will use Cloud Build again, but for more complex deloyments Cloud Deploy is also an option. Think of it as a skilled installer with a truck delivering and installing the new washing machine.

### Implementation

You are now going to use the completed SkillsMapper code that accompanies this book as an example. The code is available on GitHub at [SkillsMapper Repo](https://oreil.ly/jbVj0). If you have not already done so, you should fork the repository into your own GitHub account if you want to make any changes, but you can also use the original repository if you prefer, as it is public.

#### Creating the factory

The first step is to create the “factory,” and for this, you will create a separate project from the one that is used for running the applications, the “citadel.” All automation will be in a separate project from the applications themselves. Refer back to [Chapter 4](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch04.html#chapter\_04) if you need a reminder of how to create a project.

A suggested naming convention is to use the same name as the citadel project but with `-management` appended. For example, if the citadel project is `skillsmapper` the management project would be `skillsmapper-management`.

Having a separate project for management is a good idea for several reasons. It allows you to:

* Separate the billing and budgeting for the citadel and the management infrastructure.
* Give different people access to the citadel and the management infrastructure.
* Delete the management infrastructure without affecting the citadel.
* Have multiple citadels for different environments (e.g., `skillsmapper-development`, `skillsmapper-test`, `skillsmapper-production`) and a single management project.

For the rest of this chapter, we will assume that the management project is called `skillsmapper-management` and refer to it as the management project, and the citadel project is a citadel project named `skillsmapper-development` and refer to it as the target project.

Create two environment variables for referring to these projects:

```
export MANAGEMENT_PROJECT_ID=[MANAGEMENT_PROJECT_ID]
export TARGET_PROJECT_ID=[TARGET_PROJECT_ID]
```

#### Connecting to the source code repository

When you have a management project, you can start to build the factory. Remember to switch your gcloud context to the management project using:

```
gcloud config set project $MANAGEMENT_PROJECT_ID
```

When in the management project, you will need to connect to the repository to be able to build the application. This is done using a Cloud Build connection. As is good practice, GitHub’s credentials for the connection are stored in Secret Manager. As you are in a new project, you will need to first enable the services you are going to use, in this case, Cloud Build and Secret Manager:

```
gcloud services enable cloudbuild.googleapis.com
gcloud services enable secretmanager.googleapis.com
```

Next, permit Cloud Build to create secrets. The Cloud Build P4SA service account is a built-in product service account with a name in the format `service-⁠[PROJECT​_NUMBER]@gcp-sa-cloudbuild.iam.gserviceaccount.com`. Grant the P4SA service account permission to create secrets in Secret Manager using the following command:

```
gcloud projects add-iam-policy-binding $MANAGEMENT_PROJECT_ID \
--member='serviceAccount:service-'$(gcloud projects describe $MANAGEMENT_PROJECT_ID
--format='value(projectNumber)')'@gcp-sa-cloudbuild.iam.gserviceaccount.com' \
--role='roles/secretmanager.admin'
```

Create an environment variable to store a connection name (e.g., `skillsmapper-github-connection`):

```
export CONNECTION_NAME=[CONNECTION_NAME]
```

Then create the connection:

```
gcloud alpha builds connections create github $CONNECTION_NAME --region=$REGION
```

**TIP**

At the time of writing, this functionality is still in preview, hence the use of the `alpha` command. The syntax may have changed by the time you read this.

This will give you a link that will open your browser and guide you through connecting to your GitHub account. When completed, check that the connection has been created successfully with this command:

```
gcloud alpha builds connections describe $CONNECTION_NAME --region=$REGION
```

Look for the value of `installationState`; the stage should show the value `COMPLETE`.

Now you can connect to the SkillsMapper GitHub repository. Set the `REPO_NAME` and `REPO_URI` as follows unless you are using your own fork, in which case you should use your own repository name and URI:

```
export REPO_NAME=skillsmapper
export REPO_URI=https://github.com/SkillsMapper/skillsmapper.git
```

Then run the following command to connect to the repository:

```
gcloud alpha builds repositories create $REPO_NAME --remote-uri=$REPO_URI \
--connection=$CONNECTION_NAME --region=$REGION
```

When this command completes with the `Created repository [SkillsMapper]` message, you will have a repository connection to use with Cloud Build as the source repository. These are the raw materials for the automated assembly line. You can check it using this command:

```
gcloud alpha builds repositories list --connection=$CONNECTION_NAME --region=$REGION
```

You will see the repository listed with the name you specified, mapping to the URI of the GitHub repository.

#### Creating a container repository

Next, you need to create a container repository to store the containers that are built by the assembly line.

Create an environment variable to store the name of the container repository:

```
export CONTAINER_REPO='skillsmapper'
```

Now use the following command to create the container repository:

```
gcloud artifacts repositories create $CONTAINER_REPO --repository-format=docker --location=$REGION
```

This will create a new container repository named `skillsmapper` to store the Docker format containers Cloud Build will create. You can check using the following command:

```
gcloud artifacts repositories list --location=$REGION
```

Notice `ENCRYPTION` is set to `Google-managed key`. Like most Google Cloud services, Artifact Repository encrypts data at rest by default.

#### Implementing continuous integration with Cloud Build

In previous chapters, you have been using Cloud Build behind the scenes whether you realized it or not. Every time you used `gcloud run deploy` or `gcloud functions deploy` you have been triggering a Cloud Build pipeline in the background using your local source code as the source.

Now you are going to use it to automatically trigger a build whenever there is a commit to the main branch of the GitHub repository. You will use the skill service as an example, but it will be a similar process to the other services. The build configuration will differ slightly for the fact service and the skill service, for example, as it is a Java application rather than Go. Cloud Build is very flexible and can be configured to build almost any type of application.

First, define some environment variables:

```
export TRIGGER_NAME='skill-service-trigger'
export BRANCH_PATTERN='^main$'
export BUILD_CONFIG_FILE='skill-service/cloudbuild.yaml'
```

This is setting the name of the trigger to `skill-service-trigger`, the branch pattern to `^main$` (i.e., only trigger when there is a commit to the main branch), and the build config file to a file in the skill service directory _skill-service/cloudbuild.yaml_.

The single SkillsMapper repository contains multiple services. What you don’t want is for all services to be rebuilt and redeployed when there is a commit to the main branch. To only build and deploy the service that has changed, you can restrict the files that trigger a build with the `included-file` parameter. In the next example, only changes to files in the _skill-service_ directory will trigger a build.

Then use the following command to create the trigger. Again, note this is a beta command at the time of writing:

```
gcloud beta builds triggers create github \
  --name=$TRIGGER_NAME \
  --repository=projects/$MANAGEMENT_PROJECT_ID/locations/$REGION/connections
  /$CONNECTION_NAME/repositories/$REPO_NAME \
  --branch-pattern=$BRANCH_PATTERN \
  --build-config=$BUILD_CONFIG_FILE \
  --region=$REGION \
  --included-files="${SKILL_SERVICE_NAME}/**" \
  --substitutions=_REPOSITORY=$CONTAINER_REPO,_REGION=$REGION,
  _SERVICE_NAME=$SKILL_SERVICE_NAME,_IMAGE_NAME=$SKILL_SERVICE_IMAGE_NAME
```

This will create a trigger that will build the skill service whenever there is a commit to the main branch of the GitHub repository. You can check that the trigger has been created successfully with the following command:

```
gcloud beta builds triggers list --region=$REGION
```

You should see the trigger listed with the name you gave it. The trigger runs on git commit, but you can also run it manually with the following command:

```
gcloud beta builds triggers run $TRIGGER_NAME \
  --region=$REGION \
  --branch=main
```

This will run the trigger on the main branch. The message returned will include a link to the build log in the `metadata.build.logUrl` field, which you can open in your browser to see progress.

**TIP**

To speed up the build process by excluding files that are not needed for the build process, you can create a `.gcloudignore` in the root of the repository to exclude files. This works in a similar way as a _.gitignore_ file. For example, you can exclude all markdown files like _README.md_ files with the following entry in the _.gcloudignore_ file: `*/*.md`.

Now let’s look at the details of the Cloud Build configuration file to understand what the build pipeline is doing.

#### Understanding Cloud Build configurations

The Cloud Build configuration is a YAML file, in this case, the _cloudbuild.yaml_ file in the _skill-service_ directory.

It is made up of one or more build steps. The build process starts with a workspace containing a fresh clone of the GitHub repository for each execution and then runs each step in turn.

In the case of the skill service, it is a Go application and there are multiple steps. Each step in the configuration file uses a container image and runs a specific commands as shown in [Table 12-1](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch12.html#build-steps).

| Step                  | Container                       | Command             | Description                                                                |
| --------------------- | ------------------------------- | ------------------- | -------------------------------------------------------------------------- |
| Go version            | `gcr.io/cloud-builders/go:1.20` | `go version`        | Check the version of Go as a smoke test                                    |
| Download dependencies | `cloud-builders/go:1.20`        | `go mod` `download` | Download dependencies specified in the _go.mod_ file using Go version 1.20 |
| Run linting checks    | `golangci/golangci-lint`        | `golangci-lint run` | Run a selection of Go linters to check the code                            |
| Run unit tests        | `gcr.io/cloud-builders/go:1.20` | `go test`           | Run all unit tests using Go version 1.20                                   |
| Run security checkss  | `securego/gosec`                | `gosec`             | Run security checks on the code                                            |
| Build container image | `k8s-skaffold/pack`             | `pack`              | Build the container image using Buildpacks                                 |
| Push container image  | `cloud-builders/docker`         | `push`              | Push the container to artifact repository                                  |

This is a simple example, but there are many more steps you can add, such as running integration tests and building documentation.

You can use any container image you like, including your own custom images, as long as the Cloud Build service account can access them. Once you have set up steps in the configuration file, they will run automatically when the trigger is fired. You can be confident that the code is always built in the same way, and if you have a step to update your documentation, for example, it will always be up to date.

Cloud Build configurations support templating with substitutions of variables at runtime. You will see these being used a lot in the example configuration. Whenever you run a Cloud Build job, you have access to the following build-in variables:

`$PROJECT_ID`

The ID of the project running the build.

`$BUILD_ID`

The ID of the build.

`$PROJECT_NUMBER`

The number of the project running the build.

`$LOCATION`

The region associated with the build. This can be `global` if the build is not associated with a specific region.

These are provided automatically and cannot be overwritten.

There is also a set of built-in variables that are populated when the build is run from a trigger on Google Cloud. An example in use in this configuration is `$COMMIT_SHA`, as Cloud Build can access the Git repository and retrieve the hash of the commit in use.

This is useful as you can use it to tag the container and tie the container to the commit of the source code for debugging and tracing issues.

**TIP**

Note that Artifact Registry supports tag immutability, meaning you cannot overwrite a tag once it has been created. If you try to push a container with the same tag as an existing container, for example `latest`, you will get an error. This is a security measure to prevent malicious actors from pushing a container with the same tag as a trusted container.

You can also use pass variables using the same substitution mechanism, but these must start with an underscore (e.g., `_IMAGE_NAME`) to distinguish them from built-in substitutions.

#### Testing a build with local code

What you are aiming for is to trigger a build from a commit to the GitHub repo. However, you can trigger the build manually. The build will still run on Google Cloud but will be using your local source code as input. There used to be the option of running a build locally using a Cloud Build Emulator, but this has been deprecated.

When you run using local code, the `$COMMIT_SHA` and other similar variables will not be set automatically. Instead, you can pass them in using the `substitutions` flag. This is the same mechanism for providing custom substitutions used in the build configuration file.

Here is the command to trigger the build locally from the root of the project with substitutions:

```
gcloud builds submit --config $SKILL_SERVICE_NAME/cloudbuild.yaml . \
  --substitutions=REPOSITORY=$CONTAINER_REPO,_REGION=$REGION,
_SERVICE_NAME=$SKILL_SERVICE_NAME,_IMAGE_NAME=$SKILL_SERVICE_IMAGE_NAME,
COMMIT_SHA=$(git rev-parse HEAD)
```

Here `_REPOSITORY`, `_REGION`, `_SERVICE_NAME` and `_IMAGE_NAME` are custom substitution variables and `COMMIT_SHA` is overriding a built-in variable. Running a build locally in this way is useful for testing that your _cloudbuild.yaml_ is doing what you expect.

#### Adding continuous deployment to the Cloud Build pipeline

At the moment, the pipeline will build the container and push it to Artifact Registry. Although a new container is ready, it will not deploy to Cloud Run. The next steps handle that deployment, taking the pipeline from a continuous integration (CI) pipeline to a continuous deployment (CD) pipeline.

The final step in the pipeline is to deploy the container to Cloud Run. The step that does this from Cloud Build simply uses a container that includes the gcloud CLI `cloud-builders/gcloud` and then uses the same `gcloud run deploy` command you used to deploy manually in previous chapters.

As the deployment to Cloud Run will be to a different project than the one running the build, you will need to pass the `--project` flag to the `gcloud run deploy` command.

When deploying using Cloud Run, you have been passing a _.env.yaml_ file with environment variables used by the service. However, as this may contain sensitive information, it will not be in the GitHub repo, so you can instead pass each environment variable as a substitution to the build. The full build step will look like this and is included in _cloudbuild.yaml_:

```
  - id: 'Deploy to Cloud Run'
    name: 'gcr.io/cloud-builders/gcloud:latest'
    entrypoint: /bin/bash
    args:
      - '-c'
      - |
        gcloud run deploy ${_SERVICE_NAME} \
          --image ${_REGION}-docker.pkg
          .dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE_NAME}:$COMMIT_SHA \
          --project ${_TARGET_PROJECT_ID} \

          --region ${_REGION} \
          --update-env-vars PROJECT_ID=${_TARGET_PROJECT_ID}, \
            BUCKET_NAME=${_TARGET_PROJECT_ID}-tags,OBJECT_NAME=tags.csv, \
            SERVICE_NAME=${_SERVICE_NAME}
```

You will also need to use the `TARGET_PROJECT_ID` environment variable with the ID of the project you want to deploy to. This is the project you were working with in previous chapters, for example `skillsmapper-development`.

As the build will be running in the management prod project, you will need to grant to the Cloud Build service account in the management project the `Cloud Run Admin` role in the destination project.

First, get the email address of the Cloud Build service account in the management project:

```
export CLOUDBUILD_SA_EMAIL=$(gcloud projects describe $MANAGEMENT_PROJECT_ID \
--format='value(projectNumber)')@cloudbuild.gserviceaccount.com
```

Then grant the Cloud Build service account the `Cloud Run Admin` role in the destination project:

```
gcloud projects add-iam-policy-binding $TARGET_PROJECT_ID \
  --member=serviceAccount:$CLOUDBUILD_SA_EMAIL \
  --role=roles/run.admin
```

Now allow the Cloud Build Service to “impersonate” the Cloud Run service account:

```
gcloud projects add-iam-policy-binding $TARGET_PROJECT_ID  \
  --member=serviceAccount:$CLOUDBUILD_SA_EMAIL \
  --role=roles/iam.serviceAccountUser
```

The Cloud Run service account also will need permission to retrieve containers from the management project’s Artifact Registry. This is because the Cloud Run service account will be pulling the container from Artifact Registry and not the Cloud Build service account:

```
gcloud projects add-iam-policy-binding $MANAGEMENT_PROJECT_ID \
	--member='serviceAccount:service-'$(gcloud projects describe $TARGET_PROJECT_ID
    --format='value(projectNumber)')'@gcp-sa-cloudbuild.iam.gserviceaccount.com' \
    --role='roles/artifactregistry.reader'
```

Delete the existing trigger:

```
gcloud beta builds triggers delete $TRIGGER_NAME \
  --region=$REGION
```

Then create a new one using the new build configuration file and substitutions:

```
gcloud beta builds triggers create github \
  --name=$TRIGGER_NAME \
  --repository=projects/$MANAGEMENT_PROJECT_ID/locations/$REGION/connections
  /$CONNECTION_NAME/repositories/$REPO_NAME \
  --branch-pattern=$BRANCH_PATTERN \
  --build-config=$BUILD_CONFIG_FILE \
  --region=$REGION \
  --included-files="${SKILL_SERVICE_NAME}/**" \
  --substitutions=_REPOSITORY=$CONTAINER_REPO,_REGION=$REGION,
  _SERVICE_NAME=$SKILL_SERVICE_NAME,_IMAGE_NAME=$SKILL_SERVICE_IMAGE_NAME,
  _TARGET_PROJECT_ID=$TARGET_PROJECT_ID
```

Trigger the build manually:

```
gcloud beta builds triggers run $TRIGGER_NAME \
  --region=$REGION \
  --branch=main
```

The result of this command will be a lot of YAML but will include a URL to the build log. You could also use the `open` command to open the log in a browser by getting the command to output YAML and then parsing it with `yq` to get the log URL:

```
open "$(gcloud beta builds triggers run $TRIGGER_NAME \
  --region=$REGION \
  --branch=main \
  --format=yaml | yq eval '.metadata.build.logUrl' -)"
```

Tricks like this are useful for automating what you do in the console.

For completeness, _cloudbuild.yaml_ configurations are also included for the fact service and profile service in their respective directories in the code that accompanies this book.

### Deploying Infrastructure

In the [Appendix A](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/app01.html#appendix), there are instructions for deploying a complete version of the entire SkillMapper application using the Terraform and infrastructure as code tool. Almost everything that can be achieved on the gcloud CLI can be defined in code and applied automatically.

It is also possible to automate that deployment using Cloud Build, too, so any changes to the Terraform configuration are applied automatically by Cloud Build. This is a technique known as GitOps, as operations effectively become controlled by the content of a Git repository.

### How Much Will This Cost?

Cloud Build has a free tier where builds are performed on a machine with 1 vCPU and 4 GB RAM. At the time of writing, builds are free for the first 120 minutes per day and $0.003 per minute after that. If you would like to speed up your builds, you can use a machine with more CPU and RAM. The cost of this will depend on the machine type you choose. You can find more information on the [pricing page for Cloud Build](https://oreil.ly/2lF\_C).

Artifact Registry has a free tier where you can store up to 0.5 GB of data and transfer a certain amount of data. After that, there are monthly costs for storage and data transfer. You can find more information on the [pricing page for Artifact Registry](https://oreil.ly/Hs9CV).

## Summary

In this chapter, you created a Cloud Build pipeline that builds a container and pushes it to Artifact Registry. You also added a step to deploy the container to Cloud Run.

To create this facility, you used the following services directly:

* [Cloud Build](https://oreil.ly/WEQCs) is used to create pipelines for building and deploying services
* [Artifact Registry](https://oreil.ly/2-lRf) is used to store the container images

While you now have the factory for building and deploying a Cloud Run service from a GitHub repo, you should also understand how it is running and how to debug it. In [Chapter 13](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch13.html#chapter\_13), you will learn how to monitor and debug services by adding the observatory.
