# 14. Scaling Up

## Chapter 14. Scaling Up

In [Chapter 13](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch13.html#chapter\_13), I touched on the four golden signals of monitoring: saturation, latency, traffic, and errors. In this chapter, you can use those to help scale and secure applications.

As the amount of traffic increases, you would expect resources to become more saturated and latency to increase. If you are not careful, this can lead to errors and even downtime. You can use the golden signals to help scale your applications to meet the demands of your users.

So far, you have used a core of services from Google Cloud to build services. However, as the demands on your services increase, there are other options; in this chapter, you will be introduced to some of these and the circumstances in which employing them would make sense.

**NOTE**

The code for this chapter is in the [`scaling` folder of the GitHub repository](https://oreil.ly/nbNsV).

## Skill Service with Memorystore

At the moment, the skill service retrieves Stack Overflow tags from a storage bucket and holds them in memory in the service. This is fine for what is a relatively small number of tags, but as the number of tags increases, the memory requirements will increase to where it is no longer practical to fit in the memory of a single Cloud Run instance. You can use a database to store the tags and retrieve them on demand. This will allow you to scale the service horizontally and add more instances as the number of tags increases, only retrieving the tags when they are needed.

This would be a good case for Cloud Memorystore. Cloud Memorystore is a managed Redis service. Redis is an in-memory data structure store used as a database, cache, and message broker. Redis has built-in replication, transactions, and different levels of on-disk persistence and would be ideal for storing a large amount of simple data. However, the number of tags is unlikely to increase significantly, and in this case, it would be overkill. It is a good principle to only introduce the complexity of a new technology when a limitation of the current technology is likely to be reached.

Google Cloud has many more services than can be covered in this book that only start making sense at scale. Studying for the Google Professional Solutions Architect (discussed in this chapter) is a good way of learning about most of them and, more importantly, their use cases. For now, this chapter will look at how you can scale the fact service with two of Google Cloud’s flagship services, GKE Autopilot and Spanner.

## Fact Service with GKE Autopilot and Spanner

When building the fact service, I showed how a relatively traditional application built with Java and Spring Boot can make use of Google-specific APIs and services. This is a great way to use Google Cloud if you are fully committed, for example, to a new startup building exclusively on Google Cloud. I also showed how using Spring Cloud GCP libraries that Spring abstracts means that you are not as tied to Google Cloud as you could be. This is the approach Google would encourage because it gives you the best experience on Google Cloud.

However, this is not the only way to use Google Cloud. It is not the only way to use any cloud platform. In some organizations, becoming committed to a cloud in this way can make leaders nervous about lock-in. The question, “What happens if we need to make a forced exit?” can come up. Perhaps Google Cloud suddenly doubles prices or has a major security breach that destroys trust or causes a government or regulator to restrict use. These scenarios are unlikely, but in some industries, they are risks that need to be considered.

You also saw that there are limitations in both Cloud Run and Cloud SQL regarding how they scale. For most applications, these limitations are not a problem. However, if you are building a new application that you expect to scale to millions of users globally, you may need to consider other options. Therefore, let’s assume a situation where an organization requires a cloud-agnostic approach and scaling beyond the limitations of Cloud Run and Cloud SQL. Consider how you can take the fact service and deploy it in a way that meets those requirements.

## Requirements

Let’s look at the specific requirements for this scenario.

### User Story

The user story for this requirement is shown in [Figure 14-1](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch14.html#p6-postit).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_1401.png" alt="Scaling User Story" height="324" width="828"><figcaption></figcaption></figure>

**Figure 14-1. Scaling user story**

### Elaborated Requirements

This example also has the following specific requirements:

* Avoid using any Google-specific APIs or services in code.
* There should be no changes to the code required to host the application on another cloud provider.
* The application should be able to run on any Kubernetes platform.

In this chapter, you will look at how you can take the fact service and host it in a way that is even more agnostic to the underlying cloud provider but still follows cloud native principles. This is a great way to calm nerves by building applications that can be deployed to multiple cloud providers or even on-premises if the emerging topic of “cloud repatriation” takes off.

## Solution

This solution calls for some of Google Cloud’s more heavyweight services. Here are the details of those services.

### GKE Autopilot

In [Chapter 13](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch13.html#chapter\_13), you used Cloud Run as the runtime for your containers. Although Cloud Run is built upon Knative, which is an open source project, it is still a Google-specific implementation.

In this chapter, you will use Google Kubernetes Engine (GKE) Autopilot as the runtime for your container. GKE Autopilot is a fully managed Kubernetes service that is designed to be easy to use. It is a great way to run containers in the cloud without having to worry about the underlying infrastructure while providing more control than is available with Cloud Run.

I like Kubernetes, but I have never been a fan of the learning curve, the complexity of managing a Kubernetes cluster, or more importantly, the cost. Having a GKE cluster running will cost hundreds of dollars per month even when it is not hosting any applications.

It would be easy to spend several chapters introducing Kubernetes alone, as it is a complex subject. However, GKE Autopilot is a great way to get the power and benefits of Kubernetes without the complexity. It will always be useful to know how Kubernetes works, but in the case of GKE Autopilot, it is not essential. For now, it is enough to know that Kubernetes runs one or more containers in a unit called a _pod_ on a cluster of machines. GKE Autopilot is more cost-effective, as you pay on a per-pod basis rather than for several machines or “nodes” in Kubernetes terminology, as you would with a traditional cluster. The cluster supports autoscaling of the machines in it to match what is required at any time. This means that you only pay for the pods that are running, not the entire cluster.

GKE Autopilot is a Google-specific service, but it is built on the open source projects from Kubernetes and Istio service mesh, which are the same building blocks as other public and private clouds. This means that you can use the same container images and very similar configuration to deploy to other Kubernetes platforms, such as Amazon EKS, Azure AKS, or even IBM Cloud IKS, for that matter. This is a great way to build applications that can be deployed to multiple clouds.

### Cloud SQL

Previously, you used Cloud SQL for PostgreSQL as the database for your application. This is still a good choice, as every other public cloud provider has a similar managed PostgreSQL offering. There are also many options for running a third-party distribution of PostgreSQL on-premises or on the Kubernetes cluster itself. This means that the database is cloud-agnostic and how it is provided is not a cause for concern. However, there is a limitation to how well it can scale.

### Cloud Spanner

If you want to scale the application to millions of users globally, you need to consider how you can scale the database. Cloud SQL for PostgreSQL is a great choice for a relational database, but it can only scale vertically. This means that you can only scale it by increasing the size of the machine it runs on. The other option is to add read replicas, which as their name suggests are read-only. This can help scale reads but not writes.

The alternative is Cloud Spanner, Google’s globally distributed relational database. Cloud Spanner is a great choice for a globally distributed application, as it is designed to scale horizontally across multiple regions. It makes full use of all those primitive Google Cloud building blocks described in [Chapter 2](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch02.html#chapter\_02). Most importantly, it decouples compute from storage, meaning they can be deployed and scaled independently. However, Spanner is also a relational database, which means that you can use it similarly to Cloud SQL for PostgreSQL.

Cloud Spanner challenges the constraints of the CAP theorem, which states that a distributed system cannot simultaneously provide consistency, availability, and partition tolerance. However, it doesn’t completely break these boundaries. In the balance between these three pillars, Spanner leans toward consistency and partition tolerance, while making certain concessions in availability.

What sets Spanner apart is its innovative approach to managing availability. Leveraging highly precise clocks, it significantly reduces the availability trade-off. Spanner’s reliance on TrueTime API, which uses atomic clocks and GPS receivers, allows for precise synchronization across its global network. This system enables Spanner to make strong consistency and high availability a practical reality, making it a unique offering in the world of distributed databases.

As you will see, however, there are a few limitations that you need to be aware of so that an application can run on both Cloud SQL for PostgreSQL and Cloud Spanner without modification.

### Kubernetes Service Accounts

Previously, you created a service account to use from Cloud Run with permissions to access Cloud SQL and the secret stored in Secret Manager. You can refer to this as a Google service account.

In Kubernetes, there is also the concept of service accounts. Similarly, this is a way to grant pods access to other services. However, this is a completely separate concept from the Google service account you created before. You can refer to this as a Kubernetes service account; it is generic to Kubernetes and completely different from a Google service account.

### Workload Identity

In this chapter, the goal is for the fact service pod running in a GKE Autopilot Kubernetes cluster to be able to access the database and secret, which are Google Cloud resources. To allow a pod on GKE Autopilot to access Google Cloud resources, you need to bind the Kubernetes service account to the Google service account. This is done through a mechanism called Workload Identity.

### Skaffold

Previously, you used Skaffold to automate the building of containers and deployment to local Kubernetes. In this case, you can use it to automate your development process to a GKE Autopilot cluster on Google Cloud.

## Preparation

There are some small changes to the fact service needed to prepare it for Kubernetes and Cloud Spanner.

### Getting Ready for Kubernetes

For Cloud Run, you are not strictly required to configure health checks for the application. For GKE Autopilot, you will need to use the Kubernetes readiness and liveness probes to check the health of the application. This is a great way to ensure that the application is running correctly and is ready to receive traffic:

Liveness check

This indicates that a pod is healthy. If it fails, Kubernetes restarts the application.

Readiness check

This indicates that the application is ready to receive traffic. Kubernetes will not send traffic to the pod until it is successful.

As your Spring Boot application takes several seconds to start, it is helpful to use the readiness probe to ensure the application is ready to receive traffic before Kubernetes sends any.

Fortunately, Spring Boot provides a health endpoint that you can use for this purpose. You can configure the readiness and liveness probes to use this endpoint.

First, you need to add the Spring Boot Actuator dependency to your `pom.xml` in the `dependencies` section:

```
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
```

Then in `application.properties` you can add the following to enable the health endpoint and the readiness and liveness probes:

```
management.endpoint.health.probes.enabled=true
management.health.livenessState.enabled=true
management.health.readinessState.enabled=true
```

These expose the two endpoints:

* _Liveness_ at [_http://localhost:8080/actuator/health/liveness_](http://localhost:8080/actuator/health/liveness)
* _Readiness_ at [_http://localhost:8080/actuator/health/readiness_](http://localhost:8080/actuator/health/readiness)

You use these endpoints in the Kubernetes configuration to configure the readiness and liveness probes.

### Getting Ready for Spanner

There are a few things to consider when using Spanner. Although it is PostgreSQL compatible, it is not fully PostgreSQL compliant, and this means there are some limitations.

The first is that it does not support sequences, so it is not possible to automatically generate primary keys, as it was with Cloud SQL. This version of fact service in this chapter uses universally unique identifiers (UUIDs) for primary keys instead of an ID that is auto-incremented by the database.

Hibernate, the ORM library the fact service uses, has a nice feature of automatically updating schemas. This is not supported by Spanner, so you need to manually create the schema. Fortunately, the single table is simple in this case, so it’s not a big issue. However, this does add an extra step to the deployment process.

In Google Cloud Spanner, you can use the `TIMESTAMP` data type to store timestamp values. The precision of the `TIMESTAMP` data type is up to nanoseconds, but it does not store time zone information as Cloud SQL does. This means there is more information in the LocalDateTime Java type that can be stored in Spanner’s `TIMESTAMP` type.

To solve this issue, the common practice is to use two fields in your entity, one for the timestamp and another for the time zone. You store the timestamp as a String in a standardized format, like ISO 8601, and you store the time zone as another String. When you retrieve the data, you can parse the timestamp and apply the time zone. This is what has been done in this version of the fact service.

These are the type of limitations you need to be aware of when using Spanner; they are small but significant. It is not a drop-in replacement for PostgreSQL. An application written to work with Cloud SQL for PostgreSQL will not necessarily work with Spanner. However, an application written to work within the limitations of Spanner’s PostgreSQL will likely work with Cloud SQL for PostgreSQL. If you just target PostgreSQL, you will likely not be able to use Spanner without modification.

**TIP**

This is the trade-off you make when using a cloud native database. You get scalability and performance, but you lose some features of a traditional database. However, in this case, the benefits are large and the limitation relatively small.

### Kubernetes Configuration

The project also contains several generic Kubernetes YAML configurations in the _k8s_ directory. These would be the same for any Kubernetes platform and define how to deploy the application:

namespace.yaml

A namespace is a way to group resources in Kubernetes much like a project does in Google Cloud. This configuration defines a `facts` namespace.

deployment.yaml

In Kubernetes, the smallest deployable unit is a pod. This is made up of one or more containers. In this configuration, the pod contains two containers: the fact service instance and the Cloud SQL Proxy. A deployment is a way to deploy and scale an identical set of pods. It contains a template section with the actual pod spec.

service.yaml

A Kubernetes service is a way to provide a stable network endpoint for the pod with an IP address and port. If there are multiple instances of pods, it also distributes traffic between them and stops routing traffic if a readiness or liveness probe fails.

ingress.yaml

An ingress is a way to expose a Kubernetes services to the internet. Here you are using it to expose the fact service.

serviceaccount.yaml

A Kubernetes service account is a way to grant a pod access to other services. It is a way to provide a stable identity for the pod.

## Implementation

With the preparation done, you are now ready to deploy the application to GKE Autopilot. First, you will deploy the application to connect to Cloud SQL, as you did with the Cloud Run implementation. Then you will configure Cloud Spanner and use that as an alternative.

### Create a GKE Autopilot Cluster

Unlike Cloud Run, GKE Autopilot is a Kubernetes cluster, albeit a highly managed one, not a serverless service. You need to provision a cluster to run your application on.

Ensure you are in the citadel project where you have been deploying applications, not the management project:

```
gcloud config set project $PROJECT_ID
```

Enable the Kubernetes API:

```
gcloud services enable container.googleapis.com
```

Then create a GKE Autopilot cluster with the command:

```
gcloud container clusters create-auto $PROJECT_ID-gke \
  --project=$PROJECT_ID \
  --region=$REGION
```

**TIP**

Services like GKE (and later Spanner) are a lot more expensive than the likes of Cloud Run and Cloud SQL. I recommend keeping a close eye on your billing if using these services. Also, if you are experimenting, don’t be afraid to destroy a GKE cluster when you finish with it. You can always recreate a new one later. For example, use the following command to destroy the cluster you just created:

```
gcloud container clusters delete $PROJECT_ID-gke \
  --project=$PROJECT_ID \
  --region=$REGION
```

This will take around 10 minutes to create. On completion, a context for the cluster will be automatically added to the _kubeconfig_ file on your machine. This is a file that Kubernetes uses to know how to connect to the cluster. Alternatively, you can use the `gcloud` command to update the _kubeconfig_ file with the following command:

```
gcloud container clusters get-credentials $PROJECT_ID-gke --region $REGION --project $PROJECT_ID
```

If you have the `kubectx` command installed, you can enter it to list all the contexts in the _kubeconfig_ file. This is all the clusters available to you. You should see the context for the cluster you just created and possibly any other Kubernetes clusters you have, for example, a local Minikube.

You will then be able to use the `kubectl` command to interact with the cluster, and commands will apply to the highlighted cluster. For example, you can use the `kubectl get nodes` command to list the nodes in the cluster:

```
kubectl get nodes
-----

This should return a list of nodes in the cluster like this:

[source,text]
-----
NAME                                                  STATUS   ROLES    AGE   VERSION
gk3-skillsmapper-org-gke-nap-1q1d8az0-cdee423f-xdcw   Ready    <none>   13d   v1.24.9-gke.3200
```

As GKE Autopilot is a fully managed Kubernetes cluster, the nodes are managed by Google, and you do not have access to them. For most people, this is a good thing, as managing a Kubernetes cluster yourself can get complicated very quickly.

### Service Account Binding with Workload Identity

Kubernetes, like Google Cloud, has the concept of service accounts. These are a way to grant permissions to pods running in the cluster. You will create a Kubernetes service account and bind it to the Google service account you created earlier using Workload Identity. This will allow the pods to access the Cloud SQL instance.

This is not particularly straightforward, but when working, it provides a nice way of integrating workloads on Kubernetes with Google Cloud services without an explicit dependency on Google Cloud.

Define a name for a Kubernetes namespace and a Kubernetes service account in environment variables:

```
export K8S_NAMESPACE=facts
export K8S_SERVICE_ACCOUNT=facts-sa
```

Then create a facts namespace in Kubernetes using this command:

```
kubectl create ns facts
```

This is the Kubernetes configuration to create a service account with the name `facts-sa` in the `facts-sa` namespace `facts`:

```
apiVersion: v1
kind: ServiceAccount
metadata:
    name: $K8S_SERVICE_ACCOUNT
    namespace: $K8S_NAMESPACE
```

You can then create the Kubernetes service account, substituting the environment variables with the `envsubst` command:

```
envsubst < k8s-template/serviceaccount.yaml > k8s/serviceaccount.yaml
```

Then you can create the service account by applying the configuration:

```
kubectl apply -f k8s/serviceaccount.yaml
```

Executing this command isn’t directly creating the service account. Instead, it’s sending a declarative configuration to the Kubernetes API server. This configuration describes the desired state for a service account, namely how you intend it to exist within your Kubernetes environment.

The `kubectl apply` command allows you to assert control over the system configuration. When invoked, Kubernetes compares your input (the desired state) with the current state of the system, making the necessary changes to align the two.

To put it simply, by running `kubectl apply -f k8s/serviceaccount.yaml`, you’re instructing Kubernetes, “This is how I want the service account setup to look. Please make it so.”

Then use this command to bind the Kubernetes service account to the Google service account:

```
gcloud iam service-accounts add-iam-policy-binding \
"$FACT_SERVICE_SA@$PROJECT_ID.iam.gserviceaccount.com" \
--role roles/iam.workloadIdentityUser \
--member "serviceAccount:$PROJECT_ID.svc.id.goog[$K8S_NAMESPACE/$K8S_SERVICE_ACCOUNT]"
```

This gives the Kubernetes service account the ability to impersonate the Google service account. You then need to annotate the Kubernetes service account with the Google service account name to impersonate:

```
kubectl annotate serviceaccount $K8S_SERVICE_ACCOUNT \
iam.gke.io/gcp-service-account=$FACT_SERVICE_SA@${PROJECT_ID}.iam.gserviceaccount.com \
-n $K8S_NAMESPACE
```

The pod should now be able to access the Google Cloud resources using the Google service account via a Kubernetes service account.

You can test by deploying a test container. First, take a look at the pod configuration file _k8s-test/workload-identity-test.yaml_ with the following content:

```
apiVersion: v1
kind: Pod
metadata:
  name: workload-identity-test
  namespace: $K8S_NAMESPACE
spec:
  containers:
    - image: google/cloud-sdk:slim
      name: workload-identity-test
      command: ["sleep","infinity"]
  serviceAccountName: $K8S_SERVICE_ACCOUNT
  nodeSelector:
    iam.gke.io/gke-metadata-server-enabled: "true"
```

Then deploy, substituting environment variables using the following command:

```
envsubst < k8s-test/workload-identity-test.yaml | kubectl apply -f -
```

Observe the state of the container. This can be done by running the following command, which allows you to monitor changes in the pod’s status until it transitions to the _running_ state:

```
kubectl get pods -n $K8S_NAMESPACE -w
```

Once the Pod is up and running, you can execute a command inside it. This can be done using `kubectl exec`. The following command opens a Bash shell within the running container:

```
kubectl exec -it workload-identity-test --namespace $K8S_NAMESPACE -- /bin/bash
```

You should now find yourself at the command prompt within the container. From here, execute the following command. This will retrieve the email address associated with the Google service account in use within the container:

```
curl -H "Metadata-Flavor: Google" \
http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/email
```

The output should reveal the service account currently in use, which should align with the format _fact-service-sa@s$PROJECT\_ID.iam.gserviceaccount.com_.

Next, issue the following command to fetch a token. If the service account and its permissions are correctly set up, this command should successfully return a token:

```
curl -H "Metadata-Flavor: Google" \
http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token
```

If you received a token, this signifies that the service account is functional and properly configured. You’re now ready to deploy the application. Remember to close the shell in your container with the `exit` command. Then clean up by removing the workload-identity-test pod using:

```
kubectl delete pod workload-identity-test -n $K8S_NAMESPACE
```

### Deploying the Pod

The pod you’re about to deploy contains two containers. The first, Cloud SQL Proxy, establishes a connection to the Cloud SQL instance using permissions granted by the Google service account.

The second container holds the application. Unaware of its presence within Google Cloud or its deployment within a Kubernetes cluster, this application functions solely with the knowledge of its need to connect to a database. The connection details it requires are supplied through environment variables.

These environment variables are stored within a Kubernetes secret, specifically named `facts-db-secret`. Here’s how you can create it:

```
kubectl create secret generic facts-db-secret -n ${K8S_NAMESPACE} \
  --from-literal=username=${FACT_SERVICE_DB_USER} \
  --from-literal=password=${FACT_SERVICE_DB_PASSWORD} \
  --from-literal=database=${DATABASE_NAME}
```

You are now ready to apply the deployment. Create the _deployment.yaml_ from a template:

```
envsubst < k8s-template/deployment-cloudsql.yaml > k8s/deployment.yaml
```

Then use Skaffold, which is configured in _skaffold.yaml_, to build the container and apply all the configurations in the _k8s_ folder:

```
skaffold run
```

When completed, you can check the running pods using:

```
kubectl get pods -n $K8S_NAMESPACE
```

You should see 2/2 in the Ready column. This means both the Cloud SQL Auth Proxy and application containers are running successfully:

```
NAME                            READY   STATUS              RESTARTS   AGE
fact-service-57b9c956b6-ckvdv   2/2     Running             0          2m36s
```

With the application now running in Kubernetes, you can now make it more scalable.

### Scaling with a Horizontal Pod Autoscaler

In GKE Autopilot, as with other Kubernetes distributions, the number of instances (pods) for a service is not automatically scaled up and down by default as they are in Cloud Run. Instead, you can scale the number of pods in the cluster using a HorizontalPodAutoscaler. This will scale the number of pods based on the CPU usage of the pods. This is also slightly different to Cloud Run, as new pods are created when a threshold of CPU or memory usage is reached, rather than scaling based on the number of requests.

In the _k8s_ directory, _autoscaler.yaml_ defines the autoscaler. It is configured to scale the number of pods between 1 and 10 based on the CPU usage of the pods. The CPU usage is measured over 30 seconds, and the target CPU usage is 50%. This means that if the CPU usage of the pods is over 50% for 30 seconds, then a new pod will be created. If the CPU usage is below 50% for 30 seconds, then a pod will be deleted.

This helps ensure that there is sufficient capacity to handle requests, but it does not guarantee that there will be sufficient capacity. If there is a sudden spike in requests, then the pods may not be able to handle the requests.

However, as GKE Autopilot will automatically scale the number of nodes in the cluster, there will likely be sufficient capacity to handle the requests.

### Exposing with a Load Balancer

When using Cloud Run, you did not need to expose the application to the internet. It was automatically exposed to the internet via a load balancer. For GKE Autopilot, you need to expose the application to the internet using a Kubernetes load balancer and an ingress controller.

GKE Autopilot does have an ingress controller built in, so you don’t need to worry about configuring NGINX or similar. You can use this by creating an ingress resource and then annotating your service to use the ingress controller.

This is a point where you take the generic Kubernetes configuration and annotate it with a specific Google Cloud configuration. In this case, annotate the service configuration for the fact service to use the ingress controller. Annotate the service with the following annotation to use the ingress controller:

```
kubectl annotate service facts-service cloud.google.com/neg: '{"ingress": true}' -n $K8S_NAMESPACE
```

You can then check that the ingress is exposed:

```
kubectl get ingress -n $K8S_NAMESPACE
```

If successful, you will see an internal IP address in the ADDRESS column. This is the IP address of the load balancer:

```
NAME                   CLASS    HOSTS   ADDRESS        PORTS   AGE
fact-service-ingress   <none>   *       34.96.74.235   80      11d
```

You can retrieve this IP address to an environment variable:

```
export SERVICE_IP=$(kubectl get ingress -n $K8S_NAMESPACE \
-o jsonpath='{.items[0].status.loadBalancer.ingress[0].ip}')
```

As with Cloud Run, you can retrieve the ID token for the service account using the following command:

```
export ID_TOKEN=$(curl "https://www.googleapis
.com/identitytoolkit/v3/relyingparty/verifyPassword?key=${API_KEY}" \
-H "Content-Type: application/json" \
--data-binary "{\"email\":\"${TEST_EMAIL}\",\"password\":\"${TEST_PASSWORD}\",
\"returnSecureToken\":true}" | jq -r '.idToken')
```

You can then test the application by running the following command, including the retrieved token:

```
curl -X GET ${SERVICE_IP}/facts \
  -H "Authorization: Bearer ${ID_TOKEN}"
```

Then use Apache Bench to check the response time:

```
ab -n 3 -H "Authorization: Bearer ${ID_TOKEN}" ${SERVICE_IP}/facts
```

For me, this returned sub 100ms response time, which was substantially better than with Cloud Run. It is a useful test to compare how GKE and Cloud Run compare for different workloads.

## Switching to Spanner

Google Cloud Spanner is Google’s fully managed, scalable, relational database service. Cloud Spanner is designed to offer the transactional consistency of a traditional relational database plus the scalability and performance of a NoSQL database.

Unlike Cloud SQL, Cloud Spanner is cloud native and can scale horizontally and globally. Although it can be very expensive, it is a good fit for large-scale applications. While it is certainly overkill for the fact service at the moment, it is useful to demonstrate how to use it and how switching from Cloud SQL is possible.

The cloud-agnostic version of the fact service used in this chapter knows nothing about Google Cloud. Although it connects to a Cloud SQL database, it connects through a proxy. As far as the Spring application is concerned, there is a PostgreSQL instance running on the local host it can talk to using the PostgreSQL wire protocol. The Cloud SQL Proxy is taking care of all the networking, encryption, and authentication required.

While you can connect to Cloud Spanner natively using client libraries, it is also possible to connect to Cloud Spanner via a proxy, similar to how you have with Cloud SQL. The [PGAdapter](https://oreil.ly/WBCic) provides a PostgreSQL-compatible interface to Cloud Spanner, as again the client application can treat it as a PostgreSQL database running on the localhost. There are several different options for running the PGAdapter as a standalone Java process, a Java library, or a Docker container. As the fact service uses Kubernetes, the easiest is to use the Docker image provided by Google as a sidecar container in the same way as the Cloud SQL Proxy.

### Create a Spanner Instance

To use Cloud Spanner, first, enable the Cloud Spanner API:

```
gcloud services enable spanner.googleapis.com
```

Create environment variables for the instance and database name for a Spanner instance in the same way you would for Cloud SQL. Note that there is no need to pass credentials, as authentication is via the service account:

```
export SPANNER_INSTANCE='facts-instance'
export SPANNER_DATABASE='facts'
```

Spanner instances are configured to have a specified number of processing units. This computed capacity determines the amount of data throughput, queries per second (QPS), and storage limits of your instance. This was previously the number of nodes in the cluster, with one node being equivalent to 1,000 processing units, and one node being the smallest configuration.

This meant there was no cheap way of using Spanner. Now it is possible to specify a minimum of 100 processing units, which is equivalent to 0.1 nodes. This is a much more cost-effective way of using Spanner for small applications, development, and testing.

Create a Spanner instance using the gcloud command-line tool:

```
gcloud spanner instances create $SPANNER_INSTANCE --config=regional-$REGION \
--description="Fact Instance" --processing-units=100
```

**TIP**

Even with a minimum instance size, Cloud Spanner can be expensive. As with GKE, it is useful to be able to delete the instance when not in use if you are just experimenting. Spanner is very quick to configure again, if needed. Remove the Spanner instance with:

```
gcloud spanner instances delete $SPANNER_INSTANCE
```

When creating a Google Cloud Spanner instance, you’ll often observe a notably quick provisioning time compared to Cloud SQL. This expedited setup stems from Spanner’s unique architecture, designed for horizontal scaling across a global and distributed system. Instead of allocating traditional “instance-specific” resources, as many relational databases do, Spanner simply reserves capacity within its pre-existing, distributed infrastructure. On the other hand, Cloud SQL requires time-intensive provisioning because it establishes traditional database instances with designated resources (CPU, memory, storage) based on the user’s configuration. With Spanner, you’re seamlessly integrating into a vast, already-established system, while with Cloud SQL, you’re carving out a more personalized, dedicated space.

### Create a Spanner Database

Create a database in the Spanner instance. Spanner supports two SQL dialects, Google Standard SQL and PostgreSQL. The fact service uses PostgreSQL so create the database using the PostgreSQL dialect:

```
gcloud spanner databases create $SPANNER_DATABASE --instance=$SPANNER_INSTANCE \
--database-dialect=POSTGRESQL
```

Again, database creation is near instant.

### Authenticate the Service Account

Unlike Cloud SQL, Spanner does not have users, so you do not need to create a Kubernetes secret. However, the service account does require access to Spanner. Earlier, you linked a Kubernetes service account to the service account you created for the fact service. Now you need to give that service account access to Spanner:

```
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member=serviceAccount:${FACT_SERVICE_SA}@${PROJECT_ID}.iam.gserviceaccount.com \
  --role=roles/spanner.databaseUser
```

The fact service will use the service account to authenticate with Spanner.

### Redeploy the Fact Service

In the _k8s-template_ folder, there is a second deployment template. This time it has the fact service and a Spanner PGAdapter container. Run the following command to replace the _deployment.yaml_ with a new configuration generated from it:

```
envsubst < k8s-template/deployment-spanner.yaml > k8s/deployment.yaml
```

Then run the `skaffold` command, and then apply the new configuration:

```
skaffold run
```

You should now have a fact service running against Spanner rather than Cloud SQL, giving potential for global expansion.

## Evaluation

Let’s evaluate the solution in terms of scaling and cost.

### How Will This Solution Scale?

Here, you have seen a mixture of cloud native and traditional technologies. Although GKE Autopilot is not serverless, it is cloud native. As demand increases, more instances of the fact service will be created by the horizontal autoscaler. As more instances are scheduled, the GKE Autopilot cluster will automatically add additional nodes to deal with the extra pods.

GKE Autopilot also appears considerably faster to service requests than the same container running on Cloud Run. This could be down to the way networking is configured, with requests reaching the service by a more direct route.

This solution will not scale to zero in the same way as Cloud Run, and there will always need to be one pod running to service requests (if individual instances are still running in a single pod). Remember, however, that if demand suddenly increases, it will take a few minutes for both the GKE Autopilot cluster to provision the extra resources required for running the post and then for the pods to start.

While the service can be scaled almost indefinitely, the real bottleneck is the Cloud SQL database, which is not cloud native. There are two related limitations. The first is that the database cannot be dynamically scaled. You have to specify the tier of the machine used for the database, and while this can be changed manually with a database restart, it cannot change automatically in response to load. More importantly, there is a limit to the number of database connections from the instances of the services.

This means that if the instances increase without limit, they will exhaust the number of connections available to the database and fail to connect. For this reason, it is important to limit the number of instances so that the number (instances × connections per instance) is below the maximum number of connections available to the database.

However, you have seen that with some minor adjustments, you can allow the fact service to work with Google Cloud Spanner, a cloud native database with the potential to scale far beyond the limitations of Cloud SQL, creating a full cloud native solution.

### How Much Will This Solution Cost?

Unlike Cloud Run, GKE Autopilot does not have a cost per request; you will be billed for the pods running on the cluster and a cluster management fee per hour. At the time of writing, the first 720 hours of cluster management per month are included per account, so you effectively get one cluster free.

The cost of pods is based on the amount of CPU, memory, and ephemeral storage requested by scheduled pods. This is billed per second. The most significant cost is for CPU. Therefore, it is very important to make sure the resources you request for your pod are adequate but not excessive. Remember that a Kubernetes pod can use additional resources up to the limit specified; the requested resources are the ones that are reserved.

As each pod is charged per second, it does not make sense to keep a pod running for a second longer than it needs to. Therefore, using horizontal autoscaling to dynamically increase and decrease the number of running pods to fit demand will help keep costs down.

The cost of Cloud Spanner in this minimal configuration is under $100 per month. That is still ten times the cost of a minimal Cloud SQL instance. However, another advantage of the cloud is that it allows you to experiment with services like advanced databases for short periods, without the massive outlay of money or effort you would have if you were to experiment on-premises. On the cloud, you just switch off the service again and stop paying, so if you wanted to try Spanner for an hour for a few cents, you can.

## Summary

This chapter should have given you a glimpse at how you can go further in Google Cloud. However, it is a powerful platform with many services and features. There is a lot more to learn.

For this project, you used the following services directly:

* GKE Autopilot is used as the container runtime to run the container.
* Cloud SQL is used as the database backend for the application.
* Cloud Secrets Manager is used to securely store the database password.
* Cloud Spanner is used as an alternative database backend for the application.

[Chapter 15](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch15.html#chapter\_15) wraps up your Google Cloud journey and looks at some options for further learning.
