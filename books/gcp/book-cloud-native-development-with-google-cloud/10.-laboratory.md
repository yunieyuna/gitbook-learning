# 10. Laboratory

## Chapter 10. Laboratory

The cornerstone of effective, scalable, cloud native development is establishing a personal _laboratory_, which is the focus of this chapter. This space serves as a secure playground for innovation. Your laboratory is your development environment, typically a local workstation or laptop, where you have the freedom to experiment, iterate, and rapidly observe the results. You may do this yourself, or if you are using pair programming, with a partner.

Given that your laboratory explorations will mainly involve coding, the quality of your developer experience (DX) can make a big difference. It plays a crucial role in streamlining work and driving productivity.

The primary objective of the laboratory in cloud native development is to produce a recipe for generating a containerized microservice. This recipe should be designed so that it can be seamlessly transitioned to the _factory_, preparing it for real-world, production deployment.

The secret to individual productivity and effective local development lies in optimizing _inner loop agility_. This term refers to the time taken between writing code and validating the results. The key is to minimize this duration as much as possible, allowing for rapid testing, feedback, and improvements. By enhancing your inner loop agility, you can quickly iterate and evolve your code, ultimately accelerating the pace of your cloud native development.

**NOTE**

The code for this chapter is in the [`laboratory` folder of the GitHub repository](https://oreil.ly/WUMt2).

## The Inner Loop

The software development process can be visualized as two distinct loops, both pivoting around the central act of _pushing_ updates to source control.

The right diagram in [Figure 10-1](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch10.html#inner-loop) represents the _outer loop_, which is initiated once the revised code is pushed to the source control system. This phase incorporates the integration of new code and its testing against the preexisting code base. I cover this in [Chapter 12](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch12.html#chapter\_12).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_1001.png" alt="Inner Loop" height="1172" width="1415"><figcaption></figcaption></figure>

**Figure 10-1. Inner loop**

The _inner stage_ is your day-to-day work. It consists of three steps: write code, build it to a testable state, and run the tests. The starting point might change if you’re using test-driven development (TDD).

You write code, build it to a state where it can be tested, execute the test, and then repeat the test. Of course, in TDD, the starting point is a failing test rather than code, but the loop is the same either way.

You will likely go through these micro feedback loops hundreds of times daily as a developer. The shorter the loop, the more productive you will be. The better your tests are, the earlier you can run them and the earlier you find bugs. However, your tests must run fast, so your feedback loop remains short.

Let’s see how Google Cloud helps us build a short inner loop that forms the laboratory’s core.

**TIP**

I find it liberating if my inner loop runs efficiently on a modest laptop. This means I can be productive wherever I am without worrying about my battery life. It is also great for that all-important carbon usage.

## Containers in the Inner Loop

One of the foundations of cloud native is packaging code as containers. If you are used to developing in noncontainerized environments, you may already have a fast feedback loop. In noncompiled languages like Python or Node.js, getting feedback is a case of making a change and running the code. One of the strengths of Go, for example, is that even though it is a compiled language, it is fast to compile and run. When using Java with Spring Boot, it is also possible to have a rapid code, build, and testing of the local loop, especially when using Spring’s developer tools.

So why slow everything down by adding the complication of containers to the inner loop? Isn’t this colossal overhead? Whereas before you could just run the code, now you have to build a container, push it to a repository, and redeploy it after each code change. I have seen Java developers horrified when they are presented with Kubernetes and have their productivity destroyed as their inner loop goes from a few seconds to minutes due to all the extra steps that containers add.

There is an excellent reason to develop using containers, though; it eliminates the “it runs on my machine” issue. This is also the goal of the dev/prod parity principle of the 12-factor applications. A container is immutable; if it runs in one place, it will run everywhere. This is a valuable benefit because, as I said earlier, the sooner you can find an issue, the simpler it is to resolve. Fortunately, tools can automate and streamline the process, so the container overhead becomes insignificant.

To illustrate, let’s take a Spring Boot development inner loop as an example (see [Figure 10-2](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch10.html#sb-code-build-test)). Before containers, you would have had three steps: code, build, and test.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_1002.png" alt="sb-code-build-test" height="157" width="821"><figcaption></figcaption></figure>

**Figure 10-2. Spring Boot: code, build, test**

Add containers, and you have to build a container image, tag the image, push the image to a container registry, patch the deployment to use the new container image, and redeploy with the new image before being able to test ([Figure 10-3](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch10.html#docker-k8s-code-build-test)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_1003.png" alt="docker-k8s-code-build-test" height="157" width="1386"><figcaption></figcaption></figure>

**Figure 10-3. Spring Boot with Docker and Kubernetes**

This is where Google provides tools to automate the extra steps and eliminate the overhead ([Figure 10-4](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch10.html#sb-jib-skaffold)). The first is Skaffold; this tool automates most of these additional steps. The second is a tool needed for simplifying and automating building the container itself. For Spring Boot, Jib is a good choice, but I will cover more options later.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_1004.png" alt="sb-jib-skaffold" height="268" width="1386"><figcaption></figcaption></figure>

**Figure 10-4. Jib and Skaffold providing automation**

Tools like these form the basis of a laboratory as they allow cloud native developers to automate away the overhead of working with containers. Let’s explore some more Google Cloud tools that fit together to support you.

## Inside the Laboratory

A laboratory should provide everything needed to execute the inner loop automatically, from making a change in code to testing and getting feedback. You then break out of the loop by pushing changes into source control to be picked up by the factory’s build and testing processes.

The laboratory needs the following:

* An IDE in which to write the code
* A way of building a container
* A place to store built containers
* A way of testing containerized microservices
* A way of testing dependent services

## Choosing an IDE

When building a laboratory with Google Cloud, you don’t need to give up the integrated development environment (IDE) you are familiar with—quite the opposite. Google offers extra open source tools to enhance your IDE and options for where your IDE runs.

## Cloud Code

[Cloud Code](https://oreil.ly/bU3GP) is a set of plugins for popular IDEs that makes it easier to develop applications for Google Cloud. It is a great building block for your laboratory and for getting that fast feedback during local development.

Cloud Code provides and manages Google Cloud SDK, Skaffold, Minikube, and kubectl. It also gives tools that help provide a smooth path from the laboratory to the factory and then help with debugging deployed applications.

### Desktop IDE with Cloud Code

Cloud Code provides a plugin for many desktop IDEs:

* [VSCode](https://oreil.ly/THhba)
* [IntelliJ](https://oreil.ly/3F0Rh)
* [PyCharm](https://oreil.ly/t2G7M)
* [GoLand](https://oreil.ly/XH7lf)
* [WebStorm](https://oreil.ly/bZmqD)
* [Eclipse](https://oreil.ly/22TzQ)

### Cloud Shell Editor

Cloud Code is also a Web IDE running entirely in a browser. The [Cloud Shell Editor](https://oreil.ly/sKXXA) is based on [Eclipse Theia](https://oreil.ly/vZJcB) with a user interface based on VSCode.

It is available anywhere you have a browser through the Google Cloud console. Although it is possible to use this as your primary IDE, I find it most handy for making quick changes.

### Cloud Workstations

The newest option at the time of writing is [Cloud Workstations](https://oreil.ly/oWUw6), and for me, this is the best of both worlds: a thin client application that behaves like a desktop IDE, but the backend runs in Google Cloud on a virtual machine. This means you can have anything from a modest to a mighty machine backing your IDE on Google Cloud and still use it as if it were a desktop IDE.

However, the real benefit of both the Cloud Shell Editor and Cloud Workstations is that you are working on Google Cloud and using the Cloud’s ultrafast network. Operations that may be slow when working with a desktop IDE, like uploading a container to a container registry or accessing a Google Cloud service, are superfast, making for a great developer experience with a fast inner loop.

**TIP**

In commercial environments, code exfiltration is a concern. This is where developers with source code on their laptops can accidentally or maliciously share it with someone they shouldn’t. If this code is the company’s IP from which they make their money, this can be a big problem. Using Cloud Shell Editor or Cloud Workstations rather than a traditional IDE means source code never leaves Google Cloud, removing that risk and reducing the headache of controlling source code.

### Comparison of Cloud Code–Enabled IDEs

[Table 10-1](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch10.html#ide-comparison) summarized the difference between the IDE options.

| IDE                | Cost   | Offline | Speed | Developer experience | Security |
| ------------------ | ------ | ------- | ----- | -------------------- | -------- |
| Desktop IDE        | Low    | Yes     | Low   | High                 | Low      |
| Cloud Shell Editor | Low    | No      | High  | Medium               | High     |
| Cloud Workstations | Medium | No      | High  | High                 | High     |

## Skaffold

At the center of Cloud Code is [Skaffold](https://skaffold.dev/), a Google open source project that automates your inner loop, building, pushing, and deploying your containerized application. Skaffold aims to tighten and highly optimize your inner loop, giving you instant feedback while developing. It is a flexible tool for building a laboratory.

When configured and run, Skaffold takes the source code from your project, automatically builds the code, and packages the artifacts into a container using a mechanism of your choice. It then tags the container and makes it available for deployment. For a remote Kubernetes cluster, for example, this means automatically pushing the container to a remote repository. Skaffold then automates the task of redeploying the new container to the runtime, Kubernetes or otherwise.

## Efficiently Building Containers

The most significant bottleneck in an inner loop with containers is building the container itself. Fortunately, Skaffold supports several options.

### Using a Dockerfile

The most common way of building a container is to write a Dockerfile and use a Docker daemon on your local machine to create the container image. The problem with this is that adding this containerization step may add minutes to your inner loop, making it a disaster for productivity. The only time you used this in previous chapters was in [Chapter 9](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch09.html#chapter\_09) when you built a container for the UI. You want to keep the loop to seconds. Fortunately, there are other options available to streamline and automate the process.

### Jib

If you are building applications in Java, [Jib](https://oreil.ly/s1vla) is a Google open source project that lets you build OCI-compliant containers using a Maven or [Gradle](https://gradle.org/) plugin. You used this for the fact service in [Chapter 7](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch07.html#chapter\_07).

Jib does not require Docker to be installed and doesn’t need a Dockerfile. By default, Jib uses an OpenJDK base container image containing a Java runtime, although you can specify any base container you like. Jib then adds layers containing your application, much like adding the filling to a sandwich. It also automates the step of pushing the new image to a repository.

When building a Java container using a Dockerfile, dependencies are downloaded and the entire application is rebuilt as one layer. The great thing about Jib is that it creates multiple layers for the application in the container. Jib is then smart enough to know which dependencies or classes have changed and swaps in a thin layer with the changes, leaving the rest of the container image as before. This means building containers is fast, and containers are not rebuilt unnecessarily.

### Ko

[Ko](https://oreil.ly/UpZAu) is a similar project for Go developers. Like Jib, Ko is a fast container builder. It is great if your application is a single Go executable without dependencies required in the base image. By default, Ko uses the minimal [distroless](https://oreil.ly/n1Yf0) base image, although, like Jib, this can be changed. Again, it does not require Docker to be installed on your machine and supports multiplatform builds. You have not used Ko yet in this book, preferring to use buildpacks for the Go services; however, Ko is much faster than buildpacks.

### Buildpacks

[Cloud Native Buildpacks](https://buildpacks.io/) are an evolution of the buildpacks technology that made developer-centric platforms like Heroku and Cloud Foundry so popular. By default, buildpacks allow you to supply your code and then “automagically” inspect it to identify a builder image that knows how to build the application. For example, a `pom.xml` may cause a Maven buildpack to be used. The resultant application is then packaged into a run image.

Buildpacks are another excellent option for speeding up your inner loop. The disadvantage of buildpacks is that they require a local Docker daemon and tend to be considerably slower to rebuild than a solution like Jib or Ko. However, they are smart enough to use caching of dependencies to avoid the complete rebuilds you would have when using a Dockerfile. The big advantage of buildpacks is that you just need to provide the code and they do the rest; you don’t need to touch a Dockerfile, and buildpacks support a wide range of languages.

### Comparison of Container Build Tools

[Table 10-2](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch10.html#ide-comparison2) summarizes the difference between the container build tool options.

| Tool       | Language support | Requires Docker | Incremental build | Build from source code | Time to build |
| ---------- | ---------------- | --------------- | ----------------- | ---------------------- | ------------- |
| Dockerfile | All              | Yes             | No                | No                     | High          |
| Jib        | Java             | No              | No                | No                     | Low           |
| Ko         | Go               | No              | Yes               | No                     | Low           |
| Buildpacks | All              | Yes             | Yes               | Yes                    | Medium        |

## Deploy to a Development Container Runtime

Once the container is built and is available in a container registry, it can be redeployed to the container runtime for testing and verification. I will discuss the pros and cons of using the different container runtimes that Google Cloud uses in later chapters, but in the context of what is best for your inner loop, here are options that Skaffold supports.

### Local Kubernetes

By default, Cloud Code comes with [Minikube](https://oreil.ly/a1dk4), a local Kubernetes cluster. This is the easiest and quickest container runtime to deploy and test your containers. It is also used as an emulator for Cloud Run.

Alternatives for local Kubernetes include [Kind](https://oreil.ly/RNNe8), [k3d](https://oreil.ly/Puken), and [Docker Desktop](https://oreil.ly/MfG14).

Using a local Kubernetes is great if you have poor internet access or work offline, as you don’t need to wait for your container image to be uploaded to a remote repository. It is also likely the cheapest option. I like working on a modest laptop, and running local Kubernetes uses a lot of resources and battery, which is a downside.

Minikube is also included with the Cloud Console Editor and Cloud Workstations.

### Shared GKE

If you work in a team, you could have access to a shared [Google Kubernetes Engine (GKE) cluster](https://oreil.ly/VaI0Z). This could be centrally managed and shared by giving each developer their namespace to deploy to. While this may make sense for a team, it is likely not the best option for an individual developer as, most of the time, it will be idle but costing you money.

**TIP**

Don’t use GKE; use GKE Autopilot or Cloud Run, especially for development workloads.

### GKE Autopilot

[GKE Autopilot](https://oreil.ly/4h8dV) is an operating mode of GKE where Google manages the cluster. The significant advantage of this over normal GKE is that you only pay for the CPU, memory, and storage used when running on GKE Autopilot. For development, you only pay when your container runs, which will likely be a considerable saving to running a standard GKE cluster with idle capacity. You will get a chance to try GKE Autopilot in [Chapter 14](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch14.html#chapter\_14).

### Cloud Run

The final option is [Cloud Run](https://oreil.ly/Msman). This is an easier way of running a container than using Kubernetes. In Google Cloud, Cloud Run is a managed service built on Borg. However, in Google’s on-prem Anthos offering, Cloud Run runs on Kubernetes. Cloud Run should be suitable for running nearly all containerized microservices and pay-per-use services, just like GKE Autopilot. Out of all the Google-hosted services, it is likely the most accessible and cheapest option for testing your container in development. You have already used Cloud Run a lot in this book, and hopefully, you understand its flexibility and why it is worth considering for most services.

### Comparison of Development Container Runtimes

[Table 10-3](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch10.html#runtime-comparison) summarizes the difference between the container runtime options.

| Runtime          | Cost   | Local overhead | Speed to redeploy |
| ---------------- | ------ | -------------- | ----------------- |
| Local Kubernetes | Low    | Yes            | High              |
| Shared GKE       | High   | No             | Medium            |
| GKE Autopilot    | Medium | No             | Medium            |
| Cloud Run        | Medium | No             | Medium            |

## Choosing a Container Repository

If you are working with a local Kubernetes cluster like Minikube, there is no need to push your container to a registry, as it is available on your machine after building. However, if you are using a separate GKE cluster, GKE Autopilot, or Cloud Run, you will need to push it to a container registry. As you are in Google Cloud, it makes sense to use the provided Artifact Registry, but there are other options.

### Artifact Registry or Container Registry?

In Google Cloud, Artifact Registry is currently the recommended service for container image storage. You may see references to Container Registry, which is Artifact Registry’s predecessor. There is no real reason to use Container Registry, as Artifact Registry has all the features of Container Registry, with extra features such as being able to store other types of artifacts and supporting both regional and multiregional hosts.

### External Registry

You could alternatively use an external container registry such as [docker.io](https://docker.io/); however, this will require extra configuration for authentication. It will also be slower both for pushing containers and pulling containers for deployment, so it is one of those things where it only makes sense to use an external registry if you have a good reason.

## Choosing an Integration Testing Method

When testing locally, your application is likely to make use of services like databases or messaging. In later chapters, as these services are introduced, I will explain the options for using or emulating them in your local loop. However, here are the high-level options.

### Actual Service Instance

Using a real service for integration testing keeps to the 12-factor principle of dev/prod parity. Many services have the option to run at minimal capacity for testing. If you are working locally, there is the issue of network lag; however, if you are using one of the in-cloud options (Cloud Shell Editor or Cloud Workstations), this can work very well.

### Local Emulator

Sometimes, it just may not be cost-effective or possible to use a genuine service for integration testing. An example is the Cloud Spanner database, which used to have a high minimal cost deployment, although the introduction of fractional deployments has mitigated this. However, another is not having an internet connection. For example, suppose you are developing on an airplane or on a cruise ship (as I did once). In this case, a local emulator is a good option.

Here are some examples of emulators of Google services used in this book:

Cloud Pub/Sub

The [Pub/Sub emulator](https://oreil.ly/606j3) provides an in-memory implementation of the API for local testing purposes.

Cloud Firestore

The [Firestore emulator](https://oreil.ly/K3hs0) is part of the Firebase Local Emulator Suite, which lets you run Firestore, Realtime Database, Firebase Authentication, Cloud Functions, and Cloud Pub/Sub locally.

Cloud Spanner

The [Cloud Spanner emulator](https://oreil.ly/0AiFc) provides a local, in-memory environment for development and testing.

Cloud Storage

While not an emulator in the traditional sense, the _gsutil_ tool can interact with the local file system in a way that emulates interaction with the Cloud Storage API.

Note that local emulators may not offer complete functionality or may behave differently than the actual services in some cases. Therefore, it’s crucial to test your application in the actual Google Cloud environment before deploying it to production.

### Local Container

In scenarios where official emulators are unavailable or access to the actual service is restricted, leveraging local containers can provide a robust alternative. One of the projects that facilitate this is the [Testcontainers project](https://oreil.ly/HePis).

Testcontainers is a highly regarded Java library specifically designed for facilitating the use of Docker containers within automated testing environments. Its central purpose is to allow you to emulate various services in isolated, short-lived containers, ensuring your tests run against a fresh instance of the service every time. This attribute makes it an effective tool for improving the reproducibility and reliability of your automated testing pipeline.

The library is known for its extensive coverage, providing disposable containers for a broad range of services such as databases, web browsers (with Selenium), message queues, and more. It enables developers to execute integration tests against a wide variety of systems without needing to install them locally or on a shared test server.

In addition to its existing functionalities, Testcontainers has an active community contributing to its continuous improvement and expansion. New disposable containers mimicking different services are frequently added, further widening the spectrum of test scenarios it supports.

However, while local containers and emulators like those from Testcontainers can offer substantial benefits for local development and testing, they may not perfectly mirror the behavior of live services. Therefore, it’s always important to run final validation tests on the actual production-like services whenever possible, ensuring that your application behaves as expected under real-world conditions.

## Comparison of Service Integration Testing

[Table 10-4](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch10.html#int-test-comparison) summarizes the difference between the integration testing options.

| Service         | Cost   | Local overhead | Realism |
| --------------- | ------ | -------------- | ------- |
| Actual service  | Medium | Low            | High    |
| Local emulator  | Low    | Medium         | Medium  |
| Local container | Low    | Medium         | Medium  |
| Mock service    | Low    | Low            | Low     |

## Building an Example Laboratory

Let’s show how all this fits together with a simple example.

In this case, you’ll use Cloud Code through the Cloud Shell Editor and use the provided Minikube for inner loop testing.

Because Cloud Code and Cloud Shell come with a local Minikube, you can work effectively locally. However, if you want to use a local IDE, you can use Cloud Workstations.

**TIP**

There is no charge for using the Cloud Shell Editor. It is essentially a free virtual machine with 5 GB of free persistent disk, so the state is stored between sessions.

It also provides a 2 vCPU, 4 GB RAM Minikube cluster included with the Cloud Shell instance, making this a zero-cost way of working with Kubernetes.

### Start the Cloud Shell Editor

In your web browser, open [Google Cloud console](https://oreil.ly/2E3cx), as shown in [Figure 10-5](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch10.html#activate-cloud-shell). In the top right of the Console, click the Active Cloud Shell button to open Cloud Shell.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_1005.png" alt="Activate Cloud Shell" height="324" width="1196"><figcaption></figcaption></figure>

**Figure 10-5. Activate Cloud Shell button**

When Cloud Shell opens, click the Open Editor button to open the Cloud Shell Editor, as shown in [Figure 10-6](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch10.html#open-editor).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_1006.png" alt="Open Editor" height="108" width="474"><figcaption></figcaption></figure>

**Figure 10-6. Open Editor button**

### Clone the Code

In the Cloud Shell Editor terminal, clone the Skills Mapper project using this git command:

```
git clone https://github.com/SkillsMapper/skillsmapper
```

Now in the Cloud Shell Editor, navigate to and open the cloned project; then go to the `fact-service` directory.

### Enable Jib

You may remember this is a Java project that already uses _Jib_ to build a container.

In the project, open the _pom.xml_ and notice the Jib plugin in the `plugin` section:

```
<plugin>
    <groupId>com.google.cloud.tools</groupId>
    <artifactId>jib-maven-plugin</artifactId>
    <version>3.3.1</version>
</plugin>
```

### Init Skaffold

To initialize Skaffold, enter this command in Cloud Shell:

```
skaffold init --generate-manifests
```

Skaffold attempts to detect how your container can be built. As the `pom.xml` has the Jib plugin included, `Jib Maven Plugin` will be shown as an option.

Select `(Jib Maven Plugin (org.skillsmapper:fact-service, pom.xml))`.

When you see the prompt `Select port to forward for pom-xml-image (leave blank for none)`, enter `8080`. This will forward port 8080 of your local machine onto the Kubernetes service in the Minikube cluster, allowing you to connect.

The `--generate-manifests` argument causes Skaffold to generate Kubernetes manifests to use for deployment. As you do not have any at present, when prompted with `Do you want to write this configuration, along with the generated k8s manifests, to skaffold.yaml?`, enter `y`.

While the generated manifests are not ideal, especially as it generates default container names and labels to `pom-xml-image`, they are a good starting point.

In the Skills Mapper project, there is a modified _skaffold.yaml_ file and a _deployment.yaml_ for the fact service in the `k8s` directory:

```
apiVersion: skaffold/v4beta5
kind: Config
metadata:
  name: fact-service
build:
  artifacts:
    - image: fact-service
      jib:
        project: org.skillsmapper:fact-service
manifests:
  rawYaml:
    - k8s/deployment.yaml
```

In the build section, you have specified the `fact-service` image and the Maven project to build using Jib. The manifests section specifies the Kubernetes deployment to use to deploy to the local Minikube cluster.

### Repeat for the Skill and Profile Services

The skill service and profile service are written in Go as opposed to Java but will behave in the same way in Skaffold.

Change to the `skill-service` directory and then again run:

```
skaffold init --generate-manifests
```

This time Skaffold will use buildpacks and generate a _skaffold.yaml_ file similar to this:

```
apiVersion: skaffold/v4beta5
kind: Config
metadata:
  name: skill-service
build:
  artifacts:
    - image: skill-service
      buildpacks:
        builder: gcr.io/buildpacks/builder:v1
manifests:
  rawYaml:
    - k8s/deployment.yaml
```

However, the buildpack may take minutes to build, and that is not great for the inner loop, where speed is important. Instead, you can use the Go equivalent of Jib to get build times down to a couple seconds. In the project, there is a _skaffold.yaml_ file that uses Ko to build the container:

```
apiVersion: skaffold/v4beta5
kind: Config
metadata:
  name: skill-service
build:
  artifacts:
    - image: skill-service
      ko:
        baseImage: gcr.io/distroless/static:nonroot
        importPath: github.com/SkillsMapper/skillsmapper/skill-service
manifests:
  rawYaml:
    - k8s/deployment.yaml
```

The skill service also requires environment variables to be set. For this, use a Kubernetes ConfigMap. In the _skill-service/k8s_ directory, there is a _config⁠map.​yaml.template_ file that defines the ConfigMap. Run the following command to create the ConfigMap:

```
envsubst < k8s/configmap.yaml.template > k8s/configmap.yaml
```

The profile service follows the same pattern as the skill service. Repeat the steps in the `profile-service` directory to set up Skaffold and create the ConfigMap.

### Start Minikube

You can now start the provided Minikube cluster by entering this command in Cloud Shell:

```
minikube start
```

Check that Minikube is running but connecting to it through the Kubernetes kubectl client:

```
kubectl get nodes
```

If everything is working correctly, you should see a response like this showing your single-node Kubernetes cluster is running:

```
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   44m   v1.26.3
```

### Create a Secret for Service Account Keys

The skill service requires a Google Cloud service account key to access the Google Cloud Storage bucket.

You can retrieve a key for the skill service service account using the following command:

```
gcloud iam service-accounts keys create ${SKILL_SERVICE_SA}.json \
--iam-account ${SKILL_SERVICE_SA}@${PROJECT_ID}.iam.gserviceaccount.com
```

Then create a Kubernetes secret on the Minikube cluster containing the key using the following command:

```
kubectl create secret generic ${SKILL_SERVICE_SA}-key \
--from-file=${SKILL_SERVICE_SA}.json=${SKILL_SERVICE_SA}.json
```

When the service is created, the key will be mounted into the container at `/var/secrets/google/${SKILL_SERVICE_SA}.json` for the service to use.

To configure the profile and fact service, follow the same steps but using the `PROFILE_SERVICE_SA` and `FACT_SERVICE_SA` environment variable, respectively, in the place of `SKILL_SERVICE_SA`.

While you could configure the fact service in the same way, it has a default Spring Profile that does not use Google Cloud services, so it can be run without a service account. In a way, this is a good thing, as it means you can run the application locally without needing to connect to Google Cloud.

### Build a Container with Skaffold

This is an example of the skill service, but the same applies to both the fact and profiles services as well.

In the `skills-service` directory, use Skaffold to build a container:

```
skaffold build
```

As this is the first time to build, it will take a few seconds.

### Run Skaffold

There are now two options to deploy with Skaffold. To build and deploy the application once, use the command:

```
skaffold run --port-forward
```

When this command completes successfully, you will find your application running in your Minikube Kubernetes cluster. To check, use the command:

```
kubectl get pods
```

You should see your pod running like this:

```
NAME                            READY   STATUS    RESTARTS   AGE
skill-service-8f766466c-28589   1/1     Running   0          8s
```

In Kubernetes, a service is used to expose an application running in a pod outside the cluster. Skaffold would have also created a service. You can check this using:

```
kubectl get service
```

This will show the service running on port 8080:

```
NAME            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
skill-service   ClusterIP   None         <none>        8080/TCP   16h
```

In this case, you have used the `--port-forward` flag to forward the local port 8080 to port 8080 on the Kubernetes cluster. This means that the skill service will be available at [_http://localhost:8080_](http://localhost:8080/).

Open a second tab and enter:

```
curl -X GET "http://localhost:8080/autocomplete?prefix=java"
```

You should see the response:

```
{"results":["java","java-10","java-11","java-12","java-13",
"java-14","java-15","java-16","java-17","java-18"]}
```

You can also launch this in the browser in Cloud Shell by clicking the Web Preview button and then preview on port 8080.

Alternatively, if you would like the application to be deployed automatically every time you make a change, use the command:

```
skaffold dev
```

By doing this, Skaffold will monitor the source files for any changes. When detected, it will automatically rebuild and redeploy a new container. For Java applications, this may take a few seconds, so it is likely you will not want a deployment every time you make a change. However, knowing that, redeployment with `skaffold run` means that feedback is almost as fast as starting Spring Boot directly.

For Go applications like the skill service, the build time is so fast that you can use `skaffold dev` to get a great inner loop experience.

Another option is running the Skaffold debug command:

```
skaffold debug
```

This command starts the application in debug mode, so any breakpoints in the code will activate just as if the application was running in debug mode outside the Kubernetes cluster.

## Summary

Choosing how to build a laboratory does depend on your needs, but the options are there to build a great developer experience by optimizing your inner loop.

Here are your variables:

* Which IDE option to use and where to run it
* How to build your containers
* Which container runtime to use to run your container
* How to test your application with dependent services

In [Chapter 11](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch11.html#chapter\_11), you will be creating the citadel to further protect the API and UI you deployed in [Chapter 9](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch09.html#chapter\_09).
