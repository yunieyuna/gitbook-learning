# 6. Project 2: Skill Service With Cloud Run

## Chapter 6. Project 2: Skill Service with Cloud Run

In [Chapter 5](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch05.html#chapter\_05), you used a Cloud Function to collect Stack Overflow tags from the BigQuery public dataset and store the results as an object in Cloud Storage. In this chapter, you will use these tags to develop an interactive service that offers skill suggestions to users as they type.

**NOTE**

The code for this chapter is in the [`skill-service` folder of the GitHub repository](https://oreil.ly/6kKB8).

## Requirements

Let’s explore what’s needed for this project.

### User Story

The user story for this piece of functionality is shown in [Figure 6-1](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch06.html#p2-postit).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_0601.png" alt="Project 2 User Story" height="324" width="828"><figcaption></figcaption></figure>

**Figure 6-1. Project 2 user story**

### Elaborated Requirements

This project also has the following specific requirements:

* Suggestions should be presented when a user types three or more characters.
* 95% of suggestion requests should return suggestions in less than 500 ms as, anything longer than this may be perceived as slow.
* The solution should be reliable and low cost.
* The solution should scale to thousands of simultaneous requests without desegregation.

## Solution

What is required here is a reliable and scalable solution for looking up skills from a list of tags. Given the relatively small size of the data and the need for rapid response times, you’ll keep this data in an in-memory trie data structure.

## Summary of Services

Here is a summary of the Google Cloud services you will be using in this solution.

### Cloud Storage

In [Chapter 2](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch02.html#chapter\_02), you collected tags from BigQuery’s Stack Overflow dataset and stored them as an object in Cloud Storage. Now, you’ll retrieve that object and use it to populate the skills that your service will use to generate suggestions for the user.

### Cloud Run

You previously used Cloud Functions as the runtime for your application. However, while Cloud Functions are great for occasionally running code triggered by events, they are not intended to be used for long-running services. As the service will need to set up the trie data structure in memory, you don’t want to have to do that each time there is a request. Instead, the requirement is for a long-running service, or at least one that can handle a large number of requests once started.

As you want a service that is long-running and can scale dynamically, you will use Cloud Run. In Cloud Run, instances are referred to as services rather than functions in Cloud Functions.

Cloud Run is the underlying technology of the Cloud Function you used in [Chapter 5](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch05.html#chapter\_05). Here, using it directly gives us more control of the container and how it runs. Specifically, you can scale the service to handle thousands of simultaneous requests.

If Cloud Run was a means of transport, it would be like a rental car; you have more flexibility than a taxi, but you have to drive it yourself. However, you still don’t have to worry about the maintenance and upkeep of the car.

Cloud Run can scale in different ways:

Multiple instances

Cloud Run automatically scales up the number of container instances based on the number of requests by monitoring an internal request queue. It can also scale down to zero when no requests are pending. This follows the 12-factor principle of favoring scaling horizontally rather than vertically.

Concurrency

For languages with good concurrency support like Go or Java, it is possible to have multiple requests handled by a single instance rather than in Cloud Functions, where a function handles a single request at a time.

Resources

As with Cloud Functions, you can vertically scale an instance, allocating more memory and CPU.

However, Cloud Run cannot scale infinitely and there are limits on the number of instances and the amount of memory and CPU available. For example:

* Concurrency is limited to a maximum of 1,000 simultaneous requests per instance.
* Memory is limited to 32 GB per instance.
* File system is limited to 32 GB per instance.
* CPU is limited to 8 vCPUs per instance.
* The number of instances is limited to 100 per region.
* For larger CPU and memory, the number of instances is limited to a lower amount, and this varies depending on the capacity in the Google Cloud region.

See Cloud Run Quotas and Limitations [documentation](https://oreil.ly/dCrDT) for more details.

A single Cloud Run request is limited to 60 minutes of execution time. However, when Cloud Run does not receive requests, it will throttle down to 0 CPU and will terminate the instance after 60 minutes of inactivity.

Although Cloud Run does have limits, they are generous, and it should be possible to build many services within the restrictions. Cloud Run is a great service to use if you can; it is cost-effective since you are allocating resources directly from Borg only when you need them and not paying for them when you don’t.

**TIP**

When I first used Cloud Run, I tried to deploy a containerized version of the Ghost blogging platform with it, thinking if it did not receive much traffic, it would scale to zero, and this would be a cost-effective way of running it.

However, my Ghost instance had a significant startup time, upward of a minute. When the instance terminated after inactivity, the next request would be met with a “Preparing Ghost” message while it started up again. This is understandable, as Ghost was designed to run on a server as a long-running task and not a serverless platform. While Cloud Run is great for many use cases, it is not suitable for all applications.

However, if you reach limitations or if you are using an existing application that does not fit with the constraints of Cloud Run, it may be necessary to consider a lower-level service like GKE Autopilot. You will have an opportunity to look at this option in [Chapter 14](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch14.html#chapter\_14).

In this case, even if Cloud Run scales down and requires a new instance to serve requests, the instance should be ready quickly, and the user should not notice a significant impact.

## Implementation

The code for this project is in the _skill-service_ directory of the book’s repository. It is implemented in Go. While this may not be a language you have used before, it is worth taking a look at the code to see how it works.

The actual code for storing and retrieving the data is in the `internal/skill/autocomplete` package and is not specific to Cloud Run.

The _main.go_ file is the entry point for the application and is where the Cloud Run-specific code is located. In the `init` function, the configuration the service needs is loaded from environment variables and instances of Google Cloud-specific clients are created to interact with the Cloud Storage and Cloud Logging services. It is good to initialize these clients once and reuse them rather than creating them each time a request is received.

The `main` function sets up an HTTP server with three endpoints. This is using the popular [Gorilla Mux](https://oreil.ly/yMYzZ) library for routing requests. The three endpoints are:

`GET /readiness`

This is used by Cloud Run to determine if the service is healthy. It returns a 200 status. If it fails to return, Cloud Run will assume the instance has failed, terminate it, and start a new one.

`GET /liveness`

This is used by Cloud Run to determine if the service is ready to receive requests. It returns a 200 status code only after a flag is set when the trie data structure has been loaded into memory. This is to ensure the service is not sent requests by Cloud Run until it has the data it needs to respond to them.

`GET /autocomplete`

This is the endpoint that returns the suggestions to the user. It takes a query parameter `query` which is the text the user has typed so far. It returns a JSON array of strings that are the suggestions. This is the only endpoint that will ultimately be exposed to the user.

It is good to understand that Cloud Run just needs to provide an HTTP (or gRPC) endpoint running on port 8080 by default. There is no need to use a specific framework or library or even provide a specific entry point as with Cloud Functions. The point is, you can use whatever you want to build your service, and I am sure you will have your preferences.

Another piece of functionality in the `main` function of note is code to deal with a graceful shutdown. This is important, as Cloud Run will send a SIGTERM signal to the container when it is scaling down or terminating the instance. This is the disposability principle from the 12 factors. Applications should expect to be terminated at any time and should handle this gracefully. Here it is done by listening for the SIGTERM signal and then calling the `Shutdown` method on the HTTP server. This will allow any in-flight requests to be completed before the instance is terminated. In [Chapter 3](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch03.html#chapter\_03), I talked about services being largely stateless, but temporary state is acceptable. This is the time to clear up that temporary state.

The `populate` function retrieves the _tags.csv_ file, created by the Tag Updater function from [Chapter 5](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch05.html#chapter\_05). It accesses Cloud Storage using the `storage` client. It then loads the data into a trie data structure. This is a treelike data structure that is optimized for searching for strings. The trie is stored in memory and is used to respond to requests.

### Getting Ready for Deployment

Before getting hands-on, make sure you have a gcloud CLI either on your local machine or in Cloud Shell, and ensure you are using the same Google Cloud project as in [Chapter 5](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch05.html#chapter\_05), using the command:

```
gcloud config get project
```

You can set a `PROJECT_ID` environment variable again to make it easier to copy and paste commands:

```
export PROJECT_ID=$(gcloud config get-value project)
```

If you are not using the correct project, set it using this command:

```
gcloud config set project $PROJECT_ID
```

### Deploy with Cloud Run

You can use the defaults of Cloud Run to deploy directly from the command line to ensure it works. This will create a container and put it into the Artifact Registry, a Google service for storing arifacts, like containers.

The service is using a Go application with no dependencies, so Cloud Run allows you to build directly from the source code alone.

Set an environment variable for the service name (e.g., `skill-service`):

```
export SKILL_SERVICE_NAME=[SKILL_SERVICE_NAME]
```

Following the 12-factor principle of storing configuration in the environment, you will use an environment variable file to pass the configuration to the service.

If you have created _.env_ files, use this command and apply both the local and parent environment variables again:

```
set -a; source ../.env; source .env ;set +a
```

Create a file called _.env.yaml_ from _env.yaml.template_. This command will substitute values from your environment variable, including those set in [Chapter 5](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch05.html#chapter\_05):

```
envsubst < env.yaml.template >.env.yaml
```

Then run the following command to deploy the service to Cloud Run:

```
gcloud run deploy $SKILL_SERVICE_NAME --source . \
  --env-vars-file=.env.yaml \
  --allow-unauthenticated
```

If you are asked to create an “Artifact Registry Docker repository to store built containers,” select yes. This is where the container will be stored.

You are deploying the service from the source code in the current directory and passing an environment variable file, as you did in [Chapter 5](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch05.html#chapter\_05) with Cloud Functions.

This command is a shortcut for two commands. The first command is:

```
gcloud builds submit --pack image=[IMAGE] .
```

This builds a container using a buildpack using Cloud Build. A buildpack is a concept that existed in both the Heroku and Cloud Foundry platforms. It automatically identifies the application’s language, installs the necessary dependencies, and packages it all up into a container. Cloud Run is using what is effectively version three of the buildpack concept, [Cloud Native Buildpacks](https://buildpacks.io/).

The buildpack to use is determined automatically; in this case, the presence of `go.mod` will be enough to determine that the Go Buildpack should be used and the `main.go` built. It then stores the container image in Google Artifact Registry, where `[IMAGE]` would be the name for the image. In the shortcut command, the image name is automatically generated.

If there were a Dockerfile in the current directory, it would use that to build the container in preference to the buildpack, meaning you could use a custom container.

This is one way of adding more flexibility if you need that customization, but the good thing about Cloud Run is that you don’t need to worry about how the container is built if you don’t want to.

The second command is:

```
gcloud run deploy $SKILL_SERVICE_NAME --image [IMAGE]
```

This deploys the container to Cloud Run. If you run this command, you will be prompted to `Allow unauthenticated invocations to [SERVICE-NAME]`?``. Select `Y`` for this for now, but we will come back to the significance of this later.

As with Cloud Functions, the command will take about a minute to complete. When the service deploys successfully, you will see a message like this:

```
Building using Buildpacks and deploying container to Cloud Run service [skill-service] in project
[skillsmapper-org] region [us-central1]
  ✓ Building and deploying... Done.
  ✓ Uploading sources...
  ✓ Building Container... Logs are available at [https://console.cloud.google
    .com/cloud-build/builds/77c7c356-5269-445b-a013-12c70f542684?project=577723215354].
  ✓ Creating Revision...
  ✓ Routing traffic...
  ✓ Setting IAM Policy...
Done.
Service [skill-service] revision [skill-service-00002-lac] has been deployed and is serving 100
percent of traffic.
Service URL: https://skill-service-j7n5qulfna-uc.a.run.app
```

Use the following command to store the URL of the service in an environment variable:

```
export SKILL_SERVICE_URL=$(gcloud run services describe $SKILL_SERVICE_NAME \
  --format='value(status.url)')
```

You could then open the service in a browser if you wanted, querying for skills starting with “java”:

```
open "${SKILL_SERVICE_URL}/autocomplete?prefix=java"
```

## Smoke Testing

Rather than using a browser, however, you can test the service using `cURL` to test the basic functionality of the service. As the service is unauthenticated, you can use a GET request like this, requesting suggestions for the word `java`:

```
curl -X GET "${SKILL_SERVICE_URL}/autocomplete?prefix=java"
```

All being well, you should get a response like this:

```
{"results":["java","java-10","java-11","java-12","java-13","java-14","java-15",
"java-16","java-17","java-18"]}
```

This verifies the service is working correctly.

## Running the Service Locally

Cloud Run, like many of the Google services you will use in this book, has a local emulator. This is useful if you would like to test the application locally without having to deploy it to Cloud Run on the cloud:

```
gcloud beta code dev
```

This will download and use the same container image that Cloud Build uses and run it locally, as well as containers that support Cloud Run itself. It may take a few minutes to download the images the first time you run the command.

In the background, _minikube_, a local Kubernetes, is being run. Although Cloud Run is a managed service in Cloud Run, it can also be run on a Kubernetes cluster, as in this case. If you have the `kubectl` CLI installed, you can see it running using this code:

```
kubectl get nodes
```

You should see a single Kubernetes node like this:

```
NAME               STATUS   ROLES           AGE   VERSION
gcloud-local-dev   Ready    control-plane   48m   v1.26.3
```

When the deployment completes, you will see a local URL where the service is available: by default, `http://localhost:8080`. You can then use 1cURL1 to test the service:

```
curl -X GET "http://localhost:8080/autocomplete?prefix=java"
```

In the background, this has all been deployed to the local Kubernetes. If you are interested to see how, run this command:

```
kubectl get pods
```

You will see something similar to this:

```
NAME                                 READY   STATUS    RESTARTS   AGE
pod/skill-service-67dc67b44f-vwt9v   1/1     Running   0          83s
```

This is the pod, the Kubernetes component that contains the running skill-service container.

## Securing

As in [Chapter 5](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch05.html#chapter\_05) with Cloud Functions, this Cloud Run service is currently using a default service account with broad permissions.

Cloud Run is also allowing unauthenticated invocations of the service. This may be OK for testing, but in a production environment, you would want to secure the service, and you will see how to do that in [Chapter 11](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch11.html#chapter\_11).

However, ultimately, the combination of risks means you have code that can be called by anyone on the internet using a service account with permissions that could do damage if code with security vulnerabilities was accidentally or maliciously deployed.

For safety, you can create a new service account with the minimum permissions required to run the service. In this case, that will be permission to read the object from Cloud Storage and nothing more. This is the principle of the least privilege which was not one of the original 12 factors, as those principles did not have much to say about security. However, security was emphasized when the 12 factors were revisited, and the principle of the least privilege is a good practice recommended by all cloud providers.

## Create a Service Account

Create an environment variable to hold a service account name (e.g., `skill-service-sa`):

```
export SKILL_SERVICE_SA=[SKILL_SERVICE_SA]
```

Then create the service account with the following command:

```
gcloud iam service-accounts create ${SKILL_SERVICE_SA} \
  --display-name "${SKILL_SERVICE_NAME} service account"
```

Now grant the service account the permissions it needs by adding the Cloud Storage `objectViewer` role. This allows the service account to read objects from Cloud Storage, but not write; that would be more than the minimum privileges needed:

```
gsutil iam ch serviceAccount:$SKILL_SERVICE_SA@$PROJECT_ID.iam.gserviceaccount \
.com:objectViewer gs://$BUCKET_NAME
```

You also need to give the service account permission to write logs to Cloud Logging, Google Cloud’s aggregated logging service:

```
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member=serviceAccount:$SKILL_SERVICE_SA@$PROJECT_ID.iam.gserviceaccount.com \
  --role=roles/logging.logWriter
```

You can then update the service to use the new service account using the `gcloud run services update` command rather than redeploying the service:

```
gcloud run services update $SKILL_SERVICE_NAME --service-account \
$SKILL_SERVICE_SA@$PROJECT_ID.iam.gserviceaccount.com
```

The service will still have the same URL after the update, so check it is still working using:

```
curl -X GET "${SKILL_SERVICE_URL}/autocomplete?prefix=java"
```

Congratulations, the skill-service is now more secure.

## Performance Testing

Previously, you saw how you could test the service using `cURL`. This is a good way to test the basic functionality of the service, but it is not a good way to test the performance.

A better option is to use the Apache Bench tool to make a single request too. This is a command-line tool that can be used to test the performance of an HTTP request. Installation instructions were shown in [Chapter 4](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch04.html#chapter\_04). This command will make a single request to the service:

```
ab -n 1 -c 1 -rk "${SKILL_SERVICE_URL}/autocomplete?prefix=java"
```

You will see a detailed response, including the time taken to process the request:

```
Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:       77   77   0.0     77      77
Processing:   150  150   0.0    150     150
Waiting:      150  150   0.0    150     150
Total:        227  227   0.0    227     227
```

The output provides various measurements:

Connect

The time taken to establish the network connection to the server

Processing

The time taken by the server to process the request and generate a response

Waiting

The time the client had to wait before receiving the response

Total

The total time from the initiation of the request until the receipt of the response

In this case, the total time is 227 ms, which is good, as it is below the target of 500 ms for the service. The _connect_ time of 77 ms depends on your network conditions and isn’t something your service has control over. The _processing_ time of 150 ms is the actual time spent by the skill-service handling your request, which is an indicator of the service’s performance.

However, this is just one query. You can test the performance of the service by sending multiple requests sequentially to see how the response varies.

This command will send 100 requests (`-n`) from a single user (`-c`) to the service:

```
ab -n 100 -c 1 -rk "${SKILL_SERVICE_URL}/autocomplete?prefix=java"
```

The response will look something like this:

```
Percentage of the requests served within a certain time (ms)
  50%    144
  66%    146
  75%    147
  80%    148
  90%    160
  95%    288
  98%    323
  99%    345
 100%    345 (longest request)
```

The results look OK, with the average request time of 144 ms and the longest request taking 345 ms. Most important for us is the 95% percentile, which is 288 ms. This means that 95% of the requests are completed in less than 288 ms, which is under the target of 500 ms.

Now let’s try 100 requests from 10 concurrent users using the following command:

```
ab -n 100 -c 10 -rk "${SKILL_SERVICE_URL}/autocomplete?prefix=java"
```

This has very different results with the 95% percentile now being 1,669 ms in this case. This means that 95% of the requests are completed in less than 1,669 ms which is way over the target of 500 ms. Everyone is having a bad experience.

Let’s see if it is possible to understand what is going on by looking into the logs.

## Logging

By default, anything logged to stdout will appear in Cloud Logging as `INFO` and anything logged to stderr as `WARNING`. You can look at the logs for the service to see the requests being handled by the service:

```
gcloud beta run services logs read $SKILL_SERVICE_NAME
```

However, Cloud Logging, the Google logging service, supports structured logging, which provides much richer information. To see an example of structured logging, let’s look at the last log message for the Cloud Run service:

```
gcloud logging read "resource.labels.service_name: ${SKILL_SERVICE_NAME}" --limit 1
```

You will see that a single log message looks like this:

```
httpRequest:
  latency: 0.189538428s
  protocol: HTTP/1.1
  remoteIp: 82.xx.xx.xx
  requestMethod: GET
  requestSize: '372'
  requestUrl: https://skill-lookup-xfefn34lsa-nw.a.run.app/autocomplete?prefix=java
  responseSize: '755'
  serverIp: 216.239.34.53
  status: 200
  userAgent: ApacheBench/2.3
insertId: 63ed415f0005bb0d6ea1fbd0
labels:
  instanceId: 00f8b6bdb81dec1b6587a81c09bcb444c2c83222fc91d65eb71e410c99d852a51d68bbbb5bc93185f6
   ca718ffe4bbcd8d0e08ef1f2e15a6a63664e2cd1921a
logName: projects/p1-tag-updater-manual/logs/run.googleapis.com%2Frequests
receiveTimestamp: '2023-02-15T20:32:31.669398873Z'
resource:
  labels:
    configuration_name: skill_service
    location: europe-west2
    project_id: p1-tag-updater-manual
    revision_name: skill_service-00019-tan
    service_name: skill_service
  type: cloud_run_revision
severity: INFO
spanId: '8866891408317688295'
timestamp: '2023-02-15T20:32:31.375565Z'
trace: projects/p1-tag-updater-manual/traces/b50ba47749e17f15efa689cebf05b4bd
```

This provides a comprehensive, structured log entry with levels of severity and labels. Structured logging is a way of logging that makes it easier to filter and search logs. It is also easier to parse logs and extract information from them.

You can create structured logging using the Cloud Logging client libraries in your service. Before doing this, the service account your service runs with needs to be granted the `logging.logWriter` role.

You can add this role to the service account using the following command:

```
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member=serviceAccount:$SERVICE_ACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com \
  --role=roles/logging.logWriter
```

Now you can use the structured logging in the service. In the case of Go, the service uses the `cloud.google.com/go/logging` package. In the init function, it initializes the logging client, as it does for the Cloud Storage and BigQuery clients previously. It then creates a logger for the service:

```
	ctx := context.Background()
	loggingClient, err := logging.NewClient(ctx, projectID)
	if err != nil {
		log.Fatalf("failed to create client: %v", err)
	}
	logger = loggingClient.Logger(serviceName)
```

Logging is then added to record the time taken to populate the trie from the cloud storage object and the time taken to search the trie using structured log statements like this:

```
	logger.Log(logging.Entry{
		Severity: logging.Debug,
		Payload:  fmt.Sprintf("populate took %v", duration)})
```

After updating the service, we can look at the logs again to see the structured logging. For example, you can look at the logs:

* For your service
* With the `textPayload` containing “populate took”
* In the last minute; `freshness 1m`
* Limiting the results to five

Use this command:

```
gcloud logging read "resource.labels.service_name: ${SKILL_SERVICE_NAME} \
textPayload: populate tags took" --limit 5 --freshness 1m
```

And you can just return the `textPayload` field (the one you are interested in) using this command:

```
gcloud logging read "resource.labels.service_name: ${SKILL_SERVICE_NAME} \
textPayload: populate tags took" --limit 5 --freshness 1m --format="value(textPayload)"
```

This returns results like this:

```
populate took 1.004742261s
populate took 1.125826744s
populate took 1.007365607s
populate took 1.042601112s
populate took 1.018384088s
```

This shows us that it takes about a second to populate the trie from the cloud storage object. Similarly, you can look at the logs for the search function, which starts with a “autocomplete for” prefix using:

```
gcloud logging read "resource.labels.service_name: ${SKILL_SERVICE_NAME} \
textPayload: autocomplete for" --limit 5 --freshness 1m \
--format="value(textPayload)"
```

And you should get results like this:

```
autocomplete for java took 161.854392ms
autocomplete for java took 228.471095ms
autocomplete for java took 205.602819ms
autocomplete for java took 262.176097ms
autocomplete for java took 109.83006ms
```

This shows you that it takes between 100 ms and 250 ms to search the trie for the autocomplete results. Let’s see if you can use this to improve the performance of the service.

## Improving Performance

You have deployed to Cloud Run using the default settings, which are:

* 1 vCPU
* 512 MB memory
* Concurrency: 80
* Minimum instances: 0
* Maximum instances: 100

You can go to the Cloud Run console, to the default dashboard for the service, and look at the metrics for the service to see what is happening and how you could improve response time:

```
open \
"https://console.cloud.google.com/run/detail/${REGION}/${SKILL_SERVICE_NAME}/ ↩
metrics?project=${PROJECT_ID}"
```

There are two things to note. The first is that the `Container startup latency` is approximately two seconds. This means, if a container is not running, it takes about two seconds to start a new container. What you could do is set the minimum number of instances to 1 instead of 0 so that there is always one container running.

You may also notice that the CPU of the container gets high reaching above 80%. This means that the container is not able to process requests as fast as it could. Any requests that come in while the container is busy are queued and processed when the container is free.

You could increase the number of CPUs for a container from 1 to 2 or reduce the concurrency from 80 to 40 to reduce the number of requests that are processed at the same time.

The beauty of Cloud Run is that you can change these settings without redeploying the service. You can change the settings of the service using the `gcloud run services update` command.

First, collect a baseline of the performance of the service:

```
ab -n 100 -c 10 -rk "${SKILL_SERVICE_URL}/autocomplete?prefix=java"
```

Just look at the value for 95%. In this case, 95% of requests were served within 1,912 seconds.

Then change the number of minimum instances to 1 using this command:

```
gcloud run services update $SKILL_SERVICE_NAME --min-instances 1
```

Run the `ab` command again; the request time should stay roughly the same. However, you will have eliminated the wait for a container to start up.

Then double the number of CPUs and run the `ab` command again:

```
gcloud run services update $SKILL_SERVICE_NAME --cpu 2
```

Run the `ab` command again, you should see the 95% request time is reduced, as each request is processed faster; but it may not make as much difference as you might expect.

Another thing to try is halving the concurrency to reduce the load on the service while returning the CPU to 1. This can be done using the following command:

```
gcloud run services update $SKILL_SERVICE_NAME --cpu 1 --concurrency 40
```

Rerunning the test of 100 requests from 10 concurrent users using the following command:

```
ab -n 100 -c 10 -rk "${SKILL_SERVICE_URL}/autocomplete?prefix=java"
```

You can see how this has made performance considerably worse. This is because the service is now only processing 40 requests at a time instead of 80. This means that the requests are queued and take longer to process.

These are examples of how tweaking Cloud Run can increase or decrease the performance of the service; however, it is haphazard, as we are only observing from the outside. We do not know easily what is happening inside the service or what is affecting the performance of the service from Cloud Run and Google’s networking and load balancing.

In [Chapter 13](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch13.html#chapter\_13), you will learn how to add greater observability to the service so that you can see the impact of these changes on the performance of the service more systematically.

## How Much Will This Solution Cost?

Now let’s look at how much the solution will cost.

Cloud Run is charged based on three factors:

CPU and memory per 100 ms of execution

For example, 1 vCPU and 512 MB of memory for 1 second could be an execution unit. If you assigned 2 vCPUs and 1 GB of memory to a service, it would be charged four times the amount per 100 ms; but if the extra resources mean it completed in 500 ms instead of 1 second, you would only be billed twice the amount.

A cost per request

The first one million requests are free per month; then they are $0.40 per one million requests. Although this is a very small amount, it can add up, and in [Chapter 14](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch14.html#chapter\_14), we will look at scaling options when you have many million requests.

Network ingress and egress

As with most services in Google Cloud, there is also a charge for network ingress and egress. The requests and responses for this service are small, but again they can add up, and later in the book we will look at options for this too.

## Summary

In this introduction to Cloud Run, you have learned how to deploy a service to Cloud Run and how to monitor the performance of the service. You have also seen how to tweak the performance of the service by changing the number of CPUs and the concurrency of the service, but this is not always straightforward.

For this project, you used the following Google Cloud services directly:

* Cloud Run is used as the container runtime to run the containerized service.
* Cloud Logging is used to provide logging for the service.
* Cloud Storage is used to retrieve the previously stored list on tags.

Behind the scenes, you have also been using Cloud Build and Artifact Registry to build and store the container images, respectively, and we will discuss these later.

In [Chapter 7](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch07.html#chapter\_07), you will step up the complexity again and look at another Cloud Run service that is used to provide a REST API and is backed by a relational database.
