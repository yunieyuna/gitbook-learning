# 5. Project 1: Tag Updater With Cloud Functions

## Chapter 5. Project 1: Tag Updater with Cloud Functions

In this initial project, you will write the first part of the Skills Mapper application. You will be introduced to some of the higher-level abstractions in Google Cloud and be shown how you can solve a real-world problem at a minimal cost.

You will learn how to solve the requirement in three ways:

* Manually, using the gcloud CLI alone
* Automated, using a Cloud Function and Cloud Scheduler
* Fully automated, using the Cloud Function and Terraform to deploy

**NOTE**

The code for this chapter is in the [`tag-updater` folder of the GitHub repository](https://oreil.ly/os8n4).

## Requirements

Let’s dive into the requirements for this project.

### User Story

The user story for this piece of functionality can be written as shown in [Figure 5-1](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch05.html#p1-postit).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_0501.png" alt="" height="324" width="828"><figcaption></figcaption></figure>

**Figure 5-1. Project 1 user story**

### Elaborated Requirements

This project also has the following specific requirements:

* The list of skills should include technologies, tools, and techniques, and be comprehensive and unambiguous.
* Although new skills emerge frequently, it is not every day so limiting updates to weekly is sufficient.
* The solution should be reliable, require minimal maintenance, and be low cost.
* The resultant list of skills should be easy to consume by future services.

## Solution

Maintaining a list of technical skills is a big undertaking. Fortunately, [Stack Overflow](https://oreil.ly/kpLAQ) is already doing that by maintaining a crowdsourced list of over 63,000 tags, terms which are used to categorize questions. Google Cloud provides all Stack Overflow data, including tags, as a public dataset in BigQuery, the enterprise data warehouse service.

To obtain an up-to-date list of technical skills, you can extract them from the public dataset of BigQuery directly.

With cloud native solutions, we favor simplicity. The simplest way is to store a list of terms in a file. Cloud Storage is the Google Cloud service for storing object data like this. If you store a file in Cloud Storage, it will be easily consumable by other services.

You need a small amount of code to extract Stack Overflow tags from the BigQuery dataset and to store the resultant list of skills as a file in Cloud Storage. Cloud Functions is an effective way of running this type of glue code, as you only pay for the short amount of time the code is running. This is a serverless solution, meaning it is a fully managed service with no servers to maintain.

You need to update the list of skills once a week. Cloud Scheduler is a fully managed service that runs jobs on a schedule. You can use this to schedule the execution of a Cloud Function. You can use this to create a new list of skills every week and retry if there is a failure.

### Architecture Diagram

[Figure 5-2](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch05.html#tag-updater-design) is a diagram of the architecture you will be implementing.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098145071/files/assets/cdgc_0502.png" alt="" height="466" width="1377"><figcaption></figcaption></figure>

**Figure 5-2. Tag updater design**

You can draw similar diagrams using the [Google Cloud architecture diagramming tool](https://oreil.ly/ZAtlE), which is a great tool for drawing cloud architectures.

### Summary of Services

Here is a summary of the Google Cloud services you will be using in this solution.

#### BigQuery

BigQuery is a Google data warehouse solution designed for performing analytics on vast amounts of data. It uses SQL syntax, like a relational database. However, here you will just be using it to query a relatively small set of public data.

#### Cloud Storage

Cloud Storage is Google Cloud’s object store, similar to S3 on AWS. This is designed to store objects, text, and binary files containing unstructured data that does not need to be queried. It is a low-cost way of storing data.

#### Cloud Functions

Cloud Functions is Google Cloud’s highest-level abstraction for running code. To start with, you will use Cloud Functions because it is the simplest and most cost-effective option for running code occasionally. This is Google’s serverless offering and the nearest equivalent to AWS Lambda or Azure Functions. It is great for running the sort of glue code we need for this service.

If Cloud Functions was a means of transport, it would be like a taxi; it takes you where you want to go, but your involvement in driving is minimal. You just state the destination and Cloud Functions gets you there. It is ideally suited for short trips, and maintenance of the car is not your concern.

Cloud Functions has two generations. I will concentrate on the 2nd gen, as it is more flexible and has a larger memory limit. This supports [code written in Node.js, Python, Go, Java, Ruby, PHP or .NET Core](https://oreil.ly/\_P06t).

Code is automatically packaged into a managed container that is invoked by an event, but the container itself is hidden from you. Execution is in a slice of virtual CPU (vCPU) and memory, the slice of which you can specify. You can use a maximum of 16GiB of memory and four vCPU cores and execute for a maximum of 60 minutes. For more details, see [the quotas documentation](https://oreil.ly/sheUK). This is a big improvement on the 1st gen, which had a maximum of 8GiB of memory and a maximum of 9 minutes of execution time. However, the default of a maximum of 60 seconds, one vCPU core, and 256 MB of memory will be sufficient for this purpose.

In fact, Cloud Functions (2nd gen) is effectively a wrapper around two other services, Cloud Run and Cloud Build, which you will use in [Chapter 6](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch06.html#chapter\_06). Cloud Run is a managed container platform that allows you to run containers that are invocable via HTTP requests or events. Cloud Build is a managed build service that allows you to build containers.

#### Cloud Scheduler

Cloud Scheduler is a fully managed enterprise-grade job scheduler similar to Cron. In this case, you are going to use it to schedule the execution of the Cloud Function.

## Command Line Implementation

Before getting hands-on, make sure you have a gcloud CLI client either on your local machine or in Cloud Shell and create a new project, as described in [Chapter 4](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch04.html#chapter\_04); store the `PROJECT_ID` in an environment variable using:

```
export PROJECT_ID=$(gcloud config get project)
```

### BigQuery

To start with, you can use the `bq` command-line tool to try out the query you will use to retrieve the Stack Overflow tag names from the public dataset. This tool is installed as part of the Google Cloud SDK:

```
bq query --max_rows=10 --nouse_legacy_sql --format=csv \
"SELECT tag_name FROM bigquery-public-data.stackoverflow.tags order by tag_name"
```

Use the following flags:

`--max_rows`

Limits the number of results to 10 instead of the default 100

`--nouse_legacy_sql`

Uses [Google Standard SQL](https://oreil.ly/3Q01I) for the query

`--format=csv`

Formats the result as a CSV file

The results of this command should show the first ten Stack Overflow tag names. You will use a similar query in your service. For now, let’s output all the tags to a file named _tags.csv_. There are approximately 63,000 Stack Overflow tags, so setting the maximum rows to 100,000 will retrieve them all.

**TIP**

It is good practice to always specify the maximum rows with BigQuery, as it is billed by the amount of data queried. One day, you may accidentally write a query that queries trillions of rows and be left with a hefty bill.

Define an environment variable for the `FILE_NAME` (e.g., _tags.csv_); even when programming at the command line, it is good to follow the cloud native principle of externalizing configuration:

```
export FILE_NAME=[FILE_NAME]
```

Now you can issue the `bq` command to write the tags to the file:

```
bq query --max_rows=100000 \
  --nouse_legacy_sql \
  --format=csv \
  "SELECT tag_name FROM bigquery-public-data.stackoverflow.tags order by tag_name" >$FILE_NAME
```

You can check that it was successful by listing the number of lines in the file:

```
wc -l $FILE_NAME
```

If all goes well, the result should be approximately 63,654 lines, allowing one line for the CSV header.

### Cloud Storage

You now need to create a Cloud Storage bucket to store the file you generate from your query. You can do that with the `gsutil` command, which is also included with the gcloud CLI.

First, create a `BUCKET_NAME` environment variable with the bucket to use. Like a project ID, the bucket name must be globally unique. As your project ID is unique, you can use that as a prefix to the bucket-name (e.g., `skillsmapper-tags`):

```
export BUCKET_NAME="${PROJECT_ID}-[BUCKET_SUFFIX]"
```

Then use the `gsutil` command to create the new bucket:

```
gsutil mb gs://$BUCKET_NAME
```

With the bucket created, you can then copy the file containing the list of tags to the bucket:

```
gsutil cp $FILE_NAME gs://$BUCKET_NAME/$FILE_NAME
```

You can check that the file has been created successfully by again counting the number of lines and making sure that matches the approximately 63,654 lines you had in the file you generated:

```
gsutil cat gs://$BUCKET_NAME/$FILE_NAME | wc -l
```

Running commands like this would be one way of keeping the tags up to date. Alternatively, you could automate it into a bash script and save it as a file named _update-tags.sh_:

```
include::code/update-tags.sh
```

Then run the script providing the bucket name and file name as variables:

```
./update-tags.sh $BUCKET_NAME $FILE_NAME
```

You could then manually run the script periodically or use a cron job on a machine with Google Cloud credentials; however, there is a better cloud native approach where you can implement the task programmatically using a Cloud Function.

## Cloud Native Implementation

Here, you will implement the same functionality as the manual implementation but using a Cloud Function scheduled with Cloud Scheduler. This will allow you to automatically update the tags periodically without any manual intervention.

### Cloud Functions

As mentioned earlier, Cloud Functions can only be written in certain programming languages. The code accompanying this book includes a [Cloud Function written in Go](https://oreil.ly/W6HJk). This effectively performs the same task as the gcloud CLI steps but uses the BigQuery and Cloud Storage client libraries for Go.

Don’t worry too much about the code if you are not familiar with Go, but there are some key points to note:

```
func init() {
	// err is pre-declared to avoid shadowing client.
	var err error
	bigQueryClient, err = bigquery.NewClient(ctxBg, projectID)
	if err != nil {
		log.Fatalf("bigquery.NewClient: %v", err)
	}
	storageClient, err = storage.NewClient(ctxBg)
	if err != nil {
		log.Fatalf("storage.NewClient: %v", err)
	}
	// register http function
	functions.HTTP("tag-updater", updateTags)
}
```

In the init block above, a BigQuery and Cloud Storage client is created. These are initialized once and then reused for each invocation of the function. This is good practice, as it reduces the time needed to respond to requests. At the end of the block, a function named `updateTags` is registered as the HTTP trigger. This is the entry point for the function and is called when the function is invoked:

```
func updateTags(w http.ResponseWriter, r *http.Request) {
	var err error
	numberOfTagsRetrieved, data, err := retrieveTags()
	if err != nil {
		log.Printf("failed to retrieve tags: %v\n", err)
		http.Error(w, "retrieving tags failed", http.StatusInternalServerError)
	}
	err = writeFile(data)
	if err != nil {
		log.Printf("failed to write file: %v\n", err)
		http.Error(w, "writing file failed", http.StatusInternalServerError)
	}
	message := fmt.Sprintf("%v tags retrieved and written to %s as %s",
		numberOfTagsRetrieved, bucketName, objectName)
	log.Println(message)
	w.WriteHeader(http.StatusOK)
	fmt.Fprint(w, message)
}
```

The `updateTags` function in the code is handling an HTTP request. In response, it calls a function that retrieves the tags from BigQuery Stack Overflow dataset using the BiqQuery client. It then calls another function that writes the tags to Cloud Storage using the Cloud Storage client in a similar way to the gcloud CLI steps. Note that errors are handled by logging the error and returning an HTTP error response; success is handled by logging the success and returning an HTTP success response. This is important, as it is the HTTP response that is used by Cloud Functions to determine whether the function invocation was successful or not.

### Configuration

Just like the bash script version, the configuration for the Cloud Function should be externalized. In this case, you don’t have any secrets like passwords, but you do have three arguments to pass as environment variables, shown in [Table 5-1](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch05.html#env-vars).

| Environment variable | Description                                                                                          |
| -------------------- | ---------------------------------------------------------------------------------------------------- |
| PROJECT\_ID          | The ID of the Google Cloud project                                                                   |
| BUCKET\_ID           | The ID of the Cloud Storage bucket                                                                   |
| OBJECT\_NAME         | Cloud Storage is an object store, so when files are uploaded to it, they are referred to as objects. |

You can provide these as a YAML file when you deploy the function. The [code accompanying this book](https://oreil.ly/O56i7) shows an example of the structure:

```
PROJECT_ID: $PROJECT_ID
BUCKET_NAME: $BUCKET_NAME
OBJECT_NAME: $FILE_NAME
```

As you set environment variables earlier, you can generate an _env.yaml_ with this command:

```
envsubst < env.yaml.template > env.yaml
```

The main difference between this function and the bash script is that it writes the retrieved tags to an object in Cloud Storage directly rather than storing them to a file and then uploading the file.

Cloud Functions run in a Google Cloud region. In [Chapter 4](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch04.html#chapter\_04), you specified a default region for the Google CLI using this command:

```
gcloud config set functions/region [REGION]
```

However, I like to declare an environment variable for the region I want to deploy to (for example, `europe-west2`), so I explicitly know which region I am deploying to:

```
export REGION=[REGION]
```

Then use this to set the default region for Cloud Functions:

```
gcloud config set functions/region $REGION
```

The services Cloud Functions needs are not enabled by default, so you need to enable them for the project. As mentioned earlier, the Cloud Build service is used for building a container. The container is stored in Artifact Registry, Google Cloud’s container registry. The container is run using Google’s Cloud Run service. This means you need the following services enabled:

* `cloudfunctions.googleapis.com`—Cloud Functions
* `cloudbuild.googleapis.com`—Cloud Build
* `artifactregistry.googleapis.com`—Artifact Registry
* `run.googleapis.com`—Cloud Run

You can enable the services with this command:

```
gcloud services enable cloudfunctions.googleapis.com
gcloud services enable cloudbuild.googleapis.com
gcloud services enable artifactregistry.googleapis.com
gcloud services enable run.googleapis.com
```

Now create an environment variable for the name of the function to deploy (e.g., `tag-updater`):

```
export CLOUD_FUNCTION_NAME=[CLOUD_FUNCTION_NAME]
```

To deploy this code as a Cloud Function, you can use the following gcloud command:

```
gcloud functions deploy $CLOUD_FUNCTION_NAME \
    --gen2 \
    --runtime=go120 \
    --region=$REGION \
    --trigger-http \
    --no-allow-unauthenticated \
    --env-vars-file env.yaml
```

Here you are using the following flags:

`--gen2`

Deploys a Cloud Function using generation 2

`--runtime=go120`

Uses the Go 1.20 runtime

`--region=$REGION`

Deploys to this region

`--trigger-http`

The function should be triggered by an HTTP request

`--no-allow-unauthenticated`

The function should not be publicly accessible

`--env-vars-file`

Uses the environment variables in the _env.yaml_ file

Executing the command will take a few minutes as it works through building, deploying, and testing that the function is healthy. When it has completed, you will see a URI for the function that looks like this: `https://$CLOUD_FUNCTION_NAME-something.a.run.app`.

You can retrieve this using:

```
gcloud functions describe $CLOUD_FUNCTION_NAME --gen2 --region=$REGION \
--format='value(serviceConfig.uri)'
```

If you open this URI in a web browser, you will see a permission denied message. This is a good thing; it means an unauthenticated person cannot trigger the function.

You can also check that the function has been deployed using the command:

```
gcloud functions list
```

This should show a result similar to this, with the state showing active:

```
NAME         STATE   TRIGGER       REGION        ENVIRONMENT
tag-updater  ACTIVE  HTTP Trigger  europe-west2  2nd gen
```

You can then run the function using:

```
gcloud functions call $CLOUD_FUNCTION_NAME --gen2 --region=$REGION
```

You should see a result like this:

```
  63653 tags retrieved and written to skillsmapper-tags as tags.csv
```

To check the logs for the function to see more details, use the following command:

```
gcloud functions logs read $CLOUD_FUNCTION_NAME --gen2 --region=$REGION
```

This will give you more detailed logs, including the system logs when the function was deployed.

#### Using a Service Account

By default, the Cloud Function will have used the default service account for the project `service-ACCOUNT_ID@PROJECT_ID.iam.gserviceaccount.com`. This is not a great idea, as it gives broad access to all resources in the project.

You can create a new service account that has the minimum permissions required for the function to work and no more. Specifically, the service account needs the following permissions:

* Execute BigQuery queries using the `bigquery.jobUser` predefined role.
* Write to Cloud Storage using the `storage.objectAdmin` predefined role, as it will need to be able to both create new objects and delete previous ones.

Create an environment variable to hold a service account name (e.g., `tag-updater-sa`):

```
export SERVICE_ACCOUNT_NAME=[SERVICE_ACCOUNT_NAME]
```

Then create the service account with the following command:

```
gcloud iam service-accounts create $SERVICE_ACCOUNT_NAME \
--display-name "${CLOUD_FUNCTION_NAME} service account"
```

You can check that it has been created using:

```
gcloud iam service-accounts list
```

When created, the service account has the format \[_SER⁠VICE\_ACCOUNT\_NAME]​@\[PROJECT\_ID].iam.gserviceaccount.com_. This is how you reference it.

Now grant the service account the permissions it needs.

Add the BigQuery job user role:

```
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member=serviceAccount:$SERVICE_ACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com \
  --role=roles/bigquery.jobUser
```

Add the Cloud Storage objectAdmin role:

```
gsutil iam ch serviceAccount:$SERVICE_ACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com:objectAdmin \
gs://$BUCKET_NAME
```

Now you can redeploy the Cloud Function, specifying that the newly created service account is used with the `--service-account` flag:

```
gcloud functions deploy $CLOUD_FUNCTION_NAME \
--gen2 \
--runtime=go120 \
--service-account="${SERVICE_ACCOUNT_NAME}@${PROJECT_ID}.iam.gserviceaccount.com" \
--trigger-http \
--no-allow-unauthenticated \
--env-vars-file env.yaml
```

When the command completes, it will show the URI of the Cloud Function. Store this in an environment variable so you can reference it later:

```
export CLOUD_FUNCTION_URI=$(gcloud functions describe $CLOUD_FUNCTION_NAME \
--gen2 --format='value(serviceConfig.uri)')
```

You are now ready to test the Cloud Function.

### Testing with cURL

Your Cloud Function is secure in that it can only be evoked by an authenticated user with the correct permissions. If you try to invoke it using `cURL`, you will get a 403 Forbidden error:

```
curl $CLOUD_FUNCTION_URI
```

This is a good thing, as it means that not just anyone can invoke the function and cause it to run. However, there is a way to test it by passing an authentication token to the function:

```
curl -H "Authorization: Bearer $(gcloud auth print-identity-token)" $CLOUD_FUNCTION_URI
```

This will use the permissions of the current user to invoke the function. Again, you should see a message:

```
63653 tags retrieved and written to skillsmapper-tags as tags.csv
```

### Cloud Scheduler

You can now schedule the Cloud Function to run every Sunday at midnight.

First, you need to enable Cloud Scheduler:

```
gcloud services enable cloudscheduler.googleapis.com
```

Create an environment variable for the name of the job (e.g., `tag-updater-job`):

```
export JOB_NAME=[JOB_NAME]
```

Unfortunately, Cloud Scheduler will not be able to trigger the Cloud Functions at the moment; the function does not allow unauthenticated invocations.

You need to create another service account for the scheduler (e.g., `tag-updater-invoker-sa`):

```
export INVOKER_SERVICE_ACCOUNT_NAME=[INVOKER_SERVICE_ACCOUNT_NAME]
```

```
gcloud iam service-accounts create $INVOKER_SERVICE_ACCOUNT_NAME \
	--display-name "${CLOUD_FUNCTION_NAME} invoker service account"
```

Now grant the new service account the `run.invoker` role. Note that, as this is a 2nd gen Cloud Function, the permission to invoke the function is granted on the underlying Cloud Run service:

```
gcloud run services add-iam-policy-binding $CLOUD_FUNCTION_NAME \
  --member=serviceAccount:$INVOKER_SERVICE_ACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com \
  --role='roles/run.invoker'
```

By default, Cloud Scheduler will retry a job three times if it fails. You can change this using the `--max-retry-attempts` flag.

You authenticate with an OIDC token, which is linked to the service account that has the invoker role.

Use the following command to create the job:

```
gcloud scheduler jobs create http ${JOB_NAME} \
--schedule="0 0 * * SUN" \
--uri=${CLOUD_FUNCTION_URI} \
--max-retry-attempts=3 \
--location=${REGION} \
--oidc-service-account-email="${INVOKER_SERVICE_ACCOUNT_NAME}@${PROJECT_ID}.iam.gserviceaccount.com" \
--oidc-token-audience="${CLOUD_FUNCTION_URI}"
```

Check the status of the job in the job list:

```
gcloud scheduler jobs list --location=${REGION}
```

You should see that the job is enabled and scheduled to run each Sunday at midnight:

```
ID               LOCATION  SCHEDULE (TZ)          TARGET_TYPE  STATE
tag-updater-job  us-west2  0 0 * * SUN (Etc/UTC)  HTTP         ENABLED
```

To test the job, you can trigger it manually, overriding the schedule:

```
gcloud scheduler jobs run $JOB_NAME --location=$REGION
```

Check the status of the job:

```
gcloud scheduler jobs describe $JOB_NAME --location=$REGION
```

You will see a `lastAttemptTime`, which corresponds to when you triggered the job.

Check the log of the Cloud Function:

```
gcloud functions logs read ${CLOUD_FUNCTION_NAME} --gen2 --region=${REGION}
```

Alternatively, you can stream logs directly from the underlying Cloud Run service:

```
gcloud beta run services logs tail ${CLOUD_FUNCTION_NAME} --project ${PROJECT_ID}
```

Now check the data of the file in Cloud Storage as you did when running at the command line:

```
gsutil cat gs://$BUCKET_NAME/$OBJECT_NAME | wc -l
```

If all is well, you should see over 65,000 tags in the file.

## Terraform Implementation

As you can see, although the solution is simple, there are still many steps to set it up.

In [Chapter 12](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch12.html#chapter\_12), you will see how Terraform can be used to fully automate this type of deployment; in the [Appendix A](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/app01.html#appendix), the deployment for the whole of Skills Mapper has been automated. For now, here is a peek at how to do the same with Terraform.

To use this Terraform implementation, you need to have Terraform installed and configured, and you also need to have created a Google Cloud project, as described in [Chapter 4](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch04.html#chapter\_04). Use a new project so as not to conflict with what you have set up in this chapter.

Enable the required APIs using the following commands:

```
gcloud services enable cloudfunctions.googleapis.com
gcloud services enable cloudbuild.googleapis.com
gcloud services enable artifactregistry.googleapis.com
gcloud services enable run.googleapis.com
gcloud services enable cloudscheduler.googleapis.com
```

Now you can deploy the solution using Terraform. From the _terraform_ directory, run the following command, providing the `PROJECT_ID` environment variable of the project to deploy to:

```
terraform apply -var project_id=${PROJECT_ID}
```

To clear everything up, run the following command:

```
terraform destroy
```

The reason to introduce this here is that you may be getting put off by all the gcloud commands. They are useful for learning but not essential. When you want to move to a reproducible environment, Terraform will come to your rescue.

## Evaluation

Now let’s look at how the solution will scale and how much it will cost.

### How Will This Solution Scale?

The scaling of this solution is not a great concern, as it is a single task that runs weekly. It is also very unlikely that there will be a significant change in the number of tags to retrieve from the Stack Overflow dataset.

However, if you did want to schedule the task more frequently or even add tasks to collect data from other sources, you could easily do so by adding more Cloud Functions and changing the frequency of the Cloud Scheduler jobs.

### How Much Will This Solution Cost?

The costs of this solution are very close to zero (and I mean close). The cost will likely be less than $0.01 per month:

* _Cloud Storage_ data is charged at $0.026 per GB/month. This solution uses less than 1 MB of storage, so the cost is negligible.
* _Cloud Functions_ are charged at $0.0000002 per GB/s. This solution uses less than 256 MB of memory for less than a minute per month, so the cost is negligible.
* _Cloud Scheduler_ is charged at $0.01 per 100,000 invocations. This solution uses less than five invocations per month, so the cost is negligible too.
* _BigQuery_ queries are charged after the first 1TB of data is scanned per month. This solution uses less than 10 MB of data per month, so there will be no cost.
* You will also be charged for moving around small amounts of data between services, but again, this is negligible.

This is the type of service that makes a lot of sense in a cloud native environment. A task that may previously have needed a dedicated server can now be run for virtually nothing.

## Summary

You have built a solution that can be highly reliable and will run for minimal cost. This service should be able to sit in the background running for years uninterrupted, if needed.

The following are Google Cloud Services used in the solution:

* gcloud CLI is used for interacting with the Google Cloud API.
* `bq` is used for working with BigQuery at the command line.
* `gsutil` is used for working with Cloud Storage.
* BigQuery is used for querying the Stack Overflow public dataset.
* Cloud Storage is used as a simple way of storing the list of tags.
* Cloud Functions is used as a high-level abstraction to run code serverlessly.
* Cloud Scheduler is used as the mechanism scheduling runs of the job.

In the next project, you will take the list of tags that this service has provided and make it available for a user to select skills from.
