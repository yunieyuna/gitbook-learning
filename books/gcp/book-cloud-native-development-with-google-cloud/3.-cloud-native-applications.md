# 3. Cloud Native Applications

## Chapter 3. Cloud Native Applications

So far in this book, the discussion about the motivations and principles behind cloud native technologies addressed the _why_ and _what_. Now, let’s dive into the _how_. This chapter introduces the techniques, tools, and technologies that make the cloud native approach possible, specifically in the context of Google Cloud.

## Autonomous Components Communicating with Messages

Alan Kay, a prominent computer scientist primarily known for pioneering object-oriented programming and the development of the Smalltalk programming language in the 1970s, envisioned computer software as akin to a biological system constituted of interlinked cells. His model fosters a mindset that promotes application modularity and reusability.

The cornerstone of Kay’s concept is _messaging_, a mechanism that allows application cells to communicate with each other. Unlike traditional methods that rely on a shared state, messaging ensures interaction through sending and receiving of information. Essentially, Kay advocated for a world of autonomous components, interacting seamlessly through messages.

Interestingly, cloud native applications share striking parallels with Alan Kay’s vision, principally the attribute of loosely coupled components. This idea, in itself, is not novel. However, the innovation lies in the amalgamation of contemporary tools, techniques, and technologies that create a robust toolkit for building cloud native applications. Let’s explore this in detail, shedding light on how such elements blend to architect modern, efficient, and scalable applications in the cloud.

## Harnessing Cloud Potential with the 12-Factor App Principles

Heroku, a cloud platform service established in 2007, quickly became a favorite among developers. It offered an effortless environment for hosting applications, a haven that provided support right from the development phase to secure deployment, albeit within the platform’s specific constraints. Many of today’s cloud native platforms, including Google Cloud, strive to emulate this developer-friendly experience while minimizing limitations.

In 2012, after extensive engagement with their customers, Adam Wyner and his team at Heroku began to recognize common patterns among applications that performed exceptionally well on their platform. This observation led to the consolidation of these patterns into a cohesive set of guidelines and best practices for developing cloud native applications, known as the [12-factor app principles](https://12factor.net/).

The 12-factor app principles found extensive application in Cloud Foundry, another platform inspired by Heroku. As cloud platforms continue to evolve, these principles have proven enduring, gaining widespread acceptance as a comprehensive blueprint for designing cloud-ready software.

If you’re interviewing for a cloud native developer position, it’s likely the concept of 12-factor applications will emerge as a foundational topic. While many candidates might recollect some of the factors’ names, similar to recalling catchy acronyms like SOLID or DRY, understanding the underlying concept is far more critical. You should comprehend the problem each factor addresses and how to practically implement them.

Moreover, it’s worth noting that the 12-factor app principles were originally devised with a specific platform in mind. Therefore, it’s essential to revisit these principles in that original context before applying them to Google Cloud, to ensure their most effective use and relevance.

### I. Codebase: One Codebase Tracked in Version Control, Many Deploys

When initially encountering this factor, you might interpret it as advocating for monorepos, which advocate for housing the code for multiple projects within a single version control repository. However, this interpretation isn’t accurate.

The principle of _one codebase_ dictates that a singular codebase should serve all deployment environments, such as development, staging, and production. This approach negates the practice of having separate repositories configured for each environment or, even worse, maintaining a repository with a production-specific fix that hasn’t been integrated back into earlier environments.

The primary goal here is to ensure consistency. By deploying code from the same repository across all environments, you significantly reduce the potential for inconsistencies and, consequently, unexpected bugs. This practice bolsters reliability and predictability across all stages of the application life cycle.

### II. Dependencies: Explicitly Declare and Isolate Dependencies

Applications often rely on a multitude of external libraries and frameworks. A common mistake is to assume that a specific version of a library installed on one developer’s machine will be equally available on another’s, or even in a production environment. This assumption can lead to unexpected errors and the infamous “it works on my machine” defense.

To sidestep this issue, it’s critical to explicitly declare all dependencies, inclusive of their precise version numbers. This approach ensures all necessary components are readily available across all development and deployment environments, thus fostering consistency and eliminating potential errors.

Later in this chapter, you will see how containers can greatly facilitate this process, further enhancing the robustness and reliability of your applications.

### III. Config: Store Config in the Environment

In the application of the one codebase principle, the challenge emerges of how to manage environment-specific configurations, like database connection details or API keys, which aren’t suited for inclusion in the codebase. The answer lies in utilizing the environment itself as the storage medium for these configurations, thereby allowing them to be injected directly into the application. This can be accomplished through environment variables or distinct configuration files.

However, when dealing with sensitive information such as API keys, passwords, or other credentials, storing them directly in the environment can introduce unnecessary security risks. Exposure of environment variables, especially in multitenant environments or open source projects, could lead to unintended access to these secrets.

To mitigate this, a more secure alternative involves the use of secret management services. These specialized services, such as Google Cloud Secret Manager, offer a secure and convenient method to handle sensitive data. They store, manage, and access secrets like API keys, passwords, or certificates in a centralized and secure manner. These secrets are encrypted at rest and in transit and can be accessed programmatically by applications that require them. They also provide auditing and versioning capabilities, which help to monitor and control access to your secrets.

So, while environment-dependent configurations are stored within the environment, sensitive data should be managed using dedicated secret management services. This combination ensures that each environment is correctly configured while maintaining the confidentiality and integrity of your credentials, hence enhancing the overall security posture of your application.

**TIP**

Storing sensitive information within the codebase poses a significant risk. Cybercriminals employ sophisticated, automated tools that scour code repositories like GitHub, looking for any vulnerabilities such as embedded credentials. Once discovered, these credentials can be exploited in numerous ways. For instance, cloud credentials could allow a hacker unauthorized access to private data. Even worse, they could harness these acquired privileges to spawn an army of cloud-based virtual machines for their purposes, such as Bitcoin mining. In such scenarios, you, the account owner, might end up bearing the brunt of substantial unforeseen expenses.

### IV. Backing Services: Treat Backing Services as Attached Resources

A backing service refers to any resource or service upon which an application depends but is not incorporated within the application itself. Common examples include databases and message queues. Treating these backing services as attached resources implies that the application receives the service details through its configuration, thereby eliminating the need for any hardcoded dependencies within the application.

For instance, the application might be supplied with a uniform resource identifier (URI) and credentials to access a particular database. This approach allows for the independent scaling and management of the backing service, offering enhanced flexibility and seamless transitions between different environments. For instance, a database utilized in a development environment could be purposefully scaled down to a size significantly smaller than that in a production environment, aiding in resource management and cost efficiency.

### V. Build, Release, Run: Strictly Separate Build and Run Stages

The principle of strictly separating the build and run stages of the software development life cycle aligns with the earlier discussion of the factory and the citadel.

The factory stage is focused on building the application efficiently. The objective here is to minimize the build time and ensure the reliability of the application through rigorous testing before it is released. This phase is fully automated and generates an immutable artifact, which assures reproducibility. Consequently, this makes debugging simpler, as it limits the potential variables that could cause changes.

The citadel stage, on the other hand, is where the application artifact from the factory is run. It’s optimized for security, providing only the necessities to securely run the application artifact. If you were to include tools for building the application at this stage, it could create additional avenues for security vulnerabilities.

By separating the build and run stages, you can optimize each for different factors—efficiency and reliability in the build stage, and security in the run stage. This clear division minimizes the risk of security breaches when the application is operating within the citadel.

### VI. Processes: Execute the App as One or More Stateless Processes

At the beginning of this chapter, I compared the architecture of cloud native applications to Alan Kay’s analogy of cells—individual, loosely coupled components working together. To expand on this, each component is recommended to operate as an independent, stateless process under this principle.

_Stateless_ doesn’t mean a process without any state. Rather, it emphasizes that any state a component maintains should be stored externally, such as in a database, rather than being held in the process’s memory or local disk storage. This principle becomes especially relevant when a process restarts, as locally stored states could be lost. Notably, in the Heroku platform, persistent storage was not available, making state holding an impossibility, and necessitating this approach. Nonetheless, even in cloud native platforms where persistence is available, this principle remains important and beneficial.

Adherence to statelessness facilitates the scaling of processes, as there is no state to be lost when instances are scaled down or acquired when scaling up. It also mitigates the risk of process failures due to corruption of the internal state, thus enhancing reliability. Importantly, the absence of state allows for flexibility in relocating processes to different hardware or environments in case of failure, enabling the creation of cloud native applications that are more reliable than the infrastructure on which they run.

Although the 12-factor app principles predate microservices, the notion of a cloud native application as multiple stateless processes is foundational to the convergence of microservices and cloud native. It’s worth noting that the Heroku platform was built upon small instances called _dynos_, essentially small stateless processes, allowing for no alternative. While loose coupling is a key attribute of cloud native applications, the question of whether each component must be deployed as a separate microservice is a topic I will explore further.

Addressing a common question, the concept of statelessness doesn’t necessarily preclude all forms of state. For instance, buffering unsent data for efficiency, such as batching insert statements, might seem like a form of state. However, this represents a temporary or transient state, which is intended to be cleared in the short term. Such transient states can be lost without disrupting the overall functionality or correctness of the application. Therefore, such practices can still be considered in line with stateless principles, provided they don’t affect the overall functionality or lead to data loss.

### VII. Port Binding: Export Services via Port Binding

The principle of port binding is a fundamental aspect of how services communicate within and outside of an application. It allows a service to become accessible by attaching itself to a specific port number on a host machine. This mechanism ensures that different services can coexist on the same host machine without interfering with each other. For instance, it’s quite common for a service with an HTTP endpoint to be exposed on port 8080. As you delve deeper into the intricacies of Google Cloud in subsequent sections, you will discover how much of this port binding management is seamlessly handled by the platform itself.

### VIII. Concurrency: Scale Out via the Process Model

Traditional applications often achieve scaling through a vertical approach, which involves bolstering a single process with additional resources like CPU and memory. Given the availability of multiple CPUs, this necessitates concurrent threads to effectively utilize the added resources, consequently increasing the complexity. It’s noteworthy that Heroku, originally designed for Ruby applications, adhered to this principle likely because concurrency wasn’t a forte of Ruby at that time.

This principle of concurrency advocates the more efficient alternative of horizontal scaling, which involves adding more, smaller instances of the service rather than augmenting the size of existing instances. This approach optimizes the usage of the underlying infrastructure without escalating the complexity. An additional advantage of this strategy is the enhancement of application reliability through multiple instances, thereby reinforcing its place as a key principle in the cloud native paradigm.

### IX. Disposability: Maximize Robustness with Fast Startup and Graceful Shutdown

Cloud native applications are designed with the expectation of potential failure of the underlying infrastructure. Consequently, at any given moment, an instance may need to be terminated and restarted. This principle highlights the importance of treating instances as disposable, emphasizing their ability to shut down gracefully—avoiding data loss, for example. It’s equally crucial for new instances to be capable of starting up swiftly, thus minimizing periods of unavailability. This focus on disposability not only maximizes the robustness of the system but also prepares it to handle unexpected disruptions or demand fluctuations efficiently.

### X. Dev/Prod Parity: Keep Development, Staging, and Production as Similar as Possible

The dev/prod parity principle is another step to solving the “but it runs on my machine problem.” The idea behind it is that the development, testing, and production environment should be as similar as possible to avoid unexpected behavior when an application moves to a new environment. This includes having the same operating system, dependencies, services, and configuration wherever possible. As I will discuss later in [Chapter 12](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch12.html#chapter\_12), containers and infrastructure as code (IaC) do a lot to help with this.

### XI. Logs: Treat Logs as Event Streams

In a conventional application operating on a single machine, logs can be written to a local file for subsequent examination. However, in a cloud native system with numerous instances of components distributed across multiple machines, managing an assortment of log files on various machines is hardly practical. Rather than dealing with disparate log files, logs should be treated as event streams that are directed toward a centralized logging solution. This method enables efficient processing, correlation, and searchability of logs, thereby improving the overall visibility of system activities and aiding in debugging. In [Chapter 13](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch13.html#chapter\_13), I’ll delve into how services like Google Cloud offer outstanding features to facilitate these log management practices.

### XII. Admin Processes: Run Admin/Management Tasks as One-Off Processes

Admin and management tasks, such as database migrations, data imports, or system updates, are often required in systems and are typically run on a schedule or on demand. This principle advises isolating these types of tasks from the main application execution. By doing so, these tasks can be run independently, reducing potential side effects and simplifying their maintenance.

When making architectural decisions, it’s beneficial to refer back to the 12 factors. These principles should be viewed as guidelines rather than stringent rules. However, if you choose to deviate from them, it’s important to understand why.

## Beyond the 12 Factors

In 2016, Kevin Hoffman revisited the 12-factor app in [_Beyond the Twelve-Factor App_](https://learning.oreilly.com/library/view/beyond-the-twelve-factor/9781492042631/) (O’Reilly). His book revised and expanded the 12 factors to encompass 15 factors, accounting for advancements and learnings since 2012.

The additional factors—“API first,” Telemetry, and Security highlight the growing significance of security and observability in cloud native applications.

### API First

The “API first” principle posits that any functionality that your application provides should be exposed through a well-documented and versioned API. This approach benefits both the development of your application and its potential integration with other systems.

From a development perspective, “API first” encourages a clean separation of front end and back end code, making it easier to develop, test, and scale each independently. It also provides a clear contract for what functionality the back end provides and how it should be used, which can help to reduce bugs and improve consistency.

From an integration perspective, an “API first” approach means your application can be more easily combined with other systems. This is increasingly important in modern cloud environments, where complex systems are often composed of multiple smaller services.

### Telemetry

Telemetry involves the collection and analysis of data generated by remote systems to gain insights into their operation, usage, and performance. In a cloud native context, telemetry usually involves tracking metrics, logs, and traces from your application and its underlying infrastructure.

Telemetry is crucial for monitoring the health of your system, understanding how it’s being used, and diagnosing issues when they arise. It is also an essential part of many modern practices such as observability and site reliability engineering (SRE).

### Security

While the original 12-factor app principles include some security considerations, the renewed focus on security reflects its growing importance in the modern software landscape. This principle acknowledges that security is not an afterthought or an optional extra, but a fundamental concern that should be integrated into every stage of application development and operation.

In a cloud native context, this can involve practices like secure coding, automated vulnerability scanning, use of encryption for data at rest and in transit, proper management of secrets and credentials, use of least-privilege access controls, and ensuring regular updates and patches to all software components to protect against known vulnerabilities. This principle emphasizes that security is a shared responsibility across developers, operators, and security teams.

When considering the 12 factors, remember that they were originally devised for the Heroku platform and subsequently adopted by Cloud Foundry, a similar platform. While these principles remain relevant for modern cloud platforms, some are shaped by the constraints of the Heroku platform, which may not be applicable to Google Cloud in certain scenarios.

## Defining Components with Microservices

Traditional applications are often referred to as monolithic. In a monolithic application, all of the functionality is deployed as a single unit.

This itself is not a bad thing. Deploying a single unit is easy to manage. The problems come from not having a good separation of concerns. In a cloud native application, it is important to have a good separation of concerns by splitting the system into separate components that can be developed and tested independently. The idea is that if each component only has one reason to change, then changes do not cause side effects to other components. This allows different developers to work on different components at the same time and quickly, as they are not stepping on each other’s toes.

Unfortunately, many monolithic applications do not separate concerns sufficiently and they become “big balls of mud” that are difficult to change, killing developers’ productivity.

I joke that any book about cloud native can be measured by the time it takes to mention Martin Fowler, who popularized microservices in the early 2010s. My colleague Jeff will be disappointed I only managed to get to the third chapter. I learned about microservices from a talk by James Lewis, the often-forgotten co-coiner of the term. At first, I did not get it, but when I applied it to my work, I saw the value.

I handled an application that allowed users to submit data and produce reports on the data. If a change was needed to the reports, the whole system needed to be deployed, and at that time, no one could submit new data. By breaking apart the monolith into a data submission and a data reporting service, I could deploy changes to reporting without any interruption to submission. Similarly, I could change the submission service without interpreting reporting. This is a simple example, but it led to much happier users and made my life a lot easier as deployment became a lot easier.

Microservices are now a widely adopted approach to building modern, scalable, and resilient cloud native systems. However, because they are deployed and managed independently, there is an overhead of complexity.

When beginning an application, it may not be clear where the boundaries are, and it is not necessary to jump to microservices straight away; the important part is to separate the application into modules that have the potential to be deployed independently. If an application is initially developed as a single modular monolith, this is not a problem. As the application grows and different components need to be deployed or scaled independently, a well-structured monolith can be broken into microservices later.

However, for the rest of the book, I will use microservices synonymously with components as the building blocks of your cloud native applications. To go into much more depth on the subject of microservices, I recommend [_Building Microservices_](https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/) by Sam Newman (O’Reilly).

## Determining Component Granularity

How granular should a component be?

Simply put, if aspects of your component—like changes, failures, or scaling—can occur together, it should remain a single unit. However, the moment it requires independent alterations, failure management, or scaling, it’s time to consider breaking it down into separate services.

In his manufacturing process improvement book _The Goal_ (North River Press), Eliyahu Goldratt observes that “technology can bring benefits if, and only if, it diminishes a limitation.”

This is analogous to our cloud native approach, where our aim is to streamline the process of building and operating applications. Hence, the only rationale for dividing a component into smaller entities should be to overcome a limitation:

* When a component doesn’t yet need to change, fail, or scale independently, it is fine to keep them as modules in a modular monolith.
* When separate teams come across limitations due to sharing the code, it makes sense to split components out.
* When a piece of functionality is limited on how much it can scale, it makes sense to split it out.

Remember, the more you break a system down into fine-grained components, the more complexity you introduce. While automation and observability can mitigate this complexity to an extent, it’s crucial to always be mindful of the reasons for downsizing a component and to consider the trade-offs involved.

## Leveraging Domain-Driven Design for Defining Microservice Boundaries

In cloud native system design, one key consideration is the delineation of boundaries between microservices. Each microservice should ideally have a single responsibility, providing it with only one reason to change, thereby promoting a clear separation of concerns.

Domain-driven design (DDD), a technique centered around domain knowledge, plays a vital role in this context. When implemented in a microservices environment, DDD can enhance the system’s design and maintainability by establishing a laser focus on the business domain and enforcing a clear demarcation of responsibilities among the various microservices.

Implementing DDD within microservices offers several advantages:

Domain-driven architecture

DDD facilitates the identification and modeling of the core business domains within a system. These domains subsequently guide the microservices’ design, leading to an architecture that’s intuitively aligned with business requirements and exhibits logical consistency.

Bounded contexts

A crucial principle of DDD is defining clear boundaries—termed _bounded contexts_—for each microservice. Identified by the specific business capabilities they represent, these boundaries ensure that each microservice has a distinct scope and purpose, simplifying their understanding and maintenance.

Domain language

DDD promotes the use of a shared domain language, enhancing communication and collaboration within the development team. In a microservices environment, where different teams might handle different microservices, this is particularly important.

Evolvability

DDD supports the concept of continuous evolution and improvement, perfectly aligning with a microservices environment where the system often evolves in tandem with the business and incorporates new features over time. By maintaining focus on the business domain, DDD ensures that any changes align with the system’s overall goals and objectives.

Domain-driven design is an expansive subject with a vibrant community and dedicated conferences. Numerous resources are available for learning its concepts and practices. [_Domain-Driven Design_](https://learning.oreilly.com/library/view/domain-driven-design-tackling/0321125215/) by Eric Evans (Addison-Wesley) serves as the definitive reference. However, [_Learning Domain-Driven Design_](https://learning.oreilly.com/library/view/learning-domain-driven-design/9781098100124/) by Vlad Khononov (O’Reilly) serves as an accessible starting point for newcomers.

## Intercommunication Between Microservices: APIs and Events

In multicomponent applications, effective communication is key, and it heavily relies on well-defined contracts between components. Microservices predominantly employ two communication approaches: event-driven and API-first.

### Event-Driven Architecture

Event-driven architecture is a paradigm where microservices communicate by generating and consuming events. Triggered by system internal or external changes, events serve as contracts in an event-driven structure.

### API-First Approach

The API-first design is characterized by APIs serving as contracts that facilitate communication with a service. Essentially, the API is the intermediary between the system components. APIs, particularly HTTP APIs, are the primary means for external systems and developers to access system functionality and data.

HTTP APIs usually employ one of two protocols, REST or gRPC:

REST

Leverages a text-based format (typically JSON), offering more flexibility than gRPC but potentially leading to increased maintenance challenges.

gRPC

Utilizes a binary format (Protocol Buffers) and has a more rigid structure than REST. The API is specified in a contract (a _.proto_ file) that outlines the request and response messages and permissible operations. Owing to its defined structure, gRPC APIs can be easier to maintain and more efficient due to their binary format.

A gRPC or REST API can be designed for either synchronous or asynchronous communication. The choice is dictated by its design and implementation.

In a synchronous communication model, the client sends a request and waits for a response before proceeding. The client is blocked until a response is received or a time-out occurs. While synchronous communication is straightforward to use, it may be less efficient as the client must wait for a response before continuing.

In contrast, asynchronous communication allows the client to send a request and continue processing while waiting for a response. The client is not blocked while waiting for the response before continuing. Asynchronous communication can be more efficient as it allows the client to continue processing while waiting for a response, but it might introduce complexity due to the asynchronous handling of responses.

Both REST and gRPC can be implemented in either synchronous or asynchronous styles depending on the requirements. However, due to ease of use and familiarity, REST is often chosen when exposing APIs externally, while gRPC might be preferred for internal microservices communication due to its performance and strong contract enforcement.

[_gRPC: Up and Running_](https://learning.oreilly.com/library/view/grpc-up-and/9781492058328/) by Kasun Indrasiri and Danesh Kuruppu (O’Reilly) offers a comprehensive discussion on gRPC. API design, a vast subject in itself, is covered in many dedicated books.

## Harmonizing APIs and Events for Effective Microservice Communication

Both API-first and event-driven architectures come with distinct advantages and challenges. Understanding these, one can ingeniously combine these approaches for a more robust and flexible system design. In a composite model, an event-driven system could utilize APIs to expose its functions and data to external systems. However, the primary emphasis remains on the internal event flow, with APIs serving as the conduit rather than the focal point. As you delve further, you’ll explore how a strategically designed API can serve as a powerful gateway into an event-driven system, effortlessly harmonizing these two paradigms for superior microservice communication.

## Event Storming: A Collaborative Approach to System Design

One of the instrumental techniques often employed in conjunction with DDD is Event Storming. This collaborative, workshop-style method is particularly effective in shaping event-driven systems.

Event Storming serves as a powerful tool to comprehend, design, and enhance an organization’s processes. Its primary objective is to map out the system’s event flow and business processes, while also identifying opportunities for improvement.

In an Event Storming session, participants collaboratively chart the system’s event flow using sticky notes, each representing a distinct event such as a customer placing an order, a payment being processed, or an item being shipped. These notes are then arranged and rearranged on a board, visually depicting the event sequence and their interrelationships.

Beyond identifying process inefficiencies or bottlenecks, Event Storming can be employed to architect new processes or systems and to refine existing ones. This method synergizes effectively with other approaches such as agile development and lean principles, creating a comprehensive toolset for system improvement.

For those keen to delve deeper into Event Storming, Alberto Brandolini’s book, [“EventStorming”](https://oreil.ly/U1eUq) (Leanpub), provides practical insights. As the technique’s creator, Brandolini offers invaluable guidance on how to leverage Event Storming to its full potential.

## Achieving Portability with Containers

When I was a child, the town where I lived had a dock where ships would be unloaded of loose cargo using a crane that scooped it up and deposited it into a row of waiting trucks. Shipping containers had not made it to our town.

A few years ago, I saw huge cranes unloading standardized containers from ships and autonomous vehicles moving the containers around the terminal to await collection. It is a whole different level of scale and efficiency.

Shipping containers’ strength lies in their durability and standardized size and shape, making them easily manageable with cranes and forklifts, loading them onto trucks and trains for further transportation. Their ability to be stacked and stored efficiently, coupled with their locked and sealed nature, ensures they are secure and tamper-proof. By abstracting the cargo from the infrastructure that transports it, they revolutionize shipping logistics.

Just as shipping containers were standardized by the International Organization for Standardization (ISO) in the 1960s, containers for applications were standardized by the Open Container Initiative (OCI). Although practitioners often talk about Docker containers, this is just one implementation of the OCI container standard.

A container encapsulates everything necessary for the application to run—the application code, libraries, dependencies, and runtime—into a lightweight, standalone, and executable package. This concisely addresses the second factor of the 12-factor application methodology (see [“II. Dependencies: Explicitly Declare and Isolate Dependencies”](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch03.html#second\_factor)).

The primary advantages of containers, much like their physical counterparts, lie in their packaging and portability. When run on a host, containers are isolated from others, mitigating the risk of conflicts between coexisting applications and services.

The thing to remember is that containers are both a packaging and an isolation mechanism, and by using them, you get both benefits. Sometimes, for example, in a Python application with many dependencies, there will be a great advantage in packaging it up as a container. Other times, such as packaging to a single Go binary, the packaging will be less useful. However, in both cases, the content will give isolation at runtime.

The containers can also be immutable, and like having a locked and sealed shipping container, the content is immutable. As containers are a standard, there is also a wide range of tools for working with them, from building to checking security.

There are arguments against the need to use containers for cloud native applications around complexity and overhead. However, Google Cloud services are container-centric, so using containers gives a lot of flexibility in where those containers can run. As you will see, Google Cloud also has tools that take away a lot of the complexity and overhead concerns.

## Flexible Running with Container Runtimes

You have a container nicely packaging your microservice; now you need to choose where to run it—a container runtime. Many people automatically associate Kubernetes, the container orchestrator, with cloud native. “Containerized microservices running on Kubernetes” is almost the definition of cloud native for some. However, in this book’s definition of cloud native, I prioritize efficiency; and Kubernetes will not always be the most efficient way of running a container on Google Cloud.

To illustrate this, consider a container as a person and the container runtime as a vehicle. If you have a family of four and use a car multiple hours each day, owning a standard five-seater car makes sense. At times, the car may transport all four individuals; other times, only one or two. While the car may not be fully utilized at all times, it is always ready and possesses adequate space. Occasionally, you might even squeeze an additional passenger into the middle seat. This scenario reflects the versatility of Kubernetes.

However, if there are times when you need to accommodate more people—say, when grandparents visit monthly, and you need six seats—you might consider purchasing a seven-seater car, perhaps with two extra seats that fold away when not needed. This mirrors the functionality of Kubernetes with autoscaling, where the capacity can expand to accommodate additional load when required.

On the flip side, consider a different scenario where:

* You are living in a city.
* You undertake short, solo journeys every other day.
* Once a week, you travel for an outing with a group of friends.
* A few times a year, you travel a hundred miles to visit family with your partner.

In such a situation, does owning a car make sense? Probably not.

Now imagine a service like Uber offering an on-demand transportation service where you can order a self-driving car to accommodate anything from one to twenty people. You merely specify the number of passengers, a suitable car arrives within 30 seconds, and you pay per minute of use. Would you still own a car, or would you prefer this “carless” service?

This scenario is akin to running containers using a serverless container runtime. Yes, there may be a cluster of machines (perhaps a Kubernetes cluster) somewhere behind the scenes, but they’re so abstracted that you don’t need to worry about them.

However, imagine if you changed jobs and had a 60-minute daily commute where punctuality is critical. You might opt to own a two-seater sports car for daily commuting and use the Uber service for larger, less frequent trips. Here, the ownership of the car provides a guaranteed capacity, and on-demand service is the perfect solution for situations where you don’t need that assurance.

Likewise, different services exist on Google Cloud for running containers. Although Kubernetes is a powerful abstraction, it has a steep learning curve, and sometimes running a cluster 24/7 will not be efficient. You can use Kubernetes directly on Google Cloud using Google Kubernetes Engine (GKE) but you don’t need to. If your services run all the time with occasional spikes, it may make sense to have a Kubernetes cluster. It may make sense to use just the serverless container runtimes. It may make sense to use a combination.

Cloud Foundry, an earlier cloud native platform, introduced a succinct haiku that elegantly encapsulates the essence of cloud native services:

> Here is my source code\
> Run it on the cloud for me\
> I do not care how
>
> @onsijoe

Drawing parallels with Cloud Foundry’s approach, once you have a service running in a container on Google Cloud, you can adopt a similar philosophy: “Here is my containerized service, ensure it’s ready to handle incoming requests.” This approach offers immense flexibility. The specifics and trade-offs of various services will be explored in detail in later chapters.

## Avoiding Lock-In with Abstractions

Avoiding vendor lock-in with a particular cloud provider is always a consideration when building cloud native applications. Even though this is a book on Google Cloud, it is still worth considering the potential switching costs of moving to another cloud.

There are several strategies to mitigate lock-in:

Infrastructure based

Aim for the lowest common denominator in infrastructure. This involves building only on IaaS primitives, such as disks, databases, and virtual machines, that are universally available across all clouds. This strategy typically includes self-managing a Kubernetes distribution on the infrastructure to ensure consistency across clouds.

Kubernetes based

Target a managed Kubernetes service as the lowest common denominator. All major cloud providers offer managed Kubernetes services, so it seems logical to use Kubernetes as the common abstraction layer. However, managed Kubernetes services differ across providers, especially regarding versions and maintenance requirements, making this solution less straightforward than it may initially appear.

Open standards based

Embrace open standards and protocols that are supported across various clouds. This approach favors managed services that offer a technology available on other clouds through a higher-level abstraction since these have often become a de facto industry standard. Examples include the PostgreSQL wire protocol, Kubernetes API, Knative, and OCI Containers.

Single-cloud based

Choose the most suitable service on a single cloud, on the assumption that the benefits of using the managed service will outweigh even high switching costs if the probability of needing to switch is low. Technologies like AWS Lambda might fit this category. Even then, implementing patterns that allow developers to abstract away from specific services can make potential transitions more manageable.

As I discussed in [Chapter 2](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch02.html#chapter\_02), I believe the first two strategies often sacrifice many benefits of cloud native, particularly the ability to use managed services and the integrated experience they provide. By striving for a generic abstraction, you may reduce the switching cost, but it might also lead to losses in speed, efficiency, and potential security. Therefore, the likelihood of needing to switch should be carefully evaluated. I believe this approach only makes sense when an application must be deployed across multiple clouds simultaneously.

If Google Cloud is your chosen platform and the chances of needing to switch to another cloud are minimal, I advocate for the third approach, which is the primary focus of this book. This strategy involves favoring technologies and high-level abstractions available in any cloud, rather than focusing on individual services. While I will discuss many specific Google Cloud services in this book, I’ll always attempt to highlight where these services support an open standard or protocol that’s also available in other environments.

## Responding to Change with Extreme Programming (XP)

Cloud native benefits greatly from the techniques of extreme programming (XP) first put forward by Kent Beck and Cynthia Andres in [_Extreme Programming Explained_](https://learning.oreilly.com/library/view/extreme-programming-explained/0201616416/) (Addison-Wesley). These are a set of techniques that aim to build quality into code by emphasizing simplicity, communication, and feedback. It is a type of agile software development, which means that it is optimized to be flexible and responsive to change.

Principles include:

Small releases

XP emphasizes the importance of delivering small, usable increments of software frequently, rather than waiting until the end of the project to deliver a large, monolithic product. In cloud native, these increments can correspond to microservices.

Collaborative development

XP emphasizes the importance of collaboration between developers, customers, and other stakeholders.

Test-driven development

XP encourages developers to write automated tests for their code before writing the code itself, to ensure that the code is correct and meets the needs of the users.

Refactoring

XP encourages developers to regularly review and improve their code to keep it simple and maintainable.

XP is a flexible approach that can be adapted to the needs of different projects and organizations. It is designed to be adaptable to change and to encourage frequent communication and feedback between everyone involved in the development process.

Test-driven development in particular is a useful tool for cloud native development, as having the tests in place first means that any changes or refactoring can be made with confidence. [_Learning Test-Driven Development_](https://learning.oreilly.com/library/view/learning-test-driven-development/9781098106461) by Saleem Siddiquif (O’Reilly) is a good practical introduction to this subject.

## Building Confidence with Testing

For effective cloud native development, going beyond test-driven development to embrace testing in a broader sense is indispensable. Comprehensive testing procedures lay a foundation of confidence, enabling developers to make changes with the assurance that any newly introduced issues would be promptly identified and addressed. Given the intricate nature of cloud native applications, comprised of numerous interconnected components, it is virtually impossible to sustain a desirable pace of development without a robust testing regime.

Here are some critical testing methodologies for cloud native applications:

Unit testing

Focusing on testing isolated functional units within each microservice. It verifies the correctness of individual functions without considering their interactions with other parts of the system.

Integration testing

Concentrates on testing the interaction between microservices, or between a microservice and external services such as databases or messaging systems. It ensures that all components work harmoniously in conjunction.

Contract testing

This involves the verification of the contract between microservices, which could be event-driven or API-based. Contract testing ensures that all services adhere to the agreed-upon interfaces.

Performance testing

This evaluates the application’s performance under various load conditions, determining the system’s behavior under peak load, identifying bottlenecks, and understanding its scalability potential.

Disaster recovery testing

Critical for validating the application’s ability to recover from failures, disruptions, or catastrophes, ensuring that recovery procedures and contingency plans are effective.

Security testing

Ensures the application’s resilience against threats such as hacking or data breaches. It verifies the robustness of the system’s security measures, aiming to safeguard both the system and user data.

User acceptance testing

This is conducted from the perspective of the end user to ensure the application aligns with user expectations and requirements, affirming that it’s ready for deployment.

While testing is a cornerstone of software development regardless of the paradigm, cloud native applications, with their microservices architecture and specific scalability and resilience attributes, demand a nuanced approach. The loose coupling of components, which communicate through APIs or respond to events, calls for an increased emphasis on contract testing to secure the reliability of these interactions. Throughout this book, I will delve into these methodologies and demonstrate how they fortify cloud native development.

## Eliminating Toil with Automation

When I first started learning to program at school, my teacher said to me, “if you are doing the same thing manually more than three times, you are probably doing it wrong."” That has stuck with me. Another way of looking at it is that the best developers are lazy developers. They don’t want to build a container by hand, manually deploy a new version multiple times a day, or spend ages trawling through logs to debug errors. The most productive developers consider this work “toil” and seek to eliminate it and get back to building applications.

This is where DevOps comes from, a set of practices that combines software development (Dev) and operations (Ops) to speed up the development and increase software quality. As you will see in later chapters, this includes:

Continuous integration

Automatically building and testing changes in the factory ([Chapter 12](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch12.html#chapter\_12))

Continuous delivery

Automatically making the output of the factory ready for deployment to the citadel ([Chapter 11](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch11.html#chapter\_11))

Monitoring and logging

Exposing what is happening inside the citadel in the observatory ([Chapter 13](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch13.html#chapter\_13)) to make correcting problems straightforward

Infrastructure as code

The mechanism for building the infrastructure of the factory and the citadel in a reproducible way

Managed services

Using services managed by the cloud provider in the citadel in preference to self-management

Ultimately, when working with cloud native apps, there is the approach of “you build it, you run it.” As it is the developers that are on the hook if the applications fail at 2 a.m., there is a great incentive to make sure that doesn’t happen. If it does, there is the incentive that it be easy to fix so the developer can get back to bed. DevOps is operations performed by a developer who prefers to be writing code _and_ getting a good night’s sleep.

Google has developed a particular implementation of DevOps called site reliability engineering (SRE) that it uses for running production systems. This is explained in [_Site Reliability Engineering_](https://learning.oreilly.com/library/view/site-reliability-engineering/9781491929117) by Betsy Beyer et al. (O’Reilly). You will see how SRE principles permeate into Google Cloud in [Chapter 13](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/ch13.html#chapter\_13).

## Summary

Successfully developing cloud native applications relies on a comprehensive understanding of various tools, techniques, and technologies. While each of these subjects merits detailed study, here are the primary points to remember:

* Cloud native applications tend to be made from _autonomous components_ communicating by _messages_, not by sharing state.
* The 12 factors are useful recommendations, but remember that they were designed for earlier platforms.
* Microservices are one destination but don’t have to be the starting point.
* Event-driven and API-driven architecture are both useful approaches.
* Domain-driven design helps identify the boundaries between microservices.
* Event Storming helps map out communication in event-driven systems.
* Containerizing components provide flexibility and portability.
* Kubernetes is just one way of running containers.
* Patterns exist to help address common problems and mitigate the risk of vendor lock-in.
* Extreme programming techniques help sustain fast feedback, especially test-driven development.
* Testing increases quality and provides the confidence needed to keep going fast.
* Automation saves time, provides consistency, and lets developers focus on solving business problems.

One crucial thing to remember is that everything comes with trade-offs. There aren’t any definitive rules dictating what a cloud native application should be. Instead, there are proven principles that generally provide good results. Cloud native doesn’t impose the use of Kubernetes, microservices, or even containers. Situations may arise where these principles don’t apply, and it’s these shades of gray that underline the importance of knowing the options available and making informed decisions based on your specific circumstances.

In [Part II](https://learning.oreilly.com/library/view/cloud-native-development/9781098145071/part02.html#hands-on-projects), you’ll begin your exploration of Google Cloud, focusing on how it can be leveraged to build cloud native applications.
