# 5. Interactive Data Exploration With Vertex AI Workbench

## Chapter 5. Interactive Data Exploration with Vertex AI Workbench

In every major field of study, there is usually a towering figure who did much of the seminal work and blazed the trail for what that particular discipline would evolve into. Classical physics has Newton, relativity has Einstein, game theory has John Nash, and so on. When it comes to computational statistics (the field of study that develops computationally efficient methods for carrying out statistical operations), the towering figure is John W. Tukey. At Bell Labs, he collaborated with John von Neumann on early computer designs soon after World War II—famously, Tukey was responsible for coining the word _bit_. Later, at Princeton (where he founded its statistics department), Tukey collaborated with James Cooley to develop the fast Fourier transform, one of the first examples of using divide-and-conquer to address a formidable computational challenge.

While Tukey was responsible for many “hard science” mathematical and engineering innovations, some of his most enduring work is about the distinctly softer side of science. Unsatisfied that most of statistics overemphasized confirmatory data analysis (i.e., statistical hypothesis testing such as paired _t_-tests),[1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn83) Tukey developed a variety of approaches to do what he termed _exploratory data analysis_ (EDA)[2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn84) and many practical statistical approximations. It was Tukey who developed the box plot, jack-knifing, range test, median-median regression, and so on and gave these eminently practical methods a solid mathematical grounding by motivating them in the context of simple conceptual models that are applicable to a wide variety of datasets. In this chapter, we follow Tukey’s approach of carrying out exploratory data analysis to identify important variables, discover underlying structure, develop parsimonious models, and use those models to identify unusual patterns and values.

All of the code snippets in this chapter are available in the folder [_05\_bqnotebook_ of the book’s GitHub repository](https://github.com/GoogleCloudPlatform/data-science-on-gcp). See the _README.md_ file in that directory for instructions on how to do the steps described in this chapter.

## Exploratory Data Analysis

Ever since Tukey introduced the world to the value of EDA in 1977, the traditional first step for any data scientist has been to analyze raw data by using a variety of graphical techniques. This is not a fixed set of methods or plots; rather, it’s an approach meant to develop insight into a dataset and enable the development of robust statistical models. Specifically, as a data scientist, you should do the following:

* Test any underlying assumptions, such as that a particular value will always be present or will always lie within a certain range. For example, as discussed in [Chapter 2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch02.html#ingesting\_data\_into\_the\_cloud), the distribution of the distance between any pair of airports might help verify whether the distance is the same across the dataset or whether it reflects the actual flight distance.
* Use intuition and logic to identify important variables. Verify that these variables are, indeed, important as expected. For example, plotting the relationship between departure delay and arrival delay might validate assumptions about the recording of these variables.
* Discover underlying structures in the data (i.e., relationships between important variables and situations such as the data falling into specific statistical regimes). It might be useful to examine whether the season (summer versus winter) has an impact on how often flight delays can be made up.
* Develop a parsimonious model—a simple model with explanatory power, that you can use to hypothesize about what reasonable values in the data look like. If there is a simple relationship between departure delay and arrival delay, values of either delay that are far off the trendline might warrant further examination.
* Detect outliers, anomalies, and other inexplicable data values. This depends on having that parsimonious model. Thus, further examination of outliers from the simple trend between departure and arrival delays might lead to the discovery that such values off the trendline correspond to rerouted flights.
* Discover any potential overarching data quality problems such as the issues we found in [Chapter 3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#creating\_compelling\_dashboards) with time being recorded without being UTC.

To carry out exploratory data analysis, it is necessary to load the data in a form that makes interactive analysis possible. In this chapter, we load data into Google BigQuery, explore the data in Vertex AI Workbench, carry out quality control based on what we discover about the dataset, build a new model, and evaluate the model to ensure that it is better than the model we built in [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest). As we go about loading the data and exploring it and move on to building models and evaluating them, we’ll discuss a variety of considerations that come up, from security to pricing.

Both exploratory data analysis and the dashboard creation discussed in [Chapter 3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#creating\_compelling\_dashboards) involve the creation of graphics. However, the steps differ in two ways—in terms of the purpose and in terms of the audience. The aim of dashboard creation is to crowdsource insight into the working of models from end users and is, therefore, primarily about presenting an explanation of the models to end users. In [Chapter 3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#creating\_compelling\_dashboards), I recommended doing it very early in your development cycle, but that advice was more about Agile development and getting feedback early than about statistical rigor.[3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn85) The aim of EDA is for you, the data engineer, to develop insights about the data before you delve into developing sophisticated models. The audience for EDA is typically other members of your team and yourself, not end users. In some cases, especially if you uncover strange artifacts in the data, the audience could be the data engineering team that produces the dataset you are working with. For example, when we discovered the problem that the times were being reported in local time, with no UTC offsets, we could have relayed that information back to the US Bureau of Transportation Statistics (BTS).[4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn86) In any case, the assumption is that the audience for an EDA graphic is statistically sophisticated. Although you probably would not include a violin plot in a dashboard meant for end users,[5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn87) you would have no compunctions about using it in an EDA chart that is meant for data scientists.

Doing exploratory data analysis on large datasets poses a few challenges. To test that a particular value will always be present, for example, you would need to check every row of a tabular dataset, and if that dataset is many millions of rows, these tests can take hours. An interactive ability to quickly explore large datasets is indispensable. On Google Cloud Platform, BigQuery provides the ability to run Cloud SQL queries on unindexed datasets (i.e., your raw data) in a matter of seconds even if the datasets are in the terabyte scale. Therefore, in this chapter, we load the flight data into BigQuery.

**ANSCOMBE’S QUARTET**

The statistician Francis Anscombe illustrated that graphs are essential to good statistical analysis using a very powerful example. All four of the datasets shown in [Figure 5-1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ansombeapostrophes\_quartetdot\_figure\_fr) have the same mean, variance, linear fit, and correlation (to two decimal places) but are obviously quite different from one another.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0501.png" alt="" height="572" width="574"><figcaption></figcaption></figure>

**Figure 5-1. Ansombe’s Quartet. Figure from “Graphs in Statistical Analysis” by Francis Anscombe in American Statistician 27, no. 1 (1973): 17–21 as recreated in the seaborn documentation.**

Anscombe used the quartet to emphasize that summary statistics are not a substitute for graphic data—it’s particularly important to graph outliers to develop a holistic understanding of the data.

Identifying outliers and underlying structure typically involves using univariate and bivariate plots.[6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn88) The graphs themselves can be created using Python plotting libraries—I use a mixture of [_Matplotlib_](https://matplotlib.org/) and [_seaborn_](http://seaborn.pydata.org/) to do this. The networking overhead of moving data between storage and the graphics engine can become prohibitive if I carry out data exploration on my laptop—for exploration of large datasets to be interactive, we need to bring the analytics closer to the data. Therefore, we will want to use a cloud computer (not my laptop) to carry out graph generation. Generating graphics on a cloud computer poses a display challenge because the graphs are being generated on a Compute Engine instance that is _headless_—that is, it has no input or output devices. A Compute Engine instance has no keyboard, mouse, or monitor, and frequently has no graphics card.[7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn89) Such a machine is accessed purely through a network connection. Fortunately, desktop programs and interactive read–eval–print loops (REPLs) are no longer necessary to create visualizations. Instead, notebook servers such as [Jupyter](https://jupyter.org/) have become the standard way that data scientists create graphs and disseminate executable reports. On Google Cloud Platform, Vertex AI Workbench provides a fully managed way to run Jupyter notebooks that connect to Google Cloud Platform services.

### Exploration with SQL

Let’s start in the BigQuery console by exploring the time-corrected dataset that we created in BigQuery in [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest):

```
SELECT
   ORIGIN,
   AVG(DEP_DELAY) AS dep_delay,
   AVG(ARR_DELAY) AS arr_delay,
   COUNT(ARR_DELAY) AS num_flights
 FROM
   dsongcp.flights_tzcorr
 GROUP BY
   ORIGIN
```

The result consists of 322 airports (the order you get might be different):

| `Row` | `ORIGIN` | `dep_delay`          | `arr_delay`           | `num_flights` |
| ----- | -------- | -------------------- | --------------------- | ------------- |
| `1`   | `OTZ`    | `5.209103840682787`  | `6.562952243125903`   | `691`         |
| `2`   | `HPN`    | `11.782807151007983` | `9.087898089171965`   | `7850`        |
| `3`   | `SJU`    | `9.8362379921783`    | `2.506036485508635`   | `26257`       |
| `4`   | `ANC`    | `3.2497373643048966` | `–0.4801384732734849` | `17043`       |
| `5`   | `CVG`    | `8.826792206581548`  | `5.244408048666357`   | `21370`       |

Let’s look at just the major airports, which we can define as airports that have on average more than 10 flights a day. To do this we can filter by airports that have a sufficient number of flights:

<pre><code>WITH all_airports AS (
  SELECT
    ORIGIN,
    AVG(DEP_DELAY) AS dep_delay,
    AVG(ARR_DELAY) AS arr_delay,
    COUNT(ARR_DELAY) AS num_flights
  FROM
    dsongcp.flights_tzcorr
  GROUP BY
    ORIGIN
)

<strong>SELECT * FROM all_airports WHERE num_flights > 3650
</strong><strong>ORDER BY dep_delay DESC
</strong></code></pre>

We are thresholding the number of flights at 3,650 because there are 365 days in the dataset. The result, when I did it, was:

| `Row` | `ORIGIN` | `dep_delay`          | `arr_delay`          | `num_flights` |
| ----- | -------- | -------------------- | -------------------- | ------------- |
| `1`   | `ORD`    | `13.305085522847`    | `7.596119952650316`  | `304120`      |
| `2`   | `EWR`    | `13.182294215975096` | `3.9227994696288535` | `107849`      |
| `3`   | `BWI`    | `12.893989460498512` | `6.768316724436742`  | `92320`       |
| `4`   | `LGA`    | `12.764120915158792` | `5.043357442317552`  | `103281`      |
| `5`   | `IAD`    | `12.23048266485387`  | `4.505307971508886`  | `36643`       |

It makes sense that airports that serve major American cities experience the worst departure delays (ORD serves Chicago, EWR and LGA serve New York City, and BWI and IAD serve Washington, DC).

What if we restrict this analysis to January, reducing the number of flights threshold to 310 since there are 31 days in January?

<pre><code>WITH all_airports AS (
  SELECT
    ORIGIN,
    AVG(DEP_DELAY) AS dep_delay,
    AVG(ARR_DELAY) AS arr_delay,
    COUNT(ARR_DELAY) AS num_flights
  FROM
    dsongcp.flights_tzcorr
<strong>  WHERE EXTRACT(MONTH FROM FL_DATE) = 1
</strong>  GROUP BY
    ORIGIN
)

<strong>SELECT * FROM all_airports WHERE num_flights > 310
</strong>ORDER BY dep_delay DESC
</code></pre>

Now, we get a somewhat stranger set of airports:

| `Row` | `ORIGIN` | `dep_delay`          | `arr_delay`          | `num_flights` |
| ----- | -------- | -------------------- | -------------------- | ------------- |
| `1`   | `ASE`    | `20.86779661016949`  | `16.988095238095244` | `588`         |
| `2`   | `ORD`    | `19.96205128205124`  | `17.016131923283723` | `22316`       |
| `3`   | `JAC`    | `18.787172011661802` | `16.096209912536445` | `343`         |
| `4`   | `SBN`    | `18.491891891891886` | `16.326975476839234` | `367`         |
| `5`   | `FAT`    | `18.12554744525547`  | `17.63823529411766`  | `680`         |

I don’t recognize four of the five airports on this list, and I’m a rather frequent traveler. A short Google Search later, I learned that ASE is a ski resort (Aspen, Colorado) as is JAC (Jackson Hole, Wyoming). This makes sense—ski resorts are open only in winter, have to load up bulky baggage, and probably suffer more weather-related delays.

Using the average delay to characterize airports is not ideal, though. What if most flights to Aspen were actually on time but a few highly delayed flights (perhaps flights delayed by several hours) are skewing the average? I’d like to see a distribution function of the values of arrival and departure delays. BigQuery itself cannot help us with graphs—instead, we need to tie the BigQuery backend to a graphical, interactive exploration tool. Data scientists tend to use Jupyter Notebooks for EDA, so I’ll use Vertex AI Workbench, which offers fully managed Jupyter Notebooks.

### Reading a Query Explanation

Before I move on to Notebooks, though, we want to see if there are any red flags regarding query performance on our table in BigQuery. In the BigQuery console, there is a tab (next to Results) labeled “Execution details.” [Figure 5-2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#the\_explanation\_of\_a\_query\_in\_bigqueryd) shows the explanation of the January query.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0502.png" alt="" height="422" width="600"><figcaption></figcaption></figure>

**Figure 5-2. The explanation of a query in BigQuery.**

Our query has been executed in three stages. Expand each of the stages to see their details:

* The first stage (see [Figure 5-3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#the\_first\_stagedot)) pulls the origin, departure delay, arrival delay, and date for each flight and filters the result by looking at the month. Then, it groups them by origin, computes averages on each shard of data, and writes them to `__stage00_output. __stage00_output` is organized by the hash of the `ORIGIN`.
* The second stage (see [Figure 5-4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#the\_second\_stagedot)) reads the fields organized by `ORIGIN`, computes the average delays and count (but starting from the SHARD averages), and filters the result to ensure that the count is greater than 310. Note that the query has been optimized a bit—my `WHERE` clause was actually outside the `WITH` statement, but it has been moved here so as to minimize the amount of data written out to `__stage01_output`.
* The third stage (see [Figure 5-5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#the\_third\_stagedot)) simply sorts the rows by departure delay and writes to the output.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0503.png" alt="" height="254" width="600"><figcaption></figcaption></figure>

**Figure 5-3. The first stage.**

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0504.png" alt="" height="273" width="600"><figcaption></figcaption></figure>

**Figure 5-4. The second stage.**

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0505.png" alt="" height="196" width="600"><figcaption></figcaption></figure>

**Figure 5-5. The third stage.**

Based on the preceding second-stage optimization, is it possible to write the query itself in a better way? Yes, by using the `HAVING` keyword:

<pre><code>  SELECT
    ORIGIN,
    AVG(DEP_DELAY) AS dep_delay,
    AVG(ARR_DELAY) AS arr_delay,
    COUNT(ARR_DELAY) AS num_flights
  FROM
    dsongcp.flights_tzcorr
  WHERE EXTRACT(MONTH FROM FL_DATE) = 1
  GROUP BY
    ORIGIN
<strong>  HAVING num_flights > 310
</strong>  ORDER BY dep_delay DESC
</code></pre>

In the rest of this chapter, I will use this form of the query that avoids the `WITH` statement. By using the `HAVING` keyword, we are not relying on the query optimizer to minimize the amount of data written to `__stage01_output`.

What do the times in the graphics mean? Each stage (see [Figure 5-6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#average\_and\_maximum\_time\_spent\_by\_bigqu)) is broken into four steps: wait, read, compute, and write. The average and maximum time spent in each of these steps by the BigQuery workers is reported. So, in another example shown in [Figure 5-6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#average\_and\_maximum\_time\_spent\_by\_bigqu), BigQuery spends an average of 353 milliseconds (37 + 115 + 195 + 6) in this stage. A worker could spend as much as 608 milliseconds (49 + 308 + 242 + 9) in it, though.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0506.png" alt="" height="199" width="600"><figcaption></figcaption></figure>

**Figure 5-6. Average and maximum time spent by BigQuery workers by steps. Each part of each stage in the query explanation is depicted by a color bar that represents the fraction of time spent in that part.**

The length of the bar is the time taken by the most time-consuming step in the stage—so the length of the bar corresponds to 308 ms and the color in each bar is the fraction of that time spent in this step. In other words, the bars are all normalized to the time taken by the longest step (wait, read, compute, or write). A large difference between the average and the maximum (as in the read step of [Figure 5-6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#average\_and\_maximum\_time\_spent\_by\_bigqu)) indicates a skew—there are some workers who are doing a lot more work than others. Sometimes, it is inherent to the query, but at other times, it might be possible to rework the storage or partitions to reduce such skew.[8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn90)

The wait time is the time spent waiting for the necessary compute resources to become available—a very high value here indicates a job that could not be immediately scheduled on the cluster. A high wait time could occur if you are using the BigQuery flat-rate plan and someone else in your organization is already consuming all of the paid-for capacity. The solution would be to run your job at a different time, make your job smaller, or negotiate with the group that is using the available resources. The read step reads the data required at this stage from the table or from the output of the previous stage. A high value of read time indicates that you might consider reworking the query so that most of the data is read in the initial stages. The compute step carries out the computation required—if you run into high values here, consider whether you can carry out some of the operations in postprocessing or if you could omit the use of user-defined functions (UDFs).[9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn91) The write step writes to temporary storage or to the response and is mainly a function of the amount of data being written out in each stage—optimization of this step typically involves moving filtering options to occur in the innermost query (or earliest stage), although as we saw earlier, the BigQuery optimizer can do some of this automatically.

For all three stages in our query, the read step is what takes the most amount of time, indicating that our query is I/O bound and that the basic cost of reading the data is what dominates the query. It is clear from the numbers in the input column (6 million to 936 to 135) that we are already doing quite well at funneling the data through and processing most of the data in earlier stages. We also noticed from the explanation that BigQuery has already optimized things by moving the filtering step to the earliest possible stage—there is no way to move it any earlier because it is not possible to filter on the number of flights until that value is computed. On the other hand, if this is a frequent sort of filter, it might be helpful to add a table indicating the traffic at each airport and join with this table instead of computing the aggregate each time. It might also be possible to achieve an approximation to this by adding a column indicating some characteristic (such as the population) of the metropolitan area that each airport serves. For now, without any idea of the kinds of airports that the typical user of this dataset will be interested in, there is little to be done. We are determined to process all the data, and processing all the data requires time spent reading that data. If we don’t need statistics from all the data, we could consider sampling the data and computing our statistics on that sample instead.

## Exploratory Data Analysis in Vertex AI Workbench

Data scientists have moved en masse to using notebooks because notebooks greatly streamline the workflow of developing, visualizing, collaborating, and publishing in science.[10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn92) The contrast between the user experience of a Jupyter Notebook and the way exploratory data analysis was carried out a few years ago is stark. Take, for example, [Figure 5-7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#graph\_created\_using\_a\_complex\_workflowd), which appears in one of my papers about [different ways to track storms in weather radar images](https://oreil.ly/6pcZF). Just this single graphic required wrangling multiple languages (C++, R, Java), concepts (distributed programming, statistics), data formats (CSV, PNG, LaTeX, PDF), and collaboration mechanisms (FTP, email)\![11](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn93) Today, I’d do it all in a Jupyter Notebook with the big data analysis carried out in BigQuery or Dataflow.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0507.png" alt="" height="318" width="600"><figcaption></figcaption></figure>

**Figure 5-7. Graph created using a complex workflow.**

### Jupyter Notebooks

[JupyterLab](https://jupyter.org/) is open source software that provides an interactive scientific computing experience in a variety of languages, including Python, R, Julia, and Scala. The key unit of work is a Jupyter Notebook, which is a document that contains code, visualizations, and explanatory text. The code in the document is executed on and served from a web server that runs JupyterLab.

A key issue with notebooks is how to manage the web servers that serve them. Running the notebook server on our laptop will work, but is not ideal. Instead, we want the notebook server to run on a cloud machine for the following reasons:

* Because the code is executed on the notebook server, we want the notebook server to be able to handle bigger datasets. It is easier to get a more powerful machine on the public cloud than it is to upgrade one’s laptop. You can simplify managing the provisioning of notebook servers by running them on demand in the public cloud. This way, we can stop the machines when we leave for the day, instead of paying for machines even when we are not at work.
* Data science workloads such as machine learning require heavy, repetitive computation—typically, we scale up such workloads using GPUs. However, GPUs are expensive and become superseded by better hardware rather quickly. Ideally, you want to be able to add/remove GPUs on demand from these machines, rather than pay for GPUs all the time that we are using notebooks.
* A common pattern is to develop on small datasets and basic hardware and then, once we have the code working, to execute the code on large datasets on a more powerful machine. This ability to change the infrastructure is possible on the public cloud.
* We might even schedule the execution of these jobs periodically, or in response to an event such as the arrival of new data.

Once we say that we are going to run notebooks ephemerally on hardware that depends on the computations that we are doing, lifecycle management becomes quite important. Vertex AI Workbench on Google Cloud gives us a fully managed notebook experience.

To start a fully managed notebook in Google Cloud, visit the GCP web console, navigate to Vertex AI Workbench, and choose the tab for a Google-managed notebook. Then, create a notebook with the name dsongcp-ch5. Look at the Advanced settings and note that it’s possible to add a GPU if we want (see [Figure 5-8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#options\_when\_creating\_a\_managed\_noteboo)). Note also there is a default time period after which the notebook server will automatically shut down. We can always restart it by opening the notebook from the GCP console.

Vertex AI Workbench provides a hosted version of JupyterLab on Google Cloud; it knows how to authenticate against Google Cloud so as to provide easy access to Cloud Storage, BigQuery, Cloud Dataflow, Vertex AI Training, and so on. A few minutes after you launch a managed notebook, the console shows you a JupyterLab link. When you are done using the Notebooks instance, you can manually delete the instance from the web console. You can also stop the instance when you are not using it—you won’t be charged for the CPU resources, which are the bulk of the cost, although resources like disks will continue to be charged for. As shown in [Figure 5-8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#options\_when\_creating\_a\_managed\_noteboo), this shutdown can be made to happen automatically after an idle time period that you specify.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0508.png" alt="" height="652" width="600"><figcaption></figcaption></figure>

**Figure 5-8. Options when creating a managed notebook in Vertex AI Workbench.**

### Creating a Notebook

After you have launched the Notebooks instance and navigated to JupyterLab, you can create a new Python notebook from the launcher menu that it starts up with. Alternately, navigate to the folder in which you want this notebook to appear and select File > New Notebook.

A notebook contains two key types of cells: a _markdown_ cell is a cell with text content⁠,⁠[12](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn94) whereas a _code_ cell has source code. When you run a markdown cell, the cell contents are formatted nicely, whereas when you run a code cell, the notebook displays the output of _print_ statements from that cell.

For example, suppose that you type into a notebook the contents of [Figure 5-9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#what\_you\_type\_into\_the\_notebook\_left\_pa) (with the first cell a markdown cell, and the second cell a Python cell).

You can run the cell your cursor is in by clicking Run (or use the keyboard shortcut Ctrl/Cmd + Shift + Enter). You could also run all cells by clicking “Run all cells.” When you click this, the cells are evaluated, and the results rendered.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0509.png" alt="" height="725" width="600"><figcaption></figcaption></figure>

**Figure 5-9. What you type into the notebook (top) and what is rendered (bottom).**

Note that the markdown has been converted into a visual document, the Python code has been evaluated, and the resulting output printed out.

### Jupyter Commands

You can `git clone` the repository for this book within the notebook environment by typing:

<pre><code><strong>!git clone https://github.com/GoogleCloudPlatform/data-science-on-gcp
</strong></code></pre>

You could have also used the git icon in Vertex AI Workbench or run the preceding command from a terminal. Regardless of how you interact with git, get into the habit of practicing source-code control on changed notebooks.

The use of the exclamation point (when you type `!git` into a code cell) is an indication to Jupyter that the line is not Python code, but is instead a shell command. If you have multiple lines of a shell command, you can start a cell with `%%bash`, for example:

```
%%bash
wget tensorflow ...
pip install ...
```

### Installing Packages

Which Python packages are already installed in Notebooks, and which ones will we have to install? One way to check which packages are installed is to type the following:

<pre><code><strong>%pip freeze
</strong></code></pre>

This lists the Python packages installed. Another option is to add in imports for packages and see if they work. Let’s do that with packages that I know that we’ll need:

```
import matplotlib.pyplot as plt
import seaborn as sb
import pandas as pd
import numpy as np
```

_NumPy_ is the canonical numerical Python library that provides efficient ways of working with arrays of numbers. _Pandas_ is an extremely popular data analysis library that provides a way to do operations such as group by and filter on in-memory dataframes. _Matplotlib_ is a Matlab-inspired module to create graphs in Python. _seaborn_ provides extra graphics capabilities built on top of the basic functionality provided by Matplotlib. All these are open source packages that are installed in Vertex AI Workbench by default.

Had I needed a package that was not already installed, I could have installed it using `pip`. For example, to install the `pytz` package that we used in [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest), execute this code within a cell:

<pre><code><strong>%pip install pytz
</strong></code></pre>

Often, you will need to restart the Python kernel for the new package to be picked up (you can do this using the Restart Kernel button on the notebook ribbon user interface).

### Jupyter Magic for Google Cloud

When we used `%%bash` in the previous section, we were using a Jupyter _magic_, a syntactic element that marks what follows as special.[13](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn95) This is how Jupyter can support multiple interpreters or engines. Jupyter knows what language a cell is in by looking at the magic at its beginning. For example, try typing the following into a code cell:

<pre><code><strong>%%html
</strong><strong>This cell will print out a &#x3C;b> HTML &#x3C;/b> string.
</strong></code></pre>

You should see the HTML rendering of that string being printed out on evaluation, as depicted in [Figure 5-10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#jupyter\_magic\_for\_html\_renderingdot).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0510.png" alt="" height="135" width="600"><figcaption></figcaption></figure>

**Figure 5-10. Jupyter magic for HTML rendering.**

Jupyter magics provide a mechanism to run a wide variety of languages and ways to add some more. The BigQuery Python package has added a few magics to make the interaction with Google Cloud Platform convenient.

For example, you can run a query on your BigQuery table using the `%%bigquery` magic environment that comes with Vertex AI Workbench:

```
%%bigquery
SELECT
  COUNTIF(arr_delay >= 15)/COUNT(arr_delay) AS frac_delayed
FROM dsongcp.flights_tzcorr
```

If you get the fraction of flights that are delayed, as shown in [Figure 5-11](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#the\_percentpercentbiqquery\_magic\_enviro), all is well.

If not, look at the error message and carry out appropriate remedial actions. You might need to authenticate yourself, set the project you are working in, or change permissions on the BigQuery table.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0511.png" alt="" height="191" width="600"><figcaption></figcaption></figure>

**Figure 5-11. The `%%biqquery` magic environment that comes with Vertex AI Workbench.**

The fact that we refer to `%%bigquery` as a Jupyter magic should indicate that this is not pure Python—you can execute this only within a notebook environment. The magic, however, is simply a wrapper function for Python code.[14](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn96) If there is a piece of code that you’d ultimately want to run outside a notebook (perhaps as part of a scheduled script), it’s better to use the underlying Python and not the magic pragma:[15](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn97)

```
sql = """
SELECT
  COUNTIF(arr_delay >= 15)/COUNT(arr_delay) AS frac_delayed
FROM dsongcp.flights_tzcorr
"""
from google.cloud import bigquery
bq = bigquery.Client()
df = bq.query(sql).to_dataframe()
print(df)
```

One way to use the underlying Python is to use the `google.cloud.bigquery` package—this allows us to use code independent of the notebook environment. This is, of course, the same `bigquery` package in the Cloud Client Library that we used in Chapters [2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch02.html#ingesting\_data\_into\_the\_cloud) and [4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest). The client library includes interconnections between BigQuery results and NumPy/Pandas to simplify the creation of graphics.

## Exploring Arrival Delays

Now that we have a notebook up and running, let’s use it to do exploratory analysis of arrival delays because this is the variable we want to be able to predict.

### Basic Statistics

To pull the arrival delays corresponding to the model created in [Chapter 3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#creating\_compelling\_dashboards) (i.e., of the arrival delay for flights that depart more than 10 minutes late), we can do the following:

```
%%bigquery df
SELECT ARR_DELAY, DEP_DELAY
FROM dsongcp.flights_tzcorr
WHERE DEP_DELAY >= 10
```

This code uses the `%%bigquery` magic to run the SQL statement and stores the result set into a Pandas dataframe named df. Recall that in [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest), we did this using the Google Cloud Platform API.

After we have the dataframe, getting fundamental statistics about the two columns returned by the query is as simple as this:

```
df.describe()
```

This gives us the mean, standard deviation, minimum, maximum, and quartiles of the arrival and departure delays given that departure delay is more than 10 minutes, as illustrated in [Figure 5-12](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#getting\_the\_fundamental\_statistics\_of\_a) (see the `WHERE` clause of the query):

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0512.png" alt="" height="466" width="528"><figcaption></figcaption></figure>

**Figure 5-12. Getting the fundamental statistics of a Pandas dataframe.**

### Plotting Distributions

Beyond just the statistical capabilities of Pandas, we can also pass the Pandas dataframes and underlying NumPy arrays to plotting libraries like _seaborn_. For example, to plot a violin plot of our decision surface from [Chapter 3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#creating\_compelling\_dashboards) (i.e., of the arrival delay for flights that depart more than 10 minutes late), we can do the following:

```
sns.set_style("whitegrid")
ax = sns.violinplot(data=df, x='ARR_DELAY', inner='box', orient='h')
ax.axes.set_xlim(-50, 300);
```

This produces the graph shown in [Figure 5-13](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#violin\_plot\_of\_arrival\_delaydot).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0513.png" alt="" height="453" width="600"><figcaption></figcaption></figure>

**Figure 5-13. Violin plot of arrival delay.**

A violin plot is a _kernel density plot_;[16](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn98) that is, it is an estimate of the probability distribution function (PDF).[17](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn99) We see that, even though the distribution peaks around 10 minutes (which is the mode), deviations around this peak are skewed toward larger delays than smaller ones. Importantly, we also notice that there is only one peak—the distribution is not, for example, bimodal.

Let’s compare the violin plot for flights that depart more than 10 minutes late with the violin plot for flights that depart less than 10 minutes late and zoom in on the x-axis close to our 15-minute threshold. First, we pull all of the delays using the following:

```
%%bigquery df
SELECT ARR_DELAY, DEP_DELAY
FROM dsongcp.flights_tzcorr
```

In this query, I have dropped the `WHERE` clause. Instead, we will rely on Pandas to do the thresholding. I can now create a new column in the Pandas dataframe that is either `True` or `False` depending on whether the flight departed less than 10 minutes late:

```
df['ontime'] = df['DEP_DELAY'] < 10
```

We can graph this new Pandas dataframe using _seaborn_:

```
ax = sns.violinplot(data=df, x='ARR_DELAY', y='ontime',
                    inner='box', orient='h')
ax.set_xlim(-50, 200)
```

The difference between the previous violin plot and this one is the inclusion of the `ontime` column. This results in a violin plot (see [Figure 5-14](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#difference\_between\_violin\_plots\_of\_all)) that illustrates how different flights that depart 10 minutes late are from flights that depart early.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0514.png" alt="" height="387" width="600"><figcaption></figcaption></figure>

**Figure 5-14. Difference between violin plots of all late flights (top) versus on-time flights (bottom).**

The angular peak of the top violin plot indicates that the _seaborn_ default smoothing was too coarse. You can fix this by passing in a `gridsize` parameter:

```
ax = sns.violinplot(data=df, x='ARR_DELAY', y='ontime',
inner='box', orient='h', gridsize=1000)
```

But doing so will make the computation take much longer. The [notebook](https://oreil.ly/CmzHa) in the GitHub repository shows what the result looks like with greater smoothing.

As we discussed in [Chapter 3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#creating\_compelling\_dashboards), it is clear that the 10-minute threshold separates the dataset into two separate statistical regimes, so that the typical arrival delay for flights that depart more than 10 minutes late is skewed toward much higher values than for flights that depart more on time. We can see this in [Figure 5-14](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#difference\_between\_violin\_plots\_of\_all), both from the shape of the violin plot and from the box plot that forms its center. Note how centered the on-time flights are versus the box plot (the dark line in the center) for delayed flights.[18](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn100)

However, the extremely long, skinny tail of the violin plot is a red flag—it is an indication that the dataset might pose modeling challenges. Let’s investigate what is going on.

### Quality Control

We can continue writing queries in the notebook, but doing so on the BigQuery console gives me immediate feedback on syntax and logic errors. So, I switch over to [the BigQuery console](https://oreil.ly/cgJIR) and type in my first query:

```
SELECT
  AVG(ARR_DELAY) AS arrival_delay
FROM
  dsongcp.flights_tzcorr
GROUP BY
  DEP_DELAY
ORDER BY
  DEP_DELAY
```

This should give me the average arrival delay associated with every value of departure delay (which, in this dataset, is stored as an integer number of minutes). I got back more than one thousand rows. Are there really more than one thousand unique values of `DEP_DELAY`? What’s going on?

#### Oddball values

To look at this further, let’s add more elements to my initial query:

```
SELECT
  DEP_DELAY,
  AVG(ARR_DELAY) AS arrival_delay,
  COUNT(ARR_DELAY) AS numflights
FROM
  dsongcp.flights_tzcorr
GROUP BY
  DEP_DELAY
ORDER BY
  DEP_DELAY
```

The resulting table explains what’s going on. The first few rows have only a few flights each:

| `Row` | `DEP_DELAY` | `arrival_delay` | `numflights` |
| ----- | ----------- | --------------- | ------------ |
| `1`   | `null`      | `null`          | `0`          |
| `2`   | `-82.0`     | `-80.0`         | `1`          |
| `3`   | `-68.0`     | `-87.0`         | `1`          |
| `4`   | `-61.0`     | `-77.0`         | `1`          |
| `5`   | `-56.0`     | `-26.0`         | `1`          |

However, departure delay values of a few minutes have hundreds of thousands of flights:

| `56` | `0.0` | `-5.100334305600409`  | `328442` |
| ---- | ----- | --------------------- | -------- |
| `57` | `1.0` | `-4.188285855693881`  | `159619` |
| `58` | `2.0` | `-3.2246696399075128` | `121080` |
| `59` | `3.0` | `-2.1957821784079146` | `104177` |
| `60` | `4.0` | `-1.2101860730716607` | `92813`  |

Oddball values that are such a small proportion of the data can probably be ignored. Moreover, if the flight does really leave 82 minutes early, I’m quite sure that you won’t be on the flight, and if you are, you know that you will make the meeting. There is no reason to complicate our statistical modeling with such odd values.

#### Outlier removal: Big data is different

How can you remove such outliers? There are two ways to filter the data: one would be based on the departure delay variable itself, keeping only values that met a condition such as this:

```
WHERE dep_delay > -15
```

A second method would be to filter the data based on the number of flights:

```
WHERE numflights > 300
```

The second method—using a quality-control filter that is based on removing data for which we have insufficient examples—is preferable.

This is an important point that gets at the key difference between statistics on “normal” datasets and statistics on big data. Although I agree that the term _big data_ has become completely hyped, people who claim that big data is just data are missing a key point—the fundamental approach to problems becomes different when datasets grow sufficiently large. The way we detect outliers is just one such example.

For a dataset that numbers in the hundreds to thousands of examples, you would filter the dataset and remove values outside, say, `μ` ± 3`σ` (where `μ` is the mean and `σ` the standard deviation).[19](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn101) We can find out what the range would be by running a BigQuery query on the table:

```
SELECT
  AVG(DEP_DELAY) - 3*STDDEV(DEP_DELAY) AS filtermin,
  AVG(DEP_DELAY) + 3*STDDEV(DEP_DELAY) AS filtermax
FROM
  dsongcp.flights_tzcorr
```

This yields the range \[−102, 121] minutes so that the `WHERE` clause would become as follows:

```
WHERE dep_delay BETWEEN -102 AND 121
```

Of course, a filter that retains values in the range `μ` ± 3`σ` is based on an implicit assumption that the distribution of departure delays is Gaussian. We can avoid such an assumption by using percentiles, perhaps by omitting the top and bottom 5% of values:

```
SELECT
  APPROX_QUANTILES(DEP_DELAY, 20)
FROM
  dsongcp.flights_tzcorr
```

This would lead us to retain values in the range \[−9, 66]. Regardless of how we find the range, though, the range is based on an assumption that unusually high and low values are outliers.

On datasets that number in the hundreds of thousands to millions of examples, thresholding your input data based on value is dangerous because you can very well be throwing out valuable nuance—if there are sufficient examples of a delay of 150 minutes, it is worth modeling such a value regardless of how far off the mean it is. Customer satisfaction and “long-tail” business strategies might hinge on our systems coping well with usually small or large values. There is, therefore, a world of difference between filtering our data using:

```
WHERE dep_delay > -15
```

versus filtering it using:

```
WHERE numflights > 370
```

The first method imposes a threshold on the input data and is viable only if we are sure that a departure delay of less than −15 minutes is absurd. The second method, on the other hand, is based on how often certain values are observed—the larger our dataset grows, the less unusual any particular value becomes.

The term _outlier_ is, therefore, somewhat of a misnomer when it comes to big data. An outlier implies a range within which values are kept, with outliers being values that lie outside that range. Here, we are keeping data that meets a criterion involving frequency of occurrence—any value is acceptable as long as it occurs often enough in our data.[20](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn102)

**HOW FAR CAN YOU TRUST QUALITY FLAGS IN DATASETS?**

Many datasets include metadata about their data quality. These might even be on a row-by-row basis. Should you discard any rows whose quality is marked as being bad?

The quality flags that you find in many datasets are themselves highly suspect. Many of them are set with no basis on a holistic understanding of the environment (“the instrument was left unshielded”) but simply based on statistical analysis of the data values. The statistical techniques used are often carried over from the days of small datasets. So, if you can do your own analysis based on frequency of occurrence, you should.

Of course, if you don’t have time to examine the data, the quality flag in the dataset is better than nothing. Take it into consideration! However, the way to take it into consideration is to treat it as one more input to your model and not to discard the supposedly bad values.

In our flights dataset, we will trust flags such as whether the flight was canceled or diverted (those reflect an understanding of the environment) but carry out our own statistical analysis of values such as departure delay based on occurrence frequency.

#### Filtering data on occurrence frequency

To filter the dataset based on frequency of occurrence, we first need to compute the frequency of occurrence and then threshold the data based on it. We can accomplish this by using a `HAVING` clause:

<pre><code>SELECT
  DEP_DELAY,
  AVG(ARR_DELAY) AS arrival_delay,
  STDDEV(ARR_DELAY) AS stddev_arrival_delay,
  COUNT(ARR_DELAY) AS numflights
FROM
  dsongcp.flights_tzcorr
GROUP BY
  DEP_DELAY
<strong>HAVING
</strong><strong>  numflights > 370
</strong>ORDER BY
  DEP_DELAY
</code></pre>

Why threshold the number of flights at 370? This number derives from a guideline called the _three-sigma rule_,[21](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn103) which is traditionally the range within which we consider “nearly all values”[22](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn104) to lie. If we assume (for now; we’ll verify it soon) that at any departure delay, arrival delays are normally distributed, we can talk about things that are true for “almost every flight” if our population size is large enough. Because 99.73% of values in a Gaussian distribution lie within the three-sigma bounds, filtering our dataset so that we have at least 1 / (1 – 0.9973) = 370 examples of each input value is a rule of thumb that achieves this.[23](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn105)

How different would the results be if we were to choose a different threshold? We can look at the number of flights that are removed by different quality-control thresholds by looking at the slope of a linear model between arrival delay and departure delay using this query:

<pre><code><strong>CREATE TEMPORARY FUNCTION linear_fit(NUM_TOTAL INT64, THRESH INT64)
</strong>RETURNS STRUCT&#x3C;thresh INT64, num_removed INT64, lm FLOAT64>
AS ((
    SELECT AS STRUCT
        THRESH,
<strong>        (NUM_TOTAL - SUM(numflights)) AS num_removed,
</strong><strong>        AVG(arrival_delay * numflights) / AVG(dep_delay * numflights) AS lm
</strong>    FROM
    (
        SELECT
            DEP_DELAY,
            AVG(ARR_DELAY) AS arrival_delay,
            STDDEV(ARR_DELAY) AS stddev_arrival_delay,
            COUNT(ARR_DELAY) AS numflights
        FROM
            dsongcp.flights_tzcorr
        GROUP BY
            DEP_DELAY
    )
    WHERE numflights > THRESH
))
;
</code></pre>

Running this function for various different thresholds on `numflights` (see _exploration.ipynb_ in the GitHub repository), we get the following results:

| `Row` | `stats.thresh`           | `stats.num_removed` | `stats.lm` |
| ----- | ------------------------ | ------------------- | ---------- |
| `1`   | `1000`                   | `175873`            | `0.25`     |
| `2`   | `500`                    | `143801`            | `0.34`     |
| `3`   | `370 (three-sigma rule)` | `135518`            | `0.36`     |
| `4`   | `300`                    | `129835`            | `0.38`     |
| `5`   | `200`                    | `123640`            | `0.40`     |
| `6`   | `100`                    | `115471`            | `0.43`     |
| `7`   | `22 (two-sigma rule)`    | `108247`            | `0.45`     |
| `8`   | `10`                     | `106958`            | `0.46`     |
| `9`   | `5`                      | `106319`            | `0.46`     |

As you can see, the slope varies extremely slowly as we remove fewer and fewer flights by decreasing the threshold. Thus, the differences in the model created for thresholds of 300, 370, or 500 are quite minor. However, that model is quite different from that created if the threshold were 5 or 10. The order of magnitude of the threshold matters, but perhaps not the exact value.

### Arrival Delay Conditioned on Departure Delay

Now that we have a query that cleans up oddball values of departure delay from the dataset, we can take the query over to the Jupyter Notebook to continue our exploratory analysis and to develop a model to help us make a decision on whether to cancel our meeting.

In [Chapter 3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#creating\_compelling\_dashboards), we built a simple model based on simply thresholding the departure delay. Here, however, we see that there are many flights for each value of departure delay. Given a certain departure delay, what arrival delays are likely?

#### Distribution of arrival delays

I simply copy and paste from the BigQuery console to the notebook and give the Pandas dataframe a name, as shown here:

```
%%bigquery depdelay
SELECT
    DEP_DELAY,
    AVG(ARR_DELAY) AS arrival_delay,
    STDDEV(ARR_DELAY) AS stddev_arrival_delay,
    COUNT(ARR_DELAY) AS numflights
FROM
    dsongcp.flights_tzcorr
GROUP BY
    DEP_DELAY
HAVING numflights > 370
ORDER BY DEP_DELAY
```

We can display the first five rows of the dataframe using `[:5]`:

```
depdelay[:5]
```

The result is:

| `Row` | `DEP_DELAY` | `arrival_delay`       | `stddev_arrival_delay` | `numflights` |
| ----- | ----------- | --------------------- | ---------------------- | ------------ |
| `1`   | `-23.0`     | `-23.888646288209607` | `11.432163250582196`   | `458`        |
| `2`   | `-22.0`     | `-23.22748815165877`  | `12.590133374822704`   | `633`        |
| `3`   | `-21.0`     | `-22.29978118161926`  | `11.558312559289162`   | `914`        |
| `4`   | `-20.0`     | `-21.40782122905028`  | `12.066489232808147`   | `1432`       |
| `5`   | `-19.0`     | `-20.430769230769243` | `11.910133697086701`   | `1950`       |

Let’s plot this data to see what insight we can get. Even though we have been using _seaborn_ so far, Pandas itself has plotting functions built in:

```
ax = depdelay.plot(kind='line', x='DEP_DELAY',
              y='arrival_delay', yerr='stddev_arrival_delay')
```

This yields the plot shown in [Figure 5-15](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#relationship\_between\_departure\_delay\_an).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0515.png" alt="" height="427" width="600"><figcaption></figcaption></figure>

**Figure 5-15. Relationship between departure delay and arrival delay.**

It certainly does appear as if the relationship between departure delay and arrival delay is quite linear. The width of the standard deviation of the arrival delay is also pretty constant, on the order of 10 minutes.

#### Applying a probabilistic decision threshold

Recall from [Chapter 1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch01.html#making\_better\_decisions\_based\_on\_data) that our decision criteria are 15 minutes and 30%. If the plane is more than 30% likely to be delayed (on arrival) by more than 15 minutes, we want to send a text message asking to postpone the meeting. At what departure delay does this happen?

By computing the standard deviation of the arrival delays corresponding to each departure delay, we implicitly assumed that arrival delays are normally distributed. For now, let’s continue with that assumption. I can examine [a complementary cumulative distribution table](https://oreil.ly/JEb7T) and find where 0.3 happens. From the table, this happens at Z = 0.52.

Let’s now go back to Jupyter to plug this number into our dataset:

<pre><code><strong>Z_30 = 0.52
</strong><strong>depdelay['arr_delay_30'] = (Z_30 * depdelay['stddev_arrival_delay']) \
</strong><strong>             + depdelay['arrival_delay']
</strong>ax = plt.axes()
depdelay.plot(kind='line', x='DEP_DELAY', y='arr_delay_30',
              ax=ax, ylim=(0,30), xlim=(0,30), legend=False)
ax.set_xlabel('Departure Delay (minutes)')
ax.set_ylabel('> 30% prob of this Arrival Delay (minutes)');

x = np.arange(0, 30)
y = np.ones_like(x) * 15
ax.plot(x, y, 'r');
</code></pre>

The plotting code yields the plot depicted in [Figure 5-16](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#choosing\_the\_departure\_delay\_threshold).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0516.png" alt="" height="388" width="600"><figcaption></figcaption></figure>

**Figure 5-16. Choosing the departure delay threshold that results in a 30% probability of an arrival delay of < 15 minutes.**

Looking up the x-axis value corresponding to the decision threshold of 15 minutes (see dotted lines in [Figure 5-16](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#choosing\_the\_departure\_delay\_threshold)). It appears that our decision criteria translate to a departure delay of 13 minutes. If the departure delay is 13 minutes or more, the aircraft is more than 30% likely to be delayed by 15 minutes or more.

#### Empirical probability distribution function

The analysis in the previous section used the number 0.52, which assumes that the distribution of flights at each departure delay is normally distributed. What if we drop that assumption? We then will need to empirically determine the 30% likelihood at each departure delay. Happily, we do have at least 370 flights at each departure delay (the joys of working with large datasets!), so we can simply compute the 30th percentile for each departure delay.

We can compute the 30th percentile in BigQuery by discretizing the arrival delays corresponding to each departure delay into 100 bins and picking the arrival delay that corresponds to the 70th bin:

<pre><code>SELECT
    DEP_DELAY,
<strong>    APPROX_QUANTILES(ARR_DELAY, 101)[OFFSET(70)] AS arrival_delay_30th,
</strong>    COUNT(ARR_DELAY) AS numflights
FROM
    dsongcp.flights_tzcorr
GROUP BY
    DEP_DELAY
HAVING numflights > 370
ORDER BY DEP_DELAY
</code></pre>

The function `APPROX_QUANTILES()` takes the `ARR_DELAY` and divides it into `N` + 1 bins (here we specified `N` = 101).[24](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn106) The first bin is the approximate minimum, the last bin the approximate maximum, and the rest of the bins are what we’d traditionally consider the bins. Hence, the 70th percentile is the 71st element of the result. The `[]` syntax finds the `n`th element of that array—`OFFSET(70)` will provide the 71st element because `OFFSET` is zero-based.[25](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn107) Why 70 and not 30? Because we want the arrival delay that could happen with 30% likelihood and this implies the larger value.

The results of this query provide the empirical 30th percentile threshold for every departure delay:

| `Row` | `DEP_DELAY` | `arrival_delay_30th` | `numflights` |
| ----- | ----------- | -------------------- | ------------ |
| `1`   | `-23.0`     | `-20.0`              | `458`        |
| `2`   | `-22.0`     | `-19.0`              | `633`        |
| …     |             |                      |              |
| `39`  | `15.0`      | `14.0`               | `38835`      |
| `40`  | `16.0`      | `15.0`               | `35771`      |
| `41`  | `17.0`      | `16.0`               | `33964`      |
| …     |             |                      |              |

Plugging the query back into the Jupyter Notebook, we can avoid the Z-lookup and Z-score calculation associated with Gaussian distributions. We get the chart shown in [Figure 5-17](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#departure\_delay\_threshold\_that\_results).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0517.png" alt="" height="388" width="600"><figcaption></figcaption></figure>

**Figure 5-17. Departure delay threshold that results in a 30% likelihood of an arrival delay of < 15 min.**

#### The answer is...

From the chart in [Figure 5-16](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#choosing\_the\_departure\_delay\_threshold), our decision threshold, without the assumption of normal distribution, is 16 minutes. If a flight is delayed by more than 16 minutes, there is a greater than 30% likelihood that the flight will arrive more than 15 minutes late.

Recall that the aforementioned threshold is conditioned on rather conservative assumptions—you are going to cancel the meeting if there is more than 30% likelihood of being late by 15 minutes. What if you are a bit more audacious in your business dealings, or if this particular customer will not be annoyed by a few minutes’ wait? What if you won’t cancel the meeting unless there is a greater than 70% chance of being late by 15 minutes? The good thing is that it is easy enough to come up with a different decision threshold for different people and different scenarios out of the same basic framework.

Another thing to notice is that the addition of the actual departure delay in minutes has allowed us to make a better decision than going with just the contingency table. Using just the contingency table, we would cancel meetings whenever flights were just 10 minutes late. Using the actual departure delay and a probabilistic decision framework, we were able to avoid canceling our meeting unless flights were delayed by 16 minutes or more.

## Evaluating the Model

But how good is this advice? How many times will my advice to cancel or not cancel the meeting be the correct one? Had you asked me that question, I would have hemmed and hawed—I don’t know how accurate the threshold is because we have no independent sample. Let’s address that now—as our models become more sophisticated, an independent sample will be increasingly required.

There are two broad approaches to finding an independent sample:

* Collect new data. For example, we could go back to BTS and download 2016 data and evaluate the recommendation on that dataset.
* Split the 2015 data into two parts. Create the model on the first part (called the _training set_), and evaluate it on the second part (called the _test set_).

The second approach is more common because datasets tend to be finite. In the interest of being practical here, let’s do the same thing even though, in this instance, we could go back and get more data.[26](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn108)

When splitting the data, we must be careful. We want to ensure that both parts are representative of the full dataset (and have similar relationships to what you are predicting), but at the same time we want to make sure that the testing data is independent of the training data. To understand what this means, let’s take a few reasonable splits and talk about why they won’t work.

### Random Shuffling

We might split the data by randomly shuffling all the rows in the dataset and then choosing the first 70% as the training set, and the remaining 30% as the test set. In BigQuery, you could do that using the `RAND()` function:

<pre><code>SELECT
  ORIGIN, DEST,
  DEP_DELAY,
  ARR_DELAY
FROM
  dsongcp.flights_tzcorr
WHERE
<strong>  RAND() &#x3C; 0.7
</strong></code></pre>

The `RAND()` function returns a value between 0 and 1, so approximately 70% of the rows in the dataset will be selected by this query. However, there are several problems with using this sampling method for machine learning:

* It is not nearly as easy to get the 30% of the rows that were not selected to be in the training set to use as the test dataset.
* The `RAND()` function returns different things each time it is run, so if you run the query again, you will get a different 70% of rows. In this book, we are experimenting with different machine learning models, and this will play havoc with comparisons between models if each model is evaluated on a different test set.
* The order of rows in a BigQuery result set is not guaranteed—it is essentially the order in which different workers return their results. So, even if you could set a random seed to make `RAND()` repeatable, you’ll still not get repeatable results. You’d have to add an `ORDER BY` clause to explicitly sort the data (on an ID field, a field that is unique for each row) before doing the `RAND()`. This is not always going to be possible.

Further, on this particular dataset, random shuffling is problematic for another reason. Flights on the same day are probably subject to the same weather and traffic factors. Thus, the rows in the training set and test sets will not be independent if we simply shuffle the data. This consideration is relevant only for this particular dataset—shuffling the data and taking the first 70% will work for other datasets that don’t have this interrow dependence, as long as you have an `id` field.

We could split the data such that Jan–Sep 2015 is training data and Oct–Dec is testing data. But what if delays can be made up in summer but not in winter? This split fails the representativeness test. Neither the training dataset nor the test dataset will be representative of the entire year if we split the dataset by months.

### Splitting by Date

The approach that we will take is to find all the unique days in the dataset, shuffle them, and use 70% of these days as the training set and the remainder as the test set. For repeatability, I will store this division as a table in BigQuery.

The first step is to get all the unique days in the dataset:

```
SELECT
  DISTINCT(FL_DATE) AS FL_DATE
FROM
  dsongcp.flights_tzcorr
ORDER BY
  FL_DATE
```

The next step is to select a random 70% of these to be our training days:

```
SELECT
  FL_DATE,
  IF(ABS(MOD(FARM_FINGERPRINT(CAST(FL_DATE AS STRING)), 100)) < 70,
     'True', 'False') AS is_train_day
FROM (
  SELECT
    DISTINCT(FL_DATE) AS FL_DATE
  FROM
    dsongcp.flights_tzcorr)
ORDER BY
  FL_DATE
```

In the preceding query, the hash value of each of the unique days from the inner query is computed using the FarmHash library and the `is_train_day` field is set to `True` if the last two digits of this hash value are less than 70:[27](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn109)

| `Row` | `FL_DATE`    | `is_train_day` |
| ----- | ------------ | -------------- |
| `1`   | `2015-01-01` | `True`         |
| `2`   | `2015-01-02` | `False`        |
| `3`   | `2015-01-03` | `False`        |
| `4`   | `2015-01-04` | `True`         |
| `5`   | `2015-01-05` | `True`         |

The final step is to save this result as a table in BigQuery:

```
CREATE OR REPLACE TABLE dsongcp.trainday AS
...
```

We can join with this table whenever we want to pull out training rows. For your convenience, the preceding query is in the GitHub repository in the file _trainday.txt_, so you can simply do:

```
cat trainday.txt | bq query --nouse_legacy_sql
```

In some chapters, we won’t be using BigQuery. Just in case we aren’t using BigQuery, I will also export the table as a CSV file—we can do this on the web console, but we can also script it:

```
bq extract dsongcp.trainday gs://${BUCKET}/flights/trainday.csv
```

### Training and Testing

Now, I can go back and edit my original query to carry out the percentile using only data from my training days. To do that, I will change this string in my original query:

```
FROM
    dsongcp.flights_tzcorr
```

to:

```
FROM
    dsongcp.flights_tzcorr
JOIN dsongcp.trainday USING(FL_DATE)
WHERE is_train_day = 'True'
```

Now, the percentile is computed out only on days for which `is_train_day` is `True`.

The code to create the plot remains the same. On running it, the threshold (the x-axis value of the intersection point) remains consistent, as depicted in [Figure 5-18](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#the\_departure\_delay\_threshold\_remains\_c).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0518.png" alt="" height="388" width="600"><figcaption></figcaption></figure>

**Figure 5-18. The departure delay threshold remains consistent with earlier methods.**

This is gratifying because we get the same answer—16 minutes—after creating the empirical probabilistic model on just the training data.

Let’s formally evaluate how well our recommendation of 16 minutes does in terms of predicting an arrival delay of 15 minutes or more. To do that, we have to find the number of times that we would have wrongly canceled a meeting or missed a meeting. We can compute these numbers using this query on days that are not training days:

<pre><code>SELECT
  SUM(IF(DEP_DELAY &#x3C; 16
      AND arr_delay &#x3C; 15, 1, 0)) AS correct_nocancel,
  SUM(IF(DEP_DELAY &#x3C; 16
      AND arr_delay >= 15, 1, 0)) AS wrong_nocancel,
  SUM(IF(DEP_DELAY >= 16
      AND arr_delay &#x3C; 15, 1, 0)) AS wrong_cancel,
  SUM(IF(DEP_DELAY >= 16
      AND arr_delay >= 15, 1, 0)) AS correct_cancel
FROM (
  SELECT
    DEP_DELAY,
    ARR_DELAY
  FROM
    dsongcp.flights_tzcorr
  JOIN dsongcp.trainday USING(FL_DATE)
<strong>  WHERE is_train_day = 'False' 
</strong>)
</code></pre>

Note that unlike when I was computing the decision threshold, I am not removing outliers (i.e., thresholding on 370 flights at a specific departure delay) when evaluating the model—outlier removal is part of my training process, and the evaluation needs to be independent of that. The second point to note is that this query is run on days that are not in the training dataset. Running this query in BigQuery, I get:

| `Row` | `correct_nocancel` | `wrong_nocancel` | `wrong_cancel` | `correct_cancel` |
| ----- | ------------------ | ---------------- | -------------- | ---------------- |
| `1`   | `1259740`          | `66081`          | `52827`        | `217669`         |

We will cancel meetings corresponding to a total of 52,827 + 217,669 or around 270k flights. What fraction of the time are these recommendations correct? We can do the computation in the notebook (assuming that the dataframe is named `eval`):

```
print(eval['correct_nocancel'] /
      (eval['correct_nocancel'] + eval['wrong_nocancel']))
print(eval['correct_cancel'] /
      (eval['correct_cancel'] + eval['wrong_cancel']))
```

[Figure 5-19](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#computing\_accuracy\_on\_independent\_test) presents the results.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0519.png" alt="" height="163" width="600"><figcaption></figcaption></figure>

**Figure 5-19. Computing accuracy on independent test dataset.**

It turns out when I recommend that you not cancel your meeting, I will be correct 95% of the time, and when I recommend that you cancel your meeting, I will be correct 82% of the time.

Why is this not 70%? Because the populations are different. In creating the model, we found the 70th percentile of arrival delay given a specific departure delay. In evaluating the model, we looked at the dataset of all flights. One’s a marginal distribution, and the other’s the full one. Another way to think about this is that the 95% figure is padded by all the departure delays of more than 20 minutes when canceling the meeting is an easy call.

We could evaluate right at the decision boundary by changing our scoring function:

```
SELECT
  SUM(IF(DEP_DELAY = 15
      AND arr_delay < 15, 1, 0)) AS correct_nocancel,
  SUM(IF(DEP_DELAY = 15
      AND arr_delay >= 15, 1, 0)) AS wrong_nocancel,
  SUM(IF(DEP_DELAY = 16
      AND arr_delay < 15, 1, 0)) AS wrong_cancel,
  SUM(IF(DEP_DELAY = 16
      AND arr_delay >= 15, 1, 0)) AS correct_cancel
...
```

If we do that, evaluating only at departure delays of 15 and 16 minutes, the contingency table and ratios look like those in [Figure 5-20](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#evaluating\_only\_at\_marginal\_decisionsdo).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0520.png" alt="" height="540" width="600"><figcaption></figcaption></figure>

**Figure 5-20. Evaluating only at marginal decisions.**

As expected, we are correct to not cancel the meeting 72% of the time, close to our target of 70%. We chose the departure delay threshold of 16 minutes on the training dataset because we expected to be 70% correct in not canceling if we do so, and now we’ve proved on an independent dataset that this is the case. This model achieves the 70% correctness measure that was our target but does so by canceling fewer flights than the contingency table–based model of [Chapter 3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#creating\_compelling\_dashboards).

## Summary

In this chapter, we began to carry out exploratory data analysis. To be able to interactively analyze our large dataset, we loaded the data into BigQuery, which gave us the ability to carry out queries on millions of rows in a matter of seconds. We required sophisticated statistical plotting capabilities, and we obtained that by using a Jupyter Notebook in the form of Vertex AI Workbench.

In terms of the model itself, we were able to use nonparametric estimation of the 30th percentile of arrival delays, at each departure delay, to pick the departure delay threshold. We discovered that doing this allows us to cancel fewer meetings while attaining the same target correctness. We evaluated our decision threshold on an independent set of flights by dividing our dataset into two parts—a training set and a testing set—based on randomly partitioning the distinct days that comprise our dataset.

## Suggested Resources

To learn how to carry out EDA using Python libraries like Matplotlib, NumPy, and Pandas, read the O’Reilly Media book [_Hands-On Exploratory Data Analysis with Python_](https://www.oreilly.com/library/view/hands-on-exploratory-data/9781789537253/) by Suresh Kumar Mukhiya and Usman Ahmed. For a more theoretically grounded introduction to the topic, consider taking the [online course on EDA](https://oreil.ly/Swjj0) from Johns Hopkins.

Peruse and work through the [gallery of _seaborn_ plots](https://oreil.ly/XkazV) so that you are familiar with the different ways of visualizing data that are available.

While Jupyter is great for EDA, it is not a great environment for developing standalone Python programs. Ultimately, you will want to refactor your notebook code into functions and move them into a Python package. At that point, use a proper integrated development environment like [PyCharm](https://oreil.ly/1vWEK) or [Visual Studio Code](https://oreil.ly/teQf4). This is exactly what we will do in this book. To learn this workflow, read this 2018 article by Florian Wilhelm, [“Working Efficiently with JupyterLab Notebooks”](https://oreil.ly/xjevC). You don’t need to install Jupyter because Vertex AI Workbench manages that for you, but the rest of Florian’s advice applies.

[1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn83-marker) See [this Wikipedia article](https://oreil.ly/62pYc) for an excellent overview of statistical hypotheses testing.

[2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn84-marker) John Tukey. _Exploratory Data Analysis_. Addison-Wesley, 1977.

[3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn85-marker) James Shore. _The Art of Agile Development_, 2nd ed. With Diana Larsen, Gitte Klitgaard, and Shane Warden. O’Reilly Media, 2021.

[4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn86-marker) I did confirm with the BTS via email that the times in the dataset were, indeed, in the local time zone of the corresponding airport. The BTS being a government agency that was making the data freely available, I didn’t broach the matter of them producing a separate dataset with UTC timestamps. In a contractual relationship with a vendor, however, this is the type of change you might request as a result of EDA.

[5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn87-marker) A violin plot is a way of visualizing probability density functions. See the [seaborn documentation](https://oreil.ly/BT3nv) for examples of violin plots.

[6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn88-marker) A _univariate graph_ is a graph of just one variable. For example, we might plot the histogram of arrival delays. A _bivariate graph_ is a graph of two variables. For example, we might plot the median taxi-out time by airport.

[7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn89-marker) Special [general purpose graphics processing unit (GPU) instances exist](https://oreil.ly/oeNQp) that are used for high-performance computing applications, but for generating the graphs in this chapter, CPU instances are sufficient.

[8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn90-marker) Reducing the skew is not just about reducing the time taken by the workers who have a lot to do. You should also see if you can rework the query to combine the work carried out by the underworked workers, so that you have fewer workers overall.

[9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn91-marker) BigQuery supports UDFs in JavaScript, but the excessive use of UDFs can slow down your query, and certain UDFs can be [high-compute queries](https://oreil.ly/NnsNW).

[10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn92-marker) Jupyter Notebooks are a realization of the _literate programming_ concept envisioned by Donald Knuth in 1984. By allowing data scientists to interweave statistical and business logic in code and the output of that code within a literate statistical programming paradigm, notebooks foster replicability and reuse.

[11](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn93-marker) If you are interested in the gory details: [Figure 5-7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#graph\_created\_using\_a\_complex\_workflowd) was created by running the methods in question (PRJ, CST, etc.) on a large dataset of weather radar imagery and computing various evaluation metrics (VIL error in the graphic). For performance reasons, this was done in C++. The metric for each pair of images was written out to a text file (a different text file for each method), and it is the aggregates of those metrics that are reported in [Figure 5-7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#graph\_created\_using\_a\_complex\_workflowd). The text files had to be wrangled from the cluster of machines on which they were written out, combined by key (the method used to track storms), and then aggregated. This code, essentially a MapReduce operation, was written in Java. The resulting aggregate files were read by an R program that ranked the methods, determined the appropriate shading, and wrote out an image file in PNG format. These PNG images were incorporated into a LaTeX report, and a compiler run to create a shareable document in PDF from the LaTeX and PNG sources. It was this PDF of the paper that we could disseminate to interested colleagues.\
If the colleague then suggested a change, we’d go through the process all over again. The ordering of the programs—C++, followed by Java, followed by R, followed by LaTeX, followed by attaching the PDF to an email—was nontrivial, and there were times when we skipped something in between, resulting in incorrect graphics or text that didn’t match the graphs.

[12](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn94-marker) The _README.md_ files in the repository are another example of markdown files. Markdown is ubiquitous enough that it is worth learning the key syntax from a [cheat sheet](https://oreil.ly/QTXnS).

[13](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn95-marker) Magics use a syntax element that is not valid in the underlying language. In the case of notebooks in Python, this is the `%` symbol.

[14](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn96-marker) See [GitHub](https://oreil.ly/YCxiW) for the Python code being wrapped.

[15](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn97-marker) See _05\_bqnotebook/exploration.ipynb_ in the GitHub repository.

[16](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn98-marker) A kernel [density plot](https://oreil.ly/za47l) is just a smoothed histogram—the challenge lies in figuring out how to smooth the histogram while balancing interpretability against the loss of information. Here, I’m just letting _seaborn_ use its default settings for the smoothing bandwidth.

[17](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn99-marker) See the discussion of the PDF in [Chapter 1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch01.html#making\_better\_decisions\_based\_on\_data).

[18](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn100-marker) I created this second, zoomed-in violin plot by adding `ax.set_xlim(-50, 50)`.

[19](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn101-marker) In a Gaussian distribution, 99.7% of values lie within three standard deviations of the mean. It’s a handy way to identify outliers.

[20](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn102-marker) In this dataset, floating-point numbers have already been discretized. For example, arrival delays have been rounded to the nearest minute. If this is not the case, you will have to discretize continuous data before computing the frequency of occurrence.

[21](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn103-marker) For a normal distribution (at each departure delay, the number of flights is in the hundreds to thousands, so usual statistical thinking applies), 68.27% of values lie in the μ ± σ range, 95.45% of values lie in the `μ` ± 2`σ` range, and 99.73% of values lie in the μ ± 3σ range. That last range is termed the three-sigma rule. For more information, see the [Encyclopedia of Mathematics entry](https://oreil.ly/Da4Xp) for the three-sigma rule.

[22](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn104-marker) Traditions, of course, are different in different fields and often depend on how much data you can reasonably collect in that field. In business statistics, this three-sigma rule is quite common. In the social sciences and in medicine, two-sigma is the typical significance threshold. Meanwhile, when the Higgs boson discovery announcement was made, the significance threshold to classify it as a true discovery and not just a statistical artifact was five-sigma or 1 in 3.5 million (see the [blog at _Scientific American_](https://oreil.ly/zDo6O)).

[23](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn105-marker) It might appear fishy that this number is independent of the size of the dataset, but if you think about it, this rule of thumb has to be such that we are less likely to discard outliers the more data we have. The larger the dataset, the more likely it is that there will be 370 instances of any particular condition. Corner cases on small datasets will have enough company on very large datasets.

[24](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn106-marker) This function computes [the approximate quantiles](https://oreil.ly/qIksk) because computing the exact quantiles on large datasets, especially of floating-point values, can be very expensive in terms of space. Instead, most big data databases use some variant of [Greenwald and Khanna’s algorithm](https://oreil.ly/4BuOU) to compute approximate quantiles.

[25](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn107-marker) Had I used `ORDINAL` instead of `OFFSET`, it would have been 1-based.

[26](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn108-marker) By the time we get to [Chapter 10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#getting\_ready\_for\_mlops\_with\_vertex\_ai), I will have based so many decisions on 2015–2018 data that I will get 2019 data to act as a truly independent test set.

[27](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#ch01fn109-marker) See [the Google Open Source Blog](https://oreil.ly/3KDFA) for a description and [GitHub](https://oreil.ly/r9x0I) for the code.
