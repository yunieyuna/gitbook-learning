# 10. Getting Ready For MLOps With Vertex AI

## Chapter 10. Getting Ready for MLOps with Vertex AI

In [Chapter 9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch09.html#machine\_learning\_with\_tensorflow\_in\_ver), we developed a TensorFlow model in a Jupyter Notebook. We were able to train the model, deploy it to an endpoint, and get predictions from it from the notebook environment. While that worked for us during development, it is not a scalable workflow.

Taking a TensorFlow model that you trained in your Jupyter Notebook and deploying the SavedModel to Vertex AI doesn’t scale to hundreds of models and large teams. Retraining is going to be difficult because the ops team will have to set up all of the ops and monitoring and scheduling on top of something that is really clunky and totally nonminimal.

In order for a machine learning model to be placed into production, it needs to meet the following requirements:

* The model should be under version control. Source code control systems such as git work much better with text files (such as _.py_ files) than with mixtures of text and binaries (which is what _.ipynb_ files are).
* The entire process—from dataset creation to training to deployment—has to be driven by code. This is so that it is easy to automatically retrigger a training run using GitHub Actions or GitLab Continuous Integration whenever new changed code is checked in.
* The entire process should be invokable from a single entry point, so that the retraining can be triggered by noncode changes such as the arrival of new data in a Cloud Storage bucket.
* It should be easy to monitor the performance of models and endpoints and take measures to fix some subset of issues that arise without having to modify the model code. For example, if GPUs are getting saturated, it should be easy to add extra resources for training or serving. It should be possible to continuously evaluate the model, and if the distribution of an input feature changes or if the evaluation metric falls below a specific threshold, it should be possible to trigger model retraining.

Together, these criteria go by the name _MLOps_. Google Cloud, in general, and Vertex AI, in particular, provide a number of MLOps capabilities. However, in order to take advantage of these inbuilt capabilities, it is better if we clearly separate out the model code from the ops code and express everything in Python rather than in notebooks.

## Developing and Deploying Using Python

Jupyter Notebooks are great for development, but I strongly recommend against putting those notebooks directly into production, even though Vertex AI will allow you to do this.

What I recommend is that you convert your initial prototyping model code into a Python file and then continue all development in it. Throw away the Jupyter Notebook. The Python files will be what’s in your code repository, and will be the maintained codebase from now onwards.

Look at the code in the files _model.py_ and _train\_on\_vertexai.py_ in the code repository of this book, and use them to follow along.

**USING LOCAL PYTHON MODULE FROM JUPYTERLAB**

If you throw away the Jupyter Notebook in which you did development, how can you do future ad-hoc work and demos? What I recommend is that you invoke the extracted (and maintained) Python code from a new notebook for future experimentation, ad-hoc data analytics, or demos. For example, supposing you extract the code to a file named _model.py_, you can invoke functions in that file from JupyterLab:

```
import model
trainds = model.read_dataset(...)
```

If you find yourself changing _model.py_ as you write code in your new notebook, make sure to add this magic at the top of your notebook:

```
%autoreload
```

This [automatically reloads](https://oreil.ly/bEDD2) the module whenever you change _model.py_ so that you are not running older code.

### Writing model.py

I created the file _model.py_ by extracting all the Keras model code from the Jupyter Notebook I wrote in the previous section (_flights\_model\_tf2.ipynb_). Much of the notebook code has been extracted into a function called `train_and_evaluate.py`:

```
def train_and_evaluate(train_data_pattern, eval_data_pattern, test_data_pattern, 
                       export_dir, output_dir):
    ...
    train_dataset = read_dataset(train_data_pattern, train_batch_size)
    eval_dataset = read_dataset(eval_data_pattern, eval_batch_size, 
                                tf.estimator.ModeKeys.EVAL, num_eval_examples)

    model = create_model()
    history = model.fit(train_dataset,
                        validation_data=eval_dataset,
                        epochs=epochs,
                        steps_per_epoch=steps_per_epoch,
                        callbacks=[cp_callback])
    # export
    logging.info('Exporting to {}'.format(export_dir))
    tf.saved_model.save(model, export_dir)
```

There are three key things to note:

* The data is read from URIs specified by `train_data_pattern`,[1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#ch01fn170) `eval_data_pattern`, and `test_data_pattern` for training, validation, and test datasets, respectively.
* The model creation code is extracted out to a function called `create_model`.
* The model is written out to `export_dir`, and any other intermediate outputs are written to `output_dir`.

I get the data patterns and output directories in _model.py_ using environment variables:

```
    OUTPUT_MODEL_DIR = os.getenv("AIP_MODEL_DIR")
    TRAIN_DATA_PATTERN = os.getenv("AIP_TRAINING_DATA_URI")
    EVAL_DATA_PATTERN = os.getenv("AIP_VALIDATION_DATA_URI")
    TEST_DATA_PATTERN = os.getenv("AIP_TEST_DATA_URI")
```

These environment variables form the contract between my code and Vertex AI and are needed in order for all the automagical things to happen.

I will, however, also want to run this code outside Vertex AI (for example, during development). In such a case, the environment variable will not be set, and so the variables will all be `None`. I look for that case and set them to values in my development environment:

```
    if not OUTPUT_MODEL_DIR:
        OUTPUT_MODEL_DIR = os.path.join(OUTPUT_DIR,
                                    'export/flights_{}'.format(
                                    time.strftime("%Y%m%d-%H%M%S")))
    if not TRAIN_DATA_PATTERN:
        TRAIN_DATA_PATTERN = 'gs://{}/ch9/data/train*'.format(BUCKET)
    if not EVAL_DATA_PATTERN:
        EVAL_DATA_PATTERN = 'gs://{}/ch9/data/eval*'.format(BUCKET)
```

These files can be very small because they are only for development. Actual production runs will run inside Vertex AI where the environment variables will be set.

Once I finish extracting the code into _model.py_, I make sure it works:

```
 python3 model.py  --bucket <bucket-name> --develop   
```

I can also run it on the full dataset by dropping the `develop` flag (I suggest you visit the accompanying code to understand the _model.py_ script and its arguments). The results are the same as my Jupyter Notebook, so I can move on to invoking this from a Vertex AI pipeline.

### Writing the Training Pipeline

The training pipeline (See `train_on_vertexai.py`) needs to do five things in code:

* Load up a managed dataset in Vertex AI.
* Set up training infrastructure to run _model.py_.
* Train the model by invoking functions in _model.py_ on the managed dataset.
* Find the endpoint to which to deploy the model.
* Deploy the model to the endpoint.

Let’s look at them one by one.

First, I load up a tabular dataset (options exist for image, text, and other datasets, and for tabular data in BigQuery):

```
 data_set = aiplatform.TabularDataset.create(
        display_name='data-{}'.format(ENDPOINT_NAME),
        gcs_source=['gs://{}/ch9/data/all.csv'.format(BUCKET)]
)
```

Note that I am passing in _all_ of the data. Vertex AI will take care of splitting the data into train, validate, and test datasets and sending it to the training program. By default, the split will be random, whereas we want to split based on the daywise split that we have set up. I’ll get back to this.

Second, I create a training job passing in _model.py_, the training container image, and the serving container image:

```
model_display_name = '{}-{}'.format(ENDPOINT_NAME, timestamp)
job = aiplatform.CustomTrainingJob(
        display_name='train-{}'.format(model_display_name),
        script_path="model.py",
        container_uri=train_image,
        requirements=[],  # any extra Python packages
        model_serving_container_image_uri=deploy_image
)
```

Just as we did when we did it with bash scripts, we are assigning a timestamped name to the model.

The third step is to run the job. This involves running _model.py_ on the managed dataset on some hardware:

```
model = job.run(
        dataset=data_set,
        model_display_name=model_display_name,
        args=['--bucket', BUCKET],
        replica_count=1,
        machine_type='n1-standard-4',
        accelerator_type=aip.AcceleratorType.NVIDIA_TESLA_T4.name,
        accelerator_count=1
    )
   
```

I get back a model that I wish to deploy to a preexisting endpoint. To find an existing endpoint or create one, I then do:

```
    endpoints = aiplatform.Endpoint.list(
        filter='display_name="{}"'.format(ENDPOINT_NAME),
        order_by='create_time desc',
        project=PROJECT, location=REGION,
    )
    if len(endpoints) > 0:
        endpoint = endpoints[0]  # most recently created
    else:
        endpoint = aiplatform.Endpoint.create(
            display_name=ENDPOINT_NAME, project=PROJECT, location=REGION
        )
```

Finally, I deploy the model to the endpoint using:

```
model.deploy(
        endpoint=endpoint,
        traffic_split={"0": 100},
        machine_type='n1-standard-2',
        min_replica_count=1,
        max_replica_count=1
    )
```

That’s it! Now, we have a Python program that we can run anytime we want to retrain and/or deploy the trained model. Of course, the MLOps person will typically not replace the model wholesale, but send only a small fraction of the traffic to the model. They’ll probably also set up monitoring and continuous evaluation on the endpoint in Vertex AI. But we’ve made it easy for them to do that.

We can try out the training pipeline:

```
python3 train_on_vertexai.py --project <project> \
        --bucket <bucket-name> –develop
```

This time, though, the training happens in the managed service. The GCP web console shows me GPU utilization and the job’s logs show up in Cloud Logging.

### Predefined Split

By default, Vertex AI does a fractional split of the data (80% to training, 10% each for validation and testing). Here, however, we want to explicitly assign each row to a data split. To do this, we need to add a column to our dataset that controls the split. We can do this when creating the data:

<pre><code>CREATE OR REPLACE TABLE dsongcp.flights_all_data AS
SELECT
  IF(arr_delay &#x3C; 15, 1.0, 0.0) AS ontime,
  dep_delay,
  taxi_out,
  ...
<strong>  IF (is_train_day = 'True',
</strong><strong>      IF(ABS(MOD(FARM_FINGERPRINT(CAST(f.FL_DATE AS STRING)), 100)) &#x3C; 60, 
</strong><strong>                                                    'TRAIN', 'VALIDATE'),
</strong><strong>      'TEST') AS data_split
</strong>FROM dsongcp.flights_tzcorr f
...
   
</code></pre>

Basically, there is a column that I’m calling `data_split` that takes the values `TRAIN`, `VALIDATE`, or `TEST`. So, every row in the managed dataset is assigned to one of these three splits.

Then, when I’m training the job, I specify what the predefined splitting column is:

```
model = job.run(
        dataset=data_set,
        predefined_split_column_name='data_split',
        model_display_name=model_display_name,
```

Vertex AI will take care of the rest, including assigning all the necessary metadata to the models being trained.

### AutoML

What changes should I make in the preceding pipeline if I want to use AutoML instead of my custom training job? Well, I don’t need my own _model.py_ of course. So, instead of the `CustomTrainingJob`, I’ll use AutoML.

Setting and running the training job (Steps 3 and 4 in [“Writing the Training Pipeline”](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#writing\_the\_training\_pipeline)) now become:

<pre><code>def train_automl_model(data_set, timestamp):
    # train
    model_display_name = '{}-{}'.format(ENDPOINT_NAME, timestamp)
<strong>    job = aiplatform.AutoMLTabularTrainingJob(
</strong>        display_name='train-{}'.format(model_display_name),
        optimization_prediction_type='classification'
    )
    model = job.run(
        dataset=data_set,
        target_column='ontime',
        model_display_name=model_display_name,
        budget_milli_node_hours=(300 if develop_mode else 2000),
        disable_early_stopping=False
    )
    return job, model
 
</code></pre>

That’s the only change! The rest of the pipeline stays the same. Vertex AI provides a unified platform for ML development regardless of the ML technique you use. In fact, we can similarly change the ML framework to PyTorch or to sklearn or XGBoost and, as far as the MLOps people are concerned, there are only minimal changes.

In my _train\_on\_vertexai.py_, I switch between custom Keras code and AutoML with a command-line parameter.

How well does AutoML do? Does it beat our custom Keras model with the latitude and longitude feature crosses? Unfortunately (see [Figure 10-1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#performance\_metrics\_from\_automl\_do\_not)), AutoML reports precision, recall, area under the curve, and several other metrics. However, it does not report RMSE (as of this writing in January 2022).

Looking at the feature importance graph that is part of the GCP console for AutoML models (see [Figure 10-2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#the\_gcp\_web\_console\_for\_automl\_provides)), it appears that AutoML didn’t take much advantage of the latitude and longitude of the airports.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1001.png" alt="" height="426" width="600"><figcaption></figcaption></figure>

**Figure 10-1. Performance metrics from AutoML do not include RMSE.**

In order to compute custom evaluation metrics, we can ask AutoML to dump the evaluation data and predictions to a table in BigQuery. In order to do so, I added the following to the AutoML job:

<pre><code>model = job.run(
        dataset=data_set,
        predefined_split_column_name='data_split',
        target_column='ontime',
        model_display_name=model_display_name,
        budget_milli_node_hours=(300 if develop_mode else 2000),
        disable_early_stopping=False,
<strong>        export_evaluated_data_items=True,
</strong><strong>        export_evaluated_data_items_bigquery_destination_uri=(
</strong><strong>            '{}:dsongcp.ch9_automl_evaluated'.format(PROJECT)),
</strong><strong>        export_evaluated_data_items_override_destination=True
</strong>    )
</code></pre>

Now, when I rerun the AutoML training job, a table is created in BigQuery and I can compute the RMSE in SQL:

```
SELECT  
  SQRT(SUM(
      (CAST(ontime AS FLOAT64) - predicted_ontime.scores[OFFSET(0)])*
      (CAST(ontime AS FLOAT64) - predicted_ontime.scores[OFFSET(0)])
      )/COUNT(*))
FROM dsongcp.ch9_automl_evaluated
```

The result? AutoML Tables on the dataset achieved an RMSE of 0.199. So, our custom model with the feature crosses is better than AutoML (0.196), but AutoML came really close.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1002.png" alt="" height="696" width="600"><figcaption></figcaption></figure>

**Figure 10-2. The GCP web console for AutoML provides feature importance.**

## Hyperparameter Tuning

Our custom model is better than AutoML, but could it be even better? There are a number of hyperparameters—learning rate, batch size, number of layers/nodes in the neural network, number of buckets, number of embedding dimensions, etc. that I essentially just guessed.

For example, the number of layers and the number of hidden nodes was essentially arbitrary. As discussed earlier, more layers help the model learn more complex input spaces, but it is difficult to have an intuition about how difficult this particular problem (predicting flight delays) is. However, the choice of model architecture does matter—choosing too few layers will result in a suboptimal classifier, whereas choosing too many layers might result in overfitting. We need to select an appropriate number of layers and nodes.

The optimizer uses gradient descent, but computes the gradients on small batches. We used a batch size of 64, but that choice was arbitrary. The larger the batch size, the quicker the training run will complete because the network overhead scales with the number of batches—with larger batches, we have fewer batches to complete an epoch, and so the training will complete faster. However, if the batch size is too large, the sensitivity of the optimizer to specific data points reduces and hurts the ability of the optimizer to learn the nuances of the problem. Even in terms of efficiency, too large a batch will cause matrix multiplications to spill over from more efficient memory to less efficient memory (such as from a GPU to a CPU or thrashing from RAM to HDD). Thus, the choice of batch size matters.

There are other arbitrary choices that are specific to our model. For example, we discretized the latitude and longitude into five buckets each. What should this number of buckets actually be? Too low a number, and we will lose the discrimination ability; too high a number, and we will begin to overfit.

As a final step in improving the model, we’ll carry out an experiment with different choices for these three parameters: number of hidden units, batch size, and number of buckets. Even though we could laboriously carry out these experiments one-by-one, we will use a capability of Vertex AI called Vizier that allows for a nonlinear hyperparameter tuning approach. We’ll specify ranges for these three parameters, specify a maximum number of trials we want to try out, and have Vizier carry out a search in hyperparameter space for the best set of parameters.

To add this capability to our Python code, this is what I have to do:

* Parameterize the model in _model.py._
* Implement a shorter training run.
* Write out metrics during training.
* Implement a hyperparameter tuning pipeline.
* Run the best trial to completion.

Recall that the model training file exists as _model.py_ in the code repository of this book and the pipeline orchestrator is _train\_on\_vertexai.py_. Use the code in the two files to follow along.

### Parameterize Model

The first step is to make the hyperparameters as command-line parameters to your model. For example, in _model.py_, we might do:

```
parser.add_argument(
        '--nembeds',
        help='Embedding dimension for categorical variables',
        type=int,
        default=3
    )
```

Note that the initial guess for the variable is the default value. This allows your training script to continue working as it did before. Then, you set the variable from the command-line parameters for use by the training script:

```
args = parser.parse_args()
...
NEMBEDS = args.nembeds     
```

It’s a good idea to do this for all the hyperparameters we might ever want to tune. A good practice is to never have any hardcoded values in _model.py_—everything there needs to be an input parameter.

### Shorten Training Run

Our training run so far has involved training on the full dataset and then evaluating on the full test dataset. Doing a complete training run like that for hyperparameter tuning is expensive, wasteful, and wrong. Why?

_Expensive_

The point of hyperparameter tuning is to obtain the best set of parameters, not to obtain the best possible model. Once we find the best set of parameters, we can then train a model with those parameters to completion. Therefore, there is no need to carry out a trial to completion. We just need to train it until you know which trial is likely to end up better (see [Figure 10-3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#doing\_a\_complete\_training\_run\_for\_hyper)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1003.png" alt="" height="273" width="600"><figcaption></figcaption></figure>

**Figure 10-3. Doing a complete training run for hyperparameter tuning is expensive, wasteful, and wrong.**

WastefulUnder the assumption that your training curves won’t cross each other, a better set of parameters will be better throughout the training process, and you can stop the training well before it starts to converge. Use your training budget to do more trials, not to run those trials longer.WrongYou don’t want to evaluate the hyperparameter tuning on the test dataset. You want to compare performance on the validation dataset. Just make sure that the validation dataset is large enough for you to do this comparison between trial models meaningfully.

The way I do these modifications is to add two options to my _model.py_: one to train for a shorter time and another to skip the full evaluation:

<pre><code><strong>NUM_EXAMPLES = args['num_examples']
</strong><strong>SKIP_FULL_EVAL = args['skip_full_eval']
</strong>...
steps_per_epoch = NUM_EXAMPLES // train_batch_size
epochs = NUM_EPOCHS
eval_dataset = read_dataset(eval_data_pattern, eval_batch_size,
                    tf.estimator.ModeKeys.EVAL, num_eval_examples)
model.fit(train_dataset,
                        validation_data=eval_dataset,
                        epochs=NUM_EPOCHS,
<strong>                        steps_per_epoch=steps_per_epoch,
</strong>                        callbacks=[cp_callback, HpCallback()])
...
if not SKIP_FULL_EVAL:
        test_dataset = read_dataset(test_data_pattern, eval_batch_size, 
                                    tf.estimator.ModeKeys.TEST, None)
        final_metrics = model.evaluate(test_dataset)
        ...
else:
        logging.info("Skipping evaluation on full test dataset")
</code></pre>

What’s the deal with `steps_per_epoch` and `NUM_EXAMPLES`? Note the x-axis in [Figure 10-3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#doing\_a\_complete\_training\_run\_for\_hyper). It’s not epochs—it’s the number of examples. While it’s pretty wasteful to train on the full dataset, it can be helpful to get the same number of intermediate metrics that you would get with the full amount of training (I’ll explain why in the next step). Because you will also be hyperparameter tuning the batch size, the best way to do this is to use Virtual Epochs (see the Checkpoints pattern in [_Machine Learning Design Patterns_](https://www.oreilly.com/library/view/machine-learning-design/9781098115777/) by Valliappa Lakshmanan, Sara Robinson, and Michael Munn \[O’Reilly] for details). Steps-per-epoch is how we get virtual epochs on large datasets.

### Metrics During Training

The next modification is to write out metrics during the training process. We don’t want to just wait until the end before writing out the entire history. If we do this, then Vertex AI will also help us save costs by cutting short unproductive trials.

In Keras, to write out metrics during training we can define and use a callback:

```
METRIC = 'val_rmse'
hpt = hypertune.HyperTune()

class HpCallback(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs=None):
            if logs and METRIC in logs:
                logging.info("Epoch {}: {} = {}".format(
                             epoch, METRIC, logs[METRIC]))
                hpt.report_hyperparameter_tuning_metric(
hyperparameter_metric_tag=METRIC, metric_value=logs[METRIC], global_step=epoch)

...
history = model.fit(train_dataset,
                    ...
                    callbacks=[cp_callback, HpCallback()])
```

I’m using the `cloudml-hypertune` package to simplify the writing of metrics in a form that the TensorFlow ecosystem (TensorBoard, Vizier, etc.) can understand.

### Hyperparameter Tuning Pipeline

Now that we have modified _model.py_ to make it easy to do hyperparameter tuning, we could build an MLOps pipeline to tune our model anytime we notice it drifting.

But now, back to the hyperparameter tuning: there are two steps in the orchestration code (in _train\_on\_vertexai.py_).

First, we create a Vertex AI CustomJob to call the _model.py_ with the right parameters:

<pre><code>    tf_version = '2-' + tf.__version__[2:3]
    train_image = (
            "us-docker.pkg.dev/vertex-ai/training/tf-gpu.{}:latest"
            .format(tf_version))
    model_display_name = '{}-{}'.format(ENDPOINT_NAME, timestamp)
    trial_job = aiplatform.CustomJob.from_local_script(
        display_name='train-{}'.format(model_display_name),
<strong>        script_path="model.py",
</strong>        container_uri=train_image,
<strong>        args=[
</strong><strong>            '--bucket', BUCKET,
</strong><strong>            '--skip_full_eval', # no need to evaluate on test data
</strong><strong>            '--num_epochs', '10',
</strong><strong>            '--num_examples', '500000' # 1/10 actual size
</strong><strong>        ],
</strong>        requirements=['cloudml-hypertune'],
        replica_count=1,
        machine_type='n1-standard-4',
        accelerator_type=aip.AcceleratorType.NVIDIA_TESLA_T4.name,
        accelerator_count=1,
    )
</code></pre>

Next, we create and run a hyperparameter tuning job that will use the custom job as an individual trial:

<pre><code> hparam_job = aiplatform.HyperparameterTuningJob(
        display_name='hparam-{}'.format(model_display_name),
        custom_job=trial_job,
<strong>        metric_spec={'val_rmse': 'minimize'},
</strong>        parameter_spec={
<strong>            "train_batch_size": hpt.IntegerParameterSpec(
</strong><strong>                                   min=16, max=256, scale='log'),
</strong><strong>            "nbuckets": hpt.IntegerParameterSpec(
</strong><strong>                                   min=5, max=10, scale='linear'),
</strong><strong>            "dnn_hidden_units": hpt.CategoricalParameterSpec(
</strong><strong>               values=["64,16", "64,16,4", "64,64,64,8", "256,64,16"])
</strong>        },
        max_trial_count=4 if develop_mode else 10,
        parallel_trial_count=2,
        search_algorithm=None,  # Bayesian
    )
  
</code></pre>

Note that I am specifying the metric here to match the `METRIC` in my _model.py_ and that I’m specifying ranges for the parameters.

The parameter `train_batch_size` is an integer; we ask for it to look for values in the interval \[16, 256]—the logarithmic scale instructs the tuner that we would like to try more values at the smaller end of the range rather than the larger end of the range. This is because long-standing experience suggests that smaller batch sizes yield more accurate models.[2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#idm46519169175440) And also because I expect the effect of going from 16 to 32 to be bigger than going from 216 to 232, for example.

The `nbuckets` parameter is also an integer, but linearly distributed between 5 and 10. The FAA seems to have about 36 grid boxes into which it divides up the airspace (see [Figure 9-3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch09.html#air\_traffic\_in\_the\_usa\_is\_managed\_by\_th)). This argues for `nbuckets = 6` (since 6 × 6 = 36), but the corridors are significantly narrower in the Northeast part of the United States, and so perhaps we need more fine-grained grid cells. By specifying `nbuckets` in the range 5 to 10, we are asking the tuner to explore having between 25 and 100 grid cells into which to divide up the United States.

As for `dnn_hidden_units`, we explicitly specify a few candidates—a two-layer network, a three-layer network, and a four-layer network, and a network with many more nodes. If it turns out that any optimal parameter is near the extrema, we will repeat the hyperparameter tuning with a different range. For example, if it turns out that `nbuckets` = 10 is best, we should repeat the tuning, but trying out `nbuckets` in the range 10 to 15 the next time. Similarly, if a four-layer network turns out to be best, we will need to also try a five-layer and a six-layer network.

By default, the hyperparameter tuning service in Vertex AI (called Vizier) will use Bayesian optimization, but we can change the algorithm to GridSearch if we want.

### Best Trial to Completion

Once we launch the hyperparameter tuning job, we can look at the Vertex AI section of the GCP console to see the parameters come in (see [Figure 10-4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#results\_of\_hyperparameter\_tuning\_in\_the)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1004.png" alt="" height="296" width="600"><figcaption></figcaption></figure>

**Figure 10-4. Results of hyperparameter tuning in the GCP Vertex AI web console.**

Once we determine the best set of parameters, we can take the best set of parameters and then run the training job to completion. That will give us the model to deploy.

We can automate this as well, of course:

```
    best = sorted(hparam_job.trials, 
              key=lambda x: x.final_measurement.metrics[0].value)[0]
    logging.info('Best trial: {}'.format(best))
    best_params = []
    for param in best.parameters:
        best_params.append('--{}'.format(param.parameter_id))
        best_params.append(param.value)
    # run the best trial to completion
    model = train_custom_model(data_set, timestamp, develop_mode, 
                               extra_args=best_params)   
```

On doing this, I got a model whose RMSE was 0.195. The improvement (0.196 to 0.195) is relatively minimal. It appears that our initial guesses were not that bad.

## Explaining the Model

Why does the model believe that a certain flight will be late with a probability of 0.83? An active area of research in machine learning is to provide the reasoning that underlies a specific model production in a form that humans can understand.

One of the advantages of deploying a model into Vertex AI is that explainability is easy to add. Several techniques of explainability are supported. The one I like for tabular data is [Shapley](https://oreil.ly/GWzQQ), which apportions the credit for the prediction among the input features.

To add explainability capabilities, we will have to deploy the model with a configuration file that specifies the input and output tensors. Sending the normal prediction request to the endpoint to which the model is deployed will return a response that contains feature attributions.

### Configuring Explanations Metadata

When we create a model in Vertex AI, we can specify that it should be able to explain its predictions. Broadly speaking, explaining a prediction is more expensive than simply making the prediction because the model has to be invoked with small variants of the original request in order to estimate the affect of different parameters.

Because the model needs to be invoked with variants, we need to configure the model with information from the serving input signature. We can get the serving signature of a TensorFlow/Keras model by using the command-line tool `saved_model_cli`:

```
saved_model_cli show --tag_set serve \
     --signature_def serving_default --dir $model_dir  
```

Doing this on our flights model yields the following signature:[3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#ch01fn171)

<pre><code><strong>  inputs['arr_airport_lat'] tensor_info:
</strong>      dtype: DT_FLOAT
      shape: (-1)
      name: serving_default_arr_airport_lat:0
  inputs['arr_airport_lon'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1)
      name: serving_default_arr_airport_lon:0
  ...
The given SavedModel SignatureDef contains the following output(s):
<strong>  outputs['pred'] tensor_info:
</strong>      dtype: DT_FLOAT
      shape: (-1, 1)
      name: StatefulPartitionedCall_2:0
<strong>Method name is: tensorflow/serving/predict
</strong></code></pre>

Based on the preceding, we can write a file that I’ll call _explanations-metadata.json_:

```
{
  "inputs": {
    ...
    "arr_airport_lat": {
      "inputTensorName": "arr_airport_lat"
    },
    "arr_airport_lon": {
      "inputTensorName": "arr_airport_lon"
    },
    ...
  },
  "outputs": {
    "pred": {
      "outputTensorName": "pred"
    }
  }
}
```

Rather than hand-crafting the file, I created it using a short Python program and supplying the names of the columns:

<pre><code>cols = ('dep_delay,taxi_out,distance,dep_hour,is_weekday,' +
        'dep_airport_lat,dep_airport_lon,' +
        'arr_airport_lat,arr_airport_lon,' +
        'carrier,origin,dest')
<strong>inputs = {x: {"inputTensorName": "{}".format(x)} 
</strong>        for x in cols.split(',')}
expl = {
<strong>    "inputs": inputs,
</strong>    "outputs": {
    "pred": {
<strong>      "outputTensorName": "pred"
</strong>    }
  }
}
print(expl)
with open('explanation-metadata.json', 'w') as ofp:
    json.dump(expl, ofp, indent=2)
</code></pre>

Now that we have the configuration file that specifies what inputs the model needs when it is invoked on variants of the original query, we can create and deploy the model.

### Creating and Deploying Model

When creating the model, we specify three more options: the explanation method, how many variants of the original query to perform, and the location of the metadata configuration file we created in the previous section:

```
gcloud ai models upload ... \
     --explanation-method=sampled-shapley \
     --explanation-path-count=10 \
     --explanation-metadata-file=explanation-metadata.json
```

Of course, we could do this in Python as well. The Python SDK to create the model [supports the first two parameters](https://oreil.ly/Ki02v). Instead of passing in a metadata file, we’d pass in the metadata object itself.

Deploying the model is identical to before:

```
gcloud ai endpoints deploy-model $ENDPOINT_ID \
  --region=$REGION \
  --model=$MODEL_ID \
  --display-name=$MODEL_NAME \
  --machine-type=n1-standard-2 \
  --min-replica-count=1 \
  --max-replica-count=1 \
  --traffic-split=0=100
 
```

Note that we did not have to make any changes to the model code itself in order to add explainability to it.

### Obtaining Explanations

Once the explainability-enhanced model has been deployed, any client can request an explanation. The format of the JSON request doesn’t change. Instead of sending the request to the predict method, the request has to be sent to the explain method:

<pre><code>PROJECT=$(gcloud config get-value project)
ENDPOINT_NAME=flights_xai
ENDPOINT_ID=$(gcloud ai endpoints list --region=$REGION \
        --format='value(ENDPOINT_ID)' --filter=display_name=${ENDPOINT_NAME})

curl -X POST\
-H "Authorization: Bearer "$(gcloud auth application-default print-access-token)\
-H "Content-Type: application/json; charset=utf-8"\
-d @example_input.json\
<strong>"https://...${PROJECT}/locations/${REGION}/endpoints/${ENDPOINT_ID}:explain"
</strong></code></pre>

When I did it on a flight from GST to JNU, I got the following result:

<pre><code>{
  "explanations": [
    {
      "attributions": [
        {
          "baselineOutputValue": 0.48559775948524475,
          "instanceOutputValue": 0.98635220527648926,
          "featureAttributions": {
            "dep_hour": -0.0019751578569412228,
            "distance": 0.02608233392238617,
            "origin": 0.00673377513885498,
            "arr_airport_lat": 0.065238907933235168,
            "dest": 0.0031582355499267579,
            "taxi_out": 0.017888876795768741,
            "is_weekday": -0.0054439753293991089,
<strong>            "dep_airport_lon": 0.15576429069042211,
</strong>            "carrier": 0.0063359200954437259,
<strong>            "arr_airport_lon": 0.32970959544181822,
</strong>            "dep_airport_lat": -0.070850974321365362,
            "dep_delay": -0.03188738226890564
          },
          "outputIndex": [
            0
          ],
          "approximationError": 0.008536300316499771,
          "outputName": "pred"
        }
      ]
    }
  ],
  "deployedModelId": "48598413947699200",
  "predictions": [
    [
      0.986352205
    ]
  ]
}
</code></pre>

The two most important features are the longitudes of the arrival and departure airport, respectively. This is particularly surprising when we consider (see [Figure 10-2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#the\_gcp\_web\_console\_for\_automl\_provides)) that the AutoML model doesn’t consider the airport locations particularly important. This highlights the important difference between global importance (what we saw in [Figure 10-2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#the\_gcp\_web\_console\_for\_automl\_provides)) and local explanation (explaining an individual prediction). There are features that are unimportant in the aggregate but make a lot of difference on specific instances. Because this flight has a departure delay almost identical to the baseline, the model might be making more nuanced predictions based on the remaining features.

What makes the longitude of the airport important for a flight from GST to JNU? Well, GST is Gustavus airport in Alaska and JNU is the airport in Juneau, Alaska. In other words, this is a flight within Alaska. Alaska is a large state, and the distance between the airports (41 miles) is rather unusual. Remember that one of the reasons for gridding the airport locations was to learn nuances like flights within a state. It appears that the model has learned it!

## Summary

In this chapter, we learned how to automate the entire process by creating a Vertex training pipeline. We created a single entry point for the end-to-end training run. At this point, it is easy to make this entry point be the thing that is triggered whenever new code is checked in, when new data is received, or when changes in feature distribution or model evaluation are detected.

After we had a viable machine learning model and features, we carried out hyperparameter tuning to find optimal values of batch size, learning rate, number of buckets, and neural network architecture. We discovered that our initial, default choices were themselves quite good, but that increasing the number of layers provided a minute improvement.

For speed of experimentation, we had trained and hyperparameter-tuned the model on a sample of the full dataset. So, next, we trained the model with the chosen features and hyperparameters on the full dataset.

Finally, we also added explainability to the model and were able to get the contribution of each feature on the predicted outcome.

Remember, however, we briefly explored time-windowed aggregated features (like the average taxi-out time at an airport). However, it was unclear how we could compute it on behalf of online prediction clients. Instead, we used features such as the day of the week and hour of the day extracted from the departure time. In the next chapter, we will look at how to do machine learning, where the input features (like moving averages) have to be computed in real-time.

## Suggested Resources

In this chapter, we discussed how to get ready for MLOps. Implementing MLOps requires knowledge of how to do continuous build and continuous integration with GitHub or GitLab:

* To learn more about continuous build, see this tutorial on [GitOps-style continuous delivery with Cloud Build](https://oreil.ly/ASFrn).
* To learn more about continuous integration for data processing workflows, see this article on [setting up a CI/CD pipeline](https://oreil.ly/vIEhm).
* Watch the video [“MLOps Best Practices on Google Cloud (Cloud Next ’19)”](https://oreil.ly/cLlbs) on YouTube.

The Google Cloud whitepaper on MLOps, [“Practitioners Guide to MLOps”](https://oreil.ly/gsjSA) by Khalid Salama et al., is very comprehensive.

[1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#ch01fn170-marker) _URIs_ or Uniform Resource Identifiers are strings that identify information resources on a network. URIs are a broader category than URLs in that we can also use identifiers such as _**`gs://`**_….

[2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#idm46519169175440-marker) Geoffrey Hinton, “A Practical Guide to Training Restricted Boltzmann Machines,” University of Toronto Department of Computer Science, August 2, 2010. https://oreil.ly/hkiUn.

[3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#ch01fn171-marker) See the Jupyter Notebook _flights\_model\_tf2.ipynb_.
