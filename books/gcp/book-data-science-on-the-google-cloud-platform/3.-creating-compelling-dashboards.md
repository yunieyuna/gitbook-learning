# 3. Creating Compelling Dashboards

## Chapter 3. Creating Compelling Dashboards

In [Chapter 2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch02.html#ingesting\_data\_into\_the\_cloud), we ingested on-time performance data from the US Bureau of Transportation Statistics (BTS) so as to be able to model the arrival delay given various attributes of an airline flight—the purpose of the analysis is to cancel a meeting if the probability of the flight arriving within 15 minutes of the scheduled arrival time is less than 70%.

Before we delve into building statistical and machine learning models, it is important to explore the dataset and gain an intuitive understanding of the data—this is called _exploratory data analysis_, and it’s covered in more detail in [Chapter 5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#interactive\_data\_exploration\_with\_verte). You should always carry out exploratory data analysis for any dataset that will be used as the basis for decision making. In this chapter, though, I talk about a different aspect of depicting data—of depicting data to end users and decision makers so that they can understand the recommendation that you are making. The audience of these visual representations, called _dashboards_, that we talk about in this chapter is not other data scientists, but is instead the end users. Keep the audience in mind as we go through this chapter, especially if you come from a data science background—the purpose of a dashboard is to explain an existing model, not to develop it. A dashboard is an end-user report that is interactive, tailored to end users, and continually refreshed with new data. See [Table 3-1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#a\_dashboard\_is\_different\_from\_explorato).

|                       | For decision makers                                                                                                                                                                                                                                                        | For data scientists                                                                                                                                                                                                                    |
| --------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Usage pattern         | Dashboards                                                                                                                                                                                                                                                                 | Exploratory data analysis                                                                                                                                                                                                              |
| Kinds of depictions   | Current status, gauges, trendlines                                                                                                                                                                                                                                         | Model fits with error bars, kernel density estimates                                                                                                                                                                                   |
| What does it explain? | Model recommendations and confidence                                                                                                                                                                                                                                       | Input data, feature importance, model performance, etc.                                                                                                                                                                                |
| Data represented      | Subset of dataset, tailored to user’s context                                                                                                                                                                                                                              | Aggregate of historical data                                                                                                                                                                                                           |
| Typical tools         | Data Studio, Tableau, Qlik, Looker, plotly, D3, shiny apps, etc.                                                                                                                                                                                                           | Jupyter, Python, R Studio, S-plus, matplotlib, seaborn, Matlab, etc.                                                                                                                                                                   |
| Mode of interaction   | GUI-driven                                                                                                                                                                                                                                                                 | Code-driven                                                                                                                                                                                                                            |
| Update                | Real time                                                                                                                                                                                                                                                                  | Not real time                                                                                                                                                                                                                          |
| Covered in            | [Chapter 3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#creating\_compelling\_dashboards) , [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest) | [Chapter 5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#interactive\_data\_exploration\_with\_verte)                                                                                             |
| Example               | <p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_03in01.png" alt="" data-size="original"></p><p>From AAA fuel gage report, May 2013</p>                                                                                 | <p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_03in02.png" alt="" data-size="original"></p><p>From <a href="https://oreil.ly/YMeHv">AAA safety and educational foundation</a></p> |

Very often, this step of creating end-user visual depictions goes by the anodyne name of “visualization,” as in visualizing data. However, I have purposefully chosen not to call it by that name because there is more to this than throwing together a few bar graphs and charts. Dashboards are highly visual, interactive reports that have been designed to depict data and explain models. When used in this way, dashboards provide business intelligence (BI).

All of the code snippets in this chapter are available in the folder [_03\_sqlstudio_ of the book’s GitHub repository](https://github.com/GoogleCloudPlatform/data-science-on-gcp). See the _README.md_ file in that directory for instructions on how to do the steps described in this chapter.

## Explain Your Model with Dashboards

The purpose of this step in the modeling process is not simply to depict the data but to improve your users’ understanding of how the model behaves. Whenever you are designing the display of a dataset, evaluate the design in terms of three aspects:

* Does it accurately and honestly depict the data? This is important when the raw data itself can be a basis for decision making.
* How well does it help envision not just the raw data, but the information content embodied in the data? Will the typical user know whether they need to take action after looking at the graphic? This is crucial for the cases when you are relying on human pattern recognition and interaction to help reveal insights about the environment in which the data was collected.
* Is it constructed in such a way that it explains the model being used to provide recommendations?

You want to build displays that are always accurate and honest. At the same time, the displays need to be interactive so as to provide viewers with the ability to play with the data and gain insights. Insights that users have gained should be part of the display of that information going forward in such a way that those insights can be used to explain the data.

The last point, that of explanatory power, is very important. The idea is to disseminate data understanding throughout your company. A statistical or machine learning model that you build for your users will be considered a black box, and while you might get feedback on when it works well and when it doesn’t, you will rarely get pointed suggestions on how to actually improve that model in the field. In many cases, your users will use your model at a much more fine-grained level than you ever will because they will use your model to make a single decision, whereas in both training and evaluation, you would have been looking at model performance as a whole. Explainability is also critical to catch situations where the model is amplifying unfair bias.[1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn45)

Although this holistic overview is useful for statistical rigor, you need people taking a close look at individual cases, too. Because users are making decisions one at a time, they are analyzing the data one scenario at a time. If you provide your users not just with your recommendation, but also with an explanation of why you are recommending it, they will begin to develop insights into your model. However, your users will only be able to develop such insights into the problem and your recommendations if you give them ways to observe the data that went into your model. Give enough users ways to view and interact with your data, and you will have unleashed a never-ending wave of innovation as users suggest improvements and factors the model should be considering.

Your users have other activities that require their attention. Why would they spend their time looking at your data? One of the ways to entice them to do that is by making the depiction of the information compelling. In my experience,[2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn46) the most compelling displays are displays of real-time information in context. You can show people the average airline delay at JFK on January 12, 2012, and no one will care. But show a traveler in Chicago the average airline delay at ORD _right now_ and you will have their interest—the difference is that the data is in context (O’Hare Airport, or ORD, for a traveler in Chicago) and that it is real-time information.

In this chapter, therefore, we will look at building dashboards that combine accurate depictions with explanatory power and interactivity in a compelling package. This seems to be a strange time to be talking about building dashboards—shouldn’t the building of a dashboard wait until after we have built the best possible predictive model?

### Why Build a Dashboard First?

Building a dashboard when building a machine learning model is akin to building a form or survey tool to help you build the machine learning model. To build powerful machine learning models, you need to understand the dataset and devise features that help with prediction. By building a dashboard, you get to rope in the eventual users of the predictive model to take a close look at your data. Their fine-grained look at the data (remember that everyone is looking at the data corresponding to their context) will complement your overarching look at it. As they look at the data and keep sending suggestions and insights about the data to your team, you will be able to incorporate them into the machine learning model that you actually build.

In addition, when presented with a dataset, you should be careful that the data is the way you imagine it to be. There is no substitute for exposing and exploring the data to ensure that. Doing such exploratory data analysis with an immediately attainable milestone—building a dashboard from your dataset—is a fine way to do something real with the data and develop awareness of the subtleties of your data. Just as you often understand a concept best when you explain it to someone, building an explanatory display for your data is one of the best ways to develop your understanding of a dataset. The fact that you have to visualize the data in order to do basic preprocessing such as outlier detection makes it clear that building visual representations is work you will be doing anyway. If you are going to be doing it, you might as well do it well, with an eye toward its eventual use in production.

Eventual use in production is the third reason why you should develop the dashboard first instead of leaving it as an afterthought. Building explanatory power should be constantly on your mind as you develop the machine learning model. Giving users just the machine learning model will often go down poorly—they have no insight into why the system is recommending whatever it does. Adding explanations to the recommendations is more likely to succeed. For example, if you accompany your model prediction with five of the most salient features presented in an explanatory way, it will help make the model output more believable and trustworthy.

Even for cases in which the system performs poorly, you will receive feedback along the lines of “the prediction was wrong, but it is because Feature #3 was fishy. I think maybe you should also look at Factor Y.” In other words, shipping your machine learning model along with an explanation of its behavior gets you more satisfied users, and users whose criticism will be a lot more constructive. It can be tempting to ship the machine learning model as soon as it is ready, but if there is a dashboard already available (because you were building it in parallel), it is easier to counsel that product designers consider the machine learning model and its explanatory dashboard as the complete product.

Finally, creating a dashboard is fun. It will help you build a user base quickly and show end users what’s in it for them.

Explanations can be a double-edged sword because humans are not fully rational beings. Explanations can be the result of _apophenia_—the tendency of humans to see meaningful patterns even when there are none. This can lead to _motivated reasoning_—the tendency of humans to create justifications for decisions that are more desirable at an emotional level. The combination of apophenia and motivated reasoning can lead to just-so stories that attempt to justify whatever the state of the world is on the basis of spurious explanations. As data scientists, we should realize that we too are human. We need to be careful to set aside these biases, consider counterfactuals, and be willing to revise our initial judgments. Easier said than done, of course.

Where should these dashboards be implemented? Find out the environment that gets the largest audience of experts and eventual users and build your dashboard to target that environment.

Your users might already have a visualization interface with which they are familiar. Especially when it comes to real-time data, your users might spend their entire workday facing a visualization program that is targeted toward power users—this is true of weather forecasts, air traffic controllers, and options traders. If that is the case, look for ways to embed your visualizations into that interface. In other cases, your users might prefer that your visualizations be available from the convenience of their web browser. If this is the case, look for a visualization tool that lets you share the report as an interactive, commentable document (not just a static web page). In many cases, you might have to build multiple dashboards for different sets of users (don’t shoehorn everything into the same dashboard).

### Accuracy, Honesty, and Good Design

Because the explanatory power of a good dashboard is why we are building visualizations, it is important to ensure that our explanations are not misleading. In this regard, it is best not to do anything too surprising. Although modern-day visualization programs are chock-full of types of graphs and palettes, it is best to pair any graphic with the idiom for which it is appropriate. For example, some types of graphics are better suited to relational data than others, and some graphics are better suited to categorical data than to numerical data.

Broadly, there are four fundamental types of graphics: relational (illustrating the relationship between pairs of variables), time series (illustrating the change of a variable over time), geographical maps (illustrating the variation of a variable by location), and narratives (to support an argument). Narrative graphics are the ones in magazine spreads, which win major design awards. The other three are more worker-like.

You have likely seen enough graphical representations to realize intuitively that the graph is somehow wrong when you violate an accuracy, honesty, or aesthetic principle,[3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn47) but this section of the book lists a few of the canonical ones. For example, it is advisable to choose line graphs or scatter plots for relational graphics and to ensure that autoscaling of the axes doesn’t portray a misleading story about your data. A good design principle is that your time series graphs should be more horizontal than vertical, and that it is the data lines and not “chart junk” (grid lines, labels, etc.) that ought to dominate your graphics. Maximizing the ratio of data to space and ink is a principle that will stand you in good stead when it comes to geographical data—ensure that the domain is clipped to the region of interest, and go easy on place names and other text labels.

Just as you probably learned to write well by reading good writers, one of the best ways to develop a feel for accurate and compelling graphics is to increase your exposure to good exemplars. _The Economist_ newspaper has a [Graphic Detail blog](https://oreil.ly/AnPgC)[4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn48) that is worth following—they publish a chart, map, or infographic every weekday, and these run the gamut of the fundamental graphics types. [Figure 3-1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#this\_graphic\_from\_the\_economist\_shows\_a) shows a graphic from [the blog published on Nov. 25, 2016](https://oreil.ly/YipcK).

The graphic depicts the increase in the number of coauthors on scientific papers over the past two decades. The graphic itself illustrates several principles of good design. It is a time series, and as you’d expect of this type of graphic, the time is on the horizontal axis and the time-varying quantity (number of authors per article or the number of articles per author) is on the vertical axis. The vertical axis values start out at zero, so that the height of the graphs is an accurate indicator of magnitude. Note how minimal the chart junk is—the axes labels and gridlines are very subtle and the title doesn’t draw attention to itself. The data lines, on the other hand, are what pop out. Note also the effective use of repetition—instead of all the different disciplines (Economics, Engineering, etc.) being on the same graph, each discipline is displayed on its own panel. This serves to reduce clutter and makes the graphs easy to interpret. Each panel has two graphs, one for authors per article and the other for articles per author. The colors remain consistent across the panels for easy comparison, and the placement of the panels also encourages such comparisons. We see, for example, that the increase in number of authors per article is not accompanied by an increase in articles per author in any of the disciplines, except for Physics & Astronomy. Perhaps the physicists and astronomers are gaming the system?

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0301.png" alt="" height="348" width="600"><figcaption></figcaption></figure>

**Figure 3-1. This graphic from The Economist shows an increase in the number of authors of papers in various academic disciplines over time.**

The graphic does, however, subtly mislead viewers who are in a hurry. Take a moment and try to critique the graphic—figure out how a viewer might have been misled. It has to do with the arrangement of the panels. It appears that the creator of the graphic has arranged the panels to provide a pleasing upward trend between the panels, but this upward trend is misleading because there is no relationship between the number of authors per article in Economics in 2016 and the same quantity in Engineering in 1996. This misdirection is concerning because the graph is supposed to support the narrative of an increasing number of authors, but the increase is not from one author to six authors over two decades—the actual increase is much less dramatic (for example, from four to six in Medicine). However, a viewer who only glances at the data might wrongly believe that the increase in the number of authors is depicted by the whole graph and is therefore much more than it really is.

## Loading Data into Cloud SQL

To create dashboards to allow interactive analysis of the data, we will need to store the data in a manner that permits fast random access and aggregations. Because our flight data is tabular, SQL is a natural choice, and if we are going to be using SQL, we should consider whether a relational database meets our needs. Relational databases are a mature technology and remain the tool of choice for many business problems. Relational database technology is well known and comes with a rich ecosystem of interoperable tools. The problem of standardized access to relational databases from high-level programming languages is pretty much solved.

[PostgreSQL](https://www.postgresql.org/) is a very popular open source relational database that is used in production at many enterprises. In addition to its high performance, PostgreSQL is easy to program against—it supports ANSI SQL, geographic information system (GIS) functionality, client libraries in a variety of programming languages, and standard connector technologies such as Open Database Connectivity (ODBC) and Java Database Connectivity (JDBC).

### Create a Google Cloud SQL Instance

Google [Cloud SQL](https://cloud.google.com/sql) offers a managed database service that supports PostgreSQL, MySQL, and SQL Server. Cloud SQL manages backups, patches, updates, and even replication while providing for global availability, automatic failover, and high uptime. For best performance, choose a machine whose RAM is large enough to hold your largest table in memory—as of this writing, available machine types range from a single CPU with less than 4 GB of memory all the way to a 96 CPU machine with 624 GB of memory. Balance this desire for speed with the monthly cost of a machine, of course.

Let’s configure a Cloud SQL instance, create a database table in it, and load the table with the data we ingested into Cloud Storage. You can do all these things on the command line using `gcloud`, but let’s begin by using the [SQL section of Cloud Platform Console](https://oreil.ly/Q4OaT) and select Create Instance. Choose PostgreSQL and then fill out the form as follows (see [Figure 3-2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#creating\_a\_postgresql\_instance\_using\_th)):

* Call the instance “flights.”
* Generate a strong password by clicking on the GENERATE button.
* Choose the default PostgreSQL version.
* Choose the region where your bucket of CSV data exists.
* Choose a single zone instance since we are just trying it out. We won’t take this to production.
* Choose a Standard machine type with 2 vCPU.
* Click Create Instance, accepting all the other defaults.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0302.png" alt="" height="582" width="600"><figcaption></figcaption></figure>

**Figure 3-2. Creating a PostgreSQL instance using the web console.**

**INTERACTING WITH GOOGLE CLOUD PLATFORM**

Instead of filling out the dialog box by hand, we could have used the command-line tool `gcloud` from Cloud Shell (or any other machine that has `gcloud` installed); here’s how to do that:

```
gcloud sql instances create flights \
    --database-version=POSTGRES_13 --cpu=2 --memory=8GiB \
    --region=us-central1 --root-password=somestrongpassword
```

In the rest of the book, I will show you just the `gcloud` commands, but you don’t need to memorize them. You can use `--help` at any time on `gcloud` to get a list of options. For example:

```
gcloud sql instances create --help
```

will give you all the options available to create Cloud SQL instances (the `database-version`, its `zone`, etc.), whereas:

```
gcloud sql instances --help
```

will give you all the ways in which you can work with Cloud SQL instances (`create`, `delete`, `restart`, `export`, etc.).

In general, everything you can do on the command line is doable using the Cloud Platform Console, and vice versa. In fact, both the Cloud Platform Console and the `gcloud` command invoke representational state transfer (REST) API actions. You can invoke the same REST APIs from your programs (the APIs are documented on the Google Cloud Platform website). Here is the REST API call to create an instance from Bash:[5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn49)

```
ACCESS_TOKEN="$(gcloud auth application-default print-access-token)"
curl --header "Authorization: Bearer ${ACCESS_TOKEN}" \
     --header 'Content-Type: application/json' \
     --data '{"name":"flights", "settings": 
             {"database-version":"POSTGRES_13", ...}' \
    https://www.googleapis.com/sql/v1beta4/projects/[PROJECT-ID]/instances \
     -X POST
```

Alternatively, you can use the Cloud Client Library (available for a [variety of programming languages](https://oreil.ly/ZUpit)) to issue the REST API calls. We saw this in [Chapter 2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch02.html#ingesting\_data\_into\_the\_cloud) when we used the google.cloud.storage Python package to interact with Cloud Storage.

In summary, there are three ways to interact with Google Cloud Platform:

1.  The web console:

    ```
    The gcloud command from the command line in Cloud Shell 
    or a machine that has the gcloud SDK installed
    ```
2. Directly invoke the REST API
3.  Google Cloud Client Library (available as of this writing for Go, Java, Node.js, Python, Ruby, PHP, and C#)

    In this book, I use primarily Option 2 (from the shell) and Option 4 (from Python programs).

### Create Table of Data

In order to import data into a Postgres table, we first have to create an empty database and a table with the correct schema.

In the Cloud (web) console, navigate to the databases section of Cloud SQL, select instance “flights,” and create a new database called `bts`. This will be where we load our data.

Next, we have to create a file with the following syntax, to create a column for every field in the CSV file:

```
drop table if exists flights;

CREATE TABLE flights (
  "Year" TEXT,
  "Quarter" TEXT,
  "Month" TEXT,
  "DayofMonth" TEXT,
  "DayOfWeek" TEXT,
  "FlightDate" DATE,
  "Reporting_Airline" TEXT,
  "DOT_ID_Reporting_Airline" TEXT,
  "IATA_CODE_Reporting_Airline" TEXT,
...
```

For your convenience, the file I created is already in the Git repository, so just go to Cloud Shell, change into the _03\_sqlstudio_ directory, and do the following steps:

*   Stage the file into Google Cloud Storage (changing the bucket to one that you own):

    ```
    gsutil cp create_table.sql \
        gs://cloud-training-demos-ml/flights/ch3/create_table.sql
    ```
* In the web console, navigate to the flights instance of Cloud SQL and select IMPORT. In the form, specify the location of _create\_table.sql_ and specify that you want to create a table in the database `bts` (see [Figure 3-3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#creating\_an\_empty\_tabledot)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0303.png" alt="" height="746" width="600"><figcaption></figcaption></figure>

**Figure 3-3. Creating an empty table.**

A few seconds later, the empty table will be created.

We can now load the CSV files into this table. Start by loading the January data by browsing to _201501.csv_ in your bucket and specifying CSV as the format, `bts` as the database, and `flights` as the table (see [Figure 3-4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#populating\_the\_table\_with\_data\_from\_jan)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0304.png" alt="" height="790" width="600"><figcaption></figcaption></figure>

**Figure 3-4. Populating the table with data from January.**

Note that the user interface doesn’t provide a way to skip the first line, so the header will also get loaded as a row in the table. Fortunately, our schema calls all the fields as text, so this doesn’t pose a problem—after loading the data, we can delete the row corresponding to the header. If we have a more realistic schema, we will have to remove the header line before loading the file.

### Interacting with the Database

We can connect to the Cloud SQL instance from Cloud Shell using:[6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn50)

```
gcloud sql connect flights --user=postgres
```

In the prompt that comes up, we connect to the `bts` database:

```
\c bts;
```

Then, we can run a query to obtain the five busiest airports:

```
SELECT "Origin", COUNT(*) AS num_flights 
FROM flights GROUP BY "Origin" 
ORDER BY num_flights DESC 
LIMIT 5;
```

While this is performant because the dataset is relatively small (only January!), as I added more months, the database started to slow down.

Relational databases are particularly well suited to smallish datasets on which we wish to do ad hoc queries that involve searching and that return small subsets of data. For larger datasets, we can tune the performance of a relational database by indexing the columns of interest. Further, because relational databases typically support transactions and guarantee strong consistency, they are an excellent choice for data that will be updated often.

However, a relational database is a poor choice if your data is primarily read-only, if your dataset sizes go into the terabyte range, if you have a need to scan the full table (such as to compute the maximum value of a column), or if your data streams in at high rates. This describes our flight delay use case. So, let’s switch from a relational database to an analytics data warehouse—BigQuery. The analytics data warehouse will allow us to use SQL and is much more capable of dealing with large datasets and ad hoc queries (i.e., doesn’t need the columns to be indexed).

If you are following along with me, delete the Cloud SQL instance. We won’t need it any further in this book.[7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn51)

## Querying Using BigQuery

In [Chapter 2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch02.html#ingesting\_data\_into\_the\_cloud), we loaded the CSV data into BigQuery into a table named `flights_raw` in the dataset `dsongcp`. Let’s explore that dataset a bit—this is not a full exploratory data analysis, which I will do in [Chapter 5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#interactive\_data\_exploration\_with\_verte).

My goal here is to do “just enough” analysis on the data and then quickly pivot to building my first model. Once I have the model, I will be able to build a dashboard to explain that model. The idea is to get a first iteration out in front of users as quickly as possible. Going from ingested data to minimum viable outputs (model, dashboard, etc.) quickly is what agile development in data science looks like.

Teams that wait until they build a fully capable model before incorporating it into decision tools often build the wrong model (i.e., they solve the wrong problem because of misunderstanding how the decision will be used) or choose unviable technology (that is hard to get into production). Avoid these traps by testing your work with real users as quickly as possible!

### Schema Exploration

Navigate to the [BigQuery section](https://oreil.ly/vyCns) of the Google Cloud (web) console and select the `flights_raw` table. On the right side of the window, select Schema (see [Figure 3-5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#the\_schema\_of\_the\_flights\_raw\_table\_tha)). Which fields do you think are relevant to predicting flight arrival delays?

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0305.png" alt="" height="600" width="600"><figcaption></figcaption></figure>

**Figure 3-5. The schema of the `flights_raw` table that we loaded into BigQuery in** [**Chapter 2**](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch02.html#ingesting\_data\_into\_the\_cloud)**.**

Just looking at the schema is not enough. For example, do we really need the Year, Month, DayofMonth, and so on? Isn’t FlightDate enough? It’s best to not have duplicative data—the more columns we have, the more work we have to do to keep analysis consistent.

Similarly, which of the various Airline columns do we need? For the Airline columns, we did read the description on the BTS website in [Chapter 2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch02.html#ingesting\_data\_into\_the\_cloud), and will probably follow their recommendation that `Reporting_Airline` be the one that we use. Still, it’s worth verifying why that is.

To make decisions like this, we can use two features—the Preview tab and the Table Explorer tab (see [Figure 3-6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#preview\_of\_the\_flights\_raw\_table\_that\_w)).

### Using Preview

The best way to quickly look at a BigQuery table is to use the Preview functionality. The Preview is free, whereas doing a `SELECT * FROM` … `LIMIT 10` will incur a querying cost.

Looking at the preview (see [Figure 3-6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#preview\_of\_the\_flights\_raw\_table\_that\_w)), the Year, Month, etc., columns do seem to be redundant. (If you are following along with me, you may see different rows because the Preview just picks whatever is most handy.)

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0306.png" alt="" height="161" width="600"><figcaption></figcaption></figure>

**Figure 3-6. Preview of the `flights_raw` table that we loaded into BigQuery in** [**Chapter 2**](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch02.html#ingesting\_data\_into\_the\_cloud)**.**

Let’s check whether we can resurrect the FlightDate from the other columns and extract the date pieces from the FlightDate. We can do that with SQL:

```
SELECT 
    FORMAT("%s-%02d-%02d",
        Year,
        CAST(Month AS INT64),
        CAST(DayofMonth AS INT64)) AS resurrect,
    FlightDate,
    CAST(EXTRACT(YEAR FROM FlightDate) AS INT64) AS ex_year,
    CAST(EXTRACT(MONTH FROM FlightDate) AS INT64) AS ex_month,
    CAST(EXTRACT(DAY FROM FlightDate) AS INT64) AS ex_day,
FROM dsongcp.flights_raw
LIMIT 5
```

The result appears to bear this out:

| `Row` | `resurrect`  | `FlightDate` | `ex_year` | `ex_month` | `ex_day` |
| ----- | ------------ | ------------ | --------- | ---------- | -------- |
| `1`   | `2015-02-19` | `2015-02-19` | `2015`    | `2`        | `19`     |
| `2`   | `2015-02-20` | `2015-02-20` | `2015`    | `2`        | `20`     |
| `3`   | `2015-02-22` | `2015-02-22` | `2015`    | `2`        | `22`     |
| `4`   | `2015-02-23` | `2015-02-23` | `2015`    | `2`        | `23`     |
| `5`   | `2015-02-25` | `2015-02-25` | `2015`    | `2`        | `25`     |

But we have to be sure. Let’s print out rows where the extracted data from FlightDate is _not_ identical:

<pre><code>WITH data AS (
SELECT 
    FORMAT("%s-%02d-%02d",
        Year,
        CAST(Month AS INT64),
        CAST(DayofMonth AS INT64)) AS resurrect,
    FlightDate,
    CAST(EXTRACT(YEAR FROM FlightDate) AS INT64) AS ex_year,
    CAST(EXTRACT(MONTH FROM FlightDate) AS INT64) AS ex_month,
    CAST(EXTRACT(DAY FROM FlightDate) AS INT64) AS ex_day,
FROM dsongcp.flights_raw
)
SELECT * FROM data
<strong>WHERE resurrect != CAST(FlightDate AS STRING)
</strong></code></pre>

This query returns an empty result set, so we are sure that we can safely keep only the FlightDate column.

### Using Table Explorer

How about the Airline code? Switch to the Table Explorer tab and select the three airline columns as shown in [Figure 3-7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#selecting\_fields\_for\_table\_explorerdot).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0307.png" alt="" height="189" width="600"><figcaption></figcaption></figure>

**Figure 3-7. Selecting fields for Table Explorer.**

BigQuery analyzes the full dataset and shows the unique values in the table, as shown in [Figure 3-8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#distinct\_values\_for\_the\_three\_airline\_f).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0308.png" alt="" height="202" width="600"><figcaption></figcaption></figure>

**Figure 3-8. Distinct values for the three Airline fields.**

It is clear from the Table Explorer that we want to use either the `Reporting_Airline` or the `IATA_CODE_Reporting_Airline`. As before, checking to see if there are rows where these are different indicates that `Reporting_Airline` is sufficient.

### Creating BigQuery View

Based on such analysis on the remaining fields, I came up with the following sets of operations I want to do to the raw data to make it more usable. For example, the Departure Delay should be a floating point number and not a string. The Cancellation Code should be a boolean and not 1.00:

<pre><code><strong>CREATE OR REPLACE VIEW dsongcp.flights AS
</strong>
SELECT
  FlightDate AS FL_DATE,
  Reporting_Airline AS UNIQUE_CARRIER,
  OriginAirportSeqID AS ORIGIN_AIRPORT_SEQ_ID,
  Origin AS ORIGIN,
  DestAirportSeqID AS DEST_AIRPORT_SEQ_ID,
  Dest AS DEST,
  CRSDepTime AS CRS_DEP_TIME,
  DepTime AS DEP_TIME,
<strong>  CAST(DepDelay AS FLOAT64) AS DEP_DELAY,
</strong>  CAST(TaxiOut AS FLOAT64) AS TAXI_OUT,
  WheelsOff AS WHEELS_OFF,
  WheelsOn AS WHEELS_ON,
  CAST(TaxiIn AS FLOAT64) AS TAXI_IN,
  CRSArrTime AS CRS_ARR_TIME,
  ArrTime AS ARR_TIME,
  CAST(ArrDelay AS FLOAT64) AS ARR_DELAY,
<strong>  IF(Cancelled = '1.00', True, False) AS CANCELLED,
</strong>  IF(Diverted = '1.00', True, False) AS DIVERTED,
  DISTANCE
FROM dsongcp.flights_raw;
</code></pre>

In order to avoid repeating these casts in all queries from here on out, I am creating a view that consists of the `SELECT` statement (see the first line in the preceding listing). A view is a virtual table—we can query the view just as if it were a table:

<pre><code>SELECT 
  ORIGIN,
  COUNT(*) AS num_flights
<strong>FROM dsongcp.flights
</strong>GROUP BY ORIGIN
ORDER BY num_flights DESC
LIMIT 5
</code></pre>

Any queries that happen on the view are rewritten by the database engine to happen on the original table—conceptually, a view works as if the SQL corresponding to the view was to be inserted into every query that uses the view.

What if the view includes a `WHERE` clause so that the number of rows is much less? In such cases, it would be far more efficient to export the results into a table and query that table instead:

<pre><code><strong>CREATE OR REPLACE TABLE dsongcp.flights AS
</strong>
SELECT 
</code></pre>

But what if you export the results into a table and then the original table has a new month of data added to it? We’d have to rerun the table creation statement to make the extracted table up-to-date. In the case of a view, we wouldn’t have to do anything special—all new queries would automatically be querying the entire raw table and thus include the new month of data.

Can we have our cake and eat it too? Can we get the “live” nature of a view, but the query efficiency of a table? Yes. It’s called a _materialized view_:

<pre><code><strong>CREATE MATERIALIZED VIEW dsongcp.flights AS
</strong>
SELECT
</code></pre>

The view is materialized into a table and kept up-to-date by BigQuery. While views are free, materialized views carry an extra cost because of the extra storage and compute overhead they involve.

In this book, I’ll use a regular view during development, since it’s easy to come back and add new columns, etc. Later on, once we go to production, it’s quite simple to change it over to a materialized view—none of the client code will need to change.

## Building Our First Model

Intuitively, we feel that if the flight departure is delayed by 15 minutes, it will also tend to arrive 15 minutes late. So, our model could be that we cancel the meeting if the departure delay of the flight is 15 minutes or more. Of course, there is nothing here about the probability (recall that we wanted to cancel if the probability of an arrival delay of 15 minutes was greater than 30%). Still, it will be a quick start and give us something that we can ship now and iterate upon.

### Contingency Table

Suppose that we need to know how often we will be making the right decision if our decision rule is the following:

> If DEP\_DELAY ≥ 15, cancel the meeting; otherwise, go ahead.

There are four possibilities in the _contingency table_ or the _confusion matrix_, which you can see in [Table 3-2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#confusion\_matrix\_for\_the\_decision\_to\_ca).

|                                                          | Arrival delay < 15 minutes | Arrival delay ≥ 15 minutes |
| -------------------------------------------------------- | -------------------------- | -------------------------- |
| We did not cancel meeting (departure delay < 15 min)     | Correct (true positive)    | False positive             |
| <p>We canceled meeting<br>(departure delay ≥ 15 min)</p> | False negative             | Correct (true negative)    |

If we cancel the meeting and it turns out that the flight arrived on time (let’s call that a “positive”), it is clear that we made a wrong decision. It is arbitrary whether we refer to it as a false positive (treating the on-time arrival as a positive event) or as a false negative (treating the late arrival as a negative event). Because the dataset is called “on-time arrivals,” let’s term on-time arrival the positive event. How do we find out how often the decision rule of thresholding the departure delay at 15 minutes will tend to be correct? We can evaluate the first box in the confusion matrix using BigQuery:

```
SELECT 
    COUNT(*) AS true_positives
FROM dsongcp.flights
WHERE dep_delay < 15 AND arr_delay < 15
```

There are 4,430,885 such flights.

To compute all four values in a single statement, move the `WHERE` clause into the `SELECT` itself:

```
SELECT 
    COUNTIF(dep_delay < 15 AND arr_delay < 15) AS true_positives,
    COUNTIF(dep_delay < 15 AND arr_delay >= 15) AS false_positives,
    COUNTIF(dep_delay >= 15 AND arr_delay < 15) AS false_negatives,
    COUNTIF(dep_delay >= 15 AND arr_delay >= 15) AS true_negatives,
    COUNT(*) AS total
FROM dsongcp.flights
WHERE arr_delay IS NOT NULL AND dep_delay IS NOT NULL
```

Each of the `COUNTIF` statements counts the number of rows that match the given criterion, and `COUNT(*)` counts all rows. This way, we get to scan the table just once, and still manage to collect the four numbers that form the confusion matrix:

| `Row` | `true_positives` | `false_positives` | `false_negatives` | `true_negatives` | `total`   |
| ----- | ---------------- | ----------------- | ----------------- | ---------------- | --------- |
| `1`   | `4430885`        | `232701`          | `219684`          | `830738`         | `5714008` |

Recall that these numbers assume that we are making a decision by thresholding the departure delay at 15 minutes. But is that the best threshold?

### Threshold Optimization

Ideally, we want to try out different values of the threshold and pick the one that provides the best results. To do so, we can declare a variable called `THRESH` and use it in the query. This way, there is just one number to change when we want to try out a different threshold:

<pre><code><strong>DECLARE THRESH INT64;
</strong><strong>SET THRESH = 15;
</strong>
SELECT 
<strong>    COUNTIF(dep_delay &#x3C; THRESH AND arr_delay &#x3C; 15) AS true_positives,
</strong>    COUNTIF(dep_delay &#x3C; THRESH AND arr_delay >= 15) AS false_positives,
    COUNTIF(dep_delay >= THRESH AND arr_delay &#x3C; 15) AS false_negatives,
    COUNTIF(dep_delay >= THRESH AND arr_delay >= 15) AS true_negatives,
    COUNT(*) AS total
FROM dsongcp.flights
WHERE arr_delay IS NOT NULL AND dep_delay IS NOT NULL
</code></pre>

Still, I’d rather not run the query several times, once for each threshold. It’s not about the drudgery of it—I could avoid the manual work by using a `for` loop in a script. What I’m objecting to is scanning the table four times. The better way to do this in SQL is to declare an array of possible thresholds and then group by them:

<pre><code>SELECT 
    THRESH,
<strong>    COUNTIF(dep_delay &#x3C; THRESH AND arr_delay &#x3C; 15) AS true_positives,
</strong>    COUNTIF(dep_delay &#x3C; THRESH AND arr_delay >= 15) AS false_positives,
    COUNTIF(dep_delay >= THRESH AND arr_delay &#x3C; 15) AS false_negatives,
    COUNTIF(dep_delay >= THRESH AND arr_delay >= 15) AS true_negatives,
    COUNT(*) AS total
<strong>FROM dsongcp.flights, UNNEST([5, 10, 11, 12, 13, 15, 20]) AS THRESH
</strong>WHERE arr_delay IS NOT NULL AND dep_delay IS NOT NULL
<strong>GROUP BY THRESH
</strong></code></pre>

This way, we get to run a single query, which scans the table just once and still manages to create contingency tables for all the thresholds we want to try. The result consists of the four contingency table values for each of the seven values of the threshold:

| `Row` | `THRESH` | `true_​positives` | `false_​positives` | `false_​negatives` | `true_​negatives` | `total`   |
| ----- | -------- | ----------------- | ------------------ | ------------------ | ----------------- | --------- |
| `1`   | `5`      | `3931979`         | `144669`           | `718590`           | `918770`          | `5714008` |
| `2`   | `10`     | `4242286`         | `184944`           | `408283`           | `878495`          | `5714008` |
| `3`   | `11`     | `4288279`         | `193912`           | `362290`           | `869527`          | `5714008` |
| `4`   | `12`     | `4329146`         | `203068`           | `321423`           | `860371`          | `5714008` |
| `5`   | `13`     | `4366641`         | `212498`           | `283928`           | `850941`          | `5714008` |
| `6`   | `15`     | `4430885`         | `232701`           | `219684`           | `830738`          | `5714008` |
| `7`   | `20`     | `4542475`         | `291791`           | `108094`           | `771648`          | `5714008` |

Learn SQL. You’ll thank me later_._

This is all well and good, but recall that our goal (see [Chapter 1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch01.html#making\_better\_decisions\_based\_on\_data)) is to cancel the client meeting if the probability of arriving 15 minutes late is 30% or more. How close do we get with each of these thresholds?

To know this, we need to compute the fraction of times a decision is wrong. We can do this by calling the preceding result the contingency table, and then computing the necessary ratios:

<pre><code><strong>WITH contingency_table AS (
</strong> SELECT 
   THRESH,
   COUNTIF(dep_delay &#x3C; THRESH AND arr_delay &#x3C; 15) AS true_positives,
   COUNTIF(dep_delay &#x3C; THRESH AND arr_delay >= 15) AS false_positives,
   COUNTIF(dep_delay >= THRESH AND arr_delay &#x3C; 15) AS false_negatives,
   COUNTIF(dep_delay >= THRESH AND arr_delay >= 15) AS true_negatives,
   COUNT(*) AS total
 FROM dsongcp.flights, UNNEST([5, 10, 11, 12, 13, 15, 20]) AS THRESH
 WHERE arr_delay IS NOT NULL AND dep_delay IS NOT NULL
 GROUP BY THRESH
)

SELECT
<strong>   ROUND((true_positives + true_negatives)/total, 2) AS accuracy,
</strong><strong>   ROUND(false_positives/(true_positives+false_positives), 2) AS fpr,
</strong>   ROUND(false_negatives/(false_negatives+true_negatives), 2) AS fnr,
   *
<strong>FROM contingency_table ORDER BY accuracy ASC
</strong></code></pre>

The result now includes the accuracy, false positive rate, and false negative rate:

| `Row` | `accuracy` | `fpr`  | `fnr`      | `THRESH` | `true_​positives` | `false_​positives` | `false_​negatives` | `true_​negatives` | `total`   |
| ----- | ---------- | ------ | ---------- | -------- | ----------------- | ------------------ | ------------------ | ----------------- | --------- |
| `1`   | `0.85`     | `0.04` | `0.44`     | `5`      | `3931979`         | `144669`           | `718590`           | `918770`          | `5714008` |
| `2`   | `0.9`      | `0.04` | **`0.32`** | **`10`** | `4242286`         | `184944`           | `408283`           | `878495`          | `5714008` |
| `3`   | `0.9`      | `0.04` | **`0.29`** | **`11`** | `4288279`         | `193912`           | `362290`           | `869527`          | `5714008` |
| `4`   | `0.91`     | `0.04` | `0.27`     | `12`     | `4329146`         | `203068`           | `321423`           | `860371`          | `5714008` |
| `5`   | `0.91`     | `0.05` | `0.25`     | `13`     | `4366641`         | `212498`           | `283928`           | `850941`          | `5714008` |
| `6`   | `0.92`     | `0.05` | `0.21`     | `15`     | `4430885`         | `232701`           | `219684`           | `830738`          | `5714008` |
| `7`   | `0.93`     | `0.06` | `0.12`     | `20`     | `4542475`         | `291791`           | `108094`           | `771648`          | `5714008` |

We want to cancel the meeting whenever we think the flight will be late. Our decision will not be perfect. There are times that the decision will be wrong. What is our tolerance for error? It’s 30%. This means that:

* Flights should arrive on time (when we cancel the meeting) less than 30% of the time. So, we want the false positive rate to be 0.3 or less.
* Flights should arrive late (when we go ahead with the meeting) less than 30% of the time. So, we want the false negative rate 0.3 or less.

Looking at the preceding contingency table, which of these criteria looks like it could be a problem? That’s right—the false negative rate. The false positive rate, at 0.04 or so, is comfortably within our error tolerance.

If we are going to make the decision based on the departure delay, our choice of departure delay threshold will have to be such that the false negative rate is 0.3.

It is clear from the preceding table that if we want our decision to have a false negative rate of 30%, the departure delay threshold needs to be 10 or 11 minutes (in the dataset, departure delay is an integer, so an intermediate threshold like 10.6 minutes does not make sense). We could choose 11 minutes on the grounds that, at 11 minutes, the FNR is less than 0.3. Or we could choose 10 minutes on the grounds that it’s a nice, round number and our model is not so sophisticated that we can make decisions at a 1 minute precision.[8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn52)

If we choose a threshold of 10 minutes, we will make the correct decision 96% of the time when we don’t cancel the meeting and 68% of the time when we cancel the meeting. Overall, we are correct 90% of the time.

Note that 10 minutes is not the threshold that maximizes the overall accuracy. Had we chosen a threshold of 20 minutes, we’d cancel far fewer meetings (108,000 versus 408,000) and be correct more often overall (93%). However, that would be very conservative. Since it is not our goal to be correct 88% of the times we cancel the meeting—we only want to be correct 70% of the time—10 minutes is the right threshold.

However, we could also consider that if we can increase the threshold to 20 minutes, we would be correct far more often with very little impact on the false negative rate. Until we looked at the data, we didn’t know what was achievable, and it is possible that the original target was set in a fog of uncertainty. It might be worthwhile asking our stakeholders whether they are really wedded to the 30% false negative rate and whether we have leeway to change the trade-offs available to users of our application—a dashboard that shows the impact of a threshold is an excellent way to gauge this. If the stakeholders don’t know, it might be worth doing an A/B test with a focus group, and that’s what we are about to do next.

**IS THIS MACHINE LEARNING?**

What we did here—trying different thresholds—is at the heart of machine learning. Our model is a simple rule that has a single parameter (the departure delay threshold), and we can tune it to maximize some objective measure (here, the desired precision). We can (and will) use more complex models, and we can definitely make our search through the parameter space a lot more systematic, but this process of devising a model and tuning it is the gist of machine learning. We haven’t evaluated the model (we can’t take the 70% we got on the 12 months of 2015 data and claim that to be our model performance—we need an independent dataset to do that), and that is a key step in machine learning. However, we can plausibly claim that we have built a simple machine learning model to provide guidance on whether to cancel a meeting based on historical flight data.

## Building a Dashboard

Even this simple model is enough for us to begin getting feedback from end users. Recall that my gut instinct at the beginning of the previous section was that I needed to use a 15-minute threshold on the departure delay. Analysis of the contingency table, however, indicated that the right threshold to use was 10 minutes. I’m satisfied with this model as a first step, but will our end users be? Let’s go about building a dashboard that explains the model recommendations to end users. Doing so will also help clarify what I mean by explaining a model to end users.

There are a large number of business intelligence and visualization tools available, and many of them connect with data sources like BigQuery and Cloud SQL on Google Cloud Platform. In this chapter, we build dashboards using Data Studio, which is free and comes as part of Google Cloud Platform, but you should be able to do similar things with Tableau, QlikView, Looker, and so on.

**LOOKER OR DATA STUDIO?**

Google Cloud has two business intelligence tools—Looker and Data Studio. Data Studio is free and much more suitable for self-service use. Looker is much more capable and more suitable for enterprise use.

What do I mean by enterprise use? Here are a few examples of things that Looker can do that Data Studio can’t:

ConsistencyIt is possible for one team to define a semantic layer consisting of standard nomenclature for columns and ways of computing key performance metrics. The rest of the organization then builds dashboards starting from the semantic layer rather than from the raw data.MulticloudLooker can access data in BigQuery, Amazon Redshift, Azure SQL Data Warehouse, and Snowflake _simultaneously_ in the same report.Embedded analyticsHave you been to a website where you can see charts and graphs of your activity? This is provided by a lot of B2B applications, such as marketplaces allowing sellers to visualize their own data. Looker allows you to embed analytics in another website.Alerts and updates

Data Studio requires the user to visit the Data Studio web page and refresh the graphics. With Looker, you can push reports on a schedule or whenever an event happens.

That said, in our case, all we want is a self-serve dashboard, and Data Studio fits the bill perfectly.

For those of you with a data science background, I’d like to set expectations here—a dashboard is a way for end users to quickly come to terms with the current state of a system and is not a full-blown, completely customizable, statistical visualization package. Think about the difference between what’s rendered in the dashboard of a car versus what would be rendered in an engineering visualization of the aerodynamics of the car in a wind tunnel—that’s the difference between what we will do in Data Studio versus what we will use Vertex AI Notebooks for in later chapters. Here, the emphasis is on providing information effectively to end users—thus, the key aspects are interactivity and collaboration. With Data Studio, you can share reports similarly to Google Workspace documents; that is, you can give different colleagues viewing or editing rights, and colleagues with whom you have shared a visualization can refresh the charts to view the most current data.

### Getting Started with Data Studio

To work with Data Studio, navigate to [the Data Studio home page](https://oreil.ly/EJq0N). There are two key concepts in Data Studio: reports and data sources. A report is a set of charts and commentary that you create and share with others. The charts in the report are built from data that is retrieved from a data source. The first step, therefore, is to set up a data source. Because our data is in BigQuery, the data source we need to set up is for Data Studio to connect to BigQuery.

On the Data Studio home page, click on the Create button, click the Data Source menu item, and choose the BigQuery button, as illustrated in [Figure 3-9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#choose\_bigquery\_from\_the\_data\_source\_me).[9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn53)

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0309.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 3-9. Choose BigQuery from the Data Source menu item in Data Studio.**

Select your project, the `dsongcp` dataset, and `flights` as the table. Then, click on the Connect button. Recall that `flights` is the view that we have set up with the streamlined set of fields.

A list of fields in the table displays, with Data Studio inferring something about the fields based on a sampling of the data in that table. We’ll come back and correct some of these, but for now, just click Create Report, accepting all the prompts.

### Creating Charts

On the top ribbon, select the scatter chart icon from the “Add a chart” pulldown (see [Figure 3-10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#initial\_chart\_rendered\_by\_data\_studiodo)) and draw a rectangle somewhere in the main window; Data Studio will draw a chart. The data that is rendered is pulled from some rather arbitrary columns.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0310.png" alt="" height="429" width="600"><figcaption></figcaption></figure>

**Figure 3-10. Initial chart rendered by Data Studio.**

Ignoring the Date Range Dimension for now, there are three columns being used: the Dimension is the quantity being plotted; Metric X is along the x-axis; and Metric Y is along the y-axis. Change (if necessary) Dimension to UNIQUE\_CARRIER, Metric X to DEP\_DELAY, Metric Y to ARR\_DELAY, and change the aggregation metric for both Metric X and Metric Y to Average. Ostensibly, this should give us the average departure delay and arrival delay of different carriers. Click the Style tab and add in a linear trendline and show the data labels. [Figure 3-11](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#chart\_after\_changing\_metriccomma\_dimens) depicts the resulting chart.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0311.png" alt="" height="443" width="600"><figcaption></figcaption></figure>

**Figure 3-11. Chart after changing Metric, Dimension, and Style.**

### Adding End-User Controls

So far, our chart is static—there is nothing for end users to interact with. They get to see a pretty picture, but do not get to change anything about our graph. To permit the end user to change something about the graph, we should add _controls_ to our graph.

Let’s give our end users the ability to set a date range. On the top icon ribbon, click the “Date range control” button, as illustrated in [Figure 3-12](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#the\_date\_range\_control\_on\_the\_top\_icon).

On your chart, place the rectangle where you’d like the control to appear. Change the time window to be Fixed and set the Start Date to Jan. 1, 2015, and end date to Dec. 31, 2019.[10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn54) This is how the report will initially appear to users.

In the upper-right corner, change the toggle to switch to the View mode. This is the mode in which users interact with your report. Change the data range to Jan 1, 2015 to May 31, 2015 (see [Figure 3-13](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#the\_chart\_in\_view\_modedot)) and you should see the chart immediately update.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0312.png" alt="" height="384" width="185"><figcaption></figcaption></figure>

**Figure 3-12. The Date range control on the top icon ribbon.**

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0313.png" alt="" height="512" width="600"><figcaption></figcaption></figure>

**Figure 3-13. The chart in View mode.**

Pause a bit here and ask yourself what kind of a model the chart in [Figure 3-13](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#the\_chart\_in\_view\_modedot) would explain. Because there is a line, it strongly hints at a linear model. If we were to recommend meeting cancellations based on this chart, we’d be suggesting, based on the linear trend of arrival delay with departure delay, that departure delays of more than 20 minutes lead to arrival delays of more than 15 minutes. That, of course, was not our model—we did not do linear regression, and certainly not airline by airline. Instead, we picked a departure threshold based on a contingency table over the entire dataset. So, we should not use the preceding graph in our dashboard—it would be a misleading description of our actual model.

### Showing Proportions with a Pie Chart

How would you explain our contingency table–based thresholds to end users in a dashboard? Recall that the choice comes down to the proportion of flights that arrive more than 15 minutes after their scheduled time. That is what our dashboard needs to show.

One of the best ways to show a proportion is to use a pie chart.[11](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn55) Switch back to the Edit mode, and from the pull-down menu, select the Donut Chart button (this is a type of pie chart), and then, on your report, draw a square where you’d like the donut chart to appear (it is probably best to delete the earlier scatter plot from it). As we did earlier, we need to edit the dimensions and metrics to fit what it is that we want to display. Perhaps things will be clearer if you see what the end product ought to look like. [Figure 3-14](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#desired\_end\_result\_is\_a\_chart\_that\_show) gives you a glimpse.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0314.png" alt="" height="386" width="600"><figcaption></figcaption></figure>

**Figure 3-14. Desired end result is a chart that shows the proportion of flights that are late versus on time.**

In this chart, we are displaying the proportion of flights that arrived late versus those that arrived on time. The labeled field ON TIME versus LATE is the Dimension. The number of flights is the metric that will be apportioned between the labels. So, how do you get these values from the BigQuery view?

It is clear that there is no column in the database that indicates the total number of flights. However, Data Studio has a special value Record Count that we can use as the metric, after making sure to change the aggregate from the default Sum to Count.

The “islate” value, though, will have to be computed as a formula. Conceptually, we need to [add a new calculated field](https://oreil.ly/fWZvf) to the data that looks like this:

```
CASE WHEN
(ARR_DELAY < 15)
THEN
"ON TIME"
ELSE
"LATE"
END
```

Click on the current Dimension column and click on “Create Field.” Give the field the name `is_late`, enter the preceding formula, and change the type to Text (see [Figure 3-15](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#how\_to\_set\_up\_the\_is\_late\_definitiondot)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0315.png" alt="" height="397" width="388"><figcaption></figcaption></figure>

**Figure 3-15. How to set up the `is_late` definition.**

The pie chart is now complete and reflects the proportion of flights that are late versus those that are on time. You can switch over to the Style tab if you’d like to change the appearance of the pie chart to be similar to mine.

Because the proportion of flights that end up being delayed is the quantity on which we are trying to make decisions, the pie chart translates quite directly to our use case. However, it doesn’t tell the user what the typical delay would be. To do that, let’s create a bar (column) chart that looks like the one shown in [Figure 3-16](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#typical\_delay\_for\_each\_carrierdot).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0316.png" alt="" height="219" width="478"><figcaption></figcaption></figure>

**Figure 3-16. Typical delay for each carrier.**

Here, the labeled quantity (or Dimension) is the `Carrier`. There are two metrics being displayed: the DEP\_DELAY and ARR\_DELAY, both of which are aggregated to their averages over the dataset. [Figure 3-17](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#how\_to\_set\_the\_bar\_chart\_properties\_t) shows the specifications.

Note the Sort column at the end—it is important to have a reliable sort order in dashboards so that users become accustomed to finding the information they want in a known place. Also, the default is to use different axes for the two variables.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0317.png" alt="" height="800" width="181"><figcaption></figcaption></figure>

**Figure 3-17. How to set the bar chart properties to generate the desired chart.**

Switch over to the Style tab and change this to use a single axis. Finally, Data Studio defaults to 10 bars. In the Style tab, change this to reflect that we expect to have up to 20 unique carriers ([Figure 3-18](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#how\_to\_set\_the\_bar\_chart\_properties\_to)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0318.png" alt="" height="800" width="382"><figcaption></figcaption></figure>

**Figure 3-18. How to set the bar chart properties to generate the desired chart.**

Of course, we can now add in a date control as we did earlier to end up with the report in [Figure 3-19](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#resulting\_dashboard\_consisting\_of\_a\_pie) (“All flights” in the diagram is just a text label that I added).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0319.png" alt="" height="214" width="600"><figcaption></figcaption></figure>

**Figure 3-19. Resulting dashboard consisting of a pie chart and bar chart.**

It appears that, on average, about 80% of flights are on time and that the typical arrival delay varies between airlines but lies in a range of 0 to 15 minutes.

### Explaining a Contingency Table

Even though the dashboard we just created shows users the decision-making criterion (proportion of flights that will be late) and some characteristics of that decision (the typical arrival delay), it doesn’t actually show our model. Recall that our model involved a threshold on the departure delay. We need to show that. [Figure 3-20](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#dashboard\_consisting\_of\_three\_pairs\_of) shows what we want the dashboard to look like.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0320.png" alt="" height="616" width="600"><figcaption></figcaption></figure>

**Figure 3-20. Dashboard consisting of three pairs of pie charts and bar charts along with a date control.**

In other words, we want to show the same two charts, but for the decision thresholds that we considered—departure delays of 10, 15, and 20 minutes or more.

To get there, we need to change our data source. No longer can we populate the chart from the entire table. Instead, we should populate it from a query that pulls only those flights whose departure delay is greater than the relevant threshold. In BigQuery, we can create the views we need and use those views as data sources.[12](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn56) Here’s how:

```
CREATE OR REPLACE VIEW dsongcp.delayed_10 AS
SELECT * FROM dsongcp.flights WHERE dep_delay >= 10;

CREATE OR REPLACE VIEW dsongcp.delayed_15 AS
SELECT * FROM dsongcp.flights WHERE dep_delay >= 15;

CREATE OR REPLACE VIEW dsongcp.delayed_20 AS
SELECT * FROM dsongcp.flights WHERE dep_delay >= 20;
```

Alternatively, we can add a filter on the departure delay and allow the end user to try out different thresholds. However, because we want to explain the decision model, it is better to ensure that the 10-minute threshold is explicitly present in the dashboard.

Looking at the resulting pie chart for a 10-minute threshold ([Figure 3-20](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#dashboard\_consisting\_of\_three\_pairs\_of)), we see that it comes quite close to our target of 30% on-time arrivals. The bar chart for the 10-minute delay explains why the threshold is important. Hint: it is not about the exact numeric value of 10 minutes. It is about what the 10-minute delay is indicative of. Can you decipher what is going on? Still stuck? Look at the y-axis of the three bar charts.

Although the typical departure delay of a flight is only about 5 minutes (see the chart corresponding to all flights that we created earlier), flights that are delayed by more than 10 minutes fall into a separate statistical regime. The typical departure delay of an aircraft that departs more than 10 minutes late is around 50 minutes! A likely explanation is that a flight that is delayed by 10 minutes or more typically has a serious issue that will not be resolved quickly. If you are sitting in an airplane and it is more than 10 minutes late in departing, you might as well cancel your meeting—you are going to be sitting at the gate for a while.[13](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn57)

At this point, we have created a very simple model and created dashboards to explain the model to our end users. Our end users have a visual, intuitive way to see how often our model is correct and how often it is wrong. The model might be quite simple, but the explanation of why the model works is a satisfying one.

There is one teeny, tiny thing missing, though. Context. The dashboard that we have built so far is all about historical data, whereas real dashboards need to be timely. Our dashboard shows aggregates of flights all over the country, but our users will probably care only about the airport from which they are departing and the airport to which they are going. We have a wonderfully informative dashboard, but without such time and location context, few users would care. In [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest), we look at how to build real-time, location-aware dashboards—unfortunately, however, there is a problem with our dataset that prevents us from doing so immediately. So, we’ll spend the first part of [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest) doing some data wrangling.

Before we move on to [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest), let’s expand our discussion of business intelligence beyond dashboards.

## Modern Business Intelligence

Modern business intelligence (BI) is more than dashboards. BI is data science for business users. As such, the trends in BI are toward digitization (there is data on more and more things), democratization (more and more people can get insights from data), and integration (access to sophisticated insights from familiar tools).

Let’s look at these trends.

### Digitization

The [release notes of Data Studio](https://oreil.ly/nFLeM) are a great way to stay up-to-date with product updates and new visualization options. All Google Cloud products have release notes that are published online—take a look at the [release notes for BigQuery](https://oreil.ly/mKo2i), for example.

Once upon a time, release notes were nothing more than pages of text. But now, they are digitized and queryable. As befits a Data Cloud, the release notes for BigQuery (and all other Google Cloud products) are available as a public dataset in [BigQuery](https://oreil.ly/ItWZp). Let’s query it:

```
SELECT 
  product_name,
  DATE_TRUNC(published_at, MONTH) AS month,
  COUNT(*) AS releases
FROM `bigquery-public-data.google_cloud_release_notes.release_notes`
GROUP BY product_name, month
```

The result looks like this:

| `Row` | `product_name` | `month`      | `releases` |
| ----- | -------------- | ------------ | ---------- |
| `1`   | `BigQuery`     | `2012-05-01` | `8`        |
| `2`   | `BigQuery`     | `2012-07-01` | `5`        |
| `3`   | `BigQuery`     | `2012-08-01` | `5`        |
| `4`   | `Dataflow`     | `2017-10-01` | `6`        |
| `5`   | `Dataflow`     | `2019-02-01` | `3`        |

### Natural Language Queries

We can explore the data with Data Studio, of course, but let’s try using natural language queries. At the time of writing, this was still in alpha and one could only query one’s own tables (not a public dataset). So let’s export the preceding query to a table in our own project:

```
CREATE OR REPLACE TABLE dsongcp.monthly_releases AS

SELECT 
  product_name,
  DATE_TRUNC(published_at, MONTH) AS month,
  COUNT(*) AS releases
FROM `bigquery-public-data.google_cloud_release_notes.release_notes`
GROUP BY product_name, month
```

Now, select the option to “Ask Question.” The UI immediately proposes a few questions (see [Figure 3-21](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#some\_of\_the\_questions\_automatically\_gen)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0321.png" alt="" height="361" width="600"><figcaption></figcaption></figure>

**Figure 3-21. Some of the questions automatically generated based on the dataset.**

Let’s pick a different question, though: “top three products by total releases.” The UI generates the following SQL query automatically (see [Figure 3-22](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#natural\_language\_querying\_in\_bigquerydo)):

```
SELECT
  product_name AS product_name,
  (SUM(releases)) AS SUM_releases
FROM
  `ai-analytics-solutions.dsongcp.monthly_releases`
GROUP BY product_name
ORDER BY SUM_releases DESC
LIMIT 3;
```

Isn’t that something? Notice that even though I said “top three products,” the engine was smart enough to pick up the product\_name column. The result, in case you are curious, is:

| `Row` | `product_name`                         | `SUM_releases` |
| ----- | -------------------------------------- | -------------- |
| `1`   | `Google Kubernetes Engine`             | `749`          |
| `2`   | `Dataproc`                             | `625`          |
| `3`   | `App Engine standard environment Java` | `472`          |

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0322.png" alt="" height="663" width="600"><figcaption></figcaption></figure>

**Figure 3-22. Natural language querying in BigQuery.**

### Connected Sheets

Try this. Open up a new Google Sheet (you can do so by visiting _https://sheets.new_) and select Data > Data Connectors > Connect to BigQuery using the menu.[14](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn58)

Connect to the monthly\_releases table in your project. What we now have is a _Connected Sheet_—all operations we carry out in the sheet are actually executed in BigQuery! This allows us to interact with tables and query results that may have millions of rows!

Select “Discover Data Insights” and follow the prompts to create a chart of Google Cloud releases over time (see [Figure 3-23](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#google\_cloud\_releases\_over\_timedot)). The machines are getting smarter, and the pace of change is increasing!

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0323.png" alt="" height="419" width="600"><figcaption></figcaption></figure>

**Figure 3-23. Google Cloud releases over time.**

## Summary

In this chapter, we discussed the importance of bringing the insights of our end users into our data modeling efforts as early as possible. Bringing their insights is possible only if you make it a point to explain your models in context from the get-go.

We tried using Cloud SQL, a transactional, relational database whose management is simplified by virtue of it running on the cloud and being managed by Google Cloud Platform. However, it stopped scaling once we got to millions of flights. Transactional databases are not built for queries that involve scanning the entire table. For such queries, we want to use an analytics data warehouse. Hence, we switched to using BigQuery.

Within BigQuery, we previewed the table, selected a subset of columns, and created a view to make downstream analysis simpler.

The first model that we built was to suggest that our road warriors cancel their immediately scheduled meeting if the departure delay of the flight was more than 10 minutes. At this threshold, flights arrive late (when we go ahead with the meeting) less than 30% of the time. This would enable them to make 70% of their meetings with 15 minutes to spare.

We then built a dashboard in Data Studio to explain the contingency table model. Because our choice of threshold was driven by the proportion of flights that arrived late given a particular threshold, we illustrated the proportion using a pie chart for two different thresholds. We also depicted the average arrival delay given some departure delay—this gives users an intuitive understanding of why we recommend a 10-minute threshold.

Finally, we looked at trends in business intelligence.

## Suggested Resources

An influential three-part series on business intelligence (BI) [by Forrester Analyst Boris Evelson](https://oreil.ly/qEH9y) recommends a “layer-cake” model for modernizing BI. Unfortunately, at the time of writing, it’s behind a $1,495 paywall.[15](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn59) So, perhaps read a [summary of that article in _Forbes_](https://oreil.ly/YfkXm) by Shant Hovsepian. Accordion to Hovsepian, Evelson suggests that organizations modernize their use of BI by:

* Bringing BI to the data, rather that doing BI on extracts of the data (don’t do [“data cubes”](https://oreil.ly/LuE4P))
* Infusing AI such as [natural language into BI](https://oreil.ly/EiKHP)
* Moving BI to the public cloud to take advantage of elasticity and separation of compute and storage

Data Studio is great as a self-service dashboard, but enterprises are often short of SQL experts. Looker provides a self-service business intelligence workflow by interposing a level of indirection, called LookML. This 2021 article by Clay Porter, [“Using BigQuery & Data Studio? You Should Check Out Looker”](https://oreil.ly/bi3Vs), provides an excellent explanation. There is an [online course](https://oreil.ly/2hqDh) if you want to learn how to create dashboards in Looker.

Once you create a dashboard, you might want to embed the graphics within a website. This is called embedded analytics—it can be done through iframes or using Looker’s application programming interface. See the 2020 Google blog post [“A Step-by-Step Guide to Building and Delivering Embedded Analytics”](https://oreil.ly/alZ2L) by Sharon Zhang for more information.

[1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn45-marker) When Amazon built a hiring tool to help select resumes using machine learning, they were able to use explainability to recognize what the model was keying off—apparently, the machine learning model [penalized resumes](https://oreil.ly/wCxIV) that included terms more commonly found in women’s resumes. Amazon was able to catch this error and not use the ML model to actually evaluate candidates. Had explainability not been part of the workflow, many women would have been unfairly not considered for jobs at Amazon.

[2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn46-marker) When I worked on developing machine learning algorithms for weather prediction, nearly every one of the suggestions and feature requests that I received emerged when the person in question was looking at the real-time radar feed. There would be a storm, my colleague would watch it go up on radar, observe that the tracking of the storm was patchy, and let me know what aspect of the storm made it difficult to track. Or, someone would wake up, look at the radar image, and discover that birds leaving to forage from their roost had been wrongly tagged as a mesocyclone. It was all about real-time data. No matter how many times I asked, I never ever got anyone to look at how the algorithms performed on historical data. It was also often about Oklahoma (where our office was) because that’s what my colleagues would concentrate on. Forecasters from around the country would derisively refer to algorithms that had been hypertuned to Oklahoma supercells.

[3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn47-marker) If you are not familiar with design principles, I recommend [_The Visual Display of Quantitative Information_](https://oreil.ly/Fmt0s) by Edward Tufte (Graphics Press).

[4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn48-marker) _The Economist_ is published weekly as an 80-page booklet stapled in the center, and each page is about the size of a letter paper. However, for historical reasons, the company refers to itself as a newspaper rather than a magazine.

[5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn49-marker) I looked up the URL and the format of the message from the [REST API reference documentation](https://oreil.ly/jD7RC). All the other methods of interacting with a service (web console, `gcloud` command line, Cloud Client Library) end up invoking the REST API.

[6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn50-marker) If your organization has set up a security policy to allow access only from authorized networks, you might have to [use a SQL proxy](https://oreil.ly/cdXyL) to connect to the instance. At the time of writing, this is available only in the beta version, so use the following command: **`gcloud beta sql connect flights --user=postgres`**.

[7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn51-marker) Considering that we were using BigQuery and we are going to use BigQuery, what was the point of this detour into Cloud SQL? In many cases, your data will originally be in a relational database management system (RDBMS). If that dataset is small, you can power the dashboard off the RDBMS. Even though BigQuery is the better choice for the flights delay dataset, it won’t always be the best choice for all the projects you will work on in Google Cloud. Part of my philosophy in this book is to “show,” not “tell.” So, the reason for this section was to show this trade-off between RDBMS and a data warehouse. This will be true later on in the book as well. I’ll do a Spark ML model in [Chapter 7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#logistic\_regression\_using\_spark\_ml), but it won’t scale once we add categorical columns—in that sense, that entire chapter is a digression! I’ll stream into Bigtable in [Chapter 11](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#time\_windowed\_features\_for\_real\_time\_ma) and show that the resulting performance improvement is overkill. In summary, the point of the Cloud SQL detour was to explore the possibilities, show what the problems are, and help you develop the intuition for the trade-offs involved.

[8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn52-marker) If this argument surprises you, imagine that we are talking about a threshold of 0.1 versus a threshold of 0.11. It’s the same principle—don’t use thresholds that have a misleading number of significant digits. This is important because I’m going to show the threshold to end users in a dashboard. If I were not showing the threshold, then I could use arbitrary precision. But when showing numbers to end users, keep the number of significant digits in mind.

[9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn53-marker) Graphical user interfaces are often the fastest-changing parts of any software. So, if the user interface has changed from these screenshots by the time this book gets into your hands, please hunt around a bit. There will be some way to add a new data source.

[10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn54-marker) Or whatever the last month that you downloaded is. Just so that you don’t have to wait a long time for the data to be available in your Google Cloud project, the _ingest.sh_ script in [Chapter 2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch02.html#ingesting\_data\_into\_the\_cloud), by default, downloads only 2015 data. Change the YEAR loop in that script to download 2015 to 2019.

[11](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn55-marker) An alternative way to show proportions, especially of a time-varying whole, is a [stacked column chart.](https://oreil.ly/T51MW)

[12](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn56-marker) Data Studio does support a BigQuery query as a data source, but it is preferable to read from a view because views are more reusable.

[13](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn57-marker) Road warriors know this well. Ten minutes in, and they whip out their phones to try to get on a different flight.

[14](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn58-marker) Make sure you are using the same Google account as the one you are using for your Google Cloud project. At the time of writing, this capability is only available to Enterprise Google Workspace accounts. If you are trying this section in a personal Google Account, you likely won’t have the Data Connectors option in the Data menu.

[15](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch03.html#ch01fn59-marker) No, not a typo. It’s not $14.95. It’s $1495.
