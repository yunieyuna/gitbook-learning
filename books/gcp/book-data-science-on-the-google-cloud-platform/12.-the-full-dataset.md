# 12. The Full Dataset

## Chapter 12. The Full Dataset

In Chapters [1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch01.html#making\_better\_decisions\_based\_on\_data)–[11](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#time\_windowed\_features\_for\_real\_time\_ma), we built a system for predicting flight delays so as to provide travelers with guidance on whether they would be likely to make it to their already scheduled meetings. All of the development was carried out on one year of data. In this chapter, I will change the code to process the full dataset.

All of the code snippets in this chapter are available in the folder [_12\_fulldataset_ of the book’s GitHub repository](https://github.com/GoogleCloudPlatform/data-science-on-gcp).

## Four Years of Data

How well the final model performs can be evaluated only on truly independent data. Because we used our “test” data to evaluate different models along the way and do hyperparameter tuning, we cannot use any of the originally ingested data to evaluate the performance of the model.

Fortunately, though, I did _not_ actually use all of the available data. In order to keep the datasets small enough that the Dataflow pipelines and ML training jobs would complete in a couple hours, I have limited all my work so far to 2015. I have not used 2016–2021 data in training, model selection, or hyperparameter tuning.

Let’s fix this. What I am going to do is to train the ML model on data from 2015–2018 and assume that we put the model into production at the end of 2018. How would that model have fared in 2019? If this works well, it gives us the confidence that we can train the ML model on a few years of data and then apply it in real time. That said, you probably realize why I’m not training the model on 2015–2020 and testing on 2021 data—the world of aviation was turned on its head during 2020–2021 by the COVID-19 pandemic.[1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#ch01fn180) We will have to retrain the ML model for a post COVID-19 future.

Even between 2015–2018 and 2019, the environment would have changed; the list of carriers doing business in 2019 is likely to be different from those in 2015. Also, airline schedulers have presumably changed how they schedule flights. The economy would have been different, and this might lead to more full planes (and hence longer boarding times). Still, evaluating on 2019 data is a reasonable thing to do—after all, in the real world, we might have been using our 2015–2018 model and serving it out to our users in 2019. How would our predictions have fared?

In order to evaluate the performance of a model trained on 2015–2018 data in 2019, we need to:

* Create the dataset for training, validation, and testing.
* Train the model on the 2015–2018 dataset.
* Evaluate the model on the 2019 data.

Let’s look at each of these steps.

### Creating Dataset

Getting 2015–2019 data ready involves repeating the steps we carried out for our training dataset except doing so on the full dataset (see the _README.md_ file in [Chapter 12](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#the\_full\_dataset) for details on how to reproduce the steps).

#### Dataset split

In order to achieve this desired data split, I changed the splitting code in _flights\_transforms.py_ to be such that 2019 data is used for test while the rest is split between training (95%) and validation (5%) using `farmhash`:

<pre><code>def get_data_split_2019(fl_date):
    fl_date_str = str(fl_date)
<strong>    if fl_date_str > '2019':
</strong><strong>        data_split = 'TEST'
</strong>    else:
        # Use farm fingerprint just like in BigQuery
        x = np.abs(np.uint64(farmhash.fingerprint64(
                             fl_date_str)).astype('int64') % 100)
        if x &#x3C; 95:
            data_split = 'TRAIN'
        else:
            data_split = 'VALIDATE'
    return data_split
</code></pre>

#### Shuffling data

In [Chapter 10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#getting\_ready\_for\_mlops\_with\_vertex\_ai), when I wrote the TensorFlow code to read in the data, I shuffled the data within an in-memory buffer that is 10 times the batch size:

```
if mode == tf.estimator.ModeKeys.TRAIN:
  dataset = dataset.shuffle(batch_size*10)
  dataset = dataset.repeat()
```

I explained this as being needed for distributed training so that different workers don’t always see the same slice of the data. Another reason that we wish to shuffle the data is that it could be clumped based on the way we retrieve and process the records before writing them out. For example, our Beam pipeline processes the data in hourly time windows at each airport. So, successive records are likely to all be from the same airport and the same time. Of course, the file will not be perfectly sorted, just clumped. But such clumping can cause the model to get stuck in a local optimum. So, as our dataset size grows, in-memory shuffling will no longer be sufficient to ensure that a batch remains representative of the overall dataset instead of being from the same time or same airport.

So, let’s add a reshuffle operation to _create\_traindata.py_:

```
features = (
            features
            | 'into_global' >> beam.WindowInto(
                                 beam.window.GlobalWindows())
            | 'shuffle' >> beam.util.Reshuffle()
)
```

Note that I have been careful to remove the time windowing by putting all the elements into the global window. Had I not done that, the reshuffle would have been only within each time window, and the training data would have been clumped by time. With this change, the data will get reshuffled globally before it is written out.

#### Need for continuous training

When I ran the pipeline, though, I discovered that the time correction code failed because the _airports.csv_ that the script was using was incomplete for the new year. New airports had been built, and some airport locations had changed, so there were several unique airport codes that were not present in the original airports file. We could go out and get the latest _airports.csv_, but this doesn’t address a bigger problem. Recall that we used the airport location information in our machine learning model by creating embeddings of origin and destination airports—such features will not work properly for new airports. In the real world, especially when we work with humans and human artifacts (customers, airports, products, etc.), it is unlikely that we will be able to train a model once and keep using it from then on. Instead, models will have to be continually trained with up-to-date data. Continuous training is a necessary ingredient in machine learning. Hence, the emphasis on easy operationalization, versioning, and pipelines in Vertex AI—this is a workflow that you will have to automate.

**CONTINUOUS TRAINING**

What I am doing in this chapter is retraining the model from scratch. I am not using the 2015 model in any way. I am taking the model code and training it on 2015–2018 data.

An alternative approach is called fine-tuning. When we trained our model, we wrote out checkpoints—to train the model with new data, we would start from such a checkpointed model, load it in, and then run a few batches through it, thus adjusting the weights. This allows the model to slowly adapt to new data without losing its accumulated knowledge. It is also possible to replace nodes from a checkpointed graph, or to freeze certain layers and train only others (such as perhaps the embedding layers for the airports). If you’ve done operations such as learning rate decay, you’d continue training at the lower learning rate and not restart training with the highest learning rate. Vertex AI and TensorFlow are designed to accommodate this.

Fine-tuning is faster than retraining. So, if you are doing continuous training, it is much more common to do fine-tuning than retraining. However, fine-tuning will not provide the same overall benefit as from-scratch retraining. So, the typical approach is to fine-tune the model on a daily or weekly basis with new data, but once the amount of new data has become a significant fraction of the original data (say 5%), go back and train completely from scratch.

For now, though, I will simply change the code that looks up airport codes to deal gracefully with the error and impute a reasonable value for the machine learning code:

```
def airport_timezone(airport_id, airport_timezones_dict):
       if airport_id in airport_timezones_dict:
          return airport_timezones_dict[airport_id]
       else:
          return ('37.41', '-92.35', u'America/Chicago')
```

If the airport is not found in the dictionary, the airport location is assumed to be located at (37.41, –92.35), which corresponds to the population center of the United States in central Missouri [as estimated by the US Census Bureau](https://oreil.ly/ShGzD).

#### More powerful machines

Whereas the 2015 data consisted of about 5 million flights, the 2015–2019 dataset consists of 30 million flights. Preparing the data doesn’t simply take six times longer on the same set of machines. One of the key steps in the pipeline involves time windowing of grouped elements. Because this is a batched pipeline, Dataflow will need to sort the 30 million flights in order to do time windowing. We can no longer use the default n1-standard-1 machines that we used for the 2015 data. In order for _create\_traindata.py_ to work, I need more workers each with more memory, more compute, and more storage:

```
--worker_machine_type=m1-ultramem-40 --disk_size_gb=500
```

The M1 class of machines on Google Cloud have higher memory. I’m asking for such a high-memory machine with 40 vCPUs and a persistent disk of 500 GB to store temporary data.

Even with this increased computational power, creating the training dataset seems to take forever—when I ran it, the pipeline got bottlenecked by the ability of the machines to handle the grouped time window (see [Figure 12-1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#the\_pipeline\_to\_create\_the\_full\_datase)). Note that the `by_airport` transformation that converts the flight events into a tuple `(origin, flight)` has completed and has output 30 million tuples. On the other hand, the `group_by_airport` transform is only 79% complete. It is emitting 2,500 elements per second to subsequent stages and has processed 12.5 million airport time-windows so far.

Since the dataset has 30 million records, and we are processing only 2,500 records per second, it will take 12,000 seconds or 3.5 hours to get through this bottleneck. The stage after that computes statistics on each group (time-window at an airport), and that one is processing 800 groups/second. Since there are about 105,000 5-minute intervals in a year and we have 4 years of data and 300 airports, there are 126 million of these groups. At 800 elements/second, it will take 44 hours or around 2 days to finish that step.

In reality, it takes less time because not all airport-hour combinations have flights. For example, many airports do not have flights landing or taking off between midnight and 5:30 a.m. local time. Smaller airports will not have data for many hours. I did run the pipeline to completion, and it took 26 hours, indicating that the true number of groups is about half my estimate.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1201.png" alt="" height="702" width="600"><figcaption></figcaption></figure>

**Figure 12-1. The pipeline to create the full dataset is bottlenecked by the `group_by_airport` operation, which is able to process only 2,500 elements/second.**

Is there a way to cut down on the time and resources needed? One way would be to compromise on accuracy and process each of the years separately.[2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#ch01fn181) This would require less powerful machines and allow the pipeline to finish faster.

Since this is so expensive in terms of time and resources, I have placed the full dataset in the data-science-on-gcp bucket. You can copy the data to your bucket instead of running the Dataflow pipeline:

```
gsutil cp \
 gs://data-science-on-gcpedition2/ch12_fulldataset/all-00000-of-00001.csv \
 gs://BUCKET/ch11/data/all-00000-of-00001.csv
```

### Training Model

Once the training dataset is created, we have to choose the model to train. Should we use the AutoML model that got an RMSE of 0.198 or our wide-and-deep model with location-based feature crosses that achieved an RMSE of 0.195? There are good arguments for both.

AutoML uses a number of sophisticated models such as [neural architecture search](https://oreil.ly/eJE7F), which builds deep learning networks one layer at a time, and TabNet, which is based on a sophisticated approach called sequential attention.[3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#idm46519167777040) These types of models work better the more data you have. Just because we beat AutoML on a dataset of 5 million flights doesn’t mean that we will get to beat it using 30 million flights. On the other hand, our location-based feature cross and embeddings will improve in quality if we train them with more data. So, it is conceivable that the custom model will continue to be better than AutoML. There is no way to know. We’d have to try both approaches.

However, there is a key reason I want to use AutoML. Recall that we asked AutoML to write out evaluation data to BigQuery. That capability will come in very handy for _sliced evaluation_—for example, we can easily analyze whether our model performs better on American Airlines flights than on Southwest Airlines. While I could add the necessary code (to write out evaluation data) to our custom model, that’s a lot of work.[4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#ch01fn182) Let’s go with AutoML.

Training took about 5 hours. The feature importance graph on the full dataset was similar to the one obtained from the AutoML model trained on 2015 data (see [Figure 12-2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#the\_five\_most\_important\_features\_in\_th)). The five most important features in 2015—`dep_delay`, `taxi_out`, `origin`, `dest`, and `carrier`—remain the five most important in 2015–2018, although the order of the fourth and fifth features are switched around.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1202.png" alt="" height="257" width="600"><figcaption></figcaption></figure>

**Figure 12-2. The five most important features in the AutoML model trained on 2015 data are the five most important on 2015–2018 data as well. The first three are identical while features 4 and 5 are switched around.**

The precision and recall curves (see [Figure 12-3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#the\_ml\_model\_trained\_and\_evaluated\_on)) seem quite similar.

So far, we have been comparing RMSE (which compares the performance at all thresholds, not just 0.7) and so, we need to evaluate the 2015–2018 model the same way. Let’s do that next.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1203.png" alt="" height="529" width="600"><figcaption></figcaption></figure>

**Figure 12-3. The ML model trained and evaluated on subsets of 2015 data (top) seems to have similar performance to the ML model trained on 2015–2018 data and evaluated on 2019 data.**

### Evaluation

We can dig deeper into the performance characteristics using Vertex Workbench (see _evaluations.ipynb_).

#### RMSE

We can start with the query to compute the RMSE:

```
%%bigquery
SELECT  
  SQRT(SUM(
      (CAST(ontime AS FLOAT64) - predicted_ontime.scores[OFFSET(0)])*
      (CAST(ontime AS FLOAT64) - predicted_ontime.scores[OFFSET(0)])
      )/COUNT(*)) AS rmse
FROM dsongcp.ch10_automl_evaluated
```

This results in an RMSE of 0.1998. Rounding to three decimal places, this is 0.2, which is slightly worse when compared to the 0.198 that we got from training and evaluating on subsets of 2015 data. As expected, performance does drop a bit when we evaluate on a dataset from a completely different time period.

#### Confusion matrix

The difference between this RMSE calculation and the evaluation shown in [Figure 12-3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#the\_ml\_model\_trained\_and\_evaluated\_on) is that the RMSE is over all thresholds, whereas the precision and recall are for a single threshold. Let’s compute the confusion matrix at a specific threshold:

```
DECLARE thresh FLOAT64;
SET thresh = 0.7;

SELECT 
  *, 
  ROUND(num_1_as_1 / (num_1_as_1 + num_1_as_0), 2) AS frac_1_as_1,
  ROUND(num_1_as_0 / (num_1_as_1 + num_1_as_0), 2) AS frac_1_as_0,
  ROUND(num_0_as_1 / (num_0_as_1 + num_0_as_0), 2) AS frac_0_as_1,
  ROUND(num_0_as_0 / (num_0_as_1 + num_0_as_0), 2) AS frac_0_as_0
FROM (
    SELECT  
      thresh,
      COUNTIF(CAST(ontime AS FLOAT64) > 0.5 AND 
      predicted_ontime.scores[OFFSET(0)] > thresh) AS num_1_as_1,
      COUNTIF(CAST(ontime AS FLOAT64) > 0.5 AND 
      predicted_ontime.scores[OFFSET(0)] <= thresh) AS num_1_as_0,
      COUNTIF(CAST(ontime AS FLOAT64) <= 0.5 AND 
      predicted_ontime.scores[OFFSET(0)] > thresh) AS num_0_as_1,
      COUNTIF(CAST(ontime AS FLOAT64) <= 0.5 AND 
      predicted_ontime.scores[OFFSET(0)] <= thresh) AS num_0_as_0
    FROM dsongcp.ch10_automl_evaluated
)
```

The result shows that 4% of on-time flights are misclassified as being late, and 13% of late flights are misclassified as being on time:

| `Row` | `thresh` | `num_1_​as_1` | `num_1_​as_0` | `num_0_​as_1` | `num_0_​as_0` | `frac_1_​as_1` | `frac_1_​as_0` | `frac_0_​as_1` | `frac_0_​as_0` |   |
| ----- | -------- | ------------- | ------------- | ------------- | ------------- | -------------- | -------------- | -------------- | -------------- | - |
| `1`   | `0.7`    | `5633570`     | `245409`      | `184545`      | `1204708`     | `0.96`         | **`0.04`**     | **`0.13`**     | `0.87`         |   |

#### Impact of threshold

We can expand this to multiple thresholds using an array in SQL:

<pre><code>WITH counts AS (
    SELECT
<strong>      thresh,  
</strong>      COUNTIF(CAST(ontime AS FLOAT64) > 0.5 AND 
      predicted_ontime.scores[OFFSET(0)] > thresh) AS num_1_as_1,
      COUNTIF(CAST(ontime AS FLOAT64) > 0.5 AND 
      predicted_ontime.scores[OFFSET(0)] &#x3C;= thresh) AS num_1_as_0,
      COUNTIF(CAST(ontime AS FLOAT64) &#x3C;= 0.5 AND 
      predicted_ontime.scores[OFFSET(0)] > thresh) AS num_0_as_1,
      COUNTIF(CAST(ontime AS FLOAT64) &#x3C;= 0.5 AND 
      predicted_ontime.scores[OFFSET(0)] &#x3C;= thresh) AS num_0_as_0
<strong>    FROM UNNEST([0.5, 0.7, 0.8]) AS thresh, dsongcp.ch10_automl_evaluated
</strong>    GROUP BY thresh
)

SELECT 
  *, 
  ROUND(num_1_as_1 / (num_1_as_1 + num_1_as_0), 2) AS frac_1_as_1,
  ROUND(num_1_as_0 / (num_1_as_1 + num_1_as_0), 2) AS frac_1_as_0,
  ROUND(num_0_as_1 / (num_0_as_1 + num_0_as_0), 2) AS frac_0_as_1,
  ROUND(num_0_as_0 / (num_0_as_1 + num_0_as_0), 2) AS frac_0_as_0
FROM counts
ORDER BY thresh ASC
</code></pre>

which returns:

| `Row` | `thresh` | `num_1_​as_1` | `num_1_​as_0` | `num_0_​as_1` | `num_0_​as_0` | `frac_1_​as_1` | `frac_1_​as_0` | `frac_0_​as_1` | `frac_0_​as_0` |
| ----- | -------- | ------------- | ------------- | ------------- | ------------- | -------------- | -------------- | -------------- | -------------- |
| `1`   | `0.5`    | `5763136`     | `115843`      | `258138`      | `1131115`     | **`0.98`**     | `0.02`         | `0.19`         | **`0.81`**     |
| `2`   | `0.7`    | `5633570`     | `245409`      | `184545`      | `1204708`     | **`0.96`**     | `0.04`         | `0.13`         | **`0.87`**     |
| `3`   | `0.8`    | `5498807`     | `380172`      | `146200`      | `1243053`     | **`0.94`**     | `0.06`         | `0.11`         | **`0.89`**     |

As we’d expect, the fraction of on-time flights that we get correct decreases the higher we make our threshold. Conversely, the fraction of late flights that we get correct increases.

#### Impact of a feature

We can also check whether what the model understood about the data is reasonable. For example, the relation between model prediction and departure delay can be obtained using:

```
SELECT
  ROUND(predicted_ontime.scores[OFFSET(0)], 2) AS prob_ontime,
  AVG(CAST(dep_delay AS FLOAT64)) AS dep_delay,
FROM dsongcp.ch10_automl_evaluated
GROUP BY prob_ontime
ORDER BY prob_ontime ASC
```

The query pulls out the average departure delay associated with each predicted probability. For example, what is the average departure delay associated with model predictions of 0.8? We can plot the converse graph as well:

```
SELECT
  ROUND(predicted_ontime.scores[OFFSET(0)], 2) AS prob_ontime,
  AVG(CAST(dep_delay AS FLOAT64)) AS dep_delay,
FROM dsongcp.ch10_automl_evaluated
GROUP BY prob_ontime
ORDER BY prob_ontime ASC
```

Both graphs, when plotted, are eminently smooth and reasonable (see [Figure 12-4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#the\_average\_departure\_delay\_associated)). At higher on-time probabilities, we see lower departure delays. And at lower departure delays, we see higher probabilities. This is what we would expect.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1204.png" alt="" height="199" width="600"><figcaption></figcaption></figure>

**Figure 12-4. The average departure delay associated with probabilities predicted by the model (left) and the average prediction for specific departure delays (right).**

#### Analyzing errors

We can also analyze the difference in the relationship of departure delay to errors in the model:

<pre><code>WITH preds AS (
  SELECT
    CAST(ontime AS FLOAT64) AS ontime,
    ROUND(predicted_ontime.scores[OFFSET(0)], 2) AS prob_ontime,
    CAST(dep_delay AS FLOAT64) AS var,
  FROM dsongcp.ch10_automl_evaluated
)

SELECT 
  prob_ontime, 
<strong>  AVG(IF((ontime > 0.5 and prob_ontime &#x3C;= 0.5) or 
</strong><strong>                 (ontime &#x3C;= 0.5 and prob_ontime > 0.5), var, NULL)) AS wrong,
</strong><strong>  AVG(IF((ontime > 0.5 and prob_ontime > 0.5) or 
</strong><strong>                 (ontime &#x3C;= 0.5 and prob_ontime &#x3C;= 0.5), var, NULL)) AS correct
</strong><strong>FROM preds
</strong>GROUP BY prob_ontime
ORDER BY prob_ontime
</code></pre>

Plotting this shows that our model has a very similar dependency on departure delay (see [Figure 12-5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#the\_model\_makes\_more\_errors\_when\_the\_t)). This makes sense because departure delay is the most important feature. When we look at the next best feature (taxi-out time), differences start to show up. It’s clear that the model makes more errors when the taxi-out time is small but the flight nevertheless arrives late.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1205.png" alt="" height="203" width="600"><figcaption></figcaption></figure>

**Figure 12-5. The model makes more errors when the taxi-out time is lower than usual but the flight nevertheless arrives late.**

#### Categorical features

Analyzing the impact of categorical features is more difficult—there are more than 300 airports, and it’s difficult to make sense of so many different values.

To understand whether the model learned the difference between airports, we can examine the model behavior in terms of the probability that it predicts for a given departure delay at two different airports: New York’s JFK airport and Seattle’s SEA airport (see [Figure 12-6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#departure\_delays\_between\_onezero\_and\_f)). A 25-minute delay at Seattle is associated with a lower on-time arrival probability than the same delay at JFK. The model discounts long departure delays at New York—these are common enough that airline schedulers take them into account when publishing scheduled arrival times.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1206.png" alt="" height="271" width="394"><figcaption></figcaption></figure>

**Figure 12-6. Departure delays between 10 and 50 minutes are associated with lower probabilities predicted by the model in Seattle versus New York’s John F. Kennedy airport.**

Because there are only a handful of carriers, we can plot the probability versus departure delay relationship of all the carriers. As you can see in [Figure 12-7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#the\_model\_outputs\_different\_probabilit), at a departure delay of 20 minutes, the model predicts a lower on-time arrival probability for Alaska Airlines (AS) than for Delta Airlines (DL).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1207.png" alt="" height="585" width="600"><figcaption></figcaption></figure>

**Figure 12-7. The model outputs different probabilities when different carriers encounter the same departure delay.** [**See it in color online**](https://oreil.ly/dsgcp\_12-7)**.**

Alaska Airlines operates mostly on the West Coast of the United States and encounters significantly fewer weather-related delays than Delta Airlines. Therefore, it makes sense that a 20-minute departure delay on Alaska Airlines is more significant than the same delay on Delta Airlines.

It appears that our 2015–2018 model would have performed quite well in 2019. 2020, with its COVID-19 pandemic, is of course a completely separate story. Hopefully, we would have been continuously evaluating the predictions, caught the deterioration of the model once many flights started to get canceled, and taken our model out of production.

## Summary

In this chapter, we looked at how to train the model on the full dataset and how to evaluate model performance. Having now built an end-to-end system, work moves on to continually improving it and constantly refreshing it with data.

## Suggested Resources

When developing and evaluating ML models, keep [responsible AI principles and practices](https://oreil.ly/0JN81) in mind.

[1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#ch01fn180-marker) I suppose I could have picked a different dataset for the second edition. But most real-world datasets that are dependent in some way on human behavior exhibit dramatic changes before and after March 2020.

[2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#ch01fn181-marker) This is a compromise because the time averages at the beginning of the year will be wrong—flights from December 31 will not be available to compute the time average on January 1. Still, this affects only a minuscule number of flights and might be a reasonable compromise to make.

[3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#idm46519167777040-marker) Sercan Ö. Arik, Tomas Pfister, “TabNet: Attentive Interpretable Tabular Learning,” Proceedings of the AAAI Conference on Artificial Intelligence, 35 no. 8 (May 2021): 6679-6687. https://arxiv.org/pdf/1908.07442.pdf.

[4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch12.html#ch01fn182-marker) Can you tell that I’m ready to finish writing the book?
