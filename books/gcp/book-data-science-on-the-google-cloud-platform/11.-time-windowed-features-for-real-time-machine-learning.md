# 11. Time-Windowed Features For Real-Time Machine Learning

## Chapter 11. Time-Windowed Features for Real-Time Machine Learning

In [Chapter 8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch08.html#machine\_learning\_with\_bigquery\_ml), we briefly explored incorporating time-windowed features, such as the moving average of taxi-out delay at the originating airport, as an input to the model. We found that the time-windowed features reduced the model error. However, it was unclear how clients (who know only about the flight they are on) would be able to provide the correct value. Because of that, we decided to drop the time-windowed features. In this chapter, we will address that shortcoming by implementing a real-time, streaming machine learning pipeline that uses Cloud Dataflow and Vertex AI.

All of the code snippets in this chapter are available in the folder [_11\_realtime_ of the GitHub repository](https://github.com/GoogleCloudPlatform/data-science-on-gcp). See the _README.md_ file in that directory for instructions on how to do the steps described in this chapter.

## Time Averages

What time-windowed aggregate features did we want to use, but couldn’t? Flight arrival times are scheduled based on the average taxi-out time at the departure airport at that specific hour. The machine learning model will learn this average quite easily because we are showing the entire dataset and telling the ML model the name of the origin airport. For example, at peak hours in New York’s JFK airport, taxi-out times on the order of an hour are quite common, so airlines take that into account when publishing their flight schedules. It is only when the taxi-out time exceeds the average that we ought to be worried. Such a global average is typically not a feature that we need to incorporate into the model (although it is not harmful if we do).

On the other hand, there are time averages that need to be computed over recent flights. For example, we have an intuition that the average departure and taxi-out delays being experienced at the origin airport will have an impact on whether we are likely to arrive on time. This is even if the flight we are on happens to leave on time. Lots of flights from an airport experiencing delays are typically associated with runway closures due to weather or other reasons. This leads to a congested airspace, and so subsequent flights will also be affected both because the weather delays might persist and because the number of runways might be limited. Unlike the global average taxi-out time, a recent average of departure delay is something that needs to be computed in real time. On historical data, we’d compute it over the hour previous to the departure time of the aircraft. In real time, this computation would be carried out over streaming data.

### Apache Beam and Cloud Dataflow

We will solve the issue of augmenting the dataset with time-windowed aggregate features using Apache Beam.

#### Why Apache Beam?

Apache Beam allows us to use the same code for both batch and stream processing—for example, to compute aggregate features on historical data, and then to compute the same aggregate features in real time at prediction time (see [Figure 11-1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#the\_same\_transform\_code\_is\_used\_to\_tra)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1101.png" alt="" height="332" width="600"><figcaption></figcaption></figure>

**Figure 11-1. The same transform code is used to transform BigQuery rows in the historical data and Pub/Sub events in real time into the features used by the ML model.**

#### Why Dataflow?

Cloud Dataflow is a fully managed service for executing data processing pipelines written using Apache Beam. What does _fully managed_ mean? Think BigQuery instead of Cloud SQL. Both allow for SQL queries on the data, but where Cloud SQL was simply a hosted version of MySQL on a cloud virtual machine, BigQuery was totally serverless. This allowed for dramatically larger-scale, instantly available SQL processing with BigQuery.[1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn172) Cloud Dataflow provides a similar, serverless, autoscaling service for programmatic data pipelines.

With Cloud Dataflow, unlike with Cloud Dataproc, we don’t need to spin up a cluster in order to carry out data processing. Instead, we simply submit the code and it will get executed and autoscaled to as many machines as needed to accomplish the task effectively. We will get charged for the amount of computational resources that our job involves. Why am I using Cloud Dataflow rather than serverless Spark? Serverless Spark now gives us many of the advantages that only Dataflow used to provide. So, for the majority of use cases, use either serverless Spark or Beam-on-Dataflow depending on which API (Spark or Beam) you are more familiar with. However, in this chapter, we are doing real-time computations and because Apache Beam was designed from the ground up with streaming concepts (Beam stands for _B_atch and Str_eam_), Beam is the better choice.

When the model is invoked, the client will know the departure delay of the aircraft that they are on, but how will they know the average departure delay at their airport over the past hour? So, the serving system needs to be routinely computing the average departure and taxi-out delays at all airports. That average needs to be added to the data about the individual flight before the model is asked to make predictions. This is accomplished as shown in [Figure 11-1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#the\_same\_transform\_code\_is\_used\_to\_tra) by invoking the same Apache Beam transform code in both the historical and real-time Dataflow pipelines. The averages can then be used as features in the model.

#### Starting points

See [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest) for a gentle introduction to Apache Beam. In [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest), I used Beam Python to move data from Pub/Sub to Cloud Storage (_transform/df07.py_) and compute real-time averages (_realtime/avg03.py_) that were used to drive a dashboard. Those two files are what I’ll use as a starting point for developing the Beam pipeline. In that chapter, I skimmed through the streaming code and focused more on visualization concepts. In this chapter, I will remedy that by going through the mechanics of developing a streaming pipeline in more detail.

Similarly, see [Chapter 9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch09.html#machine\_learning\_with\_tensorflow\_in\_ver) for a gentle introduction to TensorFlow and Vertex AI. In [Chapter 9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch09.html#machine\_learning\_with\_tensorflow\_in\_ver), I developed a wide-and-deep model in Vertex AI code in a Workbench notebook and then exported the code into standalone Python files (see _model.py_ and _train\_on\_vertexai.py_ in [Chapter 9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch09.html#machine\_learning\_with\_tensorflow\_in\_ver) of the code repository) for operationalization. In this chapter, we’ll continue our ML modeling code starting from those two files.

Although I have been recommending that you edit the Python files and do development in Cloud Shell, you might prefer to develop in a Python IDE on your local laptop instead. I use PyCharm on Mac as my Python IDE. To follow along with me:

* Install [PyCharm](https://oreil.ly/4d1VR) using its installer.
* In PyCharm, create a [new virtual environment](https://oreil.ly/rvCd4), selecting Python3 as your interpreter.
* In the virtual environment _setup.py_, install the following packages:
  * `tensorflow`
  * `apache-beam[gcp]`
  * `farmhash`
  * `google-cloud-aiplatform`
  * `cloudml-hypertune`

Now you will be able to develop and run the Beam and TensorFlow programs from your laptop.

### Reading and Writing

The Beam pipeline code in _transform/df07.py_ in [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest) gives us the boilerplate for a Beam pipeline that will run in Dataflow (see _create\_traindata.py_):

```
argv = [
   '--project={0}'.format(project),
   '--job_name=ch11traindata',
   '--save_main_session',         
   '--staging_location=gs://{0}/flights/staging/'.format(bucket),
   '--temp_location=gs://{0}/flights/temp/'.format(bucket),
   '--setup_file=./setup.py',
   '--autoscaling_algorithm=THROUGHPUT_BASED',
   '--max_num_workers=20',
   '--region={}'.format(region),
   '--runner=DataflowRunner'
]


with beam.Pipeline(argv=argv) as pipeline:
   ...
```

#### Reading from BigQuery

This pipeline will need to read events from the BigQuery table `flights_tzcorr` that we wrote out in [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest) and write features to a training CSV file in Cloud Storage:

```
input_table = 'dsongcp.flights_tzcorr'
flights_output = 'gs://{}/ch11/data/'.format(bucket)
events = ( 
  pipeline
  | 'read_input' >> beam.io.ReadFromBigQuery(table=input_table)
)

(events
  | 'to_string' >> beam.Map(
                  lambda f: ','.join([str(x) for x in f.values()]))
  | 'to_gcs' >> beam.io.textio.WriteToText(
          os.path.join(flights_output, 'all'),                                              
                       file_name_suffix='.csv')
)
```

This is good, but running things on Dataflow and waiting minutes for the workers to spin up and the pipeline to start is no way to develop. I need a smaller local file so that I can develop the transformation code piecemeal. To run locally, I’ll need to use `Direc⁠t​Runner` instead of `DataflowRunner`:

```
    if input == 'local':
        argv = [
            '--runner=DirectRunner'
        ]
        
```

#### Local JSON input

In order to have an input file in the right format, let’s sample the BigQuery table to a local file:

<pre><code><strong>bq query --nouse_legacy_sql --format=json \
</strong><strong>    "SELECT * FROM dsongcp.flights_tzcorr WHERE DEP_TIME BETWEEN 
</strong>    '2015-03-10T10:00:00' AND '2015-03-10T14:00:00' " \
<strong>    | sed 's/\[//g' | sed 's/\]//g' | sed s'/\},/\}\n/g' \
</strong>    > alldata_sample.json
</code></pre>

Note that I make sure to extract the data as JSON and remove the extra array brackets because I want a new-line delimited JSON file. This is the file format that we will get from Pub/Sub in the real-time code—we will get back a JSON string corresponding to a single message. Also, rather conveniently, a BigQuery row shows up as a Python dict, which is almost (but not quite) the same format as the JSON string corresponding to what is now in each line of the input file.

Then, we can change the code to read from the sampled file and write to a local file:

<pre><code>...

        if input == 'local':
            input_file = './alldata_sample.json'
            flights_output = '/tmp/'
            events = (
                    pipeline
                    | 'read_input' >> beam.io.ReadFromText(input_file)
                    | 'parse_input' >> beam.Map(lambda line: 
<strong>                                                json.loads(line))
</strong>            )
</code></pre>

At this point, `events` is a `PCollection` of dictionaries, the same as we would have gotten if we’d read it from BigQuery.

Run this code to make sure it works and the events in the input file are being written to a CSV file. Now that reading and writing are done, we can turn our attention to the transformations that are needed.

#### Filtering

Most data pipelines process a data stream, selecting the data of interest and transforming it in some way. Choosing which data to continue processing is called filtering.

Recall that throughout Chapters [7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#logistic\_regression\_using\_spark\_ml) to [9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch09.html#machine\_learning\_with\_tensorflow\_in\_ver), we discarded canceled or diverted flights because they are not associated with an arrival delay. When creating the training dataset in Beam, we have to filter the set of events to contain only those flights that correspond to flights operating normally:

<pre><code>def is_normal_operation(event):
    for flag in ['CANCELLED', 'DIVERTED']:
        if flag in event:
            s = str(event[flag]).lower()
            if s == 'true':
                return False;  # cancelled or diverted
    return True  # normal operation


events = (events 
<strong>    | 'remove_cancelled' >> beam.Filter(is_normal_operation))
</strong></code></pre>

### Time Windowing

We would like to compute hourly averages every 5 minutes. There are three steps:

* Assign a timestamp to the events.
* Pass sliding windows across the event stream.
* Compute moving averages within each time window.

#### Assigning a timestamp

In order to proceed we first have to ensure that all our wheels-off events are timestamped. This will not be a problem in real time—events get assigned to the time that they are inserted into the Pub/Sub topic. However, this doesn’t happen to rows read from BigQuery or lines read from a file.[2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn173) Also, even in real time, the time at which the event gets inserted into Pub/Sub is not the right timestamp. Imagine a message that gets delayed due to network outages or machine failure is re-sent many hours later, and is finally inserted into Pub/Sub. We obviously don’t want to include such late-arriving records in our averages, which is what would happen if we use the time of insertion into Pub/Sub.

The way to fix this is to assign the timestamp of the wheels-off event (to be the time at which the aircraft finishes taxiing the runway and its wheels are off the ground):

```
def assign_timestamp(event):
    try:
        event_time = dt.datetime.strptime(event['WHEELS_OFF'],
                                          DATETIME_FORMAT)
        yield beam.window.TimestampedValue(event,
                                      event_time.timestamp())
    except:
        pass


events = events | 'assign_time' >> beam.FlatMap(assign_timestamp)
```

Note that the timestamp that is assigned is the Unix timestamp that we get by parsing the JSON string using `strptime`.

#### Sliding windows

We want to compute hourly averages every 5 minutes. To do that, we apply a sliding window onto the event stream:

```
WINDOW_DURATION = 60 * 60
WINDOW_EVERY = 5 * 60

event = (events
  | 'window' >> beam.WindowInto(beam.window.SlidingWindows(
                             WINDOW_DURATION, WINDOW_EVERY))
```

At this point, we have assigned to each event the hourly time window (“between 05:00 and 06:00”) within which it lies. But if we run the pipeline at this point, the results will stay the same. It’s only when we compute an aggregate value (such as the average) that the windowing starts to show its effect. The average will be computed on each time window separately, and we will end up with a moving average.

#### Computing moving average

We don’t want a single average departure delay value at 05:00 and another at 05:05. We want the averages to be computed separately for each airport. In order to do that, we need to group by the airport before computing the stats:

```
 | 'window' >> beam.WindowInto(beam.window.SlidingWindows(
                             WINDOW_DURATION, WINDOW_EVERY))
 | 'by_airport' >> beam.Map(lambda x: (x['ORIGIN'], x))
 | 'group_by_airport' >> beam.GroupByKey()
```

This is an idiom you should get familiar with. To group a collection of items, convert them into a couple of tuples where the first element of the tuple is the key to group by (here, we are using `ORIGIN`), and the second element of the tuple is the item itself:

```
 | 'by_airport' >> beam.Map(lambda x: (x['ORIGIN'], x))
```

Then, group by the key. The result will be a collection each item of which is a tuple:

```
 DFW, (x1, x2, x3, ...)
 DUL, (x1, x2, x3, ...)
```

In this GroupBy tuple, the first element is the key (the origin airport) and the second element is a list of events from that airport in this time window.

We can process the tuple to compute aggregate statistics in a function that I’ll call `add_stats`:

```
def add_stats(element):
    # all averages here are by airport
    airport = element[0]
    events = element[1]

    # how late are flights leaving?
    values = [float(event['DEP_DELAY']) for event in events]
    avg_dep_delay = float(np.mean(values))

    # similar for taxi_out
```

Because the event is just a Python dictionary, we can add the computed statistics to it. However, we have to make sure to add it to a copy of the dictionary because Beam doesn’t allow you to modify the input values to a transform function:

```
    for event in events:
       event_plus_stat = event.copy()
       event_plus_stat['AVG_DEP_DELAY'] = avg_dep_delay
       event_plus_stat['AVG_TAXI_OUT'] = avg_taxiout
       yield event_plus_stat
```

The reason we “yield” the `event_plus_stat` rather than returning it is that there will be multiple events per airport, and so multiple values generated by the `add_stats` function. Because the input to output mapping is not 1:1, the `events_and_stats` transform has to be applied with a `FlatMap` rather than simply a `Map`:

<pre><code>| 'group_by_airport' >> beam.GroupByKey()
<strong>| 'events_and_stats' >> beam.FlatMap(add_stats)
</strong></code></pre>

At this point, we will expect to have exactly as many events as we started with, except that each of the events will have two extra fields—the average departure delay and the average taxi-out time experienced at the airport the flight is departing from over the past hour.

#### Removing duplicates

The results on Cloud Storage seem correct, but on closer examination turn out to have repeated flight information:

```
1.0,...,TWF,SLC,...
1.0,...,TWF,SLC,...
1.0,...,TWF,SLC,...
1.0,...,TWF,SLC,...
1.0,...,TWF,SLC,...
```

Why? This has to do with our use of sliding windows. Recall that our sliding window is computed over an hour every 5 minutes. So, every flight is part of several overlapping panes, starting from the second one in [Figure 11-2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#a\_flight\_event\_is\_part\_of\_several\_hour), and therefore, each flight is repeated each time it falls into the pane. We would like the flight to be only part of the second window here.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1102.png" alt="" height="158" width="600"><figcaption></figcaption></figure>

**Figure 11-2. A flight event is part of several hourly time windows. A flight at 12:03 will be part of 11:05 to 12:05, 11:10 to 12:10, 11:15 to 12:15, …, 12:00 to 1:00.**

This is because the second window is the first window that includes the flight, and we’d like to write the flight event out at the end of that time window. Doing so in subsequent windows will result in duplicate flight objects.

You can clearly see this if you are running it in Dataflow when you look at the size of the input and output of the group-by-airport transform (see [Figure 11-3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#because\_a\_flight\_event\_is\_part\_of\_twel))[3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn174)—but at this point, we are not yet running in Dataflow, so we don’t have this information.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1103.png" alt="" height="394" width="600"><figcaption></figcaption></figure>

**Figure 11-3. Because a flight event is part of twelve hourly time windows, the size of the collection greatly expands after grouping, from 6 GB to 57 GB.**

The average delay does need to be computed once for every pane (this is why we have a sliding window), but each flight object should be output only once. In order to do that, we should ask Beam to inject the time window being used, and verify that we are in the latest slice:

<pre><code><strong>def add_stats(element, window=beam.DoFn.WindowParam):
</strong>    ...
<strong>    emit_end_time = window.start + WINDOW_EVERY
</strong>    for event in events:
        event_time = to_datetime(event['WHEELS_OFF']).timestamp()
<strong>        if event_time &#x3C; emit_end_time:
</strong>            event_plus_stat = event.copy()
            event_plus_stat['AVG_DEP_DELAY'] = avg_dep_delay
            event_plus_stat['AVG_TAXI_OUT'] = avg_taxiout
            yield event_plus_stat
</code></pre>

Once we do this, the duplicates go away, and we get the same number of flights in the output as we had in the input (after removing canceled and diverted flights).

## Machine Learning Training

We don’t want to train the ML model with all the raw input values. Instead, we want to use only those features that we used in [Chapter 9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch09.html#machine\_learning\_with\_tensorflow\_in\_ver)—those were the features that we determined through experimentation were important. As in earlier chapters, we will also want to add in the data split (train, validate, test) by having different days in different splits.

Once the dataset is created, we can use the code from [Chapter 9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch09.html#machine\_learning\_with\_tensorflow\_in\_ver) (appropriately modified) to train the model and evaluate it.

### Machine Learning Dataset

We start by adding a transform method to convert the raw event data into features:

<pre><code> features = (events
    ...
     | 'events_and_stats' >> beam.FlatMap(add_stats)
<strong>     | 'events_to_features' >> beam.FlatMap(create_features_and_label)
</strong> )
</code></pre>

#### Label

In the transform method, we compute the label, which should be 1.0 if the flight is on-time and zero otherwise:

```
 def create_features_and_label(event):
    try:
        model_input = {
            'ontime': 1.0 if float(event['ARR_DELAY']) < 15 else 0,
```

Then, we extract the data that we used in [Chapter 9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch09.html#machine\_learning\_with\_tensorflow\_in\_ver), naming the variables the same as the SQL query did:

```
            # same as in ch9
            'dep_delay': event['DEP_DELAY'],
            'taxi_out': event['TAXI_OUT'],
            'distance': event['DISTANCE'],
            'origin': event['ORIGIN'],
            'dest': event['DEST'],
...
            'arr_airport_lat': event['ARR_AIRPORT_LAT'],
            'arr_airport_lon': event['ARR_AIRPORT_LON'],
```

The parts of time we used (the hour of day and day of week) require us to use Python’s time library rather than SQL’s `EXTRACT` function, but we do have the equivalent functionality:

```
            'dep_hour': to_datetime(event['DEP_TIME']).hour,
            'is_weekday': 1.0 if to_datetime(event['DEP_TIME']
                                    ).isoweekday() < 6 else 0.0,
            'carrier': event['UNIQUE_CARRIER'],
```

Finally, we add in the two time averages we computed in the previous section:

```
 
            'avg_dep_delay': event['AVG_DEP_DELAY'],
            'avg_taxi_out': event['AVG_TAXI_OUT'],
        }
        yield model_input
```

Finally, what do we do if any of the features is not present in the data? A missing key exception is thrown, and we ignore that row:

```
    except Exception as e:
        # if any key is not present, don't use for training
        logging.warning('Ignoring {} because: {}'.format(event, e),
                         exc_info=True)
        pass
 
```

#### Data split

Recall that we don’t want to use a random split of the rows between training, validation, and test. Instead, we want to use a deterministic method that ensures a date used in training will never be used for testing even in future iterations of the training program. In order to do that in BigQuery SQL, we used the `FARM_FINGERPRINT` function on the airport code.

Fortunately, this function is available in the Python package `farmhash`, and we can use it to determine the data split for each row:

```
def get_data_split(date_value):
    # Use farm fingerprint just like in BigQuery
    x = np.abs(np.uint64(farmhash.fingerprint64(str(date_value))
              ).astype('int64') % 100)
    if x < 60:
        data_split = 'TRAIN'
    elif x < 80:
        data_split = 'VALIDATE'
    else:
        data_split = 'TEST'
    return data_split


'data_split': get_data_split(event['FL_DATE'])
```

The managed dataset in Vertex AI will use this column to split the data. However, in case we are training in other frameworks, we want to create separate training, validation, and testing files. We can do this with Beam by filtering the data:

```
 for split in ['ALL', 'TRAIN', 'VALIDATE', 'TEST']:
     feats = features
     if split != 'ALL':
         feats = feats | 'only_{}'.format(split) >> beam.Filter(
                               lambda f: f['data_split'] == split)
     (feats
       | '{}_to_string'.format(split) >> beam.Map(
                   lambda f: ','.join([str(x) for x in f.values()]))
       | '{}_to_gcs'.format(split) >> beam.io.textio.WriteToText(
                         os.path.join(flights_output, split.lower())
     )
```

While we can write only the CSV data, it is probably better if we add a header to the files and name them with a _.csv_ suffix. This makes it much easier to use these files for AutoML since we can have AutoML simply parse the file for column names. To do so, we define a CSV header and change the output function:

```
CSV_HEADER = 'ontime,dep_delay,taxi_out,...,avg_taxi_out,data_split'

...
   | '{}_to_gcs'.format(split) >> beam.io.textio.WriteToText(
                         os.path.join(flights_output, split.lower(),                                                            
                         file_name_suffix='.csv', header=CSV_HEADER)
```

#### Distance bug

On running the pipeline at this point, I encountered a problem—all the files were empty.

The logs indicated an exception about a missing key. The key? `DISTANCE`.

It turns out that in [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest), I had decided that the wheels-off event would not include the distance on the grounds that at the time the flight is taking off, we don’t know whether it is going to get diverted to a different airport:

<pre><code>    if len(fields["WHEELS_OFF"]) > 0:
       event = dict(fields)  # copy
<strong>       event["EVENT_TYPE"] = "wheelsoff"
</strong>       event["EVENT_TIME"] = fields["WHEELS_OFF"]
       for f in ["WHEELS_ON", "TAXI_IN", 
<strong>                 "ARR_TIME", "ARR_DELAY", "DISTANCE"]:
</strong><strong>          event.pop(f, None)  # not knowable at departure time
</strong></code></pre>

Unfortunately, I forgot about this, and in subsequent chapters, I did use the nominal distance to the intended destination as an ML feature. We even found that it was a useful feature to use.

There are two ways to address this problem. I can go back and fix the [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest) pipeline that creates the time-corrected data to include the distance field in the wheels-off event. This is the better solution.

Alternatively, I can compute the distance since I know the latitude and longitude of the origin and intended destination. This is simpler, and so, that’s what I’m going to do:[4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn175)

```
 def approx_miles_between(lat1, lon1, lat2, lon2):
    # convert to radians
    lat1 = float(lat1) * np.pi / 180.0
    lat2 = float(lat2) * np.pi / 180.0
    lon1 = float(lon1) * np.pi / 180.0
    lon2 = float(lon2) * np.pi / 180.0

    # apply Haversine formula
    d_lat = lat2 - lat1
    d_lon = lon2 - lon1
    a = (pow(np.sin(d_lat / 2), 2) +
         pow(np.sin(d_lon / 2), 2) *
         np.cos(lat1) * np.cos(lat2));
    c = 2 * np.arcsin(np.sqrt(a))
    return 6371 * c * 0.621371  # miles

...

  'distance': approx_miles_between(event['DEP_AIRPORT_LAT'],
                                  event['DEP_AIRPORT_LON'],
                                  event['ARR_AIRPORT_LAT'],
                                  event['ARR_AIRPORT_LON']),
```

#### Monitoring and verification

With this done, we can run the Beam pipeline on the full dataset using the Dataflow Runner (see _create\_traindata.py_ and _flights\_transforms.py_ in the code repository for full code):

<pre><code><strong>  python3 create_traindata --input bigquery \
</strong>          --project ... --bucket ... --region ... 
</code></pre>

While the pipeline is running, we can use the GCP console, as shown in [Figure 11-4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#steps\_of\_the\_dataflow\_pipeline\_to\_the), to monitor the progress of the job and examine the logs.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1104.png" alt="" height="748" width="600"><figcaption></figcaption></figure>

**Figure 11-4. Steps of the Dataflow pipeline to the machine learning dataset with time averages.**

The pipeline took 2 hours and 50 minutes to complete and autoscaled to 20 workers (see [Figure 11-5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#the\_dataflow\_pipeline\_autoscales\_and\_p)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1105.png" alt="" height="269" width="600"><figcaption></figcaption></figure>

**Figure 11-5. The Dataflow pipeline autoscales and processes the dataset in parallel.**

The number of elements processed at each step is reported, and we can verify that all the data was correctly processed (see [Figure 11-6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#examine\_the\_number\_of\_elements\_in\_the)). I started with 5.819 million flights. It became 5.714 million flights after removing canceled/diverted flights. That was the exact number of rows written out in all _\*.csv_.

Note, however, that the reduction in the number of flights is not from 5.819 to 5.714 million in one fell swoop. Instead, we go down to 5.730 million when we assign a timestamp and then lose a further 15,000 flights when we remove canceled or diverted flights. Why did we lose flights when we assigned a timestamp? It’s because we skipped any rows where one of the feature values was missing. Also because it takes an hour to fill up an hourly time window, and so we lost any flights from 00:00 UTC to 01:00 UTC on the first day of our dataset.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1106.png" alt="" height="536" width="600"><figcaption></figcaption></figure>

**Figure 11-6. Examine the number of elements in the input and output collections to ensure that no data has been dropped.**

### Training the Model

We can use the same code as we used in [Chapter 10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#getting\_ready\_for\_mlops\_with\_vertex\_ai). There are just a handful of changes to make.

#### Changes from Chapter 10

The code in [Chapter 10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#getting\_ready\_for\_mlops\_with\_vertex\_ai) reads and writes to paths that contain the string `ch9`. Here, we’ll change it to read and write to paths that contain the string `ch11`. Instead of deploying to the endpoint flights, let’s deploy to a new endpoint named `flights-ch11`. Also, there are now two extra numeric features.

In order to avoid maintaining two similar files, I automated these changes:

<pre><code> CHANGES = [
<strong>    ("ch10", "ch11"),
</strong>
    # train_on_vertexai.py
<strong>    ("ENDPOINT_NAME = 'flights'", "ENDPOINT_NAME = 'flights-ch11'"),
</strong>
    # model.py
    ("arr_airport_lat,arr_airport_lon", 
<strong>     "arr_airport_lat,arr_airport_lon,avg_dep_delay,avg_taxi_out")
</strong>]

for filename in ['train_on_vertexai.py', 'model.py']:
    in_filename = os.path.join('../10_vertexai', filename)
    with open(in_filename, "r") as ifp:
        with open(filename, "w") as ofp:
            for line in ifp.readlines():
                for change in CHANGES:
                    line = line.replace(change[0], change[1])
                ofp.write(line)
</code></pre>

These are the only changes that need to be made to _model.py_ and _train\_on\_vertexai.py_. Apply these changes by running:

```
 python3 change_ch10_files.py
```

#### AutoML model

Training the AutoML Tables model involves launching the trainer:

```
 python3 train_on_vertexai.py --automl \
          --project ... --bucket ... --region ...
```

As in [Chapter 9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch09.html#machine\_learning\_with\_tensorflow\_in\_ver), the trainer writes evaluation data to a table in BigQuery. I can evaluate the resulting model by running the SQL query to compute the RMSE:

```
 SELECT  
  SQRT(SUM(
      (CAST(ontime AS FLOAT64) - predicted_ontime.scores[OFFSET(0)])*
      (CAST(ontime AS FLOAT64) - predicted_ontime.scores[OFFSET(0)])
      )/COUNT(*))
FROM dsongcp.ch11_automl_evaluated 
```

The model took a little over 4 hours to train and resulted in an RMSE of 0.198, whereas the AutoML model in [Chapter 9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch09.html#machine\_learning\_with\_tensorflow\_in\_ver) had an RMSE of 0.199. It appears that the time averages add only a miniscule improvement. Indeed, when we look at the feature importance graph (see longer arrows in [Figure 11-7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#the\_average\_delays\_at\_the\_origin\_airpo)), it appears that the averages do play a role, but not a significant one.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1107.png" alt="" height="674" width="600"><figcaption></figcaption></figure>

**Figure 11-7. The average delays at the origin airport are not as significant as other features more directly related to the flight itself.**

However, real-time averages may be more useful than the global averages (shown by the shorter arrows in [Figure 11-7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#the\_average\_delays\_at\_the\_origin\_airpo))—once we have a real-time understanding of what’s happening at the airport, the global averages that can be learned from the extracts of time become less important.

#### Custom model

In [Chapter 10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#getting\_ready\_for\_mlops\_with\_vertex\_ai), we also built a custom model using feature crosses of locations. How does that model do with the two additional features?

All we did was to include the two features as additional numeric features:

<pre><code>def create_model():
    real = {
<strong>        colname: tf.feature_column.numeric_column(colname)
</strong>        for colname in
        (
          'dep_delay,taxi_out,distance,dep_hour,is_weekday,' +
          'dep_airport_lat,dep_airport_lon,' +     
<strong>          'arr_airport_lat,arr_airport_lon,avg_dep_delay,avg_taxi_out'
</strong>        ).split(',')
    }
</code></pre>

The rest of the model remains the same. The resulting RMSE is 0.195, which is pretty much what we got without the two moving averages.

As stated earlier, don’t let this discourage you from including time aggregate features in your models. They can be useful in other contexts, and you won’t know until you try. However, whether the increased complexity will be worth it is a fair point that you will want to consider.

## Streaming Predictions

In [Chapter 9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch09.html#machine\_learning\_with\_tensorflow\_in\_ver), we ingested historical flight data and used it to train a machine learning model capable of predicting whether or not a flight will be late. We deployed the trained model and demonstrated that we could get the prediction for an individual flight by sending input variables to the model in the form of a representational state transfer (REST) call.

The input variables to the model include information about the flight whose on-time performance is desired. Most of these variables—the departure delay of the flight, the distance the flight is to travel, and the time taken to taxi out to the runway—are specific to the flight itself. However, the inputs to the ML model also included two time aggregates—the average departure delay and taxi-out time at the specific departure airport—that require more effort to compute. In the previous sections of this chapter, we wrote an Apache Beam pipeline to compute these averages on the training dataset so as to be able to train the machine learning model. Then, we trained a TensorFlow model capable of using the input variables to predict whether or not the flight would be late.

The ML pipeline that we wrote deployed this model to a Vertex AI endpoint. Similar to what we did in [Chapter 9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch09.html#machine\_learning\_with\_tensorflow\_in\_ver), we can invoke the service to make predictions. However, the model now expects to see the two time averages in its input. The way we address this is shown in [Figure 11-1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#the\_same\_transform\_code\_is\_used\_to\_tra).

In this section, we will build the real-time Beam pipeline depicted at the bottom of [Figure 11-1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#the\_same\_transform\_code\_is\_used\_to\_tra). The pipeline listens to flight events, computes the time averages, and passes the enriched flight info to the model. It can add the predicted on-time performance of the flight to the flight data and write it out to a database. The resulting table can then be queried by user-facing applications that need to provide information to users of the system interested in their specific flight. Although we could do prediction on behalf of individual users of the service as and when they request the status of their specific flight, this will probably end up being wasteful. It is far more efficient to compute the on-time arrival probability once, at flight takeoff, and then simply look up the flight information as required for specific users.[5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn176) It is worth noting that we are making predictions for a flight only at the time of takeoff (and not updating it as the flight is en route) because we trained our ML model only on flight data at the time of takeoff.

### Reuse Transforms

The advantage of using Apache Beam to compute time averages is that the programming model is the same for both historical data and for real-time data. Therefore, we will be able to reuse most of our training pipeline code in real time.

The pipeline to create the training dataset carries out the following transformations that can be refactored into a method:

```
def transform_events_to_features(events):
    events = events | 'assign_time' >> beam.FlatMap(assign_timestamp)
    events = events | 'remove_cancelled' >> beam.Filter(
                                            is_normal_operation)

    # compute stats by airport, and add to events
    features = (
            events
            | 'window' >> beam.WindowInto(
         beam.window.SlidingWindows(WINDOW_DURATION, WINDOW_EVERY))
            | 'by_airport' >> beam.Map(lambda x: (x['ORIGIN'], x))
            | 'group_by_airport' >> beam.GroupByKey()
            | 'events_and_stats' >> beam.FlatMap(add_stats)
            | 'events_to_features' >> beam.FlatMap(
         lambda x: create_features_and_label(x))
    )

    return features
```

For this method to be callable from both the training and prediction pipelines, we’ll put the code in a Python package (look at the file _flights\_transforms.py_ in the folder _flightstxf_ in the code repository) and move all the methods called here—`assign_timestamp`, `is_normal_operation`, and so on—to that file as well.

We can invoke this method from _create\_traindata.py_ as follows:

```
from flightstxf import flights_transforms as ftxf
...

features = ftxf.transform_events_to_features(events)
```

There is just one small change that we have to make. The training dataset contains the label and the `data_split`, which we won’t have during prediction. Let’s protect that code with a boolean flag so that those two fields aren’t created during prediction:

<pre><code><strong>def create_features_and_label(event, for_training):
</strong>        model_input = {}

        if for_training:
            model_input.update({
                'ontime': 1.0 if float(event['ARR_DELAY']) &#x3C; 15 else 0,
            })

        # features for both training and prediction
        model_input.update({
            # same as in ch9
            'dep_delay': event['DEP_DELAY'],
            'taxi_out': event['TAXI_OUT'],
            ...
            # newly computed averages
            'avg_dep_delay': event['AVG_DEP_DELAY'],
            'avg_taxi_out': event['AVG_TAXI_OUT'],

        })

        if for_training:
            model_input.update({
                # training data split
                'data_split': get_data_split(event['FL_DATE'])
            })

        yield model_input
</code></pre>

### Input and Output

The transform code is the same between training and prediction, so we were able to move it into a common module called from both pipelines. The pipeline input, however, is different. When creating the training dataset, we read the time-corrected BigQuery dataset that contains all the fields.

When doing predictions, though, the streaming pipeline will have to make do with only the data in the `wheelsoff` event. The way that the overall architecture will work is that the streaming pipeline will listen to the `wheelsoff` topic and write predictions for every flight to a database. Client programs (such as the mobile application that a traveler uses) will query the database for the status of the flight that they are on. We have to do it this way because the mobile application will need flight predictions on-demand, whereas the streaming pipeline (and the ML model) cannot operate until the time averages are available. We show the solution architecture in [Figure 11-8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#the\_streaming\_pipeline\_writes\_out\_pred).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1108.png" alt="" height="414" width="600"><figcaption></figcaption></figure>

**Figure 11-8. The streaming pipeline writes out predictions for all flights to a database. The mobile application that provides flight information to users queries the database about specific flights.**

Recall from [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest) that the simulator publishes events into a Pub/Sub topic. We can subscribe to this topic and read the events as they stream in using:

```
input_topic = "projects/{}/topics/wheelsoff".format(project)
events = (pipeline
   | 'read_input' >> beam.io.ReadFromPubSub(topic=input_topic,                                                           
                          timestamp_attribute='EventTimeStamp')
   | 'parse_input' >> beam.Map(lambda s: json.loads(s))
)
```

As before, however, we want to be able to develop the pipeline locally. An easy way to do this is to remember that we have a historical archive of all the events in `flights_simevents` in BigQuery. We can sample this dataset, create a local file, and use it to develop the pipeline:

```
bq query --nouse_legacy_sql --format=sparse \
    "SELECT EVENT_DATA FROM dsongcp.flights_simevents 
     WHERE EVENT_TYPE = 'wheelsoff' AND EVENT_TIME BETWEEN 
     '2015-03-10T10:00:00' AND '2015-03-10T14:00:00' " \
    | grep FL_DATE \
    > simevents_sample.json
```

The output of the model should go to a database (see [Figure 11-8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#the\_streaming\_pipeline\_writes\_out\_pred)), and Apache Beam supports writing to Cloud SQL or Cloud Bigtable, in addition to BigQuery. Again, for simplicity of development, we’ll simply write to Cloud Storage for now. We can change the output sink once development is complete.

At this point, we have read the events and converted them into the features that the model requires. We now need to invoke the model to get the predicted on-time probability—that is what the end user wants to know!

### Invoking Model

In [Chapter 9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch09.html#machine\_learning\_with\_tensorflow\_in\_ver), we used Vertex AI to deploy the trained TensorFlow model to an endpoint as a web service. We demonstrated that we could invoke the model by sending a correctly formatted JSON request to the endpoint (see _call\_predict.py_ in [Chapter 10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#getting\_ready\_for\_mlops\_with\_vertex\_ai) of the repository).

The first step was to get a _stub_ that can connect to the endpoint:

```
    endpoint = aiplatform.Endpoint.list(
        filter='display_name="{}"'.format(ENDPOINT_NAME),
        order_by='create_time desc'
    )[0]    
```

In networking parlance, a stub is a local proxy for a remote object. Here, the ML model is deployed and made available as a web application accessible through the endpoint URL. That web application is the remote object. A stub is an object (in Python, Java, JavaScript, etc.) that abstracts away the details of HTTPS invocation—parameter serialization, transport encryption, authentication, and so on—so as to present a simple invocation interface to the client program. The `endpoint` class provided by the Vertex AI client library is the stub.

The next step was to call `predict()` on the endpoint stub (which will take care of the communication details):

```
    preds = endpoint.predict(input_data)
    probs = [p[0] for p in preds.predictions]
```

In the preceding code snippet, `input_data` is a correctly formatted JSON request:

```
input_data = [
        {"dep_hour": 2, "is_weekday": 1, "dep_delay": 40, "taxi_out": 17, 
        "distance": 41, "carrier": "AS",
         "dep_airport_lat": 58.42527778, "dep_airport_lon": -135.7075, 
         "arr_airport_lat": 58.35472222,
         "arr_airport_lon": -134.57472222, "origin": "GST", "dest": "JNU"},
        {"dep_hour": 22, "is_weekday": 0, "dep_delay": -7, "taxi_out": 7, 
        "distance": 201, "carrier": "HA",
         "dep_airport_lat": 21.97611111, "dep_airport_lon": -159.33888889, 
         "arr_airport_lat": 20.89861111,
         "arr_airport_lon": -156.43055556, "origin": "LIH", "dest": "OGG"}
    ]
```

We need to repeat these steps to obtain the probability for every flight. However, we would ideally like to reuse endpoint stubs, instead of looking them up again and again, once for every flight.

Also, as the preceding sample code illustrates, it is possible to send multiple flights to the model endpoint. Sending the flights one-by-one is wasteful. We should batch them up and send in a list of flights.

### Reusing Endpoint

There are two ways that we can reuse the endpoint stub—a shared endpoint stub or a per-worker endpoint stub.

#### Shared handle

One way is to create a single endpoint stub and use it from all workers. This is accomplished by using a shared handle that is shared between workers:

<pre><code>class FlightsModelInvoker(beam.DoFn):
<strong>    def __init__(self, shared_handle):      # 
</strong><strong>       self._shared_handle = shared_handle  # 
</strong>
    def process(self, input_data):
<strong>        def create_endpoint(): # 
</strong>            from google.cloud import aiplatform
            endpoint_name = 'flights-ch11'
            endpoint = aiplatform.Endpoint.list(
                filter='display_name="{}"'.format(endpoint_name),
                order_by='create_time desc'
            )[0]
 
        # call predictions and pull out probability
<strong>        endpoint = self._shared_handle.acquire(create_endpoint) # 
</strong><strong>        predictions = endpoint.predict(input_data).predictions  # 
</strong>        for idx, input_instance in enumerate(input_data):
            result = input_instance.copy()
            result['prob_ontime'] = predictions[idx][0]
            yield result

...

<strong>handle = beam.utils.shared.Shared() # 
</strong>
<strong>preds = features | 'pred' >> beam.ParDo(FlightsModelInvoker(handle))   
</strong></code></pre>

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/1.png)](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#code\_id\_11\_1)In the main pipeline, create a handle of the type `beam.utils.shared.Shared` and pass that in as a constructor parameter to the `DoFn`.[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/2.png)](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#code\_id\_11\_2)In the `DoFn`, save the handle as an instance variable.[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/3.png)](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#code\_id\_11\_3)In the process function of the `DoFn`, acquire the endpoint through the handle every time.[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/4.png)](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#code\_id\_11\_4)Provide a function that can be called to create the endpoint the first time around.

We would use the shared handle approach if the endpoint stub is very large, very expensive to create, or cannot handle concurrent access from multiple workers. That is not the case here.

If you are loading an ML model directly from a SavedModel file and calling `model.predict()` on the in-memory model from within the pipeline, then the considerations flip. Creating the model is very expensive because it involves parsing the model file and creating a set of neural network layers. Models, especially text and image models, can be extremely large. If you are memory limited, you might want to use the same model from multiple workers. Keras model functions are not thread-safe. Given this, you would want to use the shared handle approach, and not the per-worker instance approach that I’m going to use here.

#### Per-worker instance

Creating an endpoint stub is just a single network call and is not very expensive. The state of an endpoint stub is essentially just a URL. That is not very large. Moreover, the endpoint stub connects to a web application that runs on Vertex AI, which will autoscale predictions. Therefore, it’s better if workers can invoke the endpoint in parallel and not have to wait on each other.

Because we have an inexpensive operation, a small object, and a thread-safe backend, it’s better to create an endpoint stub the first time it’s needed in a worker thread and then reuse it as long as that worker thread is active.

In order to do so, we should _not_ use an instance variable like this because worker state can be passivated and reactivated many hours later on another machine:

<pre><code><strong>## THIS IS WRONG. DO NOT DO THIS
</strong>class FlightsModelInvoker(beam.DoFn):
<strong>    def __init__(self):
</strong><strong>        from google.cloud import aiplatform
</strong>        endpoint_name = 'flights-ch10'
<strong>        self.endpoint = aiplatform.Endpoint.list(
</strong>            filter='display_name="{}"'.format(endpoint_name),
            order_by='create_time desc'
        )[0]

    def process(self, input_data):
<strong>        predictions = self.endpoint.predict(input_data).predictions
</strong>        for idx, input_instance in enumerate(input_data):
            result = input_instance.copy()
            result['prob_ontime'] = predictions[idx][0]
            yield result

...

<strong>preds = features | 'pred' >> beam.ParDo(FlightsModelInvoker())
</strong></code></pre>

In general, instance variables on a Beam function should be set within the `__init__` method of a `DoFn` only for things that can be safely resurrected from a passivated object. The endpoint is not safe to resurrect in this manner because it might contain a number of cached variables.

Instead, we have to hook into the lifecycle of a `DoFn`. The contract of a `DoFn` is that the setup method will be called the first time a `DoFn` is used on a worker. So, we override the `setup()` method and initialize the endpoint instance variable there:

<pre><code>class FlightsModelInvoker(beam.DoFn):
    def __init__(self):
        self.endpoint = None

<strong>    def setup(self):
</strong>        from google.cloud import aiplatform
        endpoint_name = 'flights-ch10'
<strong>        self.endpoint = aiplatform.Endpoint.list(
</strong>            filter='display_name="{}"'.format(endpoint_name),
            order_by='create_time desc'
        )[0]
        
    def process(self, input_data):
        # call predictions and pull out probability
<strong>        predictions = self.endpoint.predict(input_data).predictions
</strong>        for idx, input_instance in enumerate(input_data):
            result = input_instance.copy()
            result['prob_ontime'] = predictions[idx][0]
            yield result
</code></pre>

With this change, we reuse endpoint stubs throughout the lifetime of a worker thread.

### Batching Predictions

While we do need to make predictions for each flight, we do not need to send the flight information to the service one-by-one. Instead of invoking the machine learning service once for each flight, we could batch up requests. If we were to invoke the predictions for 60,000 flights into batches of 60 each, we’d be making only 1,000 requests. Making fewer requests will not only reduce costs, it might also end up increasing the overall performance by having less time spent waiting for a response from the service.

Because it is possible to send multiple flights to the model endpoint, sending the flights one-by-one is wasteful. However, our pipeline uses a `ParDo`, which will send each element of the features collection one-by-one to the prediction method:

```
preds = (features
  | 'model_predict' >> beam.ParDo(FlightsModelInvoker())
)
```

We can ask Beam to send in a list of elements using the BatchElements method:

```
preds = (features
  | 'batch_instances' >> beam.BatchElements(
                          min_batch_size=1, max_batch_size=64)
  | 'model_predict' >> beam.ParDo(FlightsModelInvoker())
)
```

Recall that all aggregation (sum, mean, max, etc.) happens within a time window—this is why our average departure delay was the average delay over the most recent hour. Unfortunately, batching is also an aggregation. However, we will defeat the purpose if the batches are grouped in any way. So, let’s ask Beam to discard the time window information before batching:

```
preds = (features
  | 'into_global' >> beam.WindowInto(beam.window.GlobalWindows())
  | 'batch_instances' >> beam.BatchElements(
                          min_batch_size=1, max_batch_size=64)
  | 'model_predict' >> beam.ParDo(FlightsModelInvoker())
)
```

Note that we have specified a minimum and maximum batch size to the BatchElements method. This allows Dataflow to time how long it takes to invoke the model on different batch sizes and choose an optimal size. When I tried it, Dataflow seemed to settle on sending around 30 flights to the model each time:

```
INFO:root:Invoking ML model on 1 flights
INFO:root:Invoking ML model on 2 flights
INFO:root:Invoking ML model on 4 flights
INFO:root:Invoking ML model on 8 flights
INFO:root:Invoking ML model on 16 flights
INFO:root:Invoking ML model on 32 flights
INFO:root:Invoking ML model on 30 flights
```

For obvious reasons, Dataflow tuning the batch size is better than hard coding the number of elements to batch the features collection into—our model could grow over time, and it might turn out that our hardcoded value is too large at some point.

## Streaming Pipeline

At this point, we have a streaming pipeline that can listen to incoming flight events, compute the likelihood that a flight that’s about to take off will arrive on-time, and write the output to Cloud Storage. However, we need to add the arrival probability to a _database_, not just to a file. This is so that the database can be used to serve client applications.

### Writing to BigQuery

Fortunately, it’s quite simple to change out the output sink to BigQuery:

```
preds_schema = ','.join([
                'event_time:timestamp',
                'prob_ontime:float',
                'dep_delay:float',
...
                'avg_dep_delay:float',
                'avg_taxi_out:float',
            ])
(preds
             | 'to_bigquery' >> beam.io.WriteToBigQuery(
                        'dsongcp.streaming_preds', 
                        schema=preds_schema,
                        # write_disposition=...WRITE_TRUNCATE,                      
create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                        method='STREAMING_INSERTS'
                    )
 )
```

There are two important changes to previous times that we wrote to BigQuery:

* We do not use `WRITE_TRUNCATE`. This is because the streaming pipeline needs to insert new records, not overwrite the table.
* We ask Dataflow to stream the inserts into BigQuery so that information is available in real time. Had we omitted it, Dataflow would have staged the output to Google Cloud Storage and done periodic loads.

Is BigQuery enough for our purpose? Should I not be using Cloud SQL, Cloud Bigtable, or Spanner? I’ll answer this question later on in this section. For now, trust me when I say BigQuery is sufficient.

### Executing Streaming Pipeline

Now that the pipeline code has been written, we can start the simulation from [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest) to stream records into Cloud Pub/Sub:

```
cd ../04_streaming/simulate
python3 simulate.py --startTime "2015-02-01 00:00:00 UTC" --endTime \
  "2015-02-03 00:00:00 UTC" --speedFactor 30 --project PROJECT
```

Then, we can start the Cloud Dataflow pipeline that we have just written to consume these records:

```
python3 make_predictions.py --input pubsub \
     --project <PROJECT> --bucket <BUCKET> --region <REGION>
```

With the pipeline running, we can navigate over to the BigQuery console and verify that flight information is indeed getting streamed in:

```
SELECT 
  event_time, dest, carrier, prob_ontime 
FROM dsongcp.streaming_preds
WHERE origin = 'DFW'
ORDER BY event_time DESC
LIMIT 5
```

The preceding query is looking at flights originating at Dallas/Fort Worth (DFW) and ordering and limiting the result set so that the five latest flights are retained:

| `Row` | `event_time`              | `dest` | `carrier` | `prob_ontime` |
| ----- | ------------------------- | ------ | --------- | ------------- |
| `1`   | `2015-02-01 04:17:00 UTC` | `MSY`  | `AA`      | `0.999135375` |
| `2`   | `2015-02-01 04:11:00 UTC` | `TUL`  | `AA`      | `0.999612808` |
| `3`   | `2015-02-01 04:10:00 UTC` | `LRD`  | `MQ`      | `0.229228586` |
| `4`   | `2015-02-01 04:07:00 UTC` | `LAX`  | `AA`      | `0.809439421` |
| `5`   | `2015-02-01 04:02:00 UTC` | `SEA`  | `AA`      | `0.994473457` |

The query results change when I execute the query a few minutes later, showing that we are getting new and updated flights (note from the `speedFactor` that the simulation is run at 30 times the speed of real time):

| `Row` | `event_time`              | `dest` | `carrier` | `prob_ontime` |
| ----- | ------------------------- | ------ | --------- | ------------- |
| `1`   | `2015-02-01 05:35:00 UTC` | `PHX`  | `NK`      | `0.964711547` |
| `2`   | `2015-02-01 05:25:00 UTC` | `LAS`  | `NK`      | `0.982865453` |
| `3`   | `2015-02-01 04:50:00 UTC` | `ICT`  | `AA`      | `0.99398911`  |
| `4`   | `2015-02-01 04:44:00 UTC` | `LAS`  | `AA`      | `0.998527884` |
| `5`   | `2015-02-01 04:33:00 UTC` | `LAX`  | `AA`      | `0.991004586` |

### Late and Out-of-Order Records

Our simulation uses the flight record time to add records into Cloud Pub/Sub in precise order. In real life, though, flight records are unlikely to arrive in order. Instead, network vagaries and latencies will cause late and out-of-order records to happen. In order to simulate these essentially random effects, we should change our simulation to add a random delay to each record.

This can be done in the BigQuery SQL statement that is used by the simulation program to read in the flight records:

```
SELECT
  EVENT_TYPE,
  EVENT_TIME AS ORIGINAL_NOTIFY_TIME,
  TIMESTAMP_ADD(EVENT_TIME, 
                INTERVAL CAST (0.5 + RAND()*120 AS INT64) SECOND)
        AS NOTIFY_TIME
FROM
  dsongcp.flights_simevents
WHERE event_type = 'wheelsoff'
ORDER BY original_notify_time ASC LIMIT 5
```

Because `RAND()` returns a number that is uniformly distributed between 0 and 1, multiplying the result of `RAND()` by 120 yields a delay between 0 and 2 minutes. Running this query on the BigQuery console, we notice that it works as intended—the records now reflect some jitter:

| `Row` | `EVENT_TYPE` | `ORIGINAL_NOTIFY_TIME`    | `NOTIFY_TIME`             |
| ----- | ------------ | ------------------------- | ------------------------- |
| `1`   | `wheelsoff`  | `2014-12-31 22:14:00 UTC` | `2014-12-31 22:14:42 UTC` |
| `2`   | `wheelsoff`  | `2015-01-01 04:28:00 UTC` | `2015-01-01 04:29:22 UTC` |
| `3`   | `wheelsoff`  | `2015-01-01 05:21:00 UTC` | `2015-01-01 05:21:35 UTC` |
| `4`   | `wheelsoff`  | `2015-01-01 05:36:00 UTC` | `2015-01-01 05:36:28 UTC` |
| `5`   | `wheelsoff`  | `2015-01-01 05:45:00 UTC` | `2015-01-01 05:45:37 UTC` |

Note that the first record is delayed by 42 seconds, whereas the second record is delayed by 1 minute and 22 seconds.

#### Uniformly distributed delay

A zero delay is highly unrealistic, however. We could change the formula to simulate other scenarios. For example, if we want to have latencies between 90 and 120 seconds, we would change the `jitter` to be `CAST(90.5 + RAND()*30 AS INT64)`. The resulting distribution might look like this:

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_11in01.png" alt="Image" height="344" width="490"><figcaption></figcaption></figure>



Even this strikes me as being unrealistic. I don’t know what the delay involved with the flight messages is,[6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn177) but there seem to be two possibilities: an exponential distribution and a normal distribution.

#### Exponential distribution

An exponential distribution is the theoretical distribution associated with the time between events where the events themselves happen at a constant rate. If the network capacity is limited by the number of events, we’d observe that the delay follows an exponential distribution. To simulate this, we can create the `jitter` variable following the formula:

```
CAST(-LN(RAND()*0.99 + 0.01)*30 + 90.5 AS INT64)
```

The resulting distribution would look something like this:

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_11in02.png" alt="Image" height="344" width="490"><figcaption></figcaption></figure>



With the exponential distribution, latencies of 90 s are much more common than latencies of 150 s, but a few records will encounter unusually high latencies.

#### Normal distribution

A third alternative distribution for the delay is that it follows the law of big numbers, and that if we observe enough flight events, we might observe that the delay is normally distributed around some mean with a certain standard deviation. Of course, the delay has to be positive, so the distribution would be truncated at zero.

Generating a normally distributed random variable is hard to do with just plain SQL. Fortunately, BigQuery allows for user-defined functions (UDFs) in JavaScript. This JavaScript function uses the Marsaglia polar rule to transform a pair of uniformly distributed random variables into one that is normally distributed:

```
js = """
    var u = 1 - Math.random();
    var v = 1 - Math.random();
    var f = Math.sqrt(-2 * Math.log(u)) * Math.cos(2*Math.PI*v);
    f = f * sigma + mu;
    if (f < 0)
       return 0;
    else
       return f;
""".replace('\n', ' ')
```

The preceding JavaScript can be used to create a temporary UDF invokable from SQL:

<pre><code>sql = """
CREATE TEMPORARY FUNCTION 
<strong>trunc_rand_normal(x FLOAT64, mu FLOAT64, sigma FLOAT64)
</strong>RETURNS FLOAT64
<strong>LANGUAGE js AS "{}";
</strong>
SELECT
  trunc_rand_normal(ARR_DELAY, 90, 15) AS JITTER
FROM
  ...
""".format(js).replace('\n', ' ')
</code></pre>

The resulting distribution of `jitter` might look something like this (the preceding code used a mean of 90 and a standard deviation of 15):

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_11in03.png" alt="Image" height="344" width="490"><figcaption></figcaption></figure>



In order to experiment with different types of jitter, let’s change our simulation code to add random `jitter` to the `notify_time:`[7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn178)

<pre><code><strong>jitter = 'CAST (-LN(RAND()*0.99 + 0.01)*30 + 90.5 AS INT64)'
</strong>  
# run the query to pull simulated events
querystr = """\
SELECT
  EVENT_TYPE,
<strong>  TIMESTAMP_ADD(EVENT_TIME, INTERVAL @jitter SECOND) AS NOTIFY_TIME,
</strong>  EVENT_DATA
FROM
  dsongcp.flights_simevents
WHERE
  EVENT_TIME >= @startTime
  AND EVENT_TIME &#x3C; @endTime
ORDER BY
  EVENT_TIME ASC
"""
job_config = bq.QueryJobConfig(
<strong>       query_parameters=[
</strong>           bq.ScalarQueryParameter("jitter", "INT64", jitter),
           bq.ScalarQueryParameter("startTime", "TIMESTAMP", args.startTime),
           bq.ScalarQueryParameter("endTime", "TIMESTAMP", args.endTime),
       ]
   )
   rows = bqclient.query(querystr, job_config=job_config)
</code></pre>

In the preceding code snippet, I am using parameterized queries in BigQuery by having the query refer to variables as `@jitter`, `@startTime`, and so on. These variables are passed in as query parameters by the Python code.

#### Watermarks and triggers

The Beam programming model implicitly handles out-of-order records within the sliding window, and by default accounts for late arriving records. Beam has the concept of a _watermark_, which is the oldest unprocessed record left in the pipeline. The watermark is an inherent property of any real-time data processing system and is indicative of the lag in the pipeline. Cloud Dataflow tracks and learns this lag over time.

If we are using the time that a record was inserted into Cloud Pub/Sub as the event time, then the watermark is a strict guarantee that no data with an earlier event time will ever be observed by the pipeline after the watermark. On the other hand, if the event time is user-specified (by specifying a `timestampLabel`), then there is nothing to prevent the publishing program from inserting a really old record into Cloud Pub/Sub, so the watermark is a learned heuristic based on the observed historical lag. The concept of a watermark is more general than Cloud Pub/Sub, of course—in the case of streaming sources (such as low-power Internet of Things devices) that are intermittent, watermarking helps those delays as well.

Computation of aggregate statistics is driven by a _trigger_. Whenever a trigger fires, the pipeline calculations are carried out. Our pipeline can include multiple triggers, but each of the triggers is usually keyed off the watermark. The default behavior is that the trigger fires when the watermark passes the end of the window and then immediately whenever any late data arrives. In other words, every late-arriving record is processed individually. This prioritizes correctness over performance.

What if we add a uniformly distributed `jitter` to the simulation? Since our uniform delay is in the range of 90–120 s, the actual difference in delay between the earliest-arriving and latest-arriving records is 30 s. So, Cloud Dataflow has to keep windows open 30 s longer.

The Cloud Dataflow job monitoring web page on the Cloud Platform Console indicates the learned watermark value. We can click on any of the transform steps to view what this value is. And with a uniform delay added to the simulation, the monitoring console shows us that this is what is happening:

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_11in04.png" alt="Image" height="134" width="600"><figcaption></figcaption></figure>



We see that the simulation (righthand side of the snapshot) is sending events at 00:12:32 UTC, whereas the watermark shown by the monitoring console is at 17:11:50 Pacific Standard Time. Ignoring the 7 hours due to time zone conversion, Cloud Dataflow is keeping windows open for 42 s longer (this includes the system lag of 7 s, which is the time taken to process the records).

Unlike uniform jitter, small delays are far more likely than larger delays in exponentially distributed jitter. With exponentially distributed `jitter` added to the simulated data in the Cloud Pub/Sub pipeline, the learned watermark value is 22 seconds:

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_11in05.png" alt="Image" height="129" width="600"><figcaption></figcaption></figure>



Recall that the default trigger prioritizes correctness over performance, processing each late-arriving record one-by-one and updating the computed aggregates. Fortunately, changing this trade-off is quite easy. Here is a different trade-off:

```
beam.WindowInto(
            beam.window.SlidingWindows(...),
            trigger=AfterWatermark(late=AfterCount(10)),
            allowed_lateness=1800) # 30 minutes
```

Here, the calculations are triggered at the watermark (as before). Late records are processed 10 at a time but only if they arrive within 30 minutes after the start of the plane. Beyond that, late records are thrown away.

### Possible Streaming Sinks

Streaming the output flight records to BigQuery is acceptable for my flight delay scenario, but it may not be the right choice for your data pipeline. You should select the output sink based on four factors—access pattern, transactions, throughput, and latency.

If your primary access pattern is around long-term storage and delayed access to the data, you could simply stream to sharded files on Cloud Storage. Files on Cloud Storage can serve as staging for later import into Cloud SQL or BigQuery for later analysis of the data. In the rest of this section, I will assume that you will need to query the data in near-real time.

Recall that we receive several events for each flight—`departed`, `wheelsoff`, etc. Should we have a single row for each flight that reflects the most up-to-date state for that flight? Or can the data be append-only so that we simply keep storing flight events as they come streaming in? Is it acceptable for readers to possibly get slightly out-of-date records, or is it essential for there to be a single source of truth and consistent behavior throughout the system? The answers to these questions determine whether flight updates have to be transactional, or whether flight updates can be done in an environment that provides only eventual consistency guarantees.

How many flight events come in every second? Is this rate constant, or are there periods of peak activity? The answers here determine the throughput that the system needs to handle. If we are providing for eventual consistency, what is the acceptable latency? Once flight data is added to the database, within what time period should all readers see the new data? At the time of writing, [streaming into BigQuery supports](https://oreil.ly/kSryY) up to 1 GBps with latency on the order of a few seconds. You can achieve a latency on the order of milliseconds by turning on [BI Engine in BigQuery.](https://oreil.ly/GMjhW) To do so, all we have to do is to go to the BigQuery administration console and purchase a BI Engine capacity reservation (see [Figure 11-9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#use\_bi\_engine\_to\_provide\_clients\_lower)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1109.png" alt="" height="508" width="600"><figcaption></figcaption></figure>

**Figure 11-9. Use BI Engine to provide clients lower latencies when streaming data into BigQuery.**

For throughput needs that are higher than this, or latency requirements that are lower, we need to consider other solutions.

#### Choosing a sink

During development, we used Cloud Storage, but as depicted in [Figure 11-8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#the\_streaming\_pipeline\_writes\_out\_pred), we want the pipeline to write the predictions to a database. What database shall we use?

If transactions are not needed, and we simply need to append flight events as they come in, we can use BigQuery, text files, or Cloud Bigtable:

* As stated in the previous section, streaming flight events directly into BigQuery is a great solution for throughputs of up to 1 GBps and acceptable latencies of a few milliseconds. Many dashboard applications fall into this sweet spot.
* Cloud Dataflow also supports [streaming into text files](https://oreil.ly/DAnpd) on Cloud Storage. This is obviously useful if the primary use case is to simply save the data, not to analyze it or query it. Recall that we want client programs such as mobile applications to be able to query for a specific flight. So, this won’t work for our current use case. However, it is also a solution to consider if periodic batch updates into BigQuery will suffice. For example, we could stream into text files that are sharded by hour, and at the end of the hour, we could do a batch upload of the file into BigQuery. This is less expensive than streaming into BigQuery and can be used if hourly latencies are acceptable.
* Cloud Bigtable is a massively scalable NoSQL database service—it can handle workloads that involve hundreds of petabytes with millions of reads and writes per second at a latency that is on the order of milliseconds. Moreover, the throughput that can be handled by Cloud Bigtable scales linearly with the number of nodes—for example, if a single node supports 10,000 reads or writes in 6 ms, a Cloud Bigtable instance with a hundred nodes will support a million reads or writes in the same 6 ms interval. In addition, Cloud Bigtable automatically rebalances the data to improve query performance and availability.

On the other hand, if transactions are needed and you wish to have a single record that reflects the most current state of a flight, we could use a traditional relational database, a NoSQL transactional database, or Cloud Spanner:

* Cloud SQL, which is backed by MySQL or PostgreSQL, is useful for frequently updated, low-throughput, medium-scale data that you want to access from a variety of tools and programming languages in near-real time. Because relational technologies are ubiquitous, the tools ecosystem tends to be strongest for traditional relational databases. For example, if you have third-party, industry-specific analysis tools, it is possible that relational databases might be the only storage mechanism that they will connect to. Before choosing a traditional relational database solution, though, consider whether the use case is such that you will run into throughput and scaling limitations.
* You can scale to much larger datasets (terabytes of data) and avoid the problem of converting between hierarchical objects and flattened relational tables by using Cloud Firestore, which is a NoSQL object store. Cloud Firestore provides high throughput and scaling by designing for eventual consistency. However, it is possible to get strong (or immediate) consistency on queries that involve lookups by key or “ancestor queries” that involve entity groups. Within an entity group, one gets transactions, strong consistency, and data locality. Thus, it is possible to balance the need for high throughput and many entities while still supporting strong consistency where it matters.
* Cloud Spanner provides a strongly consistent, transactional, SQL-queryable database that is nevertheless globally available and can scale to extremely large amounts of data. Cloud Spanner offers latency on the order of milliseconds, extremely high availability (99.999% availability, which translates to downtimes of around 5 minutes a year), and maintains transactional consistency and global reach. Cloud Spanner is also fully managed, without the need for manual intervention for replication or maintenance.

In our use case, we don’t need transactions. Our incoming stream has fewer than a thousand events per second. A few seconds latency between insertion into the database and availability to applications that need the flight delay information is quite tolerable because what we might do is to simply send alerts to our users if their flight is likely to be delayed. BigQuery is fully managed, supported by many data visualization and report-creation products, and is relatively inexpensive compared to the alternative choices. Based on these considerations, streaming into BigQuery is the right choice for our use case.

#### Cloud Bigtable

However, just as a hypothetical scenario, what if our stream consisted of gigabytes of flight events per second, and our use case required that the latency be on the order of milliseconds, not seconds? This would be the case if each aircraft were to provide up-to-the-minute coordinate information while it is en route, and if the use case involved traffic control of the air space. In such a case, Cloud Bigtable would be a better choice. Let’s look at how we’d build the pipeline to write to Cloud Bigtable if this were the case.

Cloud Bigtable separates compute and storage. Tables in Cloud Bigtable are sharded into blocks of contiguous rows, called tablets. The Cloud Bigtable instance doesn’t store these tablets; instead, it stores pointers to a set of tablets. The tablets themselves are durably stored on Cloud Storage. Because of this, a node may go down, but the data remains in Cloud Storage (see [Figure 11-10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#cloud\_bigtable\_tables\_sharded\_into\_tabl)). Work may get rebalanced to a different node, and only metadata needs to be copied.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1110.png" alt="" height="224" width="600"><figcaption></figcaption></figure>

**Figure 11-10. Cloud Bigtable tables sharded into tablets.**

The data itself consists of a sorted key/value map (each row has a single key). Unlike BigQuery, Cloud Bigtable’s storage is row-wise, and the rows are stored in sorted order of their key value. Columns that are related are grouped into a _column family_, with different column families typically managed by different applications. Within a column family, columns have unique names. A specific column value at a specific row can contain multiple cells at different timestamps (the table is append-only, so all the values exist in the table). This way, we can maintain a time-series record of the value of that cell over time. For most purposes, Cloud Bigtable doesn’t care about the data type—all data is treated as raw byte strings.[8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn179)

The performance of Cloud Bigtable is best understood in terms of the arrangement of rows within a tablet (blocks of contiguous rows into which tables in Cloud Bigtable are sharded). The rows are sorted in order of the keys. To optimize the write performance of Cloud Bigtable, we want to have multiple writes happening in parallel, so that each of the Cloud Bigtable instances is writing to its own set of tablets. This is possible if the row keys do not follow a predictable order. The read performance of Cloud Bigtable is, however, more efficient if multiple rows can be read at the same time. Striking the right balance between the two contradictory goals (of having writes be distributed while making most reads be concentrated) is at the heart of effective Cloud Bigtable schema design.

**Designing tables**

At the extremes, there are two types of designs of tables in Cloud Bigtable. Short and wide tables take advantage of the sparsity of Cloud Bigtable, while tall and narrow tables take advantage of the ability to search row keys by range.

Short and wide tables use the presence or absence of a column to indicate whether or not there is data for that value. For example, suppose we run an automobile factory and the primary query we wish to support with our dataset is to determine the attributes of the parts (part ID, supplier, manufacture location, etc.) that make up a specific automobile. Imagine that we will have millions of cars, each with hundreds of thousands of parts. We could use the car serial number as the row key, and each unique part (e.g., a spark plug) could have a column associated with it (see [Figure 11-11](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#short\_and\_wide\_table\_for\_auto\_partsdot)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_1111.png" alt="" height="143" width="600"><figcaption></figcaption></figure>

**Figure 11-11. Short and wide table for auto parts.**

Each row then consists of many events and is updated as the automobile is put together on the assembly line. Because cells that have no value take up no space, we don’t have to worry about the proliferation of columns over time as new car models are introduced. Because we will tend to receive events from automobiles being manufactured at the same time, we should ensure that automobile serial numbers are not consecutive, but instead start with the assembly line number. This way, the writes will happen on different tablets in parallel, and so the writes will be efficient. At the same time, diagnostic applications troubleshooting a quality issue will query for all vehicles made on the same line on a particular day, and will therefore tend to pull consecutive rows. Service centers may be interested in obtaining all the parts associated with a specific vehicle. Because the vehicle ID is the row key, this requires reading just a single row, and so, the read performance of such a query will also be very efficient.

Tall and narrow tables often store just one event per row. Every flight event that comes in could get streamed to a new row in Cloud Bigtable. This way, we can have multiple states associated with each flight (departed, wheels-off, etc.) and the historical record of these. At the same time, for each flight, we have only 20 or so fields, all of which can be part of the same column family. This makes the streaming updates easy and intuitive.

**Designing the row key**

Although the table design of one row per event is very intuitive, we need to design the row key in a way that both writes and reads are efficient. To make the reads efficient, consider that the most common queries will involve recent flights between specific airports on a specific carrier (e.g., the status of today’s flight between SEA and SJC on AS). Using multiple rows, with a single version of an event in each row, is the simplest way to represent, understand, and query your data. Tall and narrow tables are most efficient if common queries involve just a scan range of rows. This can be achieved if the origin and destination airports are part of the key, as is the carrier. Thus, our row key can start with:

```
ORIGIN#DEST#CARRIER
```

Having the row key start with these three fields also helps with optimizing write performance. While the tablets associated with busy airports like Atlanta might get some amount of hotspotting, the overload will be counteracted by the many sleepy airports whose names also start with the letter A. An alphabetical list of airports should therefore help distribute the write load. Notice that I have the carrier at the end of the list—putting the carrier at the beginning of the row key will have the effect of overloading the tablets that contain the larger airlines (American and United); because there are only a dozen or so carriers, there is no chance of this load being counteracted by smaller carriers.

Because common queries will involve the latest data, scan performance will be improved if we could store the most current data at the top of the table. Using the timestamp in the most intuitive way: `2017-04-12T13:12:45Z` will have the opposite effect. The latest data will be at the bottom of the table. Therefore, we need to store timestamps in reverse order somehow. One way would be to convert timestamps to the number of milliseconds since 1970, and then to compute the difference of that timestamp from the maximum possible long value: (`LONG_MAX - milliseconds​Sin⁠ceEpoch)`.

Where should the timestamp go? Having the timestamp at the beginning of the row key would cause the writing to get focused on just one tablet at a time. So, the timestamp needs to be at the end of the row key. In summary, then, our row key will be of the form:

```
ORIGIN#DEST#CARRIER#ReverseTimeStamp
```

But which timestamp? We’d like all the events from a particular flight to have the same row key, so we’ll use the _scheduled_ departure time in the key. This avoids problems associated with the key being different depending on the departure delay.

**Streaming into Cloud Bigtable**

Creating a Cloud Bigtable instance to stream flight events into can be done using `gcloud`:

```
gcloud bigtable \
    instances create flights \ 
    --cluster=datascienceongcp --cluster-zone=us-central1-b \
    --description="" --instance-type=DEVELOPMENT
```

The name of my instance is `flights`, and the name of the cluster of machines is `datascienceongcp`. By choosing a development instance type, I get to limit the costs—the cluster itself is not replicated or globally available.

To stream into Cloud Bigtable from Bigtable, we need to create a set of Cloud Bigtable _mutations_ (each mutation consists of a change to a single cell):

```
class CreateRowFn(beam.DoFn):
    def process(self, event):
        key = "{}#{}#{}#{}".format(event['origin'],
event['dest'],event['carrier'],reverse_ts(event['event_time']))
        result = [] 
        for name, value in event.items():
            direct_row = row.DirectRow(row_key=key)
            direct_row.set_cell(
               name, value, event['event_time']))
            result.append(direct_row)
        return result

preds | 'to_bigtable' >> beam.io.WriteToBigTable(
            project_id=PROJECT,
            instance_id='flights',
            table_id='predictions'
                        
)
```

With these changes to the pipeline code, flight predictions from our pipeline can get streamed to Cloud Bigtable.

**Querying from Cloud Bigtable**

One of the conveniences of using BigQuery as the sink was the ability to carry out analytics using SQL even while the data was streaming in. Cloud Bigtable also provides streaming analytics, but not in SQL. Because Cloud Bigtable is a NoSQL store, the typical use case involves hand-coded client applications. We can, however, use an [HBase shell](https://oreil.ly/8oyry) to interrogate the contents of our table.

For example, we can get the latest row in the database by doing a table scan and limiting it to one:

<pre><code><strong>scan 'predictions', {'LIMIT' => 1}
</strong>hbase(main):006:0> scan 'predictions', {'LIMIT' => 1}
ROW                              COLUMN+CELL                                                                                
<strong> ABE#ATL#DL#9223370608969975807  column=FL:AIRLINE_ID, timestamp=1427891940,            
</strong><strong> ABE#ATL#DL#9223370608969975807  column=FL:ARR_AIRPORT_LAT, timestamp=1427891940                     
</strong><strong> ABE#ATL#DL#9223370608969975807  column=FL:ARR_AIRPORT_LON, timestamp=1427891940
</strong>...
</code></pre>

Because the rows are sorted in ascending order of the row key, they end up being arranged by origin airport, destination airport, and reverse timestamp. That is why we get the most current flight between two airports that start with the letter A. The command-line shell outputs one line per cell, so we get several lines even though the lines all refer to the same row (note that the row key is the same).

The advantage of the way we designed the row key is to be able to get the last few flights between a pair of airports. For example, here are `ontime` and `EVENT` columns of the latest two flights between O’Hare airport in Chicago (`ORD`) and Los Angeles (`LAX`) flown by American Airlines (`AA`):

<pre><code><strong>scan 'predictions', {STARTROW => 'ORD#LAX#AA', ENDROW => 'ORD#LAX#AB', COLUMN => 
</strong><strong>ROW                                          COLUMN+CELL                                                                                                                                            
</strong><strong> ORD#LAX#AA#9223370608929475807              column=FL:EVENT, timestamp=142792                                                                   
</strong><strong> ORD#LAX#AA#9223370608929475807              column=FL:ontime, timestamp=142792                                                                            
</strong><strong> ORD#LAX#AA#9223370608939975807              column=FL:EVENT, timestamp=142793
</strong><strong> ORD#LAX#AA#9223370608939975807              column=FL:ontime, timestamp=142793
</strong></code></pre>

Notice that the `arrived` event has the actual on-time performance (1.00), while the `wheelsoff` event has the predicted on-time arrival probability (0.73). It is possible to write an application that uses a Cloud Bigtable client API to do such queries on the Cloud Bigtable table and surface the results to end users.

## Summary

In this chapter, we augmented the machine learning training dataset with time-windowed aggregate features. We computed the average departure and taxi-out delays at the origin airport over the previous hour. This required us to compute a moving average.

Apache Beam allows us to compute a time-windowed moving average on historical data in the same way as we would on streaming data in real time. Cloud Dataflow allows us to execute data pipelines written using Beam on Google Cloud Platform in a serverless way.

In order to compute the average arrival delay, we need to emit records with a timestamp. We discovered that we had a logical error of repeated lines in our output. This turned out to be because we were computing the arrival delay on sliding windows, and these windows caused each flight object to be present in 12 windows. The solution was to determine the slice of the window that the flight object was in and to emit the flight only if it was the latest slice of the window. With these changes, the pipeline was logically correct and the entire training and evaluation datasets were created.

Invoking a model that requires moving averages requires that the model be invoked from a streaming pipeline that continually computes these moving averages. Sending requests for flights one-at-a-time could prove costly in terms of networking, money, and time. Fortunately, the mechanism that we use to invoke the service batches up the requests to the machine learning service from within our Cloud Dataflow pipeline.

Our pipeline reads data from Cloud Pub/Sub and processes it using code that is identical to that used in training. Using the same code for serving as we used in training helps us mitigate training–serving skew. We also employed watermarks and triggers to get a finer control on how to deal with late-arriving, out-of-order records.

We explored other possible streaming sinks and how to choose between them. As a hypothetical, we considered a Cloud Bigtable sink to work in situations where high throughput and low latency are required. We designed an appropriate row key to parallelize the writes while getting speedy reads.

## Suggested Resources

The Apache Beam documentation includes a section on [common pipeline patterns](https://oreil.ly/miwXT). These merit careful reading. For example, I learned from the [deadletter pattern](https://oreil.ly/K4eKk) that the `WriteToBigQuery` transform that I’ve been using for donkey’s years tells you which rows that it failed to write.

Another list worth periodically reviewing is the list of [built-in transforms](https://oreil.ly/iaeqf) in Apache Beam.

[_Streaming Systems_](https://www.oreilly.com/library/view/streaming-systems/9781491983867/) by Tyler Akidau et al. (O’Reilly) is a good way to develop a foundation in streaming systems concepts.

[1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn172-marker) Of course, Cloud SQL provides for millisecond-latency transactions while BigQuery is an analytics data warehouse that scales to petabytes. The reasons you’d use the two products are different. I’m just comparing explicit control of the lifecycle of machines in one versus the other.

[2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn173-marker) Obviously we don’t want to assign all the rows the time at which we read the data!

[3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn174-marker) Look at the size, not the number of elements in the collection—the number of elements is the number of unique airports within each time window. It’s hard to compare the number of flights (the input) to the number of airports (the output), but because the data does include (airport, list-of-flights), we can verify that the list of flights is much larger than the flights that we started out with.

[4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn175-marker) Besides, it’s [Chapter 11](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#time\_windowed\_features\_for\_real\_time\_ma), and the editor is getting impatient! I’ll fix this in the 3rd edition.

[5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn176-marker) I am assuming that the number of users of our flight delay prediction service will be a factor of magnitude more than the number of flights. This is optimistic, of course, but it is good to design assuming that we will have a successful product.

[6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn177-marker) If we had a real-time feed, we’d of course collect data on delay instead of simply guessing.

[7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn178-marker) See the `jitter` variable in _04\_streaming/simulate/simulate.py_.

[8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#ch01fn179-marker) The exception is for operations like an atomic increment, where Cloud Bigtable expects the data to be an integer.
