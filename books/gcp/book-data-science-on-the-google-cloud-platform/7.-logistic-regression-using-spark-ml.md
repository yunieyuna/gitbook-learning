# 7. Logistic Regression Using Spark ML

## Chapter 7. Logistic Regression Using Spark ML

In [Chapter 6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch06.html#bayesian\_classifier\_with\_apache\_spark\_o), we created a model based on two variables—distance and departure delay—to predict the probability that a flight will be more than 15 minutes late. We found that we could get a finer-grained decision if we used a second variable (distance) instead of using just one variable (departure delay).

Why not use all the variables in the dataset? Or at least many more of them? In particular, I’d like to use the `TAXI_OUT` variable—if it is too high, the flight will be stuck on the runway waiting for the airport tower to allow the plane to take off, and so the flight is likely to be delayed. The Naive Bayes approach in [Chapter 6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch06.html#bayesian\_classifier\_with\_apache\_spark\_o) was quite limiting in terms of being able to incorporate additional variables. As we add variables, we would need to continue slicing the dataset into smaller and smaller bins. We would then find that many of our bins would contain very few samples, resulting in decision surfaces that would not be well behaved. Remember that, after we binned the data by distance, we found that the departure delay decision boundary was quite well behaved—departure delays above a certain threshold were associated with the flight not arriving on time. Our simplification of the Bayesian classification surface to a simple threshold that varied by bin would not have been possible if the decision boundary had been noisier.[1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn131) The more variables we use, the more bins we will have, and this good behavior will begin to break down. This sort of breakdown in good behavior as the number of variables (or _dimensions_) increases is called the _curse of dimensionality_; it affects many statistical and machine learning techniques, not just the quantization-based Bayesian approach of [Chapter 6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch06.html#bayesian\_classifier\_with\_apache\_spark\_o).

All of the code snippets in this chapter are available in the folder [_07\_sparkml_ of the GitHub repository](https://github.com/GoogleCloudPlatform/data-science-on-gcp). See the _README.md_ file in that directory for instructions on how to do the steps described in this chapter.

## Logistic Regression

One way to address the breakdown in behavior as the number of variables increases is to change the approach from that of directly evaluating the probability based on the input dataset. Instead, we could attempt to fit a smooth function on the variables in the dataset (a multidimensional space) to the probability of a flight arriving late (a single-dimensional space) and use the value of that function as the estimate of the probability. In other words, we could try to find a function `f`, such that:

�(�)≈�(�0,�1,...,��-1)

In our case, `x0` could be the departure delay, `x1` the taxi-out time, `x2` the distance, and so on. Each row will have different values for the `x`’s and represent different flights. The idea is that we have a function that will take these `x`’s and somehow transform them to provide a good estimate of the probability that the flight corresponding to that row’s input variables is on time.

### How Logistic Regression Works

One of the simplest transformations of a multidimensional space to a single-dimensional one is to compute a weighted sum of the input variables, as demonstrated here:

�=�0�0+�1�1+...+��-1��-1+�

The `w`’s (called the weights) and the constant `b` (called the intercept) are constants, but we don’t initially know what they are. We need to find “good” values of the `w`’s and `b` such that the weighted sum for any row closely approximates either 1 (when the flight is on time) or 0 (when the flight is late). Because this process of finding good values is averaged over a large dataset, the value `L` is the prediction that flights with a specific departure delay, taxi-out time, and so on will arrive on time. If that number is 0.8, we would like it to be that 80% of such flights would arrive on time and 20% would be late. In other words, rather than `L` being simply 1 or 0, we’d like it to be the probability that the flight will arrive on time.

There is a problem, though. The preceding weighted sum cannot function as a probability. This is because the linear combination (`L`) can take any value, whereas a probability will need to lie between 0 and 1. One common solution for this problem is to transform the linear combination using the _logistic_ function:

�(�)=11+�-�

Fitting a logistic function of a linear combination of variables to binary outcomes (i.e., finding “good” values for the `w`’s and `b` such that the estimated `P`(`Y`) are close to the actual recorded outcome of the flight being on time) is called _logistic regression_.

In machine learning, the original linear combination, `L`, which lies between −∞ and ∞, is called the _logit_. You can see that if the logit adds up to ∞, `e–L` will be 0 and so, `P`(`Y`) will be 1. If the original linear combination adds up to −∞, then `e–L` will be ∞ and so, `P`(`Y`) will be 0. Therefore, `Y` could be an event such as the flight arriving on time and `P`(`Y`), the probability of that event happening. Because of the transformation, `P(Y)` will lie between 0 and 1 as required for anything to be a probability.

If `P`(`Y`) is the probability, the logit, `L`, is given by the following:

�����(�)1-�(�)

The _odds_ is the ratio of the probability of the event happening, `P(Y)`, to the probability of the event not happening, 1 − `P`(`Y`). Therefore, the logit can also be interpreted as the log-odds where the base of the logarithm is `e`.

[Figure 7-1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#the\_relationships\_among\_the\_probability) depicts the relationship between the logit, the probability, and the odds.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0701.png" alt="" height="184" width="600"><figcaption></figcaption></figure>

**Figure 7-1. The relationships among the probability, the logit, and the odds.**

Spend some time looking at the graphs in [Figure 7-1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#the\_relationships\_among\_the\_probability) and gaining an intuitive understanding for what the relationships mean. For example, see if you can answer this set of questions (feel free to sketch out the curves as you do your reasoning):[2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn132)

* At equal odds (i.e., the flight is as likely to be late as not), what is the logit? How about if the odds are predominantly in favor of a flight being on time?
* At what probability does the logit function change most rapidly?
* Where is the gradient (rate of change) of the logit function slowest?
* Which logit change, from 2 to 3 or from 2 to 1, will result in a greater change in probability?
* How does the value of the intercept, `b`, affect the answer to Question 4?
* Suppose the intercept is zero. If all the input variables double, what happens to the logit?
* If the logit value doubles, what happens to the probability? How does this depend on the original value of the logit?
* What logit value does it take to provide a probability of 0.95? How about 0.995?
* How extreme do the input variables have to be to attain probabilities that are close to zero or one?

Many practical considerations in classification problems in machine learning derive from this set of relationships. So, as simple as these curves are, it is important that you understand their implications.

The name _logistic regression_ is a little confusing—regression is normally a way of fitting a real-valued number, whereas classification is a way of fitting to a categorical outcome. Here, we fit the observed variables (the `x`’s) to a logit (which is real-valued)—this is the regression that is being referred to in the name. However, we then use the logistic function that has no free parameters (no weights to tune) to transform the real-valued number to a probability. Overall, therefore, logistic regression functions as a classification method.

### Spark ML Library

Given a multivariable dataset, Spark has the ability to carry out logistic regression and give us the optimal weight for each variable. Spark’s logistic regression module will give us the `w`’s and `b` if we show it a bunch of `x`’s and the corresponding `Y`’s. The logistic regression module is part of Apache Spark’s machine learning library, MLlib, which you can program against in Java, Scala, Python, or R. Spark MLlib (colloquially known as Spark ML) includes implementations of many canonical machine learning algorithms: decision trees, random forests, alternating least squares, `k`-means clustering, association rules, support vector machines, and so on. Spark can execute on a Hadoop cluster; thus, it can scale to large datasets.

The problem we are solving—to find a set of weights that optimizes model predictions based on known outcomes—is an instance of a _supervised_ learning problem. In supervised learning problems, the actual answers, called _labels_, need to be known for some dataset. As illustrated in [Figure 7-2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#in\_supervised\_learningcomma\_the\_machine), first you ask the machine (here, Spark) to learn (the `w`’s) from data (the `x`’s) that has labels (the `Y`’s). This is called _training_.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0702.png" alt="" height="275" width="600"><figcaption></figcaption></figure>

**Figure 7-2. In supervised learning, the machine (here, Spark) learns a set of parameters (the `w`’s and `b`’s) from training data that consists of inputs (`x`’s) and their corresponding labels (`Y`’s).**

The learned set of weights, along with the original equation (the logistic function of a linear combination of `x`’s), is called a _model_. After you have learned a model from the training data, you can save it to a file. Then, whenever you want to make a prediction for a new flight, you can re-create the model from that file, pass in the `x`’s in the same order, and compute the logistic function and obtain the estimate `P`(`Y`). This process, called _prediction_, might be carried out in real time in response to a request that includes the input variables, whereas the training of the model might happen less frequently, as shown in [Figure 7-3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#you\_can\_use\_the\_learned\_set\_of\_weights).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0703.png" alt="" height="339" width="600"><figcaption></figcaption></figure>

**Figure 7-3. You can use the learned set of weights to predict the value of `Y` for new data (`x`’s).**

Of course, the prediction can be carried outside of Spark—all that we’d need to do is compute the weighted sum of the inputs and the intercept, and then compute the logistic function of the weighted sum. In general, though, you should seek to use the same libraries for prediction as you use for training. This helps to mitigate _training–serving skew_, the situation that we talked about in [Chapter 2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch02.html#ingesting\_data\_into\_the\_cloud) in which the input variables in prediction are subtly different from those used in training and which leads to poorly performing models.

### Getting Started with Spark Machine Learning

To run Spark conveniently, I will continue to use the Cloud Dataproc cluster that I launched in the previous chapter. In case you deleted it, start a new one using:

```
cd 06_dataproc
./create_cluster.sh bucketname region
```

Even though we are going to develop the logistic regression code in a Jupyter Notebook, we should keep in mind that our end goal is to run the machine jobs routinely. To achieve that goal, it is important to keep a notebook with the final machine learning workflow and export this notebook into a standalone program. You can submit the standalone program to the cluster whenever the machine learning needs to be repeated on new datasets.

As in [Chapter 6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch06.html#bayesian\_classifier\_with\_apache\_spark\_o), we open the notebook from the Web Interfaces section of the Google Cloud console. Within the notebook, we start out creating a `SparkContext` variable `sc` and the `SparkSession` variable `spark`:[3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn133)

```
from pyspark.sql import SparkSession
from pyspark import SparkContext
sc = SparkContext('local', 'logistic')
spark = SparkSession \
    .builder \
    .appName("Logistic regression w/ Spark ML") \
    .getOrCreate()
```

After we do this, any line that works in the interactive shell will also work when launched from the notebook _logistic\_regression.ipynb_ or the script _logistic.py_. The application name (`logistic`) will show up in logs when the script is run.

## Spark Logistic Regression

The logistic regression implementation, `L-BFGS`, is in _pyspark.mllib_ and is named for the initials of the independent inventors (Broyden, Fletcher, Goldfarb, and Shanno)[4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn134) of a popular, iterative, fast-converging optimization algorithm. The L-BFGS algorithm is used by Spark to find the weights that minimize the _logistic loss_ function:

����(1+�-��)

over the training dataset, where `y`, the training label, is either −1 or 1 and `L` is the logit computed from the input variables, the weights, and the intercept.

So, let’s begin by adding `import` lines for the Python classes that we’ll need:

```
from pyspark.mllib.classification import LogisticRegressionWithLBFGS
from pyspark.mllib.regression import LabeledPoint
```

Knowing the details of the logistic formulation and loss function used by Spark is important. This is because different machine learning libraries use equivalent (but not identical) versions of these formulae. For example, another common approach taken by machine learning frameworks is to minimize the _cross-entropy_:

�-�����(�)-(1-�)log(1-�(�)))

Here, `y` is the training label, and `P`(`Y`) is the probabilistic output of the model. In that case, the training label will need to be 0 or 1. I won’t go through the math, but even though the two loss functions look very different, minimizing the logistic loss and minimizing the cross-entropy loss turn out to be equivalent.

Rather confusingly, the Spark documentation notes that “a binary label `y` is denoted as either +1 (positive) or −1 (negative), which is convenient for the formulation. However, the negative label is represented by 0 in _spark.mllib_ instead of −1, to be consistent with multiclass labeling.”[5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn135) In other words, Spark ML uses the logistic loss function, but requires that the labels we provide be 0 or 1. There really is no substitute for reading the documentation!

To summarize, the preprocessing that you might need to do to your input data depends on the formulation of the loss function employed by the machine learning framework you are using. Suppose that we decide to do this:

> y = 0 if arrival delay ≥ 15 minutes
>
> y = 1 if arrival delay < 15 minutes

Because we have mapped on-time flights to 1, the machine learning algorithm (after training) will predict the probability that the flight is on time.

### Creating a Training Dataset

First, let’s read in the list of training days. To do that, we need to read _trainday.csv_ from Cloud Storage, remembering that the comma-separated value (CSV) file has a header that will help with inferring its schema:

```
traindays = spark.read \
    .option("header", "true") \
    .csv('gs://{}/flights/trainday.csv'.format(BUCKET))
```

For convenience, I’ll make this a Spark SQL view, as well:

```
traindays.createOrReplaceTempView('traindays')
```

We can print the first few lines of this file:

```
spark.sql("SELECT * from traindays LIMIT 5").show()
```

This obtains the following eminently reasonable result:

```
+----------+------------+
|   FL_DATE|is_train_day|
+----------+------------+
|2015-01-01|        True|
|2015-01-02|       False|
|2015-01-03|       False|
|2015-01-04|        True|
|2015-01-05|        True|
+----------+------------+
```

While we’re developing the code (on my minimal Hadoop cluster that is running Jupyter), it would be easier to read only a small part of the dataset. Hence, we define the `inputs` variable to be just one of the shards:

```
inputs = 'gs://{}/flights/tzcorr/all_flights-00000-*'.format(BUCKET)
```

After we have developed all of the code, we can change the inputs to the full dataset:

```
#inputs = 'gs://{}/flights/tzcorr/all_flights-*'.format(BUCKET)  # FULL
```

For now, though, let’s leave the latter line commented out. We can read in the `flights` dataset as we did in [Chapter 6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch06.html#bayesian\_classifier\_with\_apache\_spark\_o):

```
flights = spark.read.json(inputs)
flights.createOrReplaceTempView('flights')
```

Training will need to be carried out on the flights that were on days for which `is_train_day` is `True`:

<pre><code>trainquery = """
SELECT
  f.*
<strong>FROM flights fJOIN traindays t
</strong><strong>ON f.FL_DATE == t.FL_DATE
</strong><strong>WHERE
</strong><strong>  t.is_train_day == 'True'
</strong>"""
traindata = spark.sql(trainquery)
</code></pre>

#### Dealing with corner cases

Let’s verify that `traindata` does contain the data we need. We can look at the first few (here, the first two) rows of the dataframe using the following:

```
traindata.head(2)
```

The result seems quite reasonable:

```
[Row(ARR_AIRPORT_LAT=33.43416667, ARR_AIRPORT_LON=-112.01166667,
ARR_AIRPORT_TZOFFSET=-25200.0, ARR_DELAY=-16.0, ARR_TIME='2015-07-28T18:20:00', 
CANCELLED=False, CRS_ARR_TIME='2015-07-28T18:36:00', 
CRS_DEP_TIME='2015-07-28T17:05:00', DEP_AIRPORT_LAT=33.9425, 
DEP_AIRPORT_LON=-118.40805556, DEP_AIRPORT_TZOFFSET=-25200.0, DEP_DELAY=-3.0, 
DEP_TIME='2015-07-28T17:02:00', DEST='PHX', DEST_AIRPORT_SEQ_ID='1410702',
DISTANCE='370.00', DIVERTED=False, FL_DATE='2015-07-28', ORIGIN='LAX', 
ORIGIN_AIRPORT_SEQ_ID='1289203', TAXI_IN=6.0, TAXI_OUT=14.0, 
UNIQUE_CARRIER='AA', WHEELS_OFF='2015-07-28T17:16:00', 
WHEELS_ON='2015-07-28T18:14:00'),
```

Date fields are dates, and airport codes are reasonable, as are the latitudes and longitudes. But eyeballing is no substitute for truly verifying that all of the values exist.

So, let’s restrict the query to fields we want:

<pre><code>SELECT
<strong>  DEP_DELAY, TAXI_OUT, ARR_DELAY, DISTANCE
</strong>FROM flights f
…
</code></pre>

Knowing that the four variables we are interested in are all floats, we can ask Spark to compute simple statistics over the full dataset:

```
traindata.describe().show()
```

The `describe()` method computes column-by-column statistics, and the `show()` method causes those statistics to be printed. We now get the following:[6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn136)

<pre><code>+-------+----------+---------+----------+---------+
|summary| DEP_DELAY| TAXI_OUT| ARR_DELAY| DISTANCE|
+-------+----------+---------+----------+---------+
<strong>|  count|    259692|   259434|    258706|   275062|
</strong>|   mean|    13.178|  16.9658|    9.7319| 802.3747|
| stddev|   41.8886|  10.9363|   45.0384|  592.254|
|    min|     -61.0|      1.0|     -77.0|     31.0|
|    max|    1587.0|    225.0|    1627.0|   4983.0|
+-------+----------+---------+----------+---------+
</code></pre>

Notice anything odd?

Notice the count statistic. There are 275,062 `DISTANCE` values, but only 259,692 `DEP_DELAY` values, and even fewer `TAXI_OUT` values. What is going on? This is the sort of thing that you will need to chase down to find the root cause. In this case, the reason has to do with flights that are scheduled but never leave the gate and flights that depart the gate but never take off. Similarly, there are flights that take off (and have a `TAXI_OUT` value) but are diverted and do not have an `ARR_DELAY`. In the data, these are denoted by `NULL`, and Spark’s `describe()` method doesn’t count `NULL`s.

We don’t want to use canceled and diverted flights for training either. One way to tighten up the selection of our training dataset would be to simply remove `NULL`s, as shown here:

<pre><code>trainquery = """
SELECT
  DEP_DELAY, TAXI_OUT, ARR_DELAY, DISTANCE
FROM flights f
JOIN traindays t
ON f.FL_DATE == t.FL_DATE
WHERE
  t.is_train_day == 'True' AND
<strong>  f.dep_delay IS NOT NULL AND
</strong><strong>  f.arr_delay IS NOT NULL
</strong>"""

traindata = spark.sql(trainquery)
traindata.describe().show()
</code></pre>

Running this gets us a consistent value of the count across all the columns:

```
|summary|  DEP_DELAY|   TAXI_OUT|   ARR_DELAY|   DISTANCE|
|  count|     258706|     258706|      258706|     258706|
```

However, I strongly encourage you not to do this. Removing `NULL`s is merely fixing the symptom of the problem. What we really want to do is to address the root cause. In this case, you’d do that by removing flights that have been canceled or diverted, and fortunately, we do have this information in the data. So, we can change the query to be the following:

<pre><code>trainquery = """
SELECT
  DEP_DELAY, TAXI_OUT, ARR_DELAY, DISTANCE
FROM flights f
JOIN traindays t
ON f.FL_DATE == t.FL_DATE
WHERE
  t.is_train_day == 'True' AND
<strong>  f.CANCELLED == 'False' AND
</strong><strong>  f.DIVERTED == 'False'
</strong>"""

traindata = spark.sql(trainquery)
traindata.describe().show()
</code></pre>

This, too, yields the same counts as when we threw away the `NULL`s, thereby demonstrating that our diagnosis of the problem was correct.

Discovering corner cases and problems with an input dataset at the time we begin training a machine learning model is quite common. In this case, I knew this problem was coming and was careful to select the `CANCELLED` and `DIVERTED` columns to be part of my input dataset (in [Chapter 2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch02.html#ingesting\_data\_into\_the\_cloud)). In real life, you will need to spend quite a bit of time troubleshooting this, potentially adding new logging operations to your ingest code to uncover the reason that underlies a simple problem. What you should not do is to simply throw away bad values.

Bad values (like NULL) are usually a symptom of a problem. Investigate the issue. Don’t simply discard bad values.

#### Creating training examples

Now that we have the training data, we can look at the documentation for `Lo⁠gist⁠i⁠c⁠RegressionModel` to determine the format of its input. The documentation indicates that each row of the training data needs to be transformed to a `LabeledPoint` whose documentation in turn indicates that its constructor requires a label and an array of features, all of which need to be floating-point numbers.

Let’s create a method that will convert each data point of our dataframe into a _training example_ (an example is a combination of the input features and the true answer):

```
def to_example(raw_data_point):
  return LabeledPoint(
              float(raw_data_point['ARR_DELAY'] < 15), # on-time?
              [ 
                  raw_data_point['DEP_DELAY'], 
                  raw_data_point['TAXI_OUT'], 
                  raw_data_point['DISTANCE'], 
              ])
```

Note that we have created a label and an array of features. Here, the features consist of three numeric fields that we pass in as-is. It is good practice to create a separate method that takes the raw data and constructs a training example because this allows us to fold in other operations as well. For example, we can begin to do preprocessing of the feature values, and having a method to construct training examples allows us to reuse the code between training and evaluation.

After we have a way to convert each raw data point into a training example, we need to apply this method to the entire training dataset. We can do this by mapping the dataset row by row:

```
examples = traindata.map(to_example)
```

### Training the Model

Now that we have a dataframe in the requisite format, we can ask Spark to fit the training dataset to the labels:

```
lrmodel = LogisticRegressionWithLBFGS.train(examples, intercept=True)
```

We’d have specified `intercept = False` if we believed that when all x = 0, the prediction needed to be 0. We have no reason to expect this, so we ask the model to find a value for the intercept.

When the `train()` method completes, the `lrmodel` will have the weights and intercept, and we can print them out:

```
print lrmodel.weights,lrmodel.intercept
```

This yields the following:[7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn137)

```
[-0.164,-0.132,0.000294] 5.1579
```

The `weights` is an array, one for each variable. These numbers, plus the formula for logistic regression, are enough to set up code for the model in any language we choose. Remember that in our labeled points, 0 indicated late arrivals and 1 indicated on-time arrivals. So, applying these weights to the departure delay, taxi-out time, and flight distance of a flight will yield the probability that the flight will be on time.

In this case, it appears that the departure delay has a weight of −0.164. The negative sign indicates that the higher the departure delay, the lower the probability that the flight will be on time (which sounds about right). On the other hand, the sign on the distance is positive, indicating that higher distances are associated with more on-time behavior. Even though we are able to look at the weights and reason with them on this dataset, such reasoning will begin to break down if the variables are not independent. If you have highly correlated input variables, the magnitudes and signs of the weights are very hard to interpret.

Let’s try out a prediction:

```
lrmodel.predict([6.0,12.0,594.0])
```

The result is 1—that is, the flight will be on time when the departure delay is 6 minutes, the taxi-out time is 12 minutes, and the flight is for 594 miles. Let’s change the departure delay from 6 minutes to 36 minutes:

```
lrmodel.predict([36.0,12.0,594.0])
```

The result now is 0—the flight won’t arrive on time.

But wait a minute. We want the output to be a probability, not 0 or 1 (the final label). To do that, we can remove the implicit threshold of 0.5:

```
lrmodel.clearThreshold()
```

With the thresholding removed, we get probabilities. The probability of arriving late increases as the departure delay increases.

By keeping two of the variables constant, it is possible to study how the probability varies as a function of one of the variables. For example, at a departure delay of 20 minutes and a taxi-out time of 10 minutes, this is how the distance affects the probability that the flight is on time:

```
dist = np.arange(10, 2000, 10)
prob = [lrmodel.predict([20, 10, d]) for d in dist]
plt.plot(dist, prob)
```

[Figure 7-4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#how\_the\_distance\_of\_the\_flight\_affects) shows the plot.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0704.png" alt="" height="345" width="504"><figcaption></figcaption></figure>

**Figure 7-4. How the distance of the flight affects the probability of on-time arrival. According to our model, longer flights tend to have higher likelihoods of arriving on time, but the effect (0.63 to 0.76) is rather minor.**

As you can see, the effect is relatively minor. The probability increases from about 0.63 to about 0.76 as the distance changes from a very short hop to a cross-continental flight. On the other hand, if we hold the taxi-out time and distance constant and examine the dependence on departure delay, we see a more dramatic impact (see [Figure 7-5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#how\_the\_departure\_delay\_of\_the\_flight\_a)):

```
delay = np.arange(-20, 60, 1)
prob = [lrmodel.predict([d, 10, 500]) for d in delay]
ax = plt.plot(delay, prob)
```

Although the probabilities are useful to be able to plot the behavior of the model in different scenarios, we do want a specific decision threshold. Recall that we want to cancel the meeting if the probability of the flight arriving on time is less than 70%. So, we can change the decision threshold:

```
lrmodel.setThreshold(0.7)
```

Now, the predictions are 0 or 1, with the probability threshold set at 0.7.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0705.png" alt="" height="344" width="491"><figcaption></figcaption></figure>

**Figure 7-5. How the departure delay of the flight affects the probability of on-time arrival. The effect of the departure delay is rather dramatic.**

### Predicting Using the Model

Now that we have a trained model, we can save it to Cloud Storage and retrieve it whenever we need to make a prediction. To save the model, we provide a location on Cloud Storage:

```
MODEL_FILE='gs://' + BUCKET + '/flights/sparkmloutput/model'
lrmodel.save(sc, MODEL_FILE)
```

To retrieve the model, we load it from the same location:

```
from pyspark.mllib.classification import LogisticRegressionModel
lrmodel = LogisticRegressionModel.load(sc, MODEL_FILE)
lrmodel.setThreshold(0.7)
```

Note that we must take care to set the decision threshold; it is not part of the model.

Now, we can use the `lrmodel` variable to carry out predictions:

```
print lrmodel.predict([36.0,12.0,594.0])
```

Obviously, this code could be embedded into a Python web application to create a prediction web service or API.

A key point to realize is that whereas model training in Spark is distributed and requires a cluster, model prediction is a pretty straightforward mathematical computation. Model training is a batch operation that requires the ability to scale out to multiple processors, but online prediction requires fast computation on a single processor. When the machine learning model is relatively small (as in our logistic regression workflow),[8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn138) hardware optimizations (like graphics processing units \[GPUs]) are not needed in the training stage. Thus, when choosing how to resize the cluster to run our machine learning training job over the full dataset, it is more cost-effective to simply add more CPUs.

When would you need GPUs in machine learning? GPUs are potentially needed in the prediction stage for small models. GPUs become useful in prediction even for small models if the system needs to provide for low latency and a high number of queries per second (QPS). Of course, had we been training a deep learning model for image classification with hundreds of layers, GPUs would have been called for both in training and in prediction.

### Evaluating a Model

Now that we have a trained model, we can evaluate its performance on the test days, a set of days that were not used in training (we created this test dataset in [Chapter 5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#interactive\_data\_exploration\_with\_verte)). To do that, we change the query to pull out the test days:

```
testquery = trainquery.replace(\
         "t.is_train_day == 'True'","t.is_train_day == 'False'")
print testquery
```

Here is the resulting query:

<pre><code>SELECT
  DEP_DELAY, TAXI_OUT, ARR_DELAY, DISTANCE
FROM flights f
JOIN traindays t
ON f.FL_DATE == t.FL_DATEWHERE
<strong>  t.is_train_day == 'False' AND
</strong>  f.CANCELLED == 'False' AND
  f.DIVERTED == 'False'
</code></pre>

We then carry out the same ML pipeline as we did during training:

```
testdata = spark.sql(testquery)
examples = testdata.map(to_example)
```

Note that we are able to reuse the function `to_example` to go from the raw data to the training examples.

As soon as we have the dataframe `examples`, we can have the model predict the label given the set of features for each row:

```
labelpred = examples.map(lambda p: \
           (p.label, lrmodel.predict(p.features)))
```

The `map` function applies the `predict` method to each row of features and creates a dataframe that contains the true label and the model prediction for each row.

To evaluate the performance of the model, we first find out how many flights we will need to cancel and how accurate we are in terms of flights we cancel and flights we don’t cancel:

```
def eval(labelpred):
    cancel = labelpred.filter(lambda (label, pred): pred == 1)
    nocancel = labelpred.filter(lambda (label, pred): pred == 0)
    corr_cancel = cancel.filter(lambda (label, pred): \
                                        label == pred).count()
    corr_nocancel = nocancel.filter(lambda (label, pred): \
                                        label == pred).count()
    return {'total_cancel': cancel.count(), \
            'correct_cancel': float(corr_cancel)/cancel.count(), \
            'total_noncancel': nocancel.count(), \
            'correct_noncancel': float(corr_nocancel)/nocancel.count()\
           }
```

Here’s what the resulting statistics turn out to be:

```
{'correct_cancel': 0.7917474551623849, 'total_noncancel': 115949,
'correct_noncancel': 0.9571363271783284, 'total_cancel': 33008}
```

As discussed in [Chapter 5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#interactive\_data\_exploration\_with\_verte), the reason the correctness percentages are not 70% is because the 70% threshold is on the marginal distribution—the accuracy percentages here are computed on the total distribution and so are padded by the easy decisions. However, let’s go back and modify the evaluation function to explicitly print out statistics around the decision threshold—this is important to ensure that we are, indeed, making a probabilistic decision.

We should clear the threshold so that the model returns probabilities and then carry out the evaluation twice: once on the full dataset, and next on only those flights that fall near the decision threshold of 0.7:

<pre><code>lrmodel.clearThreshold() # so it returns probabilities
labelpred = examples.map(lambda p: \
                         (p.label, lrmodel.predict(p.features)))
print eval(labelpred)
# keep only those examples near the decision threshold
<strong>labelpred = labelpred.filter(lambda (label, pred):\
</strong>                         pred > 0.65 and pred &#x3C; 0.75)
print eval(labelpred)
</code></pre>

Of course, we must change the evaluation code to work with probabilities instead of with categorical predictions. The four variables now become as follows:

```
cancel = labelpred.filter(lambda (label, pred): pred < 0.7)
nocancel = labelpred.filter(lambda (label, pred): pred >= 0.7)
corr_cancel = cancel.filter(lambda (label, pred): \
                             label == int(pred >= 0.7)).count()
corr_nocancel = nocancel.filter(lambda (label, pred): \
                             label == int(pred >= 0.7)).count()
```

When run, the first set of results remains the same, and the second set of results now yields this:

<pre><code>{'correct_cancel': 0.30886504799548276, 'total_noncancel': 2224,
<strong>'correct_noncancel': 0.7383093525179856, 'total_cancel': 1771}
</strong></code></pre>

Note that we are correct about 74% of the time in our decision to go ahead with a meeting (our target was 70%). Although useful to verify that the code is working as intended, the actual results are meaningless because we are not running on the entire dataset, just one shard of it. Therefore, the final step is to export the code from the notebook, remove the various `show()` and `plot()` functions, and create a submittable script.[9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn139)

We can submit the script to the Cloud Dataproc cluster from a laptop with the Cloud SDK installed, from Cloud Shell, or from the Cloud Dataproc section of the [Google Cloud Platform web console](https://oreil.ly/JF3wn). Before we submit the script, we need to resize the cluster, so that we can process the entire dataset on a larger cluster than the one on which we did development. So, what we need to do is to increase the size of the cluster, submit the script, and decrease the size of the cluster after the script is done. Alternatively, we can use an autoscaling Dataproc cluster or serverless Spark as discussed in [Chapter 6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch06.html#bayesian\_classifier\_with\_apache\_spark\_o).[10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn140)

Running _logistic.py_ on a more powerful cluster, we create a model and then evaluate it on the test days. We obtain these results from the logs:

<pre><code><strong>All flights: {'total_cancel': 291895, 'correct_cancel': 0.8122920913342127, 
</strong>'total_noncancel': 1304422, 'correct_noncancel': 0.9642401002129679}
</code></pre>

Looking at these overall results, notice that we are canceling about 292k meetings. We are accurate 96.4% of the time we don’t cancel a meeting and 81.2% of the time when we decide to cancel. Remember that we canceled 275k meetings when using the Bayesian classifier in [Chapter 6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch06.html#bayesian\_classifier\_with\_apache\_spark\_o) and were correct 83% of the time that we decided to cancel. Based on our secondary criterion, the Naive Bayes approach in [Chapter 6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch06.html#bayesian\_classifier\_with\_apache\_spark\_o) with only two variables is better than the logistic regression approach in this chapter with three variables! There is, in machine learning, no substitute for experimentation to see what works.

How about the results on the marginal distribution (i.e., on flights for which our probability is near 0.7)? These, too, are in the logs. They indicate that we are, indeed, making an appropriately probabilistic decision—our decision to go ahead with a meeting is correct 73% of the time:

<pre><code>Flights near decision threshold: {'total_cancel': 15084, 
'correct_cancel': 0.3325377883850438, 'total_noncancel': 18441,
<strong>'correct_noncancel': 0.7279431701100808}
</strong></code></pre>

A close examination of the preceding numbers indicates what is going on—we are correct to cancel 33% of the time while in [Chapter 6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch06.html#bayesian\_classifier\_with\_apache\_spark\_o), we were correct to cancel only 30% of the time. This extra 3% allows us to cancel more meetings. It is clear, then, that simply looking at the number of meetings we cancel is not a good secondary criterion of model performance. Instead, we will find a different criterion by comparing the performance of the model against ideal performance across all thresholds. We will do that in the next section.

## Feature Engineering

Still, it is unclear whether we really needed all three variables in the logistic regression model. Any variable you include in a machine learning model brings with it an increased danger of overfitting. Known as the [principle of parsimony](https://oreil.ly/F5VF3) (often referred to as Occam’s razor),[11](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn141) the idea is that it is preferable to use a simpler model to a more complex model that has similar accuracy—the fewer variables we have in our model, the better it is.

One manifestation of the principle of parsimony is that there are practical considerations involved with every new variable in a machine learning model. A hand-coded system of rules can deal with the presence or absence of values (for example, if the variable in question was not collected) for specific variables relatively easily—simply write a new rule to handle the case. On the other hand, machine learning models require the presence of enough data when that variable value is absent. Thus, a machine learning model will often be unusable if all of its variable values are not present in the data. Even if a variable value is present in some new data, it might be defined differently or computed differently, and we might have to go through an expensive retraining effort in order to use it. Thus, extra variables pose issues around the applicability of a machine learning model to new situations. We should attempt to use as few variables as possible.

### Experimental Framework

It is also unclear whether the three variables we chose are the ones that matter most. Perhaps we could use more variables from the dataset besides the three we are using. In machine learning terminology, the inputs to the model are called _features_. The features could be different from the raw input variables because the input variables could be transformed in some way before being provided to the model. The process of designing the transforms that are carried out on the raw input variables is called _feature engineering_.

To test whether a feature provides value to the model, we need to build an experimental framework. We could begin with one feature (departure delay, for example) and test whether the incorporation of a new feature (perhaps the distance) improves model performance. If it does, we keep it and try out one more feature. If not, we discard it and try the next feature on our list. This way, we get to select a subset of features and ensure that they do matter. Another approach is to train a model with all possible features, remove a feature, and retrain. If performance doesn’t go down, leave the feature out. At the end, as before, we will be left with a subset of features that matter. The second approach is preferable because it allows us to capture interactions—perhaps a feature by itself doesn’t matter, but its presence alongside another is powerful. Choosing the set of features through a systematic process is called _feature selection_.

For both feature engineering and feature selection, it is important to devise an experimental framework to test out our hypotheses as to whether a feature is needed. On which dataset should we evaluate whether a feature is important? We cannot evaluate how much it improves accuracy on the training dataset itself because the model might be fitting noise in the training data. Instead, we need an independent dataset in order to carry out feature selection. However, we cannot use the test dataset because if we use the test dataset in our model creation process, it is no longer independent and cannot be used as a good indicator of model performance. Therefore, we will split the training dataset itself into two parts—one part will be used for training, whereas the other will be held out and used to evaluate different models.

[Figure 7-6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#we\_often\_split\_a\_dataset\_into\_three\_par) shows what our experimentation framework is going to look like.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0706.png" alt="" height="397" width="600"><figcaption></figcaption></figure>

**Figure 7-6. We often split a dataset into three parts. The training dataset is used to tune the weights of a model, and the held-out dataset is used to evaluate the impact of model changes, such as feature selection. The independent test dataset is used only to gauge the performance of the final, selected model.**

First, we break the full dataset into two parts and keep one part of it for the final evaluation of models (we did this in [Chapter 6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch06.html#bayesian\_classifier\_with\_apache\_spark\_o) when we created the `traindays` dataset). This part is called the test dataset and is what we have been using for end-of-chapter evaluations. However, when we are creating several models and need to choose among them, we cannot use the test dataset. Therefore, we will split our original training dataset itself into two parts. We’ll retain the larger part of it for actual training and use the held-out portion to evaluate the model. In [Figure 7-6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#we\_often\_split\_a\_dataset\_into\_three\_par), for example, we are deciding whether to use the third input variable. We do this based on whether that feature improves model performance enough.

#### Choosing a metric

What metric should we evaluate in order to choose between two models? _Not_ the number of canceled flights! It is easy to game metrics computed from the contingency table because it is possible to change the probability threshold to get a wide range of accuracy, precision, or recall metrics.[12](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn142) A contingency table–based metric is a good way to understand the performance of a model, but not to choose between two models unless care is taken to ensure that the measure is not tunable by changing the threshold. One way to do this would be to, for example, compare precision at a fixed recall rate, but you should do this only if that fixed recall is meaningful. In our problem, however, it is the probability that is meaningful, not the precision or recall.

Hence, it is not possible to fix either of them, and we are left comparing two pairs of numbers.

Another way to avoid the problem that the metric can be gamed is to use a measure that uses the full distribution of probabilities that are output by the model. When carrying out feature selection or any other form of hyperparameter tuning, we could use a measure such as the logistic loss or cross-entropy that conveys this full distribution. As a simpler, more intuitive measure that nevertheless uses the full distribution of probabilities, let’s use the root mean squared error (RMSE) between the true labels and the probabilistic predictions:

```
totsqe = labelpred.map(
        lambda data: (data[0] - data[1]) * (data[0] - data[1])
).sum()
rmse = np.sqrt(totsqe / float(cancel.count() + nocancel.count()))
```

What is “enough” of an improvement when it comes to RMSE? There are no hard-and-fast rules. We need the model performance to improve enough to outweigh the drawbacks involved with additional inputs and the loss of agility and model runtime speed that additional input features entail. Here, I will choose to use 0.5% as my threshold. If the model performance, based on some metric we decide upon, isn’t reduced by at least 0.5% by removing a variable, I won’t use the extra variable.

#### Creating the held-out dataset

Because the held-out dataset is going to be used only for model evaluation and only within Spark, we do not need to create the held-out dataset the same way we created the test dataset in [Chapter 5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch05.html#interactive\_data\_exploration\_with\_verte). For example, we do not need to store the held-out days as a separate dataset that can be read from multiple frameworks.[13](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn143) However, the principle of repeatability still applies—every time we run our Spark program, we should get the same set of held-out days. Otherwise, it will not be possible to compare the performance of different models (because evaluation metrics depend on the dataset on which they are evaluated).

After I read the `traindays` dataset, I will add in a new temporary column called `holdout` that will be initialized from a random array:[14](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn144)

<pre><code>from pyspark.sql.functions import rand
SEED = 13
<strong>traindays = traindays.withColumn("holdout", rand(SEED) > 0.8)  # 20%
</strong>traindays.createOrReplaceTempView('traindays')
</code></pre>

I am passing in a seed so that I get the exact same array (and hence the same set of held-out days) every time I run this Spark code.

The first few rows of the `traindays` table are now as follows:

<pre><code><strong>Row(FL_DATE=u'2015-01-01', is_train_day=u'True', holdout=False),
</strong><strong>Row(FL_DATE=u'2015-01-02', is_train_day=u'False', holdout=True),
</strong>Row(FL_DATE=u'2015-01-03', is_train_day=u'False', holdout=False),
Row(FL_DATE=u'2015-01-04', is_train_day=u'True', holdout=False),
Row(FL_DATE=u'2015-01-05', is_train_day=u'True', holdout=True),
</code></pre>

Note that we have both `is_train_day` and `holdout`—obviously, we are not going to be holding out any _test_ data, so the query to pull training samples is as follows:

<pre><code>SELECT
  *
FROM flights f
JOIN traindays t
ON f.FL_DATE == t.FL_DATE
WHERE
<strong>  t.is_train_day == 'True' AND
</strong><strong>  t.holdout == False AND
</strong>  f.CANCELLED == 'False' AND
  f.DIVERTED == 'False'
</code></pre>

After we have trained the model, we evaluate it not on the test data as before, but on the held-out data:

```
evalquery = trainquery.replace("t.holdout == False",
                               "t.holdout == True")
```

After we have developed this code, we can export it out of the notebook into a standalone script so that it can be submitted conveniently.

### Feature Selection

Let’s use this experimental framework and the held-out dataset to decide whether all three input variables are important. As explained earlier, we will remove one variable at a time and check the RMSE on the evaluation dataset. Conveniently, this involves changing only the `to_example()` method from:

```
def to_example(raw_data_point):
  features = [
            fields['DEP_DELAY'],
            fields['DISTANCE'],
            fields['TAXI_OUT'],
        ]
  return LabeledPoint(
              float(fields['ARR_DELAY'] < 15), #ontime
              features)
```

to:

<pre><code>def to_example(raw_data_point):
  features = [
<strong>            # fields['DEP_DELAY'],
</strong>            fields['DISTANCE'],
            fields['TAXI_OUT'],
        ]
  return LabeledPoint(
              float(fields['ARR_DELAY'] &#x3C; 15), #ontime
              features)
</code></pre>

#### Creating a large cluster

In the last chapter, running the logistic regression program script with serverless Spark took 15 minutes. Of this, nearly two minutes were for the cluster to start and an additional three minutes for it to autoscale to a sufficient size.

We are going to run many variations of this job, and because we don’t quite know what possibilities we want to try, we want jobs that start immediately and finish quickly. Saving five minutes by having an already sized cluster ready to go will speed up our experimentation by 33% for our use case.

Therefore, let’s create a cluster consisting of 50 machines that have 8 vCPUs each:[15](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn145)

<pre><code>#!/bin/bash
gcloud dataproc clusters create ch7cluster \
  --enable-component-gateway \
  --region ${REGION} --zone ${REGION}-a \
<strong>  --master-machine-type n1-standard-4 \
</strong>  --master-boot-disk-size 500 \
<strong>  --num-workers 30 --num-secondary-workers 20 \
</strong><strong>  --worker-machine-type n1-standard-8 \
</strong>  --worker-boot-disk-size 500 \
  --project $PROJECT \
  --scopes https://www.googleapis.com/auth/cloud-platform
</code></pre>

Unfortunately, when I tried it, the script immediately exited with an error:

```
ERROR: (gcloud.dataproc.clusters.create) INVALID_ARGUMENT:
Insufficient 'CPUS' quota. Requested 404.0, available 67.0.
```

I didn’t have the necessary quota—I needed 404 CPUs, but had only 67 available.

#### Increasing quota

The number of CPUs that I’m allowed to use in a region is an example of a _soft quota_. Soft quotas are meant to guard against human error and billing surprises. They are quite easy to change.

To increase my quota, I visited [the Quotas page on the GCP web console](https://oreil.ly/g7N4s) and found the Compute Engine CPUs quota for the region I am interested in:

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_07in04.png" alt="screenshot of selecting region: us-central1" height="42" width="600"><figcaption></figcaption></figure>



My quota is 72, but I must already be using 5, which is why the error indicated that only 67 are available.

I then clicked on “Edit quotas,” requested 500, and explained why:

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_07in05.png" alt="screenshot of a filled-out Request description that reads: Needed to demonstrate experimentation in Spark" height="385" width="493"><figcaption></figcaption></figure>



Amazingly enough, the approval for my quota increase arrived in my email inbox within a minute of my requesting it. However, the email asked me to wait 15 minutes for the quota increase to get propagated throughout.

Once the quotas page showed that my quota increase had gone through, I was able to rerun the cluster creation command successfully.

#### Autoscale up and down

A 400-CPU machine cluster feels a bit dangerous, though—what if we forget and leave it running for months on end? Let’s add an autoscaling policy to this cluster (we could have done it when creating the cluster too, but creating the cluster first and then adding the autoscaling policy allows us to verify that we have the quota to go as high as needed).

First, I create a policy file:

```
workerConfig:
  minInstances: 2
  maxInstances: 30
secondaryWorkerConfig:
  maxInstances: 20
basicAlgorithm:
  cooldownPeriod: 15m
  yarnConfig:
    scaleUpFactor: 0.05
    scaleDownFactor: 1.0
    gracefulDecommissionTimeout: 1h
```

Then, I update the policy of the cluster that I just created:

```
gcloud dataproc autoscaling-policies import experiment-policy \
  --source=autoscale.yaml --region=$REGION

gcloud dataproc clusters update ch7cluster \
  --autoscaling-policy=experiment-policy --region=$REGION
```

The difference between this and serverless Spark is that once this cluster is started, we can simply submit jobs to it. There is no need to wait for the cluster to start at the beginning of each experiment. At the same time, the cluster will remain at the autoscaled size, and as long as we submit the next job to it within 15 minutes, we can continue using the machines already spun up. After an hour of no activity, the cluster is decommissioned.

Yes, this is all a bit hacky. BigQuery and its near-instantaneous spin-up, autoscaling, and spin-down are so much nicer! Still, we are in Spark world here, and this is pretty flexible for being in Spark world.

When you’re using a large cluster and submitting jobs one after the other, it is a good idea to monitor the Hadoop nodes to ensure that all the machines are available and that jobs are being spread over the entire cluster. You can access the monitoring details from the Dataproc section of the GCP web console. You should see an uptick in CPU usage during the regression part of the code and use of all 50 node managers as demonstrated in [Figure 7-7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#monitor\_the\_cpu\_usage\_and\_use\_of\_all\_th).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/dsg2_0707.png" alt="" height="356" width="371"><figcaption></figcaption></figure>

**Figure 7-7. Monitor the CPU usage and use of all the nodes during the regression phase to ensure that all the machines are being used effectively.**

In my case, monitoring revealed that simply increasing the number of workers didn’t actually spread out the processing to all the nodes. This is because Spark estimates the number of partitions based on the raw input data size (here just a few gigabytes), so that estimate is probably too low for our 50-worker cluster. Hence, I modified the reading code to add in an explicit repartitioning step:

```
traindata = spark.sql(trainquery).repartition(1000)
```

And similarly, when reading the evaluation data:

```
evaldata = spark.sql(evalquery).repartition(1000)
```

After this bit of optimization, I am able to run an experiment in about 10 minutes, as I expected, by backing out the startup and upscaling time from the serverless Spark runtime. The longer the experiment, the less of a benefit all this fine tuning provides. We are better off leaving it to serverless Spark.

#### Removing features

Now that we have a faster way of running the job, we can carry out our experiment of removing one variable at a time (i.e., in Experiment 3, we use `DEP_DELAY` and `TAXI_OUT` as input features):

| Experiment # | Variables                                                                       | RMSE  | Percent increase in RMSE |
| ------------ | ------------------------------------------------------------------------------- | ----- | ------------------------ |
| 1            | <p><code>DEP_DELAY</code><br><code>DISTANCE</code><br><code>TAXI_OUT</code></p> | 0.205 | N/A                      |
| 2            | Remove `DEP_DELAY`                                                              | 0.361 | 76%                      |
| 3            | Remove `DISTANCE`                                                               | 0.207 | 1%                       |
| 4            | Remove `TAXI_OUT`                                                               | 0.227 | 11%                      |

It is clear that some variables are dramatically more important than others, but also that all three variables do carry information and you should not discard them. Contrary to my assumption in [Chapter 6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch06.html#bayesian\_classifier\_with\_apache\_spark\_o), the distance has the least amount of impact—had I known this earlier, [Chapter 6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch06.html#bayesian\_classifier\_with\_apache\_spark\_o) might have involved Bayesian selection of departure delay and taxi-out time!

### Feature Transformations

In the previous section, we carried out feature selection to determine that the distance variable was the least important of the variables included in the model. However, there is another possibility why including the distance variable did not affect the RMSE greatly—it could be that the distance variable ranges over a wider range (thousands of miles), whereas the time intervals range to only a few minutes. Therefore, the distance variable might be overpowering the other variables—assuming that both variables are given equal weights, the impact of the distance variable will be magnified 1,000-fold over that of the time interval. In an effort to reduce the impact of distance on both the sum and the gradient, the optimization process might move the distance weight to 0. And thus, the distance might end up not playing much of a role. This is less likely in the case of logistic regression because it is a linear model, and gradients in linear models can be scaled more effectively. However, having all the variables have similar magnitudes will become important as our models become more complex.

#### Scaling

Another reason we’d like to scale all the input values so that they all have similar (and small) magnitudes is that the initial, random weights in optimization routines are often set to be in the range −1 to 1, and this is where the optimizer starts to search. Thus, starting with all the variables having less than unit magnitude can help the optimizer converge more efficiently and effectively. Following are common choices for scaling functions:

* To map the minimum value of the variable to −1 and the maximum to 1. This involves scanning the dataset to find the minimum and maximum within each column—Spark has a class called `AbsScaler` that will do this for us (but it requires an extra pass through the data). However, the choices of the minimum and maximum need not be exact, so we can use the data exploration that we carried out in previous chapters to do an approximate scaling. As long as we scale the variables the same way during training and during prediction (for example, if we scale linearly such that a distance of 30 maps to −1.0 and a distance of 6,000 maps to 1.0 both in training and during prediction), the precise minimum and maximum don’t matter.
* To map the mean of the variable within the column to 0 and values one standard deviation away to −1 or 1. The tails of the distribution will map to quite large values, but such values will also be rarer. This serves to emphasize unusual values while linearly scaling the common ones.

Let’s use Option 1, which is to do linear scaling between an approximate minimum and maximum. Let’s assume that departure delays are in the range (−30, 30), distance in the range (0, 2,000), and taxi-out times in the range (0, 20). These are approximate, but quite reasonable, and using these reasonable values helps us to avoid being overly affected by outliers in the data.[16](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn146) To map these ranges to (−1, 1), this is the transformation we would carry out on the input variables inside the `to_example()` function that converts a row into a training example:

<pre><code>def to_example(raw_data_point):
  return LabeledPoint(\
              float(raw_data_point['ARR_DELAY'] &#x3C; 15), #ontime \
              [ \
<strong>                  raw_data_point['DEP_DELAY'] / 30, \
</strong><strong>                  (raw_data_point['DISTANCE'] / 1000) - 1, \
</strong><strong>                  (raw_data_point['TAXI_OUT'] / 10) - 1, \
</strong>              ])
</code></pre>

After making these changes to scale the three variables, and running the experiment again, I see that the RMSE is unaffected—scaling doesn’t make a difference:

| Experiment #                                           | Variables                                                                                        | RMSE    | Percent improvement |
| ------------------------------------------------------ | ------------------------------------------------------------------------------------------------ | ------- | ------------------- |
| 1 (values from experiment #1 repeated for convenience) | <p>Raw values of<br><code>DEP_DELAY</code><br><code>DISTANCE</code><br><code>TAXI_OUT</code></p> | 0.20537 | N/A                 |
| 5                                                      | Scaled values of the three variables                                                             | 0.20537 | 0                   |

#### Clipping

Another possible preprocessing that we could carry out is called _clipping_. Values that are beyond what we would consider reasonable are clamped to the bounds. For example, we could treat distances of more than 2,000 miles as 2,000 miles, and departure delays of more than 30 minutes as 30 minutes. This allows the optimization algorithm to focus on the part of the data where the majority of the data lies, and not be pulled off the global minimum by outliers. Also, some error metrics can be susceptible to outliers, so it is worth doing one more experiment after clipping the input variables.

Adding clipping to scaled variables is straightforward:

<pre><code>def to_example(raw_data_point):
<strong>  def clip(x):
</strong><strong>     if x &#x3C; -1:
</strong><strong>        return -1
</strong><strong>     if x > 1:
</strong><strong>        return 1
</strong><strong>     return x
</strong>  return LabeledPoint(\
              float(raw_data_point['ARR_DELAY'] &#x3C; 15), #ontime \
              [ \
<strong>                  clip(raw_data_point['DEP_DELAY'] / 30), \
</strong><strong>                  clip((raw_data_point['DISTANCE'] / 1000) - 1), \
</strong><strong>                  clip((raw_data_point['TAXI_OUT'] / 10) - 1), \
</strong>              ])
</code></pre>

Carrying out the experiment on clipped variables, and adding in the RMSE to the table, we see these results:

| Experiment #                 | Transform                                                                                        | RMSE    | Percent improvement | Keep transform? |
| ---------------------------- | ------------------------------------------------------------------------------------------------ | ------- | ------------------- | --------------- |
| 1 (repeated for convenience) | <p>Raw values of<br><code>DEP_DELAY</code><br><code>DISTANCE</code><br><code>TAXI_OUT</code></p> | 0.20537 | N/A                 | N/A             |
| 6                            | Scaled                                                                                           | 0.20537 | None                | No              |
| 7                            | Clipped                                                                                          | 0.20538 | Negative            | No              |

It turns out that neither scaling nor clipping matters for this algorithm (logistic regression) in this framework (Spark ML). In general, though, experimenting with different preprocessing transforms should be part of your workflow. It could have a dramatic impact.

### Feature Creation

So far, we have tried out the three numeric predictors in our dataset. Why did I pick only the numeric fields in the dataset? Because the logistic regression model is, at heart, just a weighted sum. We can quite easily multiply and add numeric values (actually not all numeric values but those that are continuous),[17](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn147) but what does it mean to use a timestamp such as 2015-03-13-11:00:00 as an input variable to the logistic regression model?

We cannot simply convert such a timestamp to a numeric quantity, such as the day number within the year, and then add it to the model. One rule of thumb is that to use a value as input to a model, you should have at least 5 to 10 instances of that value appearing in the data. Columns that are too specific to a particular row or handful of rows can cause the model to become _overfit_—an overfit model is one that will perform extremely well on the training dataset (essentially, it will memorize exactly what happened at each historical timestamp—for example, that flights in the Midwest were delayed on May 11, 2015) but then not work well on new data. It won’t work well on new data because the timestamps in that data (such as May 11, 2018) will not have been observed.[18](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn148)

Therefore, we need to do smarter things with attributes such as timestamps so that they are both relevant to the problem and not too specific. We could, for example, use the hour of day as an attribute in the model. The hour of day might matter—most airports become busy in the early morning and early evening because many flights are scheduled around the daily work schedules of business travelers. In addition, delays accumulate over the day because an aircraft that is delayed in arriving is also delayed on takeoff.

Suppose that we extract the hour of day from the timestamp. Given a timestamp such as 2015-03-13-11:00:00, what is the hour? It’s 11, of course, but the 11 is in the time zone corresponding to the UTC time zone. On the other hand, we care about the time zone at the American airport because many airports are busier early in the morning and late in the evening. This is one instance for which it is local time that matters. Thus, to extract the hour of day, we will need to correct by the time zone offset and then extract the hour of day. The _feature_ hour of day is computed from two input variables—the departure time and the time zone offset.

It is worth pausing here and clarifying that I am making a distinction between the words _input_ and _feature_—the timestamp is the input, the hour of day is the feature. What the client application provides when it wants a prediction is the input, but what the ML model is trained on is the feature. The feature could be (as in the case of scaling of input variables) a transformation of an input variable. In other cases, as with the hour of day, it could be a combination of multiple input variables. The `to_example()` method is the method that converts inputs (`raw_data_point`) to _examples_ (where each example is a tuple of features and a label). Different machine learning APIs will ask for inputs, features, or examples, and it is good to be clear on what exactly the three terms mean.

Given a departure timestamp and a time zone offset,[19](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn149) we can compute the hour in local time using the time-handling code we discussed in [Chapter 4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch04.html#streaming\_data\_publication\_and\_ingest):

```
def to_example(raw_data_point):
  def get_local_hour(timestamp, correction):
      import datetime
      TIME_FORMAT = '%Y-%m-%dT%H:%M:%S'
      t = datetime.datetime.strptime(timestamp, TIME_FORMAT)
      d = datetime.timedelta(seconds=correction)
      t = t + d
      return t.hour
  return LabeledPoint(\
              float(raw_data_point['ARR_DELAY'] < 15), #ontime \
              [ \
                  raw_data_point['DEP_DELAY'], \
                  raw_data_point['TAXI_OUT'], \
                  get_local_hour(raw_data_point['DEP_TIME'], \
                           raw_data_point['DEP_AIRPORT_TZOFFSET'])
              ])
```

There is one potential problem with treating the hour of day as a straightforward number. Hour 22 and hour 2 are only 4 hours apart, and it would be good to capture this somehow. An elegant way to work with periodic variables in machine learning is to convert them to two features—sin(theta) and cos(theta), where theta in this case would be the angle of the hour hand in a 24-hour clock:[20](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn150)

<pre><code>def to_example(raw_data_point):
<strong>  def get_local_hour(timestamp, correction):
</strong>      import datetime
      TIME_FORMAT = '%Y-%m-%dT%H:%M:%S'
      t = datetime.datetime.strptime(timestamp, TIME_FORMAT)
      d = datetime.timedelta(seconds=correction)
      t = t + d
<strong>      theta = np.radians(360 * t.hour / 24.0)
</strong><strong>      return [np.sin(theta), np.cos(theta)]
</strong>
<strong>  features = [ \
</strong>                  raw_data_point['DEP_DELAY'], \
                  raw_data_point['TAXI_OUT'], \
              ]
<strong>  features.extend(get_local_hour(raw_data_point['DEP_TIME'],
</strong><strong>                                 raw_data_point['DEP_AIRPORT_TZOFFSET']))
</strong>  return LabeledPoint(\
              float(raw_data_point['ARR_DELAY'] &#x3C; 15), #ontime \
<strong>              features)
</strong></code></pre>

This encoding of a periodic variable using the sin and cos makes it two features. These two features capture the information present in the periodic variable, but do not distort the distance between two values.

Another approach would be to _bucketize_ the hour. For example, we could group hours 20 to 23 and hours 0 to 5 as “night,” hours 6 to 9 as “morning,” and so on. Obviously, bucketizing takes advantage of what human experts know about the problem. We suspect that the behavior of flight delays changes depending on the time of day—long taxi-out times during busy hours are probably built into the scheduled arrival time, but a plane that experiences a long taxi-out because the towing vehicle broke down and had to be replaced will almost definitely arrive late. Hence, our bucketizing of the hour of day relies on our intuition of what the busy hours at an airport are:

<pre><code><strong>def get_category(hour):
</strong><strong>  if hour &#x3C; 6 or hour > 20:
</strong><strong>     return [1, 0, 0]  # night
</strong>  if hour &#x3C; 10:
     return [0, 1, 0] # morning
  if hour &#x3C; 17:
     return [0, 0, 1] # mid-day
  else:
     return [0, 0, 0] # evening
<strong>def get_local_hour(timestamp, correction):
</strong>   ...
<strong>   return get_category(t.hour)
</strong></code></pre>

You might find two things odd about the preceding snippet:

* We are returning binary numbers, for example \[1, 0, 0]. The first number in the triplet captures whether the hour is at night or not. The second captures whether it is in the morning or not. In essence, we convert the hour variable which is numeric and continuous into four independent variables, each of which is 1 or 0.
* But we don’t have four binary numbers, one for each class. We have only three! Note that the vector corresponding to the last category is \[0, 0, 0] and not \[0, 0, 0, 1], as you might have expected. This is because we don’t want the four features to always add up to one—that would make them linearly dependent. This trick of dropping the last column keeps the values independent. Assuming that we have `N` categories, bucketizing will make the hour variable into `N` − 1 features.

How do we know which of these methods—the raw value, the sin/cos trick, or bucketizing—works best for the hour of day? We don’t, so we need to run an experiment and choose (note that we are using the departure delay, distance, and taxi-out, and now adding a new variable to see if it helps):

| Experiment #                 | Transform                       | RMSE    |
| ---------------------------- | ------------------------------- | ------- |
| 1 (repeated for convenience) | Without hour                    | 0.20537 |
| 8                            | raw hour                        | 0.20536 |
| 9                            | <p>sin(theta)<br>cos(theta)</p> | 0.20535 |
| 10                           | bucketize                       | 0.20538 |

The fact that we aren’t able to reduce the RMSE by adding the hour information suggests that the hour of day is already adequately captured by the scheduling rules used by the airlines. It can be tempting to simply throw away the hour information, but we should follow our systematic process of keeping all our variables and then discarding one variable at a time—it is possible that the hour of day doesn’t matter now, but it might after we include some other variables. So, for now, let’s pick one of the possibilities arbitrarily—I will use the bucketed hour as the way to create a feature from the timestamp. Of course, I could have created additional features from the timestamp input variable—day of week, season, and so on.

[Spark ML supports a rich set of feature transforms](https://oreil.ly/P3gSM)—it is a good idea to go through that list and learn the types of variables for which they are meant. Knowing the tools available to you is a prerequisite to be able to call on them when appropriate. If this is the first time you are encountering machine learning, you might be surprised by how much we are relying on our experimental framework. Running experiments like this, though, is the unglamorous work that lies behind most machine learning applications. Although my approach in this chapter has required careful record keeping, a better way would be if our machine learning framework would provide structure around experiments, not just single training operations. Spark ML [provides this functionality](https://oreil.ly/3XSXz) via the `CrossValidator` tool, but even that still requires quite a bit of scaffolding.

There is another problem, though. The runtime increased from 10 minutes to 15 minutes with the addition of the hour variable. This doesn’t bode well—we have a lot more features we want to try to include.

### Categorical Variables

How about using the airport code as a predictor? What we are doing when using the airport code as a predictor is that we are asking the ML algorithm to learn the idiosyncrasies of each airport. I remember, for example, sitting on the taxiway at New York’s LaGuardia airport for nearly 45 minutes and then being surprised when the flight arrived in Dallas a few minutes ahead of schedule! Apparently, a 45-minute taxi-out time in New York is quite common and nothing to be worried about.

To use timestamp information, we extracted a numeric part of the timestamp—the hour—and used it in our model. We tried using it in raw form, as a periodic variable, and as a bucketized set of categories. We cannot use that approach here because there is no numeric component to the letters DFW or LGA. So how can we use the airport code as an input to our model?

The trick here is to realize that bucketizing the hour was a special case of making the variable categorical. A more bludgeon-like, but often effective, approach is to do _one-hot encoding_. Essentially, the hour variable is made into 24 features. The 11th feature is 1.0 and the rest of the features 0.0 when the hour is 11, for example. One-hot encoding is the standard way to deal with _categorical_ features (i.e., features for which there is no concept of magnitude or ranking between different values of the variable).[21](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn151) This is the way we’ll need to encode the departure airport if we were minded to include it in our model—we’d essentially have one feature per airport, so that the DFW feature would be 1, and the rest of the features 0 for flights that departed from DFW.

Unlike bucketing hours, though, we need to find all the possible airport codes (called the _vocabulary_) and assign a specific binary column to them. For example, we might need to assign DFW to the 143rd column. Fortunately, we don’t need to write the code. One-hot encoding is available as a prebuilt feature transformation in Spark; we can add a new column of vectors to the `traindata` dataframe using this code:

<pre><code>def add_categorical(df):
   from pyspark.ml.feature import OneHotEncoder, StringIndexer
<strong>   indexer = StringIndexer(inputCol='ORIGIN',
</strong>                           outputCot='origin_index')
<strong>   index_model = indexer.fit(df)  # 
</strong><strong>   indexed = index_model.transform(df)  # 
</strong><strong>   encoder = OneHotEncoder(inputCol='origin_index',
</strong>                           outputCot='origin_onehot')
<strong>   return encoder.transform(indexed) # 
</strong>traindata = add_categorical(traindata)
</code></pre>

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/1.png)](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#code\_id\_7\_1)Create an index from origin airport codes (e.g., DFW) to an origin index (e.g., 143).[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/2.png)](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#code\_id\_7\_2)Transform the dataset so that all flights with `ORIGIN=DFW` have `origin_index=143`.[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098118945/files/assets/3.png)](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#code\_id\_7\_3)One-hot encode the index into a binary vector that is used as input to training.

During evaluation, the same change needs to be made to the dataset, except that the index model will need to be reused from training (so that DFW continues to map to 143). In other words, we need to save the `index_model` and carry out the last three lines before prediction. So we modify the `add_categorical()` method to:⁠[22](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn152)

<pre><code>index_model = 0
def add_categorical(df, train=False):
   from pyspark.ml.feature import OneHotEncoder, StringIndexer
<strong>   if train:
</strong>       indexer = StringIndexer(inputCol='ORIGIN',
                           outputCol='origin_index')
<strong>       index_model = indexer.fit(df)
</strong>   indexed = index_model.transform(df)
   encoder = OneHotEncoder(inputCol='origin_index',
                           outputCol='origin_onehot')
   return encoder.transform(indexed)
<strong>traindata = add_categorical(traindata, train=True)
</strong>...
evaldata = add_categorical(evaldata)
</code></pre>

This is bookkeeping to which we need to pay careful attention because doing this sort of thing incorrectly will result in training–serving skew. Spark provides a `Pipeline` mechanism to help you record which operations you carried out on the dataset so that you can repeat them when evaluating, but it introduces yet another level of abstraction into an already complex topic.

During prediction, things become even more complicated. No longer is it simply a matter of calling `lrmodel.predict()`. Instead, you will need to first construct a dataframe out of your raw input data, apply these transforms, and finally invoke the actual model.

All this is academic, however. If you are wondering why there was no RMSE stated after I added the one-hot encoded airports, it’s because my program ran out of resources. Adding the airport variable completely overwhelmed the cluster. I got memory errors and disk swapping:

```
WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache 
rdd_46_403 in memory! (computed 5.6 MiB so far)
```

I put the program out of its misery after about an hour.

The problem is the large quantity of input features one-hot encoding creates. Because there are about 300 distinct airports in our dataset,[23](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn153) the airport variable will now become about 300 separate features. The `flights` dataset is about 21 million rows with the training data being about 65% of it, or about 14 million rows, and we used only one categorical column with about 300 unique values. Yet, this brought down the machines. Real-world business datasets are larger. The “small clicks” Criteo ads data used in a Kaggle competition and demonstrated in Vertex AI, for example, is 45 million rows (the full ads dataset contains four billion rows). Nearly all of its columns are categorical, and some of them have thousands of unique values.

### Repeatable, Real Time

In the previous section, we discussed that one-hot encoding leads to a large quantity of input features. One way of reducing the explosion of input features caused by one-hot encoding is to carry out dimensionality reduction. The idea is to pass in the one-hot encoded set and ask the machine learning algorithm itself to come up with weights to combine these columns into, say, four features that are used downstream in the model. This is called creating an _embedding_. This embedding model itself will be part of the full model, and so the embedding weights can be discovered at the same time. We look at creating embeddings in [Chapter 10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#getting\_ready\_for\_mlops\_with\_vertex\_ai), when we discuss TensorFlow.

One of the side effects of having complex library code to carry out feature transformations is that it adds a dependency on the program that makes the predictions. That program needs to run Spark in order to carry out the one-hot encoding correctly—an extremely difficult situation if the program that actually makes the predictions runs on mobile phones or outside of your company’s firewall. Building a realistic machine learning pipeline with Spark, as we have seen, requires a fair bit of tooling and framework building.[24](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn154) It is easy to get started, but difficult to productionize. One way to ensure repeatability is to store the necessary transformations in the model itself.

BigQuery ML, which we will cover in [Chapter 8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch08.html#machine\_learning\_with\_bigquery\_ml), addresses the scalability issue. Vertex AI, which we use in [Chapter 10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#getting\_ready\_for\_mlops\_with\_vertex\_ai), resolves this problem by being able to deploy an autoscaling, low-latency prediction model that is accessed via a REST API.

Finally, we could improve the way we use taxi-out times. Flight arrival times are scheduled based on the average taxi-out time experienced at the departure airport at that specific hour. For example, at peak hours in New York’s JFK airport, taxi-out times on the order of an hour are quite common, so airlines take that into account when publishing their flight schedules. It is only when the taxi-out time exceeds the average that we ought to be worried. To augment the training dataset with an aggregate feature that is computed in real time like this, we will need the same code that processes batch data to also process streaming data in real time. One way to do this is to use Apache Beam. We do this in [Chapter 11](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch11.html#time\_windowed\_features\_for\_real\_time\_ma).

## Summary

In this chapter, we took our first foray into machine learning using Apache Spark. Spark ML is an intuitive, easy-to-use package, and running Spark on Cloud Dataproc gives us the ability to quickly build machine learning models on moderately sized datasets.

We created a dataset using Spark SQL and discovered that there were problems with missing values for some of the columns. Rather than simply remove the missing data, though, we found the root cause to be canceled or diverted flights and removed such flights from the dataset. We employed logistic regression, a machine learning model that provides probabilistic outputs, to predict the probability that the flight will be on time. Setting the probability threshold at 0.70 allows us to make a decision as to whether to cancel a scheduled meeting that depends on us arriving at the airport within 15 minutes of the scheduled arrival time.

We carried out feature selection and feature engineering and explored categorical features. To choose the features systematically, we devised an experimental framework in which the training dataset itself was broken into two parts and the second part used to decide whether to keep the feature or transformation in question. We also discovered some pain points when building a production machine learning system on large datasets in Spark. Primarily, these had to do with the ability to deal with scale, of carrying out more complex models, of getting low-latency predictions outside of the development framework, and of being able to use features computed in real-time windows.

## Suggested Resources

I am hesitant to recommend too many resources here. Ultimately, Spark is not the best framework for machine learning. You are better off using XGBoost, Pytorch, or TensorFlow in Vertex AI.

If you are planning to use Spark MLlib, read the concise yet complete [documentation](https://oreil.ly/gSbx1) from start to finish. Definitely try out pipelines and different types of models, but before you make your decision, make sure to compare against more modern alternatives.

Spark pipelines do not have all the capabilities such as metadata tracking that one expects out of a proper pipelines framework. A better approach to operationalize Spark MLlib is to use Vertex AI Pipelines and have it delegate to Dataproc, as described in the 2021 Medium blog post [“Sparkling Vertex AI Pipelines” by Ivan Nardini](https://oreil.ly/TkcyY). Even though we will cover Vertex Pipelines in the context of TensorFlow in [Chapter 10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch10.html#getting\_ready\_for\_mlops\_with\_vertex\_ai), Vertex AI Pipelines works on any containerized code.

While you can use Spark for basic regression, classification, and recommendation problems, it is not capable of doing advanced machine learning. A better approach is to use Spark for data preparation, but use distributed TensorFlow for machine learning. This is the [approach that was used at Yahoo](https://oreil.ly/tRNFm), where they ran both Spark and TensorFlow on the same Hadoop cluster. Avoid having to install and manage TensorFlow yourself by delegating TensorFlow training tasks to Vertex AI. See the 2017 blog post [“Using Apache Spark with TensorFlow on Google Cloud Platform”](https://oreil.ly/Mjv5V) by Bill Prin and Neeraj Kashyap for more details.

[1](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn131-marker) We also put our thumb on the scale a little. Recall that we quantized the distance variable quite coarsely. Had we quantized distance into many more bins, there would have been fewer flights in each bin.

[2](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn132-marker) Answers (but don’t take my word for it): (1) At equal odds, the probability is ½ and the logit is zero. If the odds are predominantly in favor of the flight being on time, the probability of on-time arrival is nearly 1.0 and the logit has a very large positive value. (2) The logit function changes fastest at probabilities near zero and one. (3) The gradient of the logit function is slowest near a probability of ½. (4) The change from 2 to 1 will result in a greater change in probability. Near probabilities of zero and one, larger logit changes are required to have the same impact on probability. As you get nearer to a probability of ½, smaller logit changes suffice. (5) The intercept directly impacts the value of the logit, so it moves the first curve upwards or downwards. (6) The logit doubles. (7) If the original probability is near zero or one, doubling of the logit has negligible impact (look at what happens if the logit changes from 4 to 8, for example). If the original probability is between about 0.3 and 0.7, the relationship is quite linear: doubling of the logit ends up causing a proportional increase in probability. (8) About 3, and about 5. (9) Doubling of the logit value at 0.95 is required to reach 0.995. Lots of “energy” in the input variables is required to move the probability needle at the extremes.

[3](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn133-marker) See _logistic\_regression.ipynb_ in the GitHub repository for this book.

[4](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn134-marker) The _L_ stands for low-memory—this is a limited-memory variant of the BFGS algorithm.

[5](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn135-marker) See the section on loss functions in [the Spark documentation](https://oreil.ly/W3JDv).

[6](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn136-marker) Your results might be different because the actual flight records held in your first shard (recall that the input is `all_flights-00000-*`) are possibly different.

[7](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn137-marker) Because of random seeds used in the optimization process, and different data in shards, your results will be different.

[8](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn138-marker) Very large deep neural networks, such as those used for image classification, are another story. Such models can have hundreds of layers, each with hundreds of weights. Here, we have three weights—four if you count the intercept.

[9](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn139-marker) See _logistic.py_ in _07\_sparkml_.

[10](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn140-marker) The script _submit\_spark.sh_ in _07\_sparkml_ uses the serverless Spark approach.

[11](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn141-marker) William of Ockham (later spelled Occam), who was a friar in medieval England, actually wrote “_Pluralitas non est ponenda sine necessitate_,” which translates to “Entities should not be multiplied unnecessarily.”

[12](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn142-marker) The Machine Learning Crash Course from Google has a [good explanation of these metrics](https://oreil.ly/NgJyo). Remember that we need to threshold the probabilistic output of the model to get the entries in the contingency table. A correct cancel, for example, is the situation that the flight arrived more than 15 minutes late and the predicted probability of on-time arrival was less than 0.7. The metrics evaluated on the contingency table are extremely sensitive to this choice of threshold. Different models will be different at thresholds of 0.65, 0.68, or 0.70, especially for models whose performance is quite comparable. If, for example, we want the overall correct cancel percentage to be 80%, we can change the threshold to get this. We can also change the threshold to get an overall correct cancel percentage of 20% if that is what we desire.

[13](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn143-marker) Recall that we stored the training versus test days both as a BigQuery table and as a CSV file on Cloud Storage. We had to save the `traindays` dataset to persistent storage because otherwise we would have run into the problem of different hash function implementations in Spark, Pig, and Tensorflow. There would have been no way to evaluate model performance on the same dataset.

[14](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn144-marker) See _experimentation.ipynb_ and _experiment.py_ in _07\_sparkml_.

[15](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn145-marker) See _create\_large\_cluster.sh_.

[16](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn146-marker) This presupposes that you have done exploratory data analysis, and understand your data. If you don’t know what a reasonable range is, then you could use the 5th and 95th percentiles of the column values in the training data.

[17](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn147-marker) For example, you cannot add and multiply employee ID numbers even if they are numeric. Employee ID numbers are not continuous.

[18](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn148-marker) The reason that we stratify our dataset into training and validation is to catch nonobvious instances of overfitting. If the training loss is very low, but the validation error is high, it is likely that the model has overfit. It is then up to you to diagnose the reason!

[19](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn149-marker) The time zone offset is a float, and must be added to the schema as such.

[20](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn150-marker) The distribution of hours in the dataset, we assume, follows the Fisher–von Mises distribution, which describes points distributed on an `n`-dimensional sphere. If `n` = 2, this reduces to points on a unit circle. An hour hand is just that.

[21](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn151-marker) It is not the case that all strings are categorical and all numeric columns are continuous. To use my previous example, an employee ID might be numeric but is categorical. On the other hand, student grades (A+, A, A–, B+, B, etc.) are strings but can easily be translated to a continuous variable.

[22](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn152-marker) See _experiment.py_.

[23](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn153-marker) You can check, as I did, by running `SELECT DISTINCT(ORIGIN) FROM dsongcp.tzcorr` on the BigQuery console.

[24](https://learning.oreilly.com/library/view/data-science-on/9781098118945/ch07.html#ch01fn154-marker) [One approach](https://oreil.ly/bliID) is to convert the Spark ML model to ONNX and then use ONNX runtime for serving.
