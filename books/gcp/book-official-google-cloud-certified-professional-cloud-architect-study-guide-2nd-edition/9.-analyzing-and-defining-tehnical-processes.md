# 9. Analyzing And Defining Tehnical Processes

Chapter 9\
Analyzing and Defining Technical Processes <a href="#c09_1" id="c09_1"></a>
---------------------------------------------------------------------------

THE PROFESSIONAL CLOUD ARCHITECT CERTIFICATION EXAM OBJECTIVES COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:

* 4.1 Analyzing and defining technical processes

As an architect, you will participate in several kinds of technical processes, including some that we discussed in the previous chapters, such as continuous integration (CI)/continuous delivery (CD) and post-mortem analysis. In this chapter, we will look at those processes and others, including software development lifecycle planning, testing, and validation, as well as business continuity and disaster recovery planning.

The purpose of this chapter is to describe technical processes with a focus on how they relate to business objectives. For a discussion of more technical aspects, such as the role of tools like Jenkins in CI/CD, see [Chapter 8](https://learning.oreilly.com/library/view/google-cloud-certified/9781119871057/c08.xhtml), “Designing for Reliability.”

### Software Development Lifecycle Plan <a href="#head-2-69" id="head-2-69"></a>

The _software development lifecycle_ (_SDLC)_ is a series of steps that software engineers follow to create, deploy, and maintain complicated software systems. SDLC consists of seven phases.

1. Analysis
2. Design
3. Development
4. Testing
5. Deployment
6. Documentation
7. Maintenance

Each phase focuses on a different aspect of software development. The _cycle_ in SDLC refers to the fact that even after software is created and deployed, the process of understanding business requirements and developing software to meet those needs continues.

#### Analysis <a href="#head-3-121" id="head-3-121"></a>

Software development begins with requirements. There is some problem that needs to be solved. Some problems are relatively narrow and lend themselves to obvious solutions. For example, addressing a new business requirement, such as enabling the ordering of new types of products, is pretty straightforward. Developing a system to maintain electronic patient records while enabling secure integration with other medical and insurance systems is a massive undertaking. The purpose of the analysis phase is to do the following:

* Identify the scope of the problem to address.
* Evaluate options for solving the problem.
* Assess the cost benefit of various options.

Analysis begins with understanding the problem to be solved.

**Scoping the Problem to Be Solved**

At this point, the focus is on the business or organizational problem that is to be solved. This requires a combination of domain knowledge about the problem area and software and systems knowledge to understand how to frame a solution.

Domain knowledge is often provided by people with direct experience with the requirements. For example, someone with experience in insurance claims processing can describe the workflow that claims processors use and where changes to the workflow could help improve operations. Domain experts with broad knowledge can help identify key aspects of requirements and contribute to efforts to define the scope of a problem.

**Evaluating Options**

Once the scope of the problem is decided, you should consider options for solving the problem. There may be multiple ways to address a business need, from modifying an existing system to building a new service from scratch.

A common question at this stage is, “Should the organization buy a software solution or build one?” _Commercial off-the-shelf software (COTS)_ is a term that you may hear to describe existing software solutions.

The advantages of buying software or using a service include the following:

* Faster time to solution since you do not have to build an application.
* Allows developers to focus on other business requirements for which there are no good “buy” options.
* The purchased software or service likely comes with support.

There are disadvantages to buying software, and these can include potentially high licensing costs and the inability to customize the software or service to your specific needs.

If you decide to build your own solution, then you may have the option of modifying an existing application or building a new one from scratch.

In these cases, it is important to consider how much modification is needed to make an existing application meet the business requirements. Also consider the lifecycle of the existing application that you would be modifying. Is it a mature, stable application that is well supported? Has the application been deprecated, or is it receiving only minimal support? How well is the application designed; specifically, can it be readily modified to meet the business requirements?

Modifying an existing application may be the fastest option for getting a solution into production, but it may also constrain your design choices. For example, if you are modifying a distributed Java application, it is probably best to continue to code in Java and not try to create a module written for the .NET platform.

Building from scratch allows you full control over architecture and systems design choices. However, it may require a significant investment of time and software engineering resources. One of the factors to consider at this point is the opportunity cost of having software engineers work on one project versus another. If you have to choose between implementing a new service that will give your company a competitive advantage in the market or developing a custom workflow tool to support back-office operations, it's probably a better idea to build the competitive feature and use an existing open source or commercial workflow tool.

**Cost-Benefit Analysis**

Another task in the analysis phase is to consider the costs and benefits of undertaking the project as scoped. The _opportunity cost_ described earlier is one example of a cost-benefit consideration. In addition to the opportunity cost of software engineers' time, there is also the financial cost of developing an application.

As an architect, you may be asked to help develop a cost justification for a project. The purpose of this is to allow decision-makers to compare the relative value of different kinds of projects. For example, instead of developing a new application, what if the logistics department could use the funds to add new, more efficient vehicles to the company's fleet? Which is the better choice? The answer to that question may be decided using measures such as _return on investment (ROI)_, which measures the value, or return, of making an investment.

If at the end of the analysis phase you decide to move ahead with developing software, you will move into the design phase of the SDLC.

#### Design <a href="#head-3-122" id="head-3-122"></a>

In the design phase, you map out in detail how the software will be structured and how key functions will be implemented. The design phase can often be broken into two subphases: high-level design and detailed design.

**High-Level Design**

During the _high-level design_ phase, the major subcomponents of a system are identified. For example, in the case of an insurance claims processing system, the high-level design may include the following:

* An ingestion system that receives data on insurance claims from clients
* A data validation system that performs preliminary checks to ensure that all needed information is included
* A business logic backend that evaluates claims against the patient's policy to determine benefits
* A customer communication service that sends letters describing the claim and benefits
* A management reporting component that provides details on the volume of claims processed, backlog of claims to be processed, and other key performance indicators

During high-level design, you identify the interface between components. This is especially important in microservice architectures or other forms of distributed systems. RESTful interfaces are commonly used with microservices, although GraphQL is an increasingly popular alternative for designing APIs.

If large volumes of data are passed between services, then APIs may not be the most effective interface. Instead, a messaging system, such as Cloud Pub/Sub, may be used. Instead of making a synchronous API call to another service, you can write data to a Cloud Pub/Sub topic, and the receiving service can read it from there. This is a commonly used pattern for decoupling components.

The client writing data may experience a spike in load and write more messages than the consuming service can process. In that case, the messages will remain in the queue until the reading service consumes them. This allows the first service to keep up with the workload without requiring the downstream service to keep up at the same pace. This is especially important when the client service can readily scale up, as is the case with stateless web interfaces, but the consuming service cannot readily scale up, for example, when writing to a relational database that can only vertically scale.

**Detailed Design**

_Detailed design_ focuses on how to implement each of the subcomponents. During this phase, software engineers decompose components into modules and lower-level functions that implement the capabilities of the system. This includes defining data structures, algorithms, security controls, and logging, as well as user interfaces.

As noted earlier, you will probably work with people with deep domain knowledge during the analysis phase. In the detailed design phase, it can help to work with people who will use the system directly. Again, looking at the insurance claims processing example, a domain expert may understand the overall workflow, but a person who spends their workday entering insurance claim data can help identify the requirements for user interface design.

When defining the scope of the problem to be solved, it is important to enlist the help of people with different roles relative to the system. This is especially the case when designing the user interface, which is part of a broader _user experience (UX)_.

During the high-level or detailed design phase, developers may choose libraries, frameworks, or other supporting software for use in the application. For example, if an application will perform a high volume of mathematical calculations, the team may decide to standardize on a particular math library. Similarly, if the application will be built using _object-oriented design_, developers might choose an _Object Relations Mapper (ORM)_ to facilitate developing code to interface with a relational database.

**Development, Testing, and Deployment**

During development, software engineers create software artifacts that implement a system. This can include application code, which implements functionality, and configuration files, which specify environment-specific details, such as file locations or environment variables.

Today, developers often use tools to help with development. These include integrated development environments, static analysis tools, and database administration tools. They also use source control systems, such as GitHub and Cloud Source Repositories, to support collaboration.

A key component of the development process is testing. In [Chapter 8](https://learning.oreilly.com/library/view/google-cloud-certified/9781119871057/c08.xhtml), we discussed different kinds of testing. In the development phase of the SDLC, unit testing and integration testing are commonly performed. Developers have a choice among testing tools to help with these processes. Some are language specific, while others help with a subset of testing, such as API tests. The deployment phase is discussed in detail below.

**Documentation**

There are three distinct kinds of documentation with regard to software systems.

* Developer documentation
* Operations documentation
* User documentation

![note](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781119871057/files/images/note.png) Not all software development lifecycle frameworks include documentation, but for completeness, it is included here.

_Developer documentation_ is designed for software engineers who will be working with code. Much developer documentation is in the form of inline comments within the code. This kind of documentation is especially helpful when the purpose or logic of a function or procedure is not obvious. Function or procedure-level documentation should be included for all nontrivial units of code. Other forms of developer documentation include design documents that outline the higher-level details of how a system operates.

_Operations documentation_ consists of instructions used by system administrators and DevOps engineers to deploy and maintain system operations. Operations documentation is often written with the participation of SREs. A _runbook_, for example, includes instructions on how to set up and run a service or application. It may contain troubleshooting advice as well. Operations documentation is important when responsibility for maintaining functioning systems is distributed across a team of developers. In those cases, developers may have to resolve a problem with a part of the code they never worked on. In times like that, it helps to have a set of best practices and checklists from which to work.

_User documentation_ explains how to use an application and is often written by technical writers. This kind of documentation is written for people who use the system, not necessarily the developers of the system. With the advent of Agile software practices, the features of a system can change rapidly, and user documentation should be updated on the same schedule.

**Maintenance**

_Maintenance_ is the process of keeping software running and up-to-date with business requirements.

In the past, it was common practice to have a developers' pass of an application for a system administrator to deploy and maintain. With Agile software practices, it is common for developers to maintain their own code.

Maintenance includes configuring monitoring, alerting, and logging. _Monitoring_ collects data on application and infrastructure performance. This gives developers visibility into how their applications are functioning. _Alerting_ is used to notify system administrators or developers of a condition that needs human intervention. _Logging_ is used to collect detailed information about the state of the system over time. Software engineers control the details of information saved in individual log messages; system administrators can often control the number of messages generated by adjusting the logging-level configuration parameters.

It is important for architects to understand the software development lifecycle, even if they do not code on a day-to-day basis. The boundaries between development and systems administration are blurring. In the cloud, it is now possible to configure infrastructure by specifying a description in a configuration file. Systems like Terraform, which is used to deploy and modify infrastructure, and Puppet and Chef, which are used to configure software on servers, allow developers to treat infrastructure configuration as code, typically referred to as infrastructure-as-code (IaC). Architects have substantial roles in the early phases of the SDLC, especially during analysis and high-level design. They also contribute to setting standards for tools, such as version-controlled systems and CI/CD platforms.

### Continuous Integration/Continuous Delivery <a href="#head-2-70" id="head-2-70"></a>

CI/CD is the process of incorporating code into a baseline of software, testing it, and if the code passes tests, releasing it for use. A technical discussion of CI/CD can be found in [Chapter 8](https://learning.oreilly.com/library/view/google-cloud-certified/9781119871057/c08.xhtml). Here, we will discuss why it is used and some alternative approaches that you may encounter as an architect.

#### Business Drivers to Adopt CI/CD <a href="#head-3-123" id="head-3-123"></a>

A key benefit of CI/CD is that new features can be rolled out quickly for use by customers. In the past, new features may have been bundled together and released as part of a major update. For example, developers of an application may have released updates every six months. This was a common practice when software was purchased and shipped on magnetic media. The cost of continuous delivery in that situation would have been prohibitive, including the cost of having system administrators constantly updating their deployed systems.

Today, CI/CD is a practical option because tools help developers roll code out quickly and because much of the software we deploy nowadays is delivered to users as a service. This means that developers can deploy their code to servers in a data center or cloud platform, and users of the service get access to the latest code as soon as it is deployed.

![note](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781119871057/files/images/note.png) This is not always true. Developers sometimes selectively release new capabilities to customers using a mechanism called _feature flags_. These are runtime configurations that can modify the behavior of an application without deploying anything new.

CI/CD works well when code can be validated using automated tests. While this is the case for many business systems, CI/CD may not be appropriate for some _safety-critical software systems_, for example, software used with medical devices or used in aviation, factory automation, or autonomous vehicles. In these cases, the software may need to pass rigorous validation procedures that include human review. _Business-critical software_ and _security-critical software_ may also have more demanding testing requirements, but those may be able to be incorporated into the CI/CD process. It is not uncommon, for example, to require a human code review before code is pushed from a version control system to the CI/CD tool for deployment.

CI/CD is a widely used alternative to big-bang releases that batch changes into infrequent bulk updates. CI/CD, however, should be implemented with sufficient testing and human review as required based on software safety and criticality characteristics.

#### CI/CD Building Blocks <a href="#head-3-124" id="head-3-124"></a>

CI/CD is a practice that builds on several types of tools, including the following:

* Version control
* Secrets management
* Automated builds
* Deployment pipelines

These building blocks are commonly used together in a process known as _GitOps_, which is the practice of managing infrastructure and software deployment using software engineering tools including version control services, like Git.

**Version Control**

Version control is the practice of tracking changes to software. The basic idea is that teams of developers need to collaborate on software development, and that requires tools that track changes, enable multiple changes to source code at one time, and allow developers to revert to previous versions of software when needed.

Early source code management had single centralized servers that stored files related to a development project. Developers would check out files from the central repository to work on them and then check them in when they were done. The centralized model would block developers if the centralized server was down so, today, most developers use distributed version control systems such as Git.

In distributed version control systems, developers copy or clone complete copies of repositories to work from. Git also has mechanisms to track incremental changes by multiple developers so that a consolidated state of the set of files is always available.

Cloud Source Repository is a managed service in Google Cloud for version control.

**Secrets Management**

We often need to use secrets when working with services in the cloud, including API keys, certificates, passwords, and other data we would not want widely known. An obvious bad practice is hard-coding secrets into application or configuration code. For example, if a service needs to authenticate to a database using a username and password, the service will need to get those credentials somewhere. Rather than putting the credentials in a script or application code, they should be stored in a secrets manager.

Secrets managers like HashiCorp Vault and Google Cloud's Secret Manager are secure storage systems for managing secrets in a secure, centralized service. Secrets managers can store key-value pairs where keys are the name of the secret and values are the actual secret information, such as the value of the API key.

Secret Manager in Google Cloud provides for replication policies that allow you to control where secrets are stored and audit logging so you can track changes to secrets. Access to Secret Manager is controlled through IAM access controls.

**Deployment Pipelines**

Modern applications are built using many components, libraries, and modules. Build pipelines automate the process of creating deployable artifacts. Deployment pipelines are used to release previously built artifacts for use. Cloud Build is a managed service in Google Cloud that allows you to build pipelines to build, test, and deploy software. Cloud Build can create container images that can be deployed to run your services. Those images are stored in _Artifact Registry_, a managed centralized repository for images and private packages.

Cloud Build pipelines can deploy services to virtual machines, Kubernetes clusters, and serverless services, as well as Firebase. An important step of deployment pipelines is testing and scanning. With Cloud Build, you can scan images for known vulnerabilities before they are released.

When working Kubernetes Engine or Cloud Run, you can also require binary authorization, which is a security control to ensure only trusted containers run in your infrastructure. Binary Authorization requires that images be signed by trusted providers and then verifies those signatures before releasing the image.

### Troubleshooting and Post-Mortem Analysis Culture <a href="#head-2-71" id="head-2-71"></a>

Complicated and complex systems fail, sometimes in ways that we do not anticipate. There are a number of ways to respond to this reality.

We can extend more effort to ensure that our software is correct. Formal methods, such as refinements from specification and the use of theorem provers, are appropriate for safety-critical systems. In other cases, these formal methods can be too costly and slow down the release of new features more than is warranted by business requirements.

_Chaos engineering_, which is the practice of introducing failures into a system to better understand the consequences of those failures and identify unanticipated failure modes, is another approach. Obviously, chaos engineering cannot be used in safety-critical or security-critical systems, but it is a useful tool for many business applications. Netflix's _Simian Army_ is a collection of chaos engineering tools that introduce failures at various levels of infrastructure from instances to availability zones.

Another way to accommodate failures is to learn as much as possible from them. This is part of the philosophy behind post-mortem culture in software development. There are two types of post-mortems: one for incidents and another for projects.

#### Incident Post-Mortems <a href="#head-3-125" id="head-3-125"></a>

An _incident_ is an event that disrupts a service. Incidents can be fairly limited in scope. For example, a service may lag and not meet service-level objectives for a few minutes. This may impact a small number of customers, but there is no data loss or significant loss of functionality. At the other end of the spectrum are incidents that impact large numbers of customers and entail data loss. This can occur in the event of a severe error, such as a configuration change that leads to a cascading failure, or the loss of access to a zone for an application that is not designed for failover within a region or globally. An _incident post-mortem_ is a review of the causes of an incident, assessment of the effectiveness of responses to the incident, and discussions of lessons learned.

**Learning from Minor Incidents**

Minor incidents in themselves may not provide much opportunity to learn—at least from a single incident. A period of lag in application processing can happen for many reasons, from network failures to resource saturation. If such an incident happens once, it may be caused by factors that are unlikely to occur again, such as a hardware failure in a network device. If we encounter a series of minor incidents, however, there may be a systemic problem present that can be expected to continue if it is not addressed.

In the application lag example, a misconfigured load balancer may not be distributing load optimally across nodes in an instance group and that leads to processing lag for some nodes. This may not be as serious as some other incidents, but it should be corrected. This type of incident is an opportunity to evaluate procedures for making configuration changes and verifying that configuration files are correct. A simple typo in the name of a configuration parameter may trigger a warning message that the parameter is unknown, but the configuration process continues. The result could be that the parameter that you had intended to set to a custom value uses the default parameter value instead.

Once you have identified this kind of problem, you could follow up with a couple of remediations. You could, for example, develop a static code analysis script that checks that parameter names are all in a list of valid parameter names. Alternatively, or in addition, you could set an alert on the corresponding log file to send a notification to a DevOps engineer when a parameter warning message appears in the configuration log.

Minor incidents can help you to identify weak spots in your procedures without significant adverse effects on system users. Major incidents are also learning opportunities, but they come with a higher cost.

**Learning from Major Incidents**

When a large portion of users is adversely affected by a disruption in service or there is a loss of data, then we are experiencing a major incident. The first thing to do in a major incident is to restore service. This may involve a number of people with different responsibilities, such as software developers, network engineers, and database administrators. [Chapter 8](https://learning.oreilly.com/library/view/google-cloud-certified/9781119871057/c08.xhtml) includes a discussion of good practices for responding to major incidents. In this chapter, the focus is how to learn from these incidents.

Major incidents in complex systems frequently occur when two or more adverse events occur at or near the same time. If one node in a distributed database runs out of disk space at the same time that a hardware failure causes a network partition, then the impact will be worse than if either of those occurred in isolation. In this example, the impact of the problem in one node cannot be remediated by shifting database traffic to another node because part of the network is down. This combination of failures can be foreseen, but as architects, we should assume that there will be failure modes that we do not anticipate.

Major incidents can be valuable learning opportunities, but they need to be treated as such. Engineers responding to an incident should follow established procedures, such as identifying an incident manager, notifying business owners, and documenting decisions and steps taken to correct the problem.

A _timeline of events_ is helpful after the fact for understanding what was done. A timeline that includes notes about the reasoning behind a decision is especially helpful. Note-taking is not the highest priority during an incident, but capturing as much detail as possible should be a goal. These do not need to be formal, well-structured notes. A thread on a team message channel can capture details about how the team thought through the problem and how they tried to solve it.

After the incident is resolved, the team should review the response. The goal of the review is to understand what happened, why it happened, and how it could be prevented in the future. A post-mortem review is definitely not about assigning blame. Engineers should feel free to disclose mistakes without fear of retribution. This is sometimes called a _blameless culture_. Complex systems fail even when we all do our best, so in many cases, there is no one to blame. In other cases, we make mistakes. Part of the value of an experienced engineer is the lessons they have learned by making mistakes.

#### Project Post-Mortems <a href="#head-3-126" id="head-3-126"></a>

Another type of post-mortem that is sometimes used in software engineering is a _project post-mortem_. These are reviews of a project that review the way work was done. The goal in these post-mortems is to identify issues that might have slowed down work or caused problems for members of the team.

Project post-mortems are helpful for improving team practices. For example, a team may decide to include additional integration testing before deploying changes to the most complicated parts of an application. They may also change how they document decisions, so all team members know where to find answers to questions about issues discussed in the past.

Like incident post-mortems, project post-mortems should be blameless. The goal of these reviews is to improve a team's capabilities.

### IT Enterprise Processes <a href="#head-2-72" id="head-2-72"></a>

Large organizations need ways to manage huge numbers of software projects, operational systems, and expanding infrastructures. Over time, IT professionals have developed good practices for managing information technology systems at an enterprise level. As an architect, you should be aware of these kinds of processes because they can inform architecture choices and impose requirements on the systems you design. One of the most comprehensive sets of IT enterprise practices is known as ITIL.

_ITIL_, which initially stood for Information Technology Infrastructure Library, is a set of service management practices. These practices help with planning and executing IT operations in a coordinated way across a large organization. _ISO/IEC 20000_ is an international standard for IT service management that is similar to ITIL. ITIL and ISO/IEC 20000 may be helpful for large organizations, but small organizations may find that the overhead of the recommended practices outweighs the benefits.

The ITIL model is organized around four dimensions.

* Organizations and people: This dimension is about how people and groups contribute to IT processes.
* Information and technology products: This dimension relates to information technology services within an organization.
* Partners and suppliers: These are external organizations that provide information technology services.
* Value streams and processes: These are the activities executed to realize the benefits of information technologies.

ITIL also organizes management practices into three groups.

* General management practices: These practices include strategy, portfolio, and architecture management.
* Service management practices: These practices include business analysis, service catalog management, availability management, and service desk management.
* Technical management practices: These practices are related to deployment management, infrastructure and platform management, and software development management.

Architects working with large enterprises may need to work within the ITIL, ISO/IEC 20000, or similar enterprise IT processes. The most important thing to know about these practices is that they are used to manage and optimize a wide array of IT operations and tasks. For architects accustomed to thinking about servers, load balancers, and access controls, it is helpful to keep in mind the challenges of coordinating a large and diverse portfolio of services and infrastructure.

![note](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781119871057/files/images/note.png) The current version of ITIL is ITIL 4, and the definitive documentation about it is available here:

[`www.tsoshop.co.uk/Business-and-Management/AXELOS-Global-Best-Practice/ITIL-4`](https://www.tsoshop.co.uk/Business-and-Management/AXELOS-Global-Best-Practice/ITIL-4/)

ISO/IEC 20000 is defined by the International Organization for Standardization. The definitive ISO/IEC 20000 guide is available here:

[`www.iso.org/standard/51986.html`](https://www.iso.org/standard/51986.html)

### Business Continuity Planning and Disaster Recovery <a href="#head-2-73" id="head-2-73"></a>

Another enterprise-scale process to which architects contribute is _business continuity planning_. This is planning for keeping business operations functioning in the event of a large-scale natural or human-made disaster. A part of business continuity planning is planning for operations of information systems throughout, or despite the presence of, disasters. This is called _disaster recovery_.

#### Business Continuity Planning <a href="#head-3-127" id="head-3-127"></a>

Business continuity planning is a broad challenge. It tries to answer the question, “How can we keep the business operating in the event of large-scale disruption of services on which our business depends?” Large-scale disruptions include extreme weather events, such as Category 5 hurricanes, or other disasters, such as 7.0 magnitude or greater earthquakes. These kinds of events can cause major community-scale damage to power, water, transportation, and communication systems. To enable business operations to continue in spite of such events requires considerable planning. This includes defining the following:

* Disaster plan
* Business impact analysis
* Recovery plan
* Recovery time objectives

A _disaster plan_ documents a strategy for responding to a disaster. It includes information such as where operations will be established, which services are the highest priority, what personnel are considered vital to recovery operations, and plans for dealing with insurance carriers and maintaining relationships with suppliers and customers.

A _business impact analysis_ describes the possible outcomes of different levels of disaster. Minor disruptions, such as localized flooding, may shut down offices in a small area. In that case, employees who are available to work can be assigned to other offices, and their most important data can be restored from cloud backups. A major disruption that includes loss of power to a data center may require a more extreme response, such as deploying infrastructure to GCP and replicating all services in the cloud. Business impact analysis includes cost estimates as well.

The _recovery plan_ describes how services will be restored to normal operations. Once key services, such as power and access to physical infrastructure, are restored, business can start to move operations back to their usual location. This may be done incrementally to ensure that physical infrastructure is functioning as expected.

The recovery plan will also include _recovery time objectives (RTOs)_. These prioritize which services should be restored first and the time expected to restore them.

#### Disaster Recovery <a href="#head-3-128" id="head-3-128"></a>

_Disaster recovery (DR)_ is the subset of business continuity planning that focuses specifically on IT operations. DR starts with planning. Teams responsible for services should have plans in place to be able to deploy their services in a production environment other than the one they typically use. For example, if a service usually runs in an on-premises data center, the team should have a plan for running that service in the cloud. The plan should include scripts configured for the disaster recovery environment.

The DR plan should also include a description of roles for establishing services in the DR environment. The process may require a DevOps engineer, a network engineer, and possibly a database administrator, for example. DR plans should have procedures in place for replicating access controls of the normal operating environment. Someone should not have a different set of roles and permissions in the DR environment than they do in the normal production environment.

DR plans should be tested by executing them as if there were a disaster. This can be time-consuming, and some might argue that focusing on delivering features is a higher priority, but DR planning must include testing. Depending on the type of business or organization, there may be industry or government regulations that require specific DR planning and other activities.

A DR plan should have clear guidance on when to switch to a DR environment. For example, if a critical service cannot be restored within a specified period of time, that service is started in the DR environment. In the event that two or more critical services are not functioning in the normal production environment, then all services may be switched to the DR environment. The criteria for switching to a DR environment is a business decision. It may be informed by the organization's risk tolerance, existing service-level agreements with customers, and expected costs for running a DR environment and then restoring to a normal production environment.

For architects, business continuity planning sets the boundaries for establishing disaster plans for IT services and infrastructure. Architects may be called on to contribute to disaster planning. A common challenge is restoring a service in DR to a state as close as possible to the last good state of the system in the production environment. This can require keeping database replicas in a DR environment and copying committed code to a backup repository each time the production version control system is updated.

Business continuity planning and DR should be considered when defining the architecture of systems. DR planning is not something that can be added on after a system is designed, or at least not optimally. Organizations may have to choose between comprehensive DR plans that incur high costs and less comprehensive plans that cost less but result in degraded service. Architects can help business decision-makers understand the costs and trade-offs of various DR scenarios.

### Summary <a href="#head-2-74" id="head-2-74"></a>

Cloud architects contribute to and participate in a wide range of technical and business processes. Some are focused on individual developers and small teams. The software development lifecycle is a series of phases developers go through to understand business requirements, plan the architecture of a solution, design a detailed implementation, develop the code, deploy for use, and maintain it.

Some software development projects can use highly automated CI/CD procedures. This allows for rapid release of features and helps developers catch bugs or misunderstood requirements faster than batch updates that were common in the past.

As systems become more complicated and fail in unanticipated ways, it is important to learn from those failures. A post-mortem analysis provides the means to learn from minor and major incidents in a blameless culture.

Large enterprises with expansive portfolios of software employ additional organization-level processes in order to manage dynamic IT and business environments. ITIL is a well-established set of enterprise practices for managing the general, service, and technical aspects of an organization's IT operations.

Business continuity planning is the process of preparing for major disruptions in an organization's ability to deliver services. Disaster planning is a subset of business continuity planning and focuses on making IT services available in the event of a disaster.

### Exam Essentials <a href="#head-2-75" id="head-2-75"></a>

* Understand that information systems are highly dynamic and individual developers, teams, businesses, and other organizations use technical processes to manage the complexity of these environments. Technical processes have been developed to help individual developers and entire organizations function and operate in a coordinated fashion. SDLC processes focus on creating, deploying, and maintaining code. Other processes include CI/CD, post-mortem analysis, and business continuity planning.
* Know that the first stage of the SDLC is analysis. This involves identifying the scope of the problem to address, evaluating options for solving the problem, and assessing the costs/benefits of various options. Options should include examining building versus buying. Cost considerations should include the opportunity costs of developers' time and the competitive value of the proposed software development effort.
* Understand the difference between high-level and detailed design. High-level design focuses on major subcomponents of a system and how they integrate. Architecture decisions, such as when to use asynchronous messaging or synchronous interfaces, are made during the high-level design. Detailed design describes how subcomponents will be structured and operate. This includes decisions about algorithms and data structures. Decisions about frameworks and libraries may be made during either high-level or detailed design.
* Know the three kinds of documentation. Developer documentation is for other software engineers to help them understand application code and how to modify it. Operations documentation is for DevOps engineers and site reliability engineers (SREs) so that they can keep systems functioning. A runbook is documentation that describes steps to run an application and correct operational problems. User documentation is for users of the system, and it explains how to interact with the system to have it perform the functions required by the user.
* Understand the benefits of CI/CD. CI/CD is the process of incorporating code into a baseline of software, testing it, and if the code passes tests, releasing it for use. A key benefit of CI/CD is that new features can be rolled out for use by customers quickly. This may not always be an option. For example, safety-critical software may require substantial testing and validation before it can be changed.
* Know what a post-mortem is and why it is used. Post-mortems are reviews of incidents or projects with the goal of improving services or project practices. Incidents are disruptions to services. Major incidents are often the result of two or more failures within a system. Post-mortems help developers better understand application failure modes and learn ways to mitigate risks of similar incidents. Post-mortems are best conducted without assigning blame.
* Understand that enterprises with large IT operations need enterprise-scale management practices. Large organizations need ways to manage large numbers of software projects, operational systems, and expanding infrastructures. Over time, IT professionals have developed good practices for managing information technology systems at an enterprise level. One of the most comprehensive sets of IT enterprise practices is ITIL.
* Know why enterprises use business continuity planning and disaster recovery planning. These are ways of preparing for natural or human-made disasters that disrupt an organization's ability to deliver services. Disaster planning is a component of business continuity planning. Disaster planning includes defining the criteria for declaring a disaster, establishing and switching to a DR environment, and having a plan for restoring normal operations. DR plans should be tested regularly.

### Review Questions <a href="#head-2-76" id="head-2-76"></a>

1. A team of early career software engineers has been paired with an architect to work on a new software development project. The engineers are anxious to get started coding, but the architect objects to that course of action because there has been insufficient work prior to development. What steps should be completed before beginning development according to SDLC?
   1. Business continuity planning
   2. Analysis and design
   3. Analysis and testing
   4. Analysis and documentation
2. In an analysis meeting, a business executive asks about research into COTS. What is this executive asking about?
   1. Research related to deciding to build versus buying a solution
   2. Research about a Java object relational mapper
   3. A disaster planning protocol
   4. Research related to continuous operations through storms (COTS), a business continuity practice
3. Business decision-makers have created a budget for software development over the next three months. There are more projects proposed than can be funded. What measure might the decision-makers use to choose projects to fund?
   1. Mean time between failures (MTBF)
   2. Recovery time objectives (RTO)
   3. Return on investment (ROI)
   4. Marginal cost displacement
4. A team of developers is working on a backend service to implement a new business process. They are debating whether to use arrays, lists, or hash maps. In what stage of the SDLC are these developers at present?
   1. Analysis
   2. High-level design
   3. Detailed design
   4. Maintenance
5. An engineer is on call for any service-related issues with a service. In the middle of the night, the engineer receives a notification that a set of APIs is returning HTTP 500 error codes to most requests. What kind of documentation would the engineer turn to first?
   1. Design documentation
   2. User documentation
   3. Operations documentation
   4. Developer documentation
6. As a developer, you write code in your local environment, and after testing it, you commit it or write it to a version control system. From there it is automatically incorporated with the baseline version of code in the repository. What is the process called?
   1. Software continuity planning
   2. Continuous integration (CI)
   3. Continuous development (CD)
   4. Software development lifecycle (SDLC)
7. As a consulting architect, you have been asked to help improve the reliability of a distributed system with a large number of custom microservices and dependencies on third-party APIs running in a hybrid cloud architecture. You have decided that at this level of complexity, you can learn more by experimenting with the system than by studying documents and code listings. So, you start by randomly shutting down servers and simulating network partitions. This is an example of what practice?
   1. Irresponsible behavior
   2. Integration testing
   3. Load testing
   4. Chaos engineering
8. There has been a security breach at your company. A malicious actor outside of your company has gained access to one of your services and was able to capture data that was passed into the service from clients. Analysis of the incident finds that a developer included a private key in a configuration file that was uploaded to a version control repository. The repository is protected by several defensive measures, including role-based access controls and network-level controls that require VPN access to reach the repository. As part of backup procedures, the repository is backed up to a cloud storage service. The folder that stores the backup was mistakenly granted public access privileges for up to three weeks before the error was detected and corrected. During the post-mortem analysis of this incident, one of the objectives should be to
   1. Identify the developer who uploaded the private key to a version control repository. They are responsible for this incident.
   2. Identify the system administrator who backed up the repository to an unsecured storage service. They are responsible for this incident.
   3. Identify the system administrator who misconfigured the storage system. They are responsible for this incident.
   4. Identify ways to better scan code checked into the repository for sensitive information and perform checks on cloud storage systems to identify weak access controls.
9. You have just been hired as a cloud architect for a large financial institution with global reach. The company is highly regulated, but it has a reputation for being able to manage IT projects well. What practices would you expect to find in use at the enterprise level that you might not find at a startup?
   1. Agile methodologies
   2. SDLC
   3. ITIL
   4. Business continuity planning
10. A software engineer asks for an explanation of the difference between business continuity planning and DR planning. What would you say is the difference?
    1. There is no difference; the terms are synonymous.
    2. They are two unrelated practices.
    3. DR is a part of business continuity planning, which includes other practices for continuing business operations in the event of an enterprise-level disruption of services.
    4. Business continuity planning is a subset of disaster recovery.
11. In addition to ITIL, there are other enterprise IT process management frameworks. Which other standard might you reference when working on enterprise IT management issues?
    1. ISO/IEC 20000
    2. Java Coding Standards
    3. PEP-8
    4. ISO/IEC 27002
12. A minor problem repeatedly occurs with several instances of an application that causes a slight increase in the rate of errors returned. Users who retry the operation usually succeed on the second or third attempt. By your company's standards, this is considered a minor incident. Should you investigate this problem?
    1. No. The problem is usually resolved when users retry.
    2. No. New feature requests are more important.
    3. Yes. But only investigate if the engineering manager insists.
    4. Yes. Since it is a recurring problem, there may be an underlying bug in code or weakness in the design that should be corrected.
13. A CTO of a midsize company hires you to consult on the company's IT practices. During preliminary interviews, you realize that the company does not have a business continuity plan. What would you recommend they develop first with regard to business continuity?
    1. Recovery time objectives (RTO)
    2. An insurance plan
    3. A disaster plan
    4. A service management plan
14. A developer codes a new algorithm and tests it locally. They then check the code into the team's version control repository. This triggers an automatic set of unit and integration tests. The code passes, and it is integrated into the baseline code and included in the next build. The build is released and runs as expected for 30 minutes. A sudden spike in traffic causes the new code to generate a large number of errors. What might the team decide to do after the post-mortem analysis of this incident?
    1. Fire the developer who wrote the algorithm.
    2. Have at least two engineers review all of the code before it is released.
    3. Perform stress tests on changes to code that may be sensitive to changes in load.
    4. Ask the engineering manager to provide additional training to the engineer who revised the algorithm.
15. Your company's services are experiencing a high level of errors. Data ingest rates are dropping rapidly. Your data center is located in an area prone to hurricanes, and these events are occurring during peak hurricane season. What criteria do you use to decide to invoke your disaster recovery plan?
    1. When your engineering manager says to invoke the disaster recovery plan
    2. When the business owner of the service says to invoke the disaster recovery plan
    3. When the disaster plan criteria for invoking the disaster recovery plan are met
    4. When the engineer on call says to invoke the disaster recovery plan
