# 8. Designing For Reliability

Chapter 8\
Designing for Reliability <a href="#c08_1" id="c08_1"></a>
----------------------------------------------------------

THE PROFESSIONAL CLOUD ARCHITECT CERTIFICATION EXAM OBJECTIVES COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:

* 4.3 Developing procedures to ensure reliability of solutions in production (e.g., chaos engineering, penetration testing)
* 6.1 Monitoring/logging/profiling/alerting solution
* 6.2 Deployment and release management
* 6.3 Assisting with the support for deployed solutions
* 6.4 Evaluating quality control measures

A reliable system continuously provides its service. Reliability is closely related to availability. _Reliability_ is a probability, specifically, the probability that a system will be able to process some specified workload for some period of time. _Availability_ is a measure of the percentage of time that a system is functioning and able to meet some specified workload. The difference between reliability and availability is important to keep in mind when thinking about metrics and service-level agreements (SLAs). When thinking broadly about ensuring that services are functioning and, if not, that they can be restored quickly, the two concepts may seem to overlap.

In this book, and for the purposes of the Google Cloud Professional Architect certification exam, we will focus on reliability from a reliability engineering perspective. That is, how do we design, monitor, and maintain services so that they are reliable and available? This is a broad topic that will get a high-level review in this book. For a detailed study of reliability engineering, see the book _Site Reliability Engineering: How Google Runs Production Systems_ edited by Betsy Beyer, Chris Jones, Jennifer Petoff, and Niall Richard Murphy (O'Reilly, 2016). Many of the topics presented here, and especially those in the “System Reliability” section of this chapter, are discussed in detail in the SRE book, as it is commonly known.

### Improving Reliability with Cloud Operations Suite <a href="#head-2-63" id="head-2-63"></a>

It is difficult, if not impossible, to provide reliable software services without insights into how that software is functioning. The state of software systems is constantly changing, especially in the cloud, where infrastructure as well as code can change frequently. Also, demands on applications change.

* A new service might be more popular than anticipated, so additional compute infrastructure is needed to meet demand.
* Seasonal variations, such as holiday shopping, can lead to expected high workloads.
* An error in a service may be disrupting a workflow, resulting in a backlog of unprocessed tasks.
* A database runs out of persistent storage and can no longer execute critical transactions.
* The cache hit ratio is dropping for an application because the memory size is no longer sufficient to meet the needs of the service; this does not block operations, but it does slow down workload processing because of increased latency reading data from persistent storage instead of memory.

There are many other ways that things can go wrong in complicated, distributed systems. In addition, even when services are operating as expected, we may be using more resources than needed. Having detailed data about the state of applications and infrastructure can help us maintain reliable and cost-efficient services.

Google Cloud Platform offers Cloud Operations Suite, formerly known as _Stackdriver_, a comprehensive set of services for collecting data on the state of applications and infrastructure. Specifically, it supports three ways of collecting and receiving reliability information.

* Monitoring: This service is used to help understand performance and utilization of applications and resources.
* Logging: This service is used to collect service-specific details about the operations of services.
* Alerting: This service is used to notify responsible parties about issues with applications or infrastructure that need attention.

Together, these three components of Cloud Operations Suite provide a high degree of observability into cloud systems.

Google Cloud also has Cloud Profiler for continuous CPU and heap profiling, Cloud Trace for distributed tracing and identifying high latency services, and Cloud Debugger for inspecting the state of running applications. These tools are primarily useful for developing new application code and diagnosing problems with running code.

#### Monitoring with Cloud Monitoring <a href="#head-3-111" id="head-3-111"></a>

_Monitoring_ is the practice of collecting measurements of key aspects of infrastructure and application performance. Examples include average CPU utilization over the past minute, the number of bytes written to a network interface, and the maximum memory utilization over the past hour. These measurements, which are known as _metrics_, are made repeatedly over time and constitute a time series of measurements.

**Metrics**

_Metrics_ have a particular pattern that includes a property of an entity, a time range, and a numeric value. GCP has defined metrics for a wide range of entities, including the following:

* GCP services, such as BigQuery, Cloud Storage, and Compute Engine
* Operating system and application metrics that are collected by Cloud Monitoring agents that run on VMs
* Anthos metrics, which include Kubernetes and Istio metrics
* AWS metrics that measure performance of Amazon Web Services resources, such as EC2 instances
* External metrics including metrics defined in Prometheus, a popular open source monitoring tool

In addition to the metric name, value, and time range, metrics can have labels associated with them. This is useful when querying or filtering resources that you are interested in monitoring.

**Time Series**

A _time series_ is a set of metrics recorded with a time stamp. Often, metrics are collected at a specific interval, such as every second or every minute. A time series is associated with a monitored entity. [Table 8.1](https://learning.oreilly.com/library/view/google-cloud-certified/9781119871057/c08.xhtml#c08-tbl-0001) shows an example of a CPU utilization time series for a VM. Time is shown in epochs, which is the number of seconds that have elapsed since midnight of January 1, 1970, in the UTC time zone, excluding leap seconds.

| Time       | CPU Utilization |
| ---------- | --------------- |
| 1563838664 | 76              |
| 1563838724 | 67              |
| 1563838784 | 68              |
| 1563838844 | 62              |
| 1563838904 | 71              |
| 1563838964 | 73              |
| 1563839024 | 73              |
| 1563839084 | 61              |
| 1563839144 | 65              |

Cloud Monitoring provides an API for working with time-series metrics. The API supports the following:

* Retrieving time series in a project, based on metric name, resource properties, and other attributes
* Grouping resources based on properties
* Listing group members
* Listing metric descriptors
* Listing monitored entities descriptors

Common ways of working with metrics are using dashboards and alerting.

**Dashboards**

_Dashboards_ are visual displays of time series. For example, [Figure 8.1](https://learning.oreilly.com/library/view/google-cloud-certified/9781119871057/c08.xhtml#c08-fig-0001) shows a simple dashboard with several time-series metrics.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781119871057/files/images/c08f001.png" alt="Snapshot of service dashboard showing time-series data." height="646" width="1565"><figcaption><p><a href="https://learning.oreilly.com/library/view/google-cloud-certified/9781119871057/c08.xhtml#R_c08-fig-0001">FIGURE 8.1</a> Service dashboard showing time-series data</p></figcaption></figure>

Dashboards are customized by users to show data that helps monitor and meet service-level objectives or diagnose problems with a particular service. Dashboards are especially useful for determining correlated failures. For example, one dashboard may indicate that you are not meeting your service-level objective of response time for purchase transactions. You might then switch to a dashboard that shows performance metrics for your application server, database, and cache. From there, you could determine which of the main three components of the systems are having performance issues. If, for example, the cache hit rate is unusually low, that can lead to greater IO load on the database, which can increase the latency of purchase transactions. Crafting informative dashboards is often an iterative process. You may know key performance indicators that you should monitor when you first create a service, but over time you may monitor additional metrics. For example, if you had an incident because of a lag in a messaging service, you may want to monitor several metrics of the messaging service to catch potential problems before they occur.

Of course, watching a dashboard for potential problems is not the most efficient use of DevOps engineers' time. Reliable systems also depend on alerting and automation to notify engineers when a problem arises and needs their attention.

#### Alerting with Cloud Monitoring <a href="#head-3-112" id="head-3-112"></a>

_Alerting_ is the process of monitoring metrics and sending notifications when some custom-defined conditions are met. The goal of alerting is to notify someone when there is an incident or condition that cannot be automatically remediated and that puts service-level objectives at risk. If you are concerned about having enough CPU capacity for intermittent spikes in workload, you may want to run your application servers at an average of 70 percent utilization or less. If utilization is greater than 80 percent, then you may want to be notified.

**Policies, Conditions, and Notifications**

Alerting policies are sets of conditions, notification specifications, and selection criteria for determining resources to monitor.

Conditions are rules that determine when a resource is in an unhealthy state. Alerting users can determine what constitutes an unhealthy state for their resources. Determining what is healthy and what is unhealthy is not always well defined. For example, you may want to run your instances with the highest average CPU utilization possible, but you also want to have some spare capacity for short-term spikes in workload. (For sustained increases in workload, autoscaling should be used to add instances to handle the persistent extra load.)

Finding the optimal threshold may take some experimentation. If the threshold is too high, you may find that performance degrades, and you are not notified. On the other hand, if the threshold is too low, you may receive notifications for incidents that do not warrant your intervention. These are known as _false alerts_. It is important to keep false alerts to a minimum. Otherwise, engineers may suffer “alert fatigue,” in which case there are so many unnecessary alerts that engineers are less likely to pay attention to them.

**Reducing Alerts**

In addition to tuning alerts, reliability can be improved by automatically responding to changes in workload or other conditions. In Compute Engine, an application can run in an instance group of VMs with load balancing and autoscaling enabled. In this case, engineers will need to set thresholds for when to add or remove instances, but using autoscaling can reduce the number of times notifications are sent about VM resources utilization.

A further step toward automation is to use managed services when possible. Instead of running a relational database in Compute Engine to support a data warehouse, you could use BigQuery. In that case, there is no need to monitor servers because it is a serverless product. Of course, there are servers running BigQuery software, but they are managed by Google, which handles monitoring and incident response.

When you are alerted to a condition that needs your attention, you may need to collect information about the state of the system. Logs are excellent sources of such information.

#### Logging with Cloud Logging <a href="#head-3-113" id="head-3-113"></a>

_Cloud Logging_ is a centralized log management service. Logs are collections of messages that describe events in a system. Unlike metrics that are collected at regular intervals, log messages are written only when a particular type of event occurs. Here are some examples:

* If a new user account is added to an operating system and that account is granted root privileges, a log message with details about that account and who created it may be written to an audit log.
* When a database connection error message is received by an application, the application may write a log message with data about the time and the type of operation that would have been performed.
* Applications running on the Java virtual machine may need to reclaim memory that is no longer in use. This process is called _garbage collection_. Log messages may be written at the start and end of garbage collection.

Cloud Logging provides the ability to store, search, analyze, and monitor log messages from a variety of applications and cloud resources. An important feature of Cloud Logging is that it can store logs from virtually any application or resource, including GCP resources, other cloud resources, or applications running on premises. The Cloud Logging API accepts log messages from any source.

Some of the most important features of Cloud Logging are its search and analysis features. In addition to text searching, logs can be easily exported to BigQuery for more structured SQL-based analysis. Logging is also integrated with _Cloud Monitoring_, so log messages can trigger notifications from the alerting system. Metrics can also be created from log messages, so the functionality of _Cloud Monitoring_ is available for logs as well.

Applications and resources can create a large volume of log data. Cloud Logging will retain log messages for 30 days. If you would like to keep logs for longer, then you can export them to Cloud Storage or BigQuery for longer retention periods.

Log Analytics is a new feature in Pre-GA at the time of writing that allows you to directly query log data using BigQuery and SQL. When using Log Analytics, your log data is stored in a data set that is managed by BigQuery.

Logs can also be streamed to Cloud Pub/Sub if you would like to use third-party tools to perform near real-time operations on the log data.

To summarize, Cloud Operations Suite provides three essential tools for observing and understanding the state of services: monitoring, alerting, and log management. _Monitoring_ collects basic data about a wide array of resource characteristics and their performance over time. _Alerting_ evaluates metric data to determine when some predefined conditions are met, such as high CPU utilization for an extended period of time. _Logging_ collects messages about specific events related to application and resource operations. The combination of all three provides a foundation for understanding how complicated systems function and can help improve reliability.

#### Open Source Observability Tools <a href="#head-3-114" id="head-3-114"></a>

In addition to Google Cloud–specific services like Cloud Monitoring and Cloud Logging, you may also encounter widely used open source tools for monitoring and visualization. Prometheus and Grafana are two such open source tools.

**Prometheus**

Prometheus is a monitoring tool that collects metrics data from targets, such as applications, by scrapping HTTP endpoints of target services. The Prometheus project is hosted by the Cloud Native Computing Foundation, which also hosts Kubernetes.

Prometheus uses a multidimensional data model based on key-value pairs. For example, a metric about transaction latency may be collected at a specific point in time, in a particular environment, in a specific pod, and running a specific version of the application. The multidimensional model makes it relatively easy to query large data sets with many dimensions. PromQL is the query language used in Prometheus.

Prometheus includes a server for collecting metrics, client libraries for instrumenting applications and services, and an alert manager.

Managed Service for Prometheus is a managed service providing a Prometheus-compatible monitoring stack. At the time of writing, this service is available in Pre-GA release. This service uses Google's in-memory time-series database called Monarch, which Google uses to monitor its services.

![note](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781119871057/files/images/note.png) For more on Monarch, see Monarch: Google's Planet-Scale In-Memory Time-Series Database at [`research.google/pubs/pub50652`](https://research.google/pubs/pub50652/).

**Grafana**

Grafana is an open source visualization tool often used with Prometheus. Grafana queries data from existing data sources rather than importing data into a Grafana-managed data store. Grafana can pull data from monitoring services, relational databases, and time-series databases, as well as other sources.

One of the reasons Grafana is widely used is that it can bring different data together in a dashboard so you can visualize metrics from multiple systems at one time. There is no need for complex data integration pipelines before visualizing data.

### Release Management <a href="#head-2-64" id="head-2-64"></a>

_Release management_ is the practice of deploying code and configuration changes to environments, such as production, test, staging, and development environments. It is an integral part of _DevOps_, which combines software engineering and system administration efforts.

In the past, it was common to separate responsibilities for developing software and maintaining it in production. This kind of specialization often helps improve efficiency because individuals can develop skills and knowledge around a narrow domain. In economics, this is known as the _division of labor_. While division of labor can be efficient in many areas, it is less efficient in modern software development environments. Engineers who write code are often the best people to resolve problems with that code in production. Also, with proper tools, software engineers can deploy code to production efficiently so that it does not dramatically cut into the engineers' ability to write code.

Release management is important for reliability because it enables developers to put corrected code into production quickly. There should be no need to wait for some batch release of new code when a bug is found. With release management practices, developers can deploy code frequently and with small changes. This helps get fixes out faster, and it also reduces the risk of introducing new bugs by releasing small changes instead of a large set of changes. When problematic code is released, the same tools that help with deployments can be used to roll back bad deployments.

Release management tools also provide repositories for capturing information about releases. This can help promote standardized release procedures, which in turn can capture best practices over time and improve overall reliability.

#### Continuous Delivery <a href="#head-3-115" id="head-3-115"></a>

_Continuous delivery (CD)_ is the practice of releasing code soon after it is completed and after it passes all tests. CD is an automated process—there is usually no human in the loop. This allows for rapid deployment of code, but since humans other than the developer do not need to review the code prior to release, there is a higher risk of introducing bugs than there would be if a human were in the loop.

In practice, software developers should write comprehensive tests to detect bugs as early as possible. Teams that regularly write comprehensive tests and deploy in small increments find the trade-off favors rapid release. Other teams may find that they optimize their release management practice by having a human quality assurance (QA) engineer review the code and write additional tests.

When a human such as a QA engineer is involved, code may be ready for deployment but not deployed until it is reviewed.

**Tests**

A _test_ is a combination of input data and expected output. For example, if you were to test a new calculator function, you might pass the input 2 + 3 to the function and expect to see `5` as the output. There are several different kinds of tests used during deployments. These include the following:

* Unit tests
* Integration tests
* Acceptance tests
* Load testing

Tests promote reliability by reducing the risk that deployed code has errors that can disrupt the functioning of a service.

**Unit Tests**

A _unit test_ is a test that checks the smallest unit of testable code. This could be a function, an API endpoint, or another entry point into a program. Unit tests are designed to find bugs within the smallest unit. In the case of the previous calculator example, the unit test is checking the `+` operation in the calculator program.

**Integration Tests**

_Integration tests_ test a combination of units. For example, a RESTful API endpoint may receive a string expression that is passed to the calculator program. The purpose of an integration test is to ensure that the API endpoint properly passes the data to the calculator, receives the results, and returns the results to the caller. Even in this simple example, a number of things can go wrong. The API may pass a string that exceeds the maximum string size accepted by the calculator. The calculator may return its results in a data type that is incompatible with the data type the API function expects. These kinds of bugs are not likely to be caught with simple unit tests.

Integration tests can happen at multiple levels, depending on the complexity of the applications being tested. Testing a low-level API may only test the endpoint code and a single function called by that endpoint. In a more complicated scenario, an integration test can check an API endpoint that calls another API that runs some business logic on a server, which in turn has to query a database.

**Acceptance Tests**

Unit tests and integration tests help developers ensure that their code is functioning as they expect and want. _Acceptance tests_ are designed to assure business owners of the system that the code meets the business requirements of the system. A system can pass rigorous unit and integration tests and still fail to meet business requirements. For example, a code release may have unintentionally disabled some functionality that is unrelated to the code change, or a developer may have forgotten to code a necessary feature. If a developer forgets to include a feature, they are not likely to add a test for that feature.

**Load Testing**

_Load testing_ is used to understand how a system will perform under a particular set of conditions. A load test creates workloads for the system. This can be as simple as a series of API calls in a fixed time interval. It could also put a heavier-than-expected load on a system. This is useful for testing autoscaling within a cluster and rate limiting or other defensive measures within an application.

Load testing may find bugs that were not uncovered during integration testing. For example, under normal integration testing, a database may have no problem keeping up with the query load. Under heavy loads, the database may not be able to respond to all queries within a reasonable period of time. In that case, the connection will time out. This is a well-understood potential error, so the function should check for it. If it does not, the function could return an error to the code that called it, which in turn could then generate another error that is returned to its calling code. The pattern of returning errors can continue all the way up the application stack. Load testing is especially important when a system may be subject to spiking workloads.

**Deployment Strategies**

There are several different ways that engineers can deploy code to production systems. In many cases, applications run a collection of servers, and each server runs a separate instance of the application. Also, applications often have a single endpoint that all clients access. For example, an online retailer may have hundreds of servers running its catalog application, but all clients using that catalog use a URL that points to a load balancer for the application. Since many cloud applications are distributed like this, engineers have several options for updating the software on each server.

* Complete deployment
* Rolling deployment
* Canary deployment
* Blue/Green deployment

**Complete Deployment**

A _complete deployment_ updates all instances of the modified code at once. This was a common practice with teams that used waterfall methodologies. It is still the only option in some cases, for example, in the case of a single server running a monolithic application.

The other deployment strategies are generally preferred because they reduce the risk of introducing problematic code to all users at once or because they mitigate that risk by providing for rapid switch to the previously running code. Also, complete deployment may cause a service disruption.

**Rolling Deployment**

A _rolling deployment_ incrementally updates all servers over a period of time. For example, in a 10-server cluster, a deployment may be released to only one server at first. After a period of time, if there are no problems detected, a second server will be updated. This process continues until all of the servers are updated.

An advantage of rolling deployments is that you expose only a subset of users to the risk of disruptive code. In this example, assuming a balanced distribution of users, only 10 percent of the users were exposed to the initial deployment. Also, it is generally possible to do a rolling deployment without any service disruptions.

**Canary Deployment**

In a _canary deployment_, engineers release new code, but no traffic is routed to it at first. Once it is deployed, engineers route a small amount of traffic to the deployment. As time passes, if no problems are found, more traffic can be routed to the servers with the newly deployed code.

With a canary deployment, you may want to choose users randomly to route to the new version of code, or you may have criteria for choosing users. A web service that offers a free and paid version of its product may prefer to route free users to new deployments rather than expose paying customers to freshly released code.

**Blue/Green Deployment**

A _Blue/Green deployment_ strategy uses two production environments, named Blue and Green. They are configured similarly but run different code. At any point in time, one of them (for instance, Green) is the active production environment processing a live workload. The other (in other words, Blue) is available to deploy updated versions of software or new services where those changes can be tested. When testing is complete, workload is shifted from the current environment (Green) to the recently updated environment (Blue).

Blue/Green deployments mitigate the risk of a bad deployment by allowing developers and DevOps engineers to switch between environments quickly. For example, after testing the Blue environment, the workload is routed to the Blue environment. A sudden spike in workload reveals a bug in a service that causes a lag in processing a particular transaction. As soon as the problem is detected, which should be quickly because the environments should be instrumented so that DevOps engineers can monitor key service levels, the workload can be routed back to the Green environment.

If a deployment is completely stateless, then switching between the Blue/Green deployments can operate as just described. In reality, applications often require some kind of state management, such as recording data in a relational database. In this case, it is possible for the Blue and Green environments each to have databases with different information. DevOps engineers and developers will need to have a strategy for addressing this.

One approach is to have a single database that is used by both deployments. If there are only changes to the stateless application layer, then both the Blue and Green deployments can write to the same database. If you need to switch deployments, both will have access to the same data. When the database itself needs to be changed, it is a good practice to use a script to update the database schema to support both the current and the new application versions. Then test and verify that the database changes are correct before switching the application layer to the new environment. Once both the application and database changes are working correctly in the new environment, another database refactoring script can be run to remove any database structures from the older version of the database that are no longer needed.

#### Continuous Integration <a href="#head-3-116" id="head-3-116"></a>

_Continuous integration (CI)_ is the practice of incorporating new code into an established code base as soon as it is complete. For example, a software engineer may have a new algorithm to reduce the time and CPU required to perform a calculation. The code change is isolated to a single function. The software engineers tested the change in their local development and environment and are ready to perform the final tests and release the code to the staging environment.

If the software engineers had to build and compile the code manually, execute tests, and then deploy the code to the production environment for each change, it would make sense to wait until there were multiple changes and then release the code. After all, each of those steps can be time-consuming, and a software engineer's time should be maximized for developing features and services. Bundling multiple changes to code and releasing them as a batch was a common practice before the advent of DevOps. Today, we understand that continually integrating changes into baseline code and then testing and releasing that code is a faster and more efficient way to build software.

Another consequence of manual build and test is that it is much more likely than an automated process to introduce random and unexpected inconsistencies in the process.

Software engineers are able to integrate new code continuously into the production code base because of automated tools.

Tools include version control and test and build tools such as GitHub and Cloud Source Repository. _GitHub_ is a widely used code repository, and _Cloud Source Repository_ is Google Cloud's version control system. Developers can more easily collaborate when they all use a single repository that can keep track of changes to every component in a system. GitHub and Cloud Source Repository support other common practices, such as automatically executing tests or other checks such as verifying stylistic coding conventions and using the results as quality gates. Lint, for example, is the first static code analyzer developed for C. Today there are static code checkers for the many commonly used programming languages.

_Jenkins_ is a widely used CI tool that builds and tests code. Jenkins supports plugins to add functionality, such as integration with a particular version control system or support for different programming languages. _Google Cloud Build_ is a GCP service that provides software building services, and it is integrated with other GCP services, such as Cloud Source Repository.

CI and continuous delivery (CD) are widely used practices that contribute to systems reliability by enabling small, incremental changes to production systems while providing mechanisms to quickly roll back problematic changes that disrupt service functionality.

### Systems Reliability Engineering <a href="#head-2-65" id="head-2-65"></a>

While there are many aspects of systems engineering that we could discuss, this section focuses on building systems that are resilient to excessive loads and cascading failures.

#### Overload <a href="#head-3-117" id="head-3-117"></a>

One thing that service designers cannot control is the load that users may want to place on a system at any time. It is a good practice to assume that, at some point, your services will have more workload coming in than they can process. This can happen for a number of reasons, including the following:

* An external event, such as a marketing campaign that prompts new customers to start using a service
* An external event, such as an organizational change in a customer's company that triggers an increase in the workload that they send to your service
* An internal event, such as a bad deployment that causes an upstream service to generate more connection requests than ever generated previously
* An internal event, such as another development team releasing a new service that works as expected but puts an unexpected load on your service

While software engineers and site reliability engineers (SREs) cannot prevent overloads, they can decide how to respond to them.

One factor to consider when responding to overload is the criticality of an operation. Some operations may be considered more important than others and should always take precedence. Writing a message to an audit log, for example, may be more important from a business perspective than responding to a user query within a few seconds. For each of the following overload responses, be sure to take criticality into account when applying these methods.

**Shedding Load**

One of the simplest ways to respond to overload is to shed or drop data that exceeds the system's capacity to deal with it. A naïve load-shedding strategy would start dropping data as soon as some monitoring condition is met, for example, if CPU utilization exceeds some percentage for a period of time or if there are more connection requests than there are available connections in a connection pool. This approach may be easy to implement, but it is something of a crude tool: it does not consider criticality or variations in service-level agreements with users.

A variation on this approach is to categorize each type of operation or request that can be placed on a service according to criticality. The service can then implement a shedding mechanism that drops lower criticality requests first for some period of time. If overload continues, the next higher category of critical operations can be shed. This can continue until there is no longer an overload condition or until the highest-priority operations are being dropped.

Another approach is to consider service-level agreements with various users. The load from customers who use a service for free may be dropped before the load of paying customers. A business may define a specific volume of workload allowed for a customer based on the amount of money the customer pays for the service. Once the customer exceeds that limit, for example, 10,000 requests per minute, future requests are dropped until the next time period starts.

Software engineers can also use statistics to help decide when to shed requests. For example, a time series of measurements from an array of IoT devices may send data every 10 seconds. This approach could be used to sample the input stream to estimate descriptive statistics, such as the mean and standard deviation of the measurements over a period of time.

**Degrading Quality of Service**

Depending on the nature of a service, it may be possible to provide partial or approximate results.

For example, a service that runs a distributed query over multiple servers may return only the results from some of the servers instead of waiting for servers that are taking too long to respond. This is a common strategy for web search.

Alternatively, a service could use a sample of data rather than the entire population of data points to estimate the answer to a query. For instance, a data warehouse query that ranks sales regions by total sales last month could sample 10 percent of all sales data to create a sorted list of regions. Assuming that the sample is random and that the data is normally distributed, the ranking based on the sample is likely to be the same as the ranking that would be generated if every data point in the population were considered.

An advantage of responding to overload with a degraded service is that the service provides results rather than errors. This requires some planning on the part of software engineers to accommodate degraded or partial results. This approach does not work in all cases. There is no approximately completing a transaction, for example.

**Upstream Throttling**

Rather than having a service shed load or return approximate results, a calling service can slow down the rate at which it makes requests. This is known as _upstream throttling_. A client of a service can detect that calls to a service are returning errors or timing out. At that point, the calling service can cache the requests and wait until performance of the downstream service recovers. This requires planning on the part of software engineers, who must have a mechanism to hold requests. Alternatively, the client can shed requests instead of saving them to process at a later time.

Shedding is a good strategy if the data is time sensitive and no longer valuable after some period of time. Consider a downstream process that displays a chart of data from IoT devices for the previous 30 minutes. Data that arrived 30 minutes late would not be displayed and could be discarded without adversely affecting the quality of the visualizations.

When late-arriving data is still of value, then that data can be cached in the calling service or in an intermediate storage mechanism between the calling service and the called service. Message queues, like Cloud Pub/Sub, are often used to decouple the rate at which a service makes a request of another service and the rate at which the downstream service processes those requests.

One way to implement upstream throttling is to use the _Circuit Breaker pattern_. This design pattern uses an object that monitors the results of a function or service call. If the number of errors increases beyond a threshold, then the service stops making additional requests. This is known as _tripping_ the circuit breaker. Since the calling service stops sending new requests, the downstream service may be able to clear any backlog of work without having to contend with additional incoming requests.

The circuit breaker function waits some random period of time and then tries to make a request. If the request succeeds, the function can slowly increase the rate of requests while monitoring for errors. If there are few errors, then the calling service can continue to increase the rate of calls until it is back to normal operating rates.

#### Cascading Failures <a href="#head-3-118" id="head-3-118"></a>

_Cascading failures_ occur when a failure in one part of a distributed system causes a failure in another part of the system, which in turn causes another failure in some other service, and so on. For example, if a server in a cluster fails, additional load will be routed to other servers in the cluster. This could cause one of those servers to fail, which in turn will further increase the load on the remaining healthy servers.

Consider the example of a database failure. Imagine that a relational database attempts to write data to persistent storage but the device is full. The low-level data storage service that tries to find a block for the data fails to find one. This failure cascades up the database software stack to the code that issued the corresponding `INSERT` operation.

The `INSERT` operation in turn returns an error to the application function that tried to write the data. That application code detects the error and decides to retry the operation every 10 seconds until it succeeds or until it tries six times and fails each time. Now imagine that multiple calls to the database are failing and responding with the same strategy of retrying.

Now the application is held up for up to an additional 60 seconds for each transaction. This slows the processing of that service's workload. This causes a backlog of requests that in turn causes other services to fail. This kind of ripple effect of failures is a significant risk in distributed systems.

Cascading failures are often caused by resource exhaustion. Too much load is placed on system resources like memory, CPU, or database connections until those resources run out. At that point, services start to fail. A common response to this is to limit the load on failing services.

One way to deal with cascading failures is to use the Circuit Breaker design pattern described earlier. This can reduce the load of services and give them a chance to catch up on their workloads. It also gives calling services a chance to degrade gracefully by implementing a degraded quality-of-service strategy.

Another way to deal with cascading failures is to degrade the quality of service. This conserves resources, such as CPU, by reducing the amount of resources allocated to each service call.

When there is a risk of a cascading failure, the best one can hope for may be to contain the number and range of errors returned while services under stress recover.

To reduce the risk of cascading failures, load test services and autoscale resources. Be sure to set autoscaling parameters so that resources are added before failures begin. When setting autoscale parameters, it is important to be aware of the initialization time required before a new instance of a resource is fully available. For example, if the startup time for a service instance is more than 2 seconds, you shouldn't expect to see an improvement in the load until 5 to 15 seconds after the service instance starts. Also, consider the time that is required to bring a resource online. Plan for some period of time between when the need to scale is detected and the time that resource becomes available. Also consider when to release resources when the load drops.

If you are quick to drop resources, you may find that you have to add resources again when the load increases slightly. Adding and releasing resources in quick succession, known as _thrashing_, should be avoided. It is better to have spare capacity in resources rather than try to run resources at capacity.

#### Testing for Reliability <a href="#head-3-119" id="head-3-119"></a>

Testing is an important part of ensuring reliability. There are several kinds of tests, and all should be used to improve reliability. Testing for reliability includes practices used in CI/CD but adds others as well. These tests may be applied outside of the CI/CD process. They include the following:

* Unit tests
* Integration tests
* System tests
* Reliability stress tests

**Unit Tests**

_Unit tests_ are the simplest type of test. They are performed by software developers to ensure that the smallest unit of testable code functions as expected. As described in the discussion of CI/CD, these tests are often automated and performed before code is released outside the development environment.

**Integration Tests**

_Integration tests_ determine whether functional units operate as expected when used together. For example, an integration test may be used to check that an API function with an SQL query executes properly when executed against a staging database.

**System Tests**

_System tests_ include all integrated components and test whether an entire system functions as expected. These usually start with simple “sanity checks” that determine whether all of the components function under the simplest conditions.

Next, additional load is placed on the system using a performance test. This should uncover any problems with meeting expected workloads when the system is released to production.

A third type of system test is a _regression test_. This is designed to ensure that bugs that have been corrected in the past are not reintroduced to the system at a later time. Developers should create tests that check for specific bugs and execute each of those during testing.

**Reliability Stress Tests**

_Reliability stress tests_ place increasingly heavy load on a system until it breaks. The goal of these tests is to understand when a system will fail and how it will fail. For example, will excessive load trigger failures in the database before errors in the application layer? If the memory cache fails, will that immediately trigger a failure in the database?

Stress tests are useful for understanding cascading failures through a system. This can help guide your monitoring strategy so that you are sure to monitor components that can trigger cascading failures. It can also inform how you choose to invest development time to optimize reliability by focusing on potentially damaging cascading failures.

Another form of stress testing uses _chaos engineering_ tools such as Simian Army, which is a set of tools developed by Netflix to introduce failures randomly into functioning systems in order to study the impact of those failures. This is an example of the practice of chaos engineering. For more on Simian Army, see [`medium.com/netflix-techblog/the-netflix-simian-army-16e57fbab116`](https://medium.com/netflix-techblog/the-netflix-simian-army-16e57fbab116).

#### Incident Management and Post-Mortem Analysis <a href="#head-3-120" id="head-3-120"></a>

_Incidents_ are events that have a significant adverse impact on a service's ability to function. An incident may occur when customers cannot perform a task, retrieve information, or otherwise use a system that should be available.

Incidents may have a narrow impact, such as just affecting internal teams that depend on a service, or they can have broad impact, such as affecting all customers. Incidents are not minor problems that adversely affect only a small group, for example, a single team of developers. Incidents are service-level disruptions that impact multiple internal teams or external customers.

_Incident management_ is a set of practices that is used to identify the cause of a disruption, determine a response, implement the corrective action, and record details about the incident and decisions made in real time.

When an incident occurs, it is good practice to do the following:

* Identify an incident commander to coordinate a response.
* Have a well-defined operations team that analyzes the problems at hand and makes changes to the system to correct or remediate the problem.
* Maintain a log of actions taken, which is helpful during post-mortem analysis.

The goal of incident management is to correct problems and restore services to customers or other system users as soon as possible. There should be less focus on why the problem occurred or identifying who is responsible than on solving the immediate problem.

After an incident is resolved, it is a good practice to conduct a _post-mortem analysis_. During a post-mortem meeting, engineers share information about the chain of events that led to the incident. This could be something as simple as a bad deployment or resource exhaustion, or it could be more complicated, such as a failure in a critical service, for instance, DNS name resolution or an unexpected combination of unlikely events that alone would not have been problematic but together created a difficult-to-predict set of circumstances that led to the incident.

The goal of a post-mortem analysis is to identify the causes of an incident, understand why it happened, and determine what can be done to prevent it from happening again. Post-mortems do not assign blame; this is important for fostering an atmosphere of trust and honesty needed to ensure that all relevant details are understood. Post-mortems are opportunities to learn from failures. It is important to document conclusions and decisions for future reference.

### Summary <a href="#head-2-66" id="head-2-66"></a>

Reliability is a property of systems that measures the probability that a service will be available for a period of time. Several factors contribute to creating and maintaining reliable systems, including monitoring systems using metrics, logs, and alerts.

Continuous integration and continuous delivery are commonly used practices for managing the release of code. These practices reduce risk by emphasizing the frequent release of small changes. The use of automation helps to reduce the risk that a bad deployment will disrupt services for too long.

Systems reliability engineering is a set of practices that incorporates software engineering practices with operations management. These practices recognize that failures will occur, and the best way to deal with those failures is to set well-defined service-level objectives and service-level indicators, monitor systems to detect indications of failure, and learn from failures using techniques such as post-mortem analysis. Systems should be architected to anticipate problems, such as overloading and cascading failures. Testing is an essential part of promoting highly reliable systems.

### Exam Essentials <a href="#head-2-67" id="head-2-67"></a>

* Know the role of monitoring, logging, and alerting in maintaining reliable systems. Monitoring collects metrics, which are measurements of key attributes of a system, such as utilization rates. Metrics are often analyzed in a time series. Logging is used to record significant events in an application or infrastructure component. Alerting is the process of sending notifications to human operators when some condition is met indicating that a problem needs human intervention. Conditions are often of the form that a resource measurement exceeds some threshold for a specified period of time.
* Understand continuous delivery and continuous integration. CI/CD is the practice of releasing code soon after it is completed and after it passes all tests. This allows for rapid deployment of code. Continuous integration is the practice of incorporating code changes into baseline code frequently. Code is kept in a version control repository that is designed to support collaboration among multiple software engineers.
* Know the different kinds of tests that are used when deploying code. These include unit tests, integration tests, acceptance tests, and load testing. Unit tests check the smallest unit of functional code. Integration tests check that a combination of units function correctly together. Acceptance tests determine whether code meets the requirements of the system. Load testing measures how well the system responds to increasing levels of load.
* Understand that systems reliability engineering is a practice that combines software engineering practices with operations management to reduce risk and increase the reliability of systems. The core tenets of systems reliability engineering include the following:
  * Automating systems operations as much as possible
  * Understanding and accepting risk and implementing practices that mitigate risk
  * Learning from incidents
  * Quantifying service-level objectives and service-level indicators
  * Measuring performance
* Know that systems reliability engineering includes design practices, such as planning for overload, cascading failures, and incident response. Overload is when a workload on a system exceeds the capabilities of the system to process the workload in the time allowed. Ways of dealing with overload include load shedding, degrading service, and upstream or client throttling. Cascading failures occur when a failure leads to an action that causes further failures. An example is that in response to a failure in a server in a cluster, a load balancer shifts additional workload to the remaining healthy servers, causing them to fail due to overload. Incident response is the practice of controlling failures by using a structured process to identify the cause of a problem, correct the problem, and learn from the problem.
* Know testing is an important part of reliability engineering. There are several kinds of tests, and all should be used to improve reliability. Testing for reliability includes practices used in CI/CD but adds others as well, particularly stress testing. These tests may be applied outside of the CI/CD process.

### Review Questions <a href="#head-2-68" id="head-2-68"></a>

1. As an SRE, you are assigned to support several applications. In the past, these applications have had significant reliability problems. You would like to understand the performance characteristics of the applications, so you create a set of dashboards. What kind of data would you display on those dashboards?
   1. Metrics and time-series data measuring key performance attributes, such as CPU utilization
   2. Detailed log data from syslog
   3. Error messages output from each application
   4. Results from the latest acceptance tests
2. After determining the optimal combination of CPU and memory resources for nodes in a Kubernetes cluster, you want to be notified whenever CPU utilization exceeds 85 percent for 5 minutes or when memory utilization exceeds 90 percent for 1 minute. What would you have to specify to receive such notifications?
   1. An alerting condition
   2. An alerting policy
   3. A logging message specification
   4. An acceptance test
3. A compliance review team is seeking information about how your team handles high-risk administration operations, such as granting operating system users root privileges. Where could you find data that shows your team tracks changes to user privileges?
   1. In metric time-series data
   2. In alerting conditions
   3. In audit logs
   4. In ad hoc notes kept by system administrators
4. Release management practices contribute to improving reliability by which one of the following?
   1. Advocating for object-oriented programming practices
   2. Enforcing waterfall methodologies
   3. Improving the speed and reducing the cost of deploying code
   4. Reducing the use of stateful services
5. A team of software engineers is using release management practices. They want developers to check code into the central team code repository several times during the day. The team also wants to make sure that the code that is checked is functioning as expected before building the entire application. What kind of tests should the team run before attempting to build the application?
   1. Unit tests
   2. Stress tests
   3. Acceptance tests
   4. Compliance tests
6. Developers have just deployed a code change to production. They are not routing any traffic to the new deployment yet, but they are about to send a small amount of traffic to servers running the new version of code. What kind of deployment are they using?
   1. Blue/Green deployment
   2. Before/After deployment
   3. Canary deployment
   4. Stress deployment
7. You have been hired to consult with an enterprise software development that is starting to adopt Agile and DevOps practices. The developers would like advice on tools that they can use to help them collaborate on software development in the Google Cloud. What version control software might you recommend?
   1. Jenkins and Cloud Source Repositories
   2. Syslog and Cloud Build
   3. GitHub and Cloud Build
   4. GitHub and Cloud Source Repositories
8. A startup offers a software-as-a-service solution for enterprise customers. Many of the components of the service are stateful, and the system has not been designed to allow incremental rollout of new code. The entire environment has to be running the same version of the deployed code. What deployment strategy should they use?
   1. Rolling deployment
   2. Canary deployment
   3. Stress deployment
   4. Blue/Green deployment
9. A service is experiencing unexpectedly high volumes of traffic. Some components of the system are able to keep up with the workload, but others are unable to process the volume of requests. These services are returning a large number of internal server errors. Developers need to release a patch as soon as possible that provides some relief for an overloaded relational database service. Both memory and CPU utilization are near 100 percent. Horizontally scaling the relational database is not an option, and vertically scaling the database would require too much downtime. What strategy would be the fastest to implement?
   1. Shed load
   2. Increase connection pool size in the database
   3. Partition the workload
   4. Store data in a Pub/Sub topic
10. A service has detected that a downstream process is returning a large number of errors. The service automatically slows down the number of messages it sends to the downstream process. This is an example of what kind of strategy?
    1. Load shedding
    2. Upstream throttling
    3. Rebalancing
    4. Partitioning
