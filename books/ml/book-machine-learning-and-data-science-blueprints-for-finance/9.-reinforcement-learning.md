# 9. Reinforcement Learning

## Chapter 9. Reinforcement Learning

Incentives drive nearly everything, and finance is not an exception. Humans do not learn from millions of labeled examples. Instead, we often learn from positive or negative experiences that we associate with our actions. Learning from experiences and the associated rewards or punishments is the core idea behind reinforcement learning (RL).[1](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864666585896)

Reinforcement learning is an approach toward training a machine to find the best course of action through optimal policies that maximize rewards and minimize punishments.

The RL algorithms that empowered _AlphaGo_ (the first computer program to defeat a professional human Go player) are also finding inroads into finance. Reinforcement learning‚Äôs main idea of _maximizing the rewards_ aligns beautifully with several areas in finance, including algorithmic trading and portfolio management. Reinforcement learning is particularly suitable for algorithmic trading, because the concept of a _return-maximizing agent_ in an uncertain, dynamic environment has much in common with an investor or a trading strategy that interacts with financial markets. Reinforcement learning‚Äìbased models go one step further than the price prediction‚Äìbased trading strategies discussed in previous chapters and determine rule-based policies for actions (i.e., place an order, do nothing, cancel an order, and so on).

Similarly, in portfolio management and asset allocation, reinforcement learning‚Äìbased algorithms do not yield predictions and do not learn the structure of the market implicitly. They do more. They directly learn the policy of changing the portfolio allocation weights dynamically in the continuously changing market. Reinforcement learning models are also useful for order execution problems, which involve the process of completing a buy or sell order for a market instrument. Here, the algorithms learn through trial and error, figuring out the optimal path of execution on their own.

Reinforcement learning algorithms, with their ability to tackle more nuances and parameters within the operational environment, can also produce derivatives hedging strategies. Unlike traditional finance-based hedging strategies, these hedging strategies are optimal and valid under real-world market frictions, such as transaction costs, market impact, liquidity constraints, and risk limits.

In this chapter, we cover three reinforcement learning‚Äìbased case studies covering major finance applications: algorithmic trading, derivatives hedging, and portfolio allocation. In terms of the model development steps, the case studies follow a standardized seven-step model development process presented in [Chapter 2](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch02.html#Chapter2). Model development and evaluation are key steps for reinforcement learning, and these steps will be emphasized. With multiple concepts in machine learning and finance implemented, these case studies can be used as a blueprint for any other reinforcement learning‚Äìbased problem in finance.

In [‚ÄúCase Study 1: Reinforcement Learning‚ÄìBased Trading Strategy‚Äù](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#CaseStudy1RL), we demonstrate the use of RL to develop an algorithmic trading strategy.

In [‚ÄúCase Study 2: Derivatives Hedging‚Äù](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#CaseStudy2RL), we implement and analyze reinforcement learning‚Äìbased techniques to calculate the optimal hedging strategies for portfolios of derivatives under market frictions.

In [‚ÄúCase Study 3: Portfolio Allocation‚Äù](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#CaseStudy3RL), we illustrate the use of a reinforcement learning‚Äìbased technique on a dataset of cryptocurrency in order to allocate capital into different cryptocurrencies to maximize risk-adjusted returns. We also introduce a reinforcement learning‚Äìbased _simulation environment_ to train and test the model.



In addition to the points mentioned above, readers will understand the following points by the end of this chapter:

* Key components of reinforcement learning (i.e., reward, agent, environment, action, and policy).
* Model-based and model-free algorithms for reinforcement learning along with policy and value-based models.
* Fundamental approaches to solving reinforcement learning problems, such as Markov decision processes (MDP), temporal difference (TD) learning, and artificial neural networks (ANNs).
* Methods to train and test value-based and policy-based reinforcement learning algorithms using artificial neural networks and deep learning.
* How to set up an agent or simulation environment for reinforcement learning problems using Python.
* How to design and implement a problem statement related to algorithmic trading strategy, portfolio management, and instrument hedging in a classification-based machine learning framework.

## THIS CHAPTER‚ÄôS CODE REPOSITORY

A Python-based Jupyter notebook for all the case studies presented in this chapter is included under the folder [Chapter 9 - Reinforcement Learning](https://oreil.ly/Fp0xD) in the code repository for this book. To work through any machine learning problems in Python involving RL models (such as DQN or policy gradient) presented in this chapter, readers need to modify the template slightly to align with their problem statement.

## Reinforcement Learning‚ÄîTheory and Concepts

Reinforcement learning is an extensive topic covering a wide range of concepts and terminology. The theory section of this chapter covers the items and topics listed in [Figure 9-1](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#RLConcepts).[2](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864666519384)

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_0901.png" alt="mlbf 0901" height="469" width="1039"><figcaption></figcaption></figure>

**Figure 9-1. RL summary of concepts**

In order to solve any problem using RL, it is important to first understand and define the RL components.

### RL Components

The main components of an RL system are agent, actions, environment, state, and reward.

Agent

The entity that performs actions.

Actions

The things an agent can do within its environment.

Environment

The world in which the agent resides.

State

The current situation.

Reward

The immediate return sent by the environment to evaluate the last action by the agent.

The goal of reinforcement learning is to learn an optimal strategy through experimental trials and relatively simple feedback loops. With the optimal strategy, the agent is capable of actively adapting to the environment to maximize the rewards. Unlike in supervised learning, these reward signals are not given to the model immediately. Instead, they are returned as a consequence of a sequence of actions that the agent makes.

An agent‚Äôs actions are usually conditioned on what the agent perceives from the environment. What the agent perceives is referred to as the observation or the state of the environment. [Figure 9-2](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#RLComp) summarizes the components of a reinforcement learning system.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_0902.png" alt="mlbf 0902" height="594" width="836"><figcaption></figcaption></figure>

**Figure 9-2. RL components**

The interaction between the agent and the environment involves a sequence of actions and observed rewards in time, ÔøΩ=1,2...ÔøΩ. During the process, the agent accumulates knowledge about the environment, learns the optimal policy, and makes decisions on which action to take next so as to efficiently learn the best policy. Let‚Äôs label the state, action, and reward at time step _t_ as ÔøΩÔøΩ,ÔøΩÔøΩ...ÔøΩÔøΩ, respectively. Thus, the interaction sequence is fully described by one episode (also known as ‚Äútrial‚Äù or ‚Äútrajectory‚Äù), and the sequence ends at the terminal state ÔøΩÔøΩ:ÔøΩ1,ÔøΩ1,ÔøΩ2,ÔøΩ2,ÔøΩ2...ÔøΩÔøΩ.

In addition to the five components of reinforcement learning mentioned so far, there are three additional components of reinforcement learning: policy, value function (and Q-value), and model of the environment. Let us discuss the components in detail.

#### Policy

A policy is an algorithm or a set of rules that describes how an agent makes its decisions. More formally, a policy is a function, usually denoted as _œÄ_, that maps a state (_s_) and an action (_a_):

ÔøΩÔøΩ=ÔøΩ(ÔøΩÔøΩ)

This means that an agent decides its action given its current state. The policy can be can be either deterministic or stochastic. A deterministic policy maps a state to actions. On the other hand, a stochastic policy outputs a probability distribution over actions. It means that instead of being sure of taking action _a_, there is a probability assigned to the action given a state.

Our goal in reinforcement learning is to learn an optimal policy (which is also referred to as ÔøΩ\*). An optimal policy tells us how to act to maximize return in every state.

#### Value function (and Q-value)

The goal of a reinforcement learning agent is to learn to perform a task well in an environment. Mathematically, this means maximizing the future reward, or cumulative discounted reward, ÔøΩ, which can be expressed in the following equation as a function of reward function ÔøΩ at different times:

ÔøΩÔøΩ=ÔøΩÔøΩ+1+ÔøΩÔøΩÔøΩ+2+...=‚àë0‚àûÔøΩÔøΩÔøΩÔøΩ+ÔøΩ+1

The discounting factor ÔøΩ is a value between 0 and 1 to penalize the rewards in the future, as future rewards do not provide immediate benefits and may have higher uncertainty. Future reward is an important input to the value function.

The value function (or state value) measures the attractiveness of a state through a prediction of future reward ÔøΩÔøΩ. The value function of a state _s_ is the expected return, with a policy ÔøΩ if we are in this state at time _t_:

ÔøΩ(ÔøΩ)=ÔøΩ\[ÔøΩÔøΩ|ÔøΩÔøΩ=ÔøΩ]

Similarly, we define the action-value function (Q-value) of a state-action pair (ÔøΩ,ÔøΩ) as:

ÔøΩ(ÔøΩ,ÔøΩ)=ÔøΩ\[ÔøΩÔøΩ|ÔøΩÔøΩ=ÔøΩ,ÔøΩÔøΩ=ÔøΩ]

So the value function is the expected return for a state following a policy ÔøΩ. The Q-value is the expected reward for the state-action pair following a policy ÔøΩ.

The value function and the Q-value are interconnected as well. Since we follow the target policy ÔøΩ, we can make use of the probability distribution over possible actions and the Q-values to recover the value function:

ÔøΩ(ÔøΩ)=‚àëÔøΩ‚ààÔøΩÔøΩ(ÔøΩ,ÔøΩ)ÔøΩ(ÔøΩ|ÔøΩ)

The preceding equation represents the relationship between the value function and Q-value.

The relationship between reward function (ÔøΩ), future rewards (ÔøΩ), value function, and Q-value is used to derive the Bellman equations (discussed later in this chapter), which are one of the key components of many reinforcement learning models.

#### Model

The model is a descriptor of the environment. With the model, we can learn or infer how the environment would interact with and provide feedback to the agent. Models are used for _planning_, by which we mean any way of deciding on a course of action by considering possible future situations. A model of the stock market, for example, is tasked with predicting what the prices will look like in the future. The model has two major parts: _transition probability function_ (_P_) and _reward function_. We already discussed the reward function. The transition function (_P_) records the probability of transitioning from one state to another after taking an action.

Overall, an RL agent may be directly or indirectly trying to learn a policy or value function shown in [Figure 9-3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#ModelValPolicy). The approach to learning a policy varies depending on the RL model type. When we fully know the environment, we can find the optimal solution by using _model-based approaches_.[3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864666357976) When we do not know the environment, we follow a _model-free approach_ and try to learn the model explicitly as part of the algorithm.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_0903.png" alt="mlbf 0903" height="508" width="659"><figcaption></figcaption></figure>

**Figure 9-3. Model, value, and policy**

#### RL components in a trading context

Let‚Äôs try to understand what the RL components correspond to in a trading setting:

Agent

The agent is our trading agent. We can think of the agent as a human trader who makes trading decisions based on the current state of the exchange and their account.

Action

There would be three actions: _Buy, Hold,_ and _Sell_.

Reward function

An obvious reward function would be the _realized PnL (Profit and Loss)_. Other reward functions can be _Sharpe ratio_ or _maximum drawdown_.[4](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864666341672) There can be a wide range of complex reward functions that offer a trade-off between profit and risk.

Environment

The environment in a trading context would be the _exchange_. In the case of trading on an exchange, we do not observe the complete state of the environment. Specifically, we are unaware of the other agents, and what an agent observes is not the true state of the environment but some derivation of it.

This is referred to as a _partially observable Markov decision process_ (POMDP). This is the most common type of environment that we encounter in finance.

### RL Modeling Framework

In this section, we describe the core framework of reinforcement learning used across several RL models.

#### Bellman equations

Bellman equations refer to a set of equations that decompose the value function and Q-value into the immediate reward plus the discounted future values.

In RL, the main aim of an agent is to get the most expected sum of rewards from every state it lands in. To achieve that, we must try to get the optimal value function and Q-value; the Bellman equations help us to do so.

We use the relationship between reward function (R), future rewards (G), value function, and Q-value to derive the Bellman equation for value function, as shown in [Equation 9-1](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#BelEqValFunc).

**Equation 9-1. Bellman equation for value function**

ÔøΩ(ÔøΩ)=ÔøΩ\[ÔøΩÔøΩ+1+ÔøΩÔøΩ(ÔøΩÔøΩ+1)|ÔøΩÔøΩ=ÔøΩ]

Here, the value function is decomposed into two parts; an immediate reward, ÔøΩÔøΩ+1, and the discounted value of the successor state, ÔøΩÔøΩ(ÔøΩÔøΩ+1), as shown in the preceding equation. Hence, we have broken down the problem into the immediate reward and the discounted successor state. The state value _V(s)_ for the state _s_ at time _t_ can be computed using the current reward ÔøΩÔøΩ+1 and the value function at the time _t_+1. This is the Bellman equation for value function. This equation can be maximized to get an equation called Bellman Optimality Equation for value function, represented by _V\*(s)_.

We follow a very similar algorithm to estimate the optimal state-action values (Q-values). The simplified iteration algorithms for value function and Q-value are shown in Equations [9-2](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#IterationAlgoV) and [9-3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#IterationAlgoQ).

**Equation 9-2. Iteration algorithm for value function**

ÔøΩÔøΩ+1(ÔøΩ)=ÔøΩÔøΩÔøΩÔøΩ‚àëÔøΩ‚Ä≤ÔøΩÔøΩÔøΩ‚Ä≤ÔøΩÔøΩÔøΩÔøΩ‚Ä≤ÔøΩ+ÔøΩÔøΩÔøΩ(ÔøΩ‚Ä≤)

**Equation 9-3. Iteration algorithm for Q-value**

ÔøΩÔøΩ+1(ÔøΩ,ÔøΩ)=‚àëÔøΩ‚Ä≤ÔøΩÔøΩÔøΩ‚Ä≤ÔøΩ\[ÔøΩÔøΩÔøΩ‚Ä≤ÔøΩ+ÔøΩ\*ÔøΩÔøΩ‚Ä≤ÔøΩÔøΩ\*ÔøΩÔøΩ(ÔøΩ‚Ä≤,ÔøΩ‚Ä≤)]

where

* ÔøΩÔøΩÔøΩ‚Ä≤ÔøΩ is the transition probability from state _s_ to state s‚Ä≤, given that action _a_ was chosen.
* ÔøΩÔøΩÔøΩ‚Ä≤ÔøΩ is the reward that the agent gets when it goes from state _s_ to state s‚Ä≤, given that action _a_ was chosen.

Bellman equations are important because they let us express values of states as values of other states. This means that if we know the value function or Q-value of _s_t+1, we can very easily calculate the value of _s_t. This opens a lot of doors for iterative approaches for calculating the value for each state, since if we know the value of the next state, we can know the value of the current state.

If we have complete information about the environment, the iteration algorithms shown in Equations [9-2](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#IterationAlgoV) and [9-3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#IterationAlgoQ) turn into a planning problem, solvable by dynamic programming that we will demonstrate in the next section. Unfortunately, in most scenarios, we do not know ÔøΩÔøΩÔøΩ‚Ä≤ or ÔøΩÔøΩÔøΩ‚Ä≤ and thus cannot apply the Bellman equations directly, but they lay the theoretical foundation for many RL algorithms.

#### Markov decision processes

Almost all RL problems can be framed as Markov decision processes (MDPs). MDPs formally describe an environment for reinforcement learning. A Markov decision process consists of five elements: ÔøΩ=ÔøΩ,ÔøΩ,ÔøΩ,ÔøΩ,ÔøΩ, where the symbols carry the same meanings as defined in the previous section:

* _S_: a set of states
* _A_: a set of actions
* _P_: transition probability
* _R_: reward function
* _Œ≥_: discounting factor for future rewards

MDPs frame the agent‚Äìenvironment interaction as a sequential decision problem over a series of time steps t = 1, ‚Ä¶, T. The agent and the environment interact continually, the agent selecting actions and the environment responding to these actions and presenting new situations to the agent, with the aim of coming up with an optimal policy or strategy. Bellman equations form the basis for the overall algorithm.

All states in MDP have the Markov property, referring to the fact that the future depends only on the current state, not on the history.

Let us look into an example of MDP in a financial context and analyze the Bellman equation. Trading in the market can be formalized as an MDP, which is a process that has specified transition probabilities from state to state. [Figure 9-4](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#MDP) shows an example of MDP in the financial market, with a set of states, transition probability, action, and reward.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_0904.png" alt="mlbf 0904" height="862" width="1014"><figcaption></figcaption></figure>

**Figure 9-4. Markov decision process**

The MDP presented here has three states: bull, bear, and stagnant market, represented by three states (s0, s1, s2). The three actions of a trader are hold, buy, and sell, represented by a0, a1, a2, respectively. This is a hypothetical setup in which we assume that transition probabilities are known and the action of the trader leads to a change in the state of the market. In the subsequent sections we will look at approaches for solving RL problems without making such assumptions. The chart also shows the transition probabilities and the rewards for different actions. If we start in state s0 (bull market), the agent can choose between actions a0, a1, a2 (sell, buy, or hold). If it chooses action buy (a1), it remains in state s0 with certainty, and without any reward. It can thus decide to stay there forever if it wants. But if it chooses action hold (a0), it has a 70% probability of gaining a reward of +50, and remaining in state s0. It can then try again to gain as much reward as possible. But at some point, it is going to end up instead in state s1 (stagnant market). In state s1 it has only two possible actions: hold (a0) or buy (a1). It can choose to stay put by repeatedly choosing action a1, or it can choose to move on to state s2 (bear market) and get a negative reward of ‚Äì250. In state s3 it has no other choice than to take action buy (a1), which will most likely lead it back to state s0 (bull market), gaining a reward of +200 on the way.

Now, by looking at this MDP, it is possible to come up with an optimal policy or a strategy to achieve the most reward over time. In state s0 it is clear that action a0 is the best option, and in state s2 the agent has no choice but to take action a1, but in state s1 it is not obvious whether the agent should stay put (a0) or sell (a2).

Let‚Äôs apply the following Bellman equation as per [Equation 9-3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#IterationAlgoQ) to get the optimal Q-value:

ÔøΩÔøΩ+1(ÔøΩ,ÔøΩ)=‚àëÔøΩ‚Ä≤ÔøΩÔøΩÔøΩ‚Ä≤ÔøΩ\[ÔøΩÔøΩÔøΩ‚Ä≤ÔøΩ+ÔøΩ\*ÔøΩÔøΩ‚Ä≤ÔøΩÔøΩ\*ÔøΩÔøΩ(ÔøΩ‚Ä≤,ÔøΩ‚Ä≤)]

```
import numpy as np
nan=np.nan # represents impossible actions
#Array for transition probability
P = np.array([ # shape=[s, a, s']
[[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],
[[0.0, 1.0, 0.0], [nan, nan, nan], [0.0, 0.0, 1.0]],
[[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]],
])

# Array for the return
R = np.array([ # shape=[s, a, s']
[[50., 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],
[[50., 0.0, 0.0], [nan, nan, nan], [0.0, 0.0, -250.]],
[[nan, nan, nan], [200., 0.0, 0.0], [nan, nan, nan]],
])
#Actions
A = [[0, 1, 2], [0, 2], [1]]
#The data already obtained from yahoo finance is imported.

#Now let's run the Q-Value Iteration algorithm:
Q = np.full((3, 3), -np.inf) # -inf for impossible actions
for state, actions in enumerate(A):
    Q[state, actions] = 0.0 # Initial value = 0.0, for all possible actions
discount_rate = 0.95
n_iterations = 100
for iteration in range(n_iterations):
    Q_prev = Q.copy()
    for s in range(3):
        for a in A[s]:
            Q[s, a] = np.sum([
                T[s, a, sp] * (R[s, a, sp] + discount_rate * np.max(Q_prev[sp]))
        for sp in range(3)])
print(Q)
```

`Output`

```
[[109.43230584 103.95749333  84.274035  ]
 [  5.5402017          -inf   5.83515676]
 [        -inf 269.30353051         -inf]]
```

This gives us the optimal policy (Q-value) for this MDP, when using a discount rate of 0.95. Looking for the highest Q-value for each of the states: in a bull market (s0) choose action hold (a0); in a stagnant market (s1) choose action sell (a2); and in a bear market (s2) choose action buy (a1).

The preceding example is a demonstration of a dynamic programming (DP) algorithm for obtaining optimal policy. These methods make an unrealistic assumption of complete knowledge of the environment but are the conceptual foundations for most other approaches.

#### Temporal difference learning

Reinforcement learning problems with discrete actions can often be modeled as Markov decision processes, as we saw in the previous example, but in most cases the agent initially has no insight into the transition probabilities. It also does not know what the rewards are going to be. This is where temporal difference (TD) learning can be useful.

A TD learning algorithm is very similar to the value iteration algorithm ([Equation 9-2](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#IterationAlgoV)) based on the Bellman equation but is tweaked to take into account the fact that the agent has only partial knowledge of the MDP. In general, we assume that the agent initially knows only the possible states and actions and nothing more. For example, the agent uses an exploration policy, a purely random policy, to explore the MDP, and as it progresses, the TD learning algorithm updates the estimates of the state values based on the transitions and rewards that are actually observed.

The key idea in TD learning is to update the value function V(_St_) toward an estimated return ÔøΩÔøΩ+1+ÔøΩÔøΩ(ÔøΩÔøΩ+1) (known as the _TD target_). The extent to which we want to update the value function is controlled by the _learning rate_ hyperparameter _Œ±_, which defines how aggressive we want to be when updating our value. When _Œ±_ is close to zero, we‚Äôre not updating very aggressively. When _Œ±_ is close to one, we‚Äôre simply replacing the old value with the updated value:

ÔøΩ(ÔøΩÔøΩ)‚ÜêÔøΩ(ÔøΩÔøΩ)+ÔøΩ(ÔøΩÔøΩ+1+ÔøΩÔøΩ(ÔøΩÔøΩ+1)‚ÄìÔøΩ(ÔøΩÔøΩ))

Similarly, for Q-value estimation:

ÔøΩ(ÔøΩÔøΩ,ÔøΩÔøΩ)‚ÜêÔøΩ(ÔøΩÔøΩ,ÔøΩÔøΩ)+ÔøΩ(ÔøΩÔøΩ+1+ÔøΩÔøΩ(ÔøΩÔøΩ+1,ÔøΩÔøΩ+1)‚ÄìÔøΩ(ÔøΩÔøΩ,ÔøΩÔøΩ))

Many RL models use the TD learning algorithm that we will see in the next section.

#### Artificial neural network and deep learning

Reinforcement learning models often leverage an artificial neural network and deep learning methods to approximate a value or policy function. That is, ANN can learn to map states to values, or state-action pairs to Q-values. ANNs use _coefficients_, or _weights_, to approximate the function relating inputs to outputs. In the context of RL, the learning of ANNs means finding the right weights by iteratively adjusting them in such a way that the rewards are maximized. Refer to [3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#Chapter3) and [5](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch05.html#Chapter5) for more details on methods related to ANN (including deep learning).

### Reinforcement Learning Models

Reinforcement learning can be categorized into _model-based_ and _model-free_ algorithms, based on whether the rewards and probabilities for each step are readily accessible.

#### Model-based algorithms

Model-based algorithms try to understand the environment and create a model to represent it. When the RL problem includes well-defined transition probabilities and a limited number of states and actions, it can be framed as a _finite MDP_ for which dynamic programming (DP) can compute an exact solution, similar to the previous example.[5](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864665781608)

#### Model-free algorithms

Model-free algorithms try to maximize the expected reward only from real experience, without a model or prior knowledge. Model-free algorithms are used when we have incomplete information about the model. The agent‚Äôs policy _œÄ(s)_ provides the guideline on what is the optimal action to take in a certain state with the goal of maximizing the total rewards. Each state is associated with a value function _V(s)_ predicting the expected amount of future rewards we are able to receive in this state by acting on the corresponding policy. In other words, the value function quantifies how good a state is. Model-free algorithms are further divided into _value-based_ and _policy-based_. Value-based algorithms learn the state, or Q-value, by choosing the best action in a state. These algorithms are generally based upon temporal difference learning that we discussed in the RL framework section. Policy-based algorithms (also known as _direct policy search_) directly learn an optimal policy that maps state to action (or tries to approximate optimal policy, if true optimal policy is not attainable).

In most situations in finance, we do not fully know the environment, rewards, or transition probabilities, and we must fall back to model-free algorithms and related approaches.[6](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864665772264) Hence, the focus of the next section and of the case studies will be the model-free methods and related algorithms.

[Figure 9-5](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#TaxRLModels) shows a taxonomy of model-free reinforcement learning. We highly recommend that readers refer to _Reinforcement Learning: An Introduction_ for a more in-depth understanding of the algorithms and the concepts.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_0905.png" alt="mlbf 0905" height="782" width="1184"><figcaption></figcaption></figure>

**Figure 9-5. Taxonomy of RL models**

In the context of model-free methods, temporal difference learning is one of the most used approaches. In TD, the algorithm refines its estimates based on its own prior estimates. The value-based algorithms _Q-learning_ and _SARSA_ use this approach.

Model-free methods often leverage an artificial neural network to approximate a value or policy function. _Policy gradient_ and _deep Q-network (DQN)_ are two commonly used model-free algorithms that use artificial neutral networks. Policy gradient is a policy-based approach that directly parameterizes the policy. Deep Q-network is a value-based method that combines deep learning with _Q-learning_, which sets the learning objective to optimize the estimates of Q-value.[7](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864665764040)

#### Q-Learning

_Q-learning_ is an adaptation of TD learning. The algorithm evaluates which action to take based on a Q-value (or action-value) function that determines the value of being in a certain state and taking a certain action at that state. For each state-action pair _(s, a)_, this algorithm keeps track of a running average of the rewards, _R_, which the agent gets upon leaving the state _s_ with action _a_, plus the rewards it expects to earn later. Since the target policy would act optimally, we take the maximum of the Q-value estimates for the next state.

The learning proceeds _off-policy_‚Äîthat is, the algorithm does _not_ need to select actions based on the policy that is implied by the value function alone. However, convergence requires that all state-action pairs continue to be updated throughout the training process, and a straightforward way to ensure that this occurs is to use an _Œµ-greedy_ policy, which is defined further in the following section.

The steps of Q-learning are:

1. At time step _t_, we start from state _st_ and pick an action according to Q-values, ÔøΩÔøΩ=ÔøΩÔøΩÔøΩÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ).
2. We apply an _Œµ-greedy_ approach that selects an action randomly with a probability of _Œµ_ or otherwise chooses the best action according to the Q-value function. This ensures the exploration of new actions in a given state while also exploiting the learning experience.[8](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864665739960)
3. With action _at_, we observe reward _Rt+1_ and get into the next state _St+1_.
4.  We update the action-value function:

    ÔøΩ(ÔøΩÔøΩ,ÔøΩÔøΩ)‚ÜêÔøΩ(ÔøΩÔøΩ,ÔøΩÔøΩ)+ÔøΩ(ÔøΩÔøΩ+1+ÔøΩmaxÔøΩÔøΩ(ÔøΩÔøΩ+1,ÔøΩÔøΩ)‚ÄìÔøΩ(ÔøΩÔøΩ,ÔøΩÔøΩ))
5. We increment the time step, _t = t+1_, and repeat the steps.

Given enough iterations of the steps above, this algorithm will converge to the optimal Q-value.

#### SARSA

SARSA is also a TD learning‚Äìbased algorithm. It refers to the procedure of updating the Q-value by following a sequence of ...ÔøΩÔøΩ,ÔøΩÔøΩ,ÔøΩÔøΩ+1,ÔøΩÔøΩ+1,ÔøΩÔøΩ+1,.... The first two steps of SARSA are similar to the steps of Q-learning. However, unlike Q-learning, SARSA is an _on-policy_ algorithm in which the agent grasps the optimal policy and uses the same to act. In this algorithm, the policies used for _updating_ and for _acting_ are the same. Q-learning is considered an _off-policy_ algorithm.

#### Deep Q-Network

In the previous section, we saw how Q-learning allows us to learn the optimal Q-value function in an environment with discrete state actions using iterative updates based on the Bellman equation. However, Q-learning may have the following drawbacks:

* In cases where the state and action space are large, the optimal Q-value table quickly becomes computationally infeasible.
* Q-learning may suffer from instability and divergence.

To address these shortcomings, we use ANNs to approximate Q-values. For example, if we use a function with parameter _Œ∏_ to calculate Q-values, we can label the Q-value function as _Q(s,a;Œ∏)_. The deep Q-learning algorithm approximates the Q-values by learning a set of weights, _Œ∏_, of a multilayered deep Q-network that maps states to actions. The algorithm aims to greatly improve and stabilize the training procedure of Q-learning through two innovative mechanisms:

Experience replay

Instead of running Q-learning on state-action pairs as they occur during simulation or actual experience, the algorithm stores the history of state, action, reward, and next state transitions that are experienced by the agent in one large _replay memory_. This can be referred to as a _mini-batch_ of observations. During Q-learning updates, samples are drawn at random from the replay memory, and thus one sample could be used multiple times. Experience replay improves data efficiency, removes correlations in the observation sequences, and smooths over changes in the data distribution.

Periodically updated target

_Q_ is optimized toward target values that are only periodically updated. The Q-network is cloned and kept frozen as the optimization targets every _C_ step (_C_ is a hyperparameter). This modification makes the training more stable as it overcomes the short-term oscillations. To learn the network parameters, the algorithm applies _gradient descent_[9](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864665665352) to a loss function defined as the squared difference between the DQN‚Äôs estimate of the target and its estimate of the Q-value of the current state-action pair, _Q(s,a:Œ∏)_. The loss function is as follows:

ÔøΩ(ÔøΩÔøΩ)=ùîº\[ÔøΩ+ÔøΩmaxÔøΩ‚Ä≤ÔøΩ(ÔøΩ‚Ä≤,ÔøΩ‚Ä≤;ÔøΩÔøΩ‚Äì1)‚ÄìÔøΩ(ÔøΩ,ÔøΩ;ÔøΩÔøΩ)2]

The loss function is essentially a mean squared error (MSE) function, where ÔøΩ+ÔøΩmaxÔøΩ‚Ä≤ÔøΩ(ÔøΩ‚Ä≤,ÔøΩ‚Ä≤;ÔøΩÔøΩ‚Äì1) represents the target value and ÔøΩ\[ÔøΩ,ÔøΩ;ÔøΩÔøΩ] represents the predicted value. _Œ∏_ are the weights of the network, which are computed when the loss function is minimized. Both the target and the current estimate depend on the set of weights, underlining the distinction from supervised learning, in which targets are fixed prior to training.

An example of the DQN for the trading example containing buy, sell, and hold actions is represented in [Figure 9-6](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#DQN). Here, we provide the network only the state (_s_) as input, and we receive Q-values for all possible actions (i.e., buy, sell, and hold) at once. We will be using DQN in the first and third case studies of this chapter.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_0906.png" alt="mlbf 0906" height="412" width="762"><figcaption></figcaption></figure>

**Figure 9-6. DQN**

#### Policy gradient

_Policy gradient_ is a policy-based method in which we learn a policy function, _œÄ_, which is a direct map from each state to the best corresponding action at that state. It is a more straightforward approach than the value-based method, without the need for a Q-value function.

Policy gradient methods learn the policy directly with a parameterized function respect to _Œ∏, œÄ(a|s;Œ∏)_. This function can be a complex function and might require a sophisticated model. In policy gradient methods, we use ANNs to map state to action because they are efficient at learning complex functions. The loss function of the ANN is the opposite of the expected return (cumulative future rewards).

The objective function of the policy gradient method can be defined as:

ÔøΩ(ÔøΩ)=ÔøΩÔøΩÔøΩ(ÔøΩ1)=ùîºÔøΩÔøΩ\[ÔøΩ1]

where _Œ∏_ represents a set of weights of the ANN that maps states to actions. The idea here is to maximize the objective function and compute the weights (_Œ∏_) of the ANN.

Since this is a maximization problem, we optimize the policy by taking the _gradient ascent_ (as opposed to gradient descent, which is used to minimize the loss function), with the partial derivative of the objective with respect to the policy parameter _Œ∏_:

ÔøΩ‚ÜêÔøΩ+ÔøΩÔøΩÔøΩÔøΩ(ÔøΩ)

Using gradient ascent, we can find the best _Œ∏_ that produces the highest return. Computing the gradient numerically can be done by perturbing _Œ∏_ by a small amount _Œµ_ in the kth dimension or by using an analytical approach for deriving the gradient.

We will be using the policy gradient method for case study 2 later in this chapter.

### Key Challenges in Reinforcement Learning

So far, we have covered only what reinforcement learning algorithms can do. However, several shortcomings are outlined below:

Resource efficiency

Current deep reinforcement learning algorithms require vast amounts of time, training data, and computational resources in order to reach a desirable level of proficiency. Thus, making reinforcement learning algorithms trainable under limited resources will continue to be an important issue.

Credit assignment

In RL, reward signals can occur significantly later than actions that contributed to the result, complicating the association of actions with their consequences.

Interpretability

In RL, it is relatively difficult for a model to provide any meaningful, intuitive relationships between input and their corresponding output that can be easily understood. Most advanced reinforcement learning algorithms incorporate deep neural networks, which make interpretability even more difficult due to a large number of layers and nodes inside the neural network.

Let us look at the case studies now.

## Case Study 1: Reinforcement Learning‚ÄìBased Trading Strategy

Algorithmic trading primarily has three components: _policy development_, _parameter optimization_, and _backtesting_. The policy determines what actions to take based on the current state of the market. Parameter optimization is performed using a search over possible values of strategy parameters, such as thresholds or coefficients. Finally, backtesting assesses the viability of a trading strategy by exploring how it would have played out using historical data.

RL is based around coming up with a policy to maximize the reward in a given environment. Instead of needing to hand code a rule-based trading policy, RL learns one directly. There is no need to explicitly specify rules and thresholds. Their ability to decide policy on their own makes RL models very suitable machine learning algorithms to create automated algorithmic trading models, or _trading bots_.

In terms of _parameter optimization_ and _backtesting_ steps, RL allows for end-to-end optimization and maximizes (potentially delayed) rewards. Reinforcement learning agents are trained in a simulation, which can be as complex as desired. Taking into account latencies, liquidity, and fees, we can seamlessly combine the backtesting and parameter optimization steps without needing to go through separate stages.

Additionally, RL algorithms learn powerful policies parameterized by artificial neural networks. RL algorithms can also learn to adapt to various market conditions by experiencing them in historical data, given that they are trained over a long-time horizon and have sufficient memory. This allows them to be much more robust to changing markets than supervised learning‚Äìbased trading strategies, which, due to the simplistic nature of the policy, may not have a parameterization powerful enough to learn to adapt to changing market conditions.

Reinforcement learning, with its capability to easily handle policy, parameter optimization, and backtesting, is ideal for the next wave of algorithmic trading. Anecdotally, it seems that several of the more sophisticated algorithmic execution desks at large investment banks and hedge funds are beginning to use reinforcement learning to optimize their decision making.

In this case study, we will create an end-to-end trading strategy based on reinforcement learning. We will use the Q-learning approach with deep Q-network (DQN) to come up with a policy and an implementation of the trading strategy. As discussed before, the name ‚ÄúQ-learning‚Äù is in reference to the ÔøΩ(ÔøΩ,ÔøΩ) function, which returns the expected reward based on the state _s_ and provided action _a_. In addition to developing a specific trading strategy, this case study will discuss the general framework and components of a reinforcement learning‚Äìbased trading strategy.



In this case study, we will focus on:

* Understanding the key components of an RL framework from a trading strategy standpoint.
* Evaluating the Q-learning method of RL in Python by defining an agent, followed by training and testing setup.
* Implementing a deep neural network to be used for RL problems in Python using the Keras package.
* Understanding the class structure of Python programming while implementing an RL-based model.
* Understanding the intuition and interpretation of RL-based algorithms.

### Blueprint for Creating a Reinforcement Learning‚ÄìBased Trading Strategy

#### 1. Problem definition

In the reinforcement learning framework for this case study, the algorithm takes an action (buy, sell, or hold) depending on the current state of the stock price. The algorithm is trained using a deep Q-learning model to perform the best action. The key components of the reinforcement learning framework for this case study are:

Agent

Trading agent.

Action

Buy, sell, or hold.

Reward function

Realized profit and loss (PnL) is used as the reward function for this case study. The reward depends on the action: sell (realized profit and loss), buy (no reward), or hold (no reward).

State

A sigmoid function[10](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864665424200) of the differences of past stock prices for a given time window is used as the state. State _St_ is described as (ÔøΩÔøΩ-ÔøΩ+1,ÔøΩÔøΩ-1,ÔøΩÔøΩ), where ÔøΩÔøΩ=ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ(ÔøΩÔøΩ‚ÄìÔøΩÔøΩ‚Äì1), ÔøΩÔøΩ is price at time _t_, and ÔøΩ is the time window size. A sigmoid function converts the differences of the past stock prices into a number between zero and one, which helps to normalize the values to probabilities and makes the state simpler to interpret.

Environment

Stock exchange or the stock market.

## SELECTING THE RL COMPONENTS FOR A TRADING STRATEGY

Formulating an intelligent behavior for a reinforcement learning‚Äìbased trading strategy begins with identification of the correct components of the RL model. Hence, before we go into the model development, we should carefully identify the following RL components:

Reward function

This is an important parameter, as it decides whether the RL algorithm will learn to optimize the appropriate metric. In addition to the return or PnL, the reward function can incorporate risk embedded in the underlying instrument or include other parameters such as volatility or maximum drawdown. It can also include the transaction costs of the buy/sell actions.

State

State determines the observations that the agent receives from the environment for taking a decision. The state should be representative of current market behavior as compared to the past and can also include values of any signals that are believed to be predictive or items related to market microstructure, such as volume traded.

The data that we will use will be the S\&P 500 closing prices. The data is extracted from Yahoo Finance and contains ten years of daily data from 2010 to 2019.

#### 2. Getting started‚Äîloading the data and Python packages

**2.1. Loading the Python packages**

The list of libraries used for all of the steps of model implementation, from _data loading_ to _model evaluation_, including deep learning‚Äìbased model development, are included here. The details of most of these packages and functions have been provided in Chapters [2](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch02.html#Chapter2), [3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#Chapter3), and [4](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch04.html#Chapter4). The packages used for different purposes have been separated in the Python code here, and their usage will be demonstrated in different steps of the model development process.

`Packages for reinforcement learning`

```
import keras
from keras import layers, models, optimizers from keras import backend as K
from collections import namedtuple, deque
from keras.models import Sequential
from keras.models import load_model
from keras.layers import Dense
from keras.optimizers import Adam
```

`Packages/modules for data processing and visualization`

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import read_csv, set_option
import datetime
import math
from numpy.random import choice
import random
from collections import deque
```

**2.2. Loading the data**

The fetched data for the time period of 2010 to 2019 is loaded:

```
dataset = read_csv('data/SP500.csv', index_col=0)
```

#### 3. Exploratory data analysis

We will look at descriptive statistics and data visualization in this section. Let us have a look at the dataset we have:

```
# shape
dataset.shape
```

`Output`

```
(2515, 6)
```

```
# peek at data
set_option('display.width', 100)
dataset.head(5)
```

`Output`

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in02.png" alt="mlbf 09in02" height="182" width="500"><figcaption></figcaption></figure>



The data has a total of 2,515 rows and six columns, which contain the categories _open_, _high_, _low_, _close_, _adjusted close price_, and _total volume_. The adjusted close price is the closing price adjusted for the split and dividends. For the purpose of this case study, we will be focusing on the closing price.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in03.png" alt="mlbf 09in03" height="263" width="393"><figcaption></figcaption></figure>



The chart shows that S\&P 500 has been in an upward-trending series between 2010 and 2019. Let us perform the data preparation.

#### 4. Data preparation

This step is important in order to create a meaningful, reliable, and clean dataset that can be used without any errors in the reinforcement learning algorithm.

**4.1. Data cleaning**

In this step, we check for NAs in the rows and either drop them or fill them with the mean of the column:

```
#Checking for any null values and removing the null values'''
print('Null Values =', dataset.isnull().values.any())
```

`Output`

```
Null Values = False
```

As there are no null values in the data, there is no need to perform any further data cleaning.

#### 5. Evaluate algorithms and models

This is the key step of the reinforcement learning model development, where we will define all the relevant functions and classes and train the algorithm. In the first step, we prepare the data for the training set and the test set.

**5.1. Train-test split**

In this step, we partition the original dataset into training set and test set. We use the test set to confirm the performance of our final model and to understand if there is any overfitting. We will use 80% of the dataset for modeling and 20% for testing:

```
X=list(dataset["Close"])
X=[float(x) for x in X]
validation_size = 0.2
train_size = int(len(X) * (1-validation_size))
X_train, X_test = X[0:train_size], X[train_size:len(X)]
```

**5.2. Implementation steps and modules**

The overall algorithm of this case study (and of reinforcement learning in general) is a bit complex as it requires building _class-based code structure_ and the simultaneous use of many modules and functions. This additional section was added for this case study to provide a functional explanation of what is happening in the program.

The algorithm, in simple terms, decides whether to buy, sell, or hold when provided with the current market price.

[Figure 9-7](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#RLTrading) provides an overview of the training of the Q-learning-based algorithm in the context of this case study. The algorithm evaluates which action to take based on a Q-value, which determines the value of being in a certain state and taking a certain action at that state.

As per [Figure 9-7](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#RLTrading), the state (_s_) is decided on the basis of the current and historical behavior of the price (Pt, Pt‚Äì1,‚Ä¶). Based on the current state, the action is ‚Äúbuy.‚Äù With this action, we observe a reward of _$10_ (i.e., the PnL associated with the action) and move into the next state. Using the current reward and the next state‚Äôs Q-value, the algorithm updates the Q-value function. The algorithm keeps on moving through the next time steps. Given sufficient iterations of the steps above, this algorithm will converge to the optimal Q-value.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_0907.png" alt="mlbf 0907" height="542" width="1060"><figcaption></figcaption></figure>

**Figure 9-7. Reinforcement learning for trading**

The deep Q-network that we use in this case study uses an ANN to approximate Q-values; hence, the action value function is defined as _Q(s,a;Œ∏)_. The deep Q-learning algorithm approximates the Q-value function by learning a set of weights, _Œ∏_, of a multilayered DQN that maps states to actions.

#### Modules and functions

Implementing this DQN algorithm requires implementation of several functions and modules that interact with each other during the model training. Here is a summary of the modules and functions:

Agent class

The agent is defined as ‚ÄúAgent‚Äù class. This holds the variables and member functions that perform the Q-learning. An object of the `Agent` class is created using the training phase and is used for training the model.

Helper functions

In this module, we create additional functions that are helpful for training.

Training module

In this step, we perform the training of the data using the variables and the functions defined in the agent and helper methods. During training, the prescribed action for each day is predicted, the rewards are computed, and the deep learning‚Äìbased Q-learning model weights are updated iteratively over a number of episodes. Additionally, the profit and loss of each action is summed to determine whether an overall profit has occurred. The aim is to maximize the total profit.

We provide a deep dive into the interaction between the different modules and functions in [‚Äú5.5. Training the model‚Äù](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#training\_model\_cs).

Let us look at each of these in detail.

**5.3. Agent class**

The `agent` class consists of the following components:

* `Constructor`
* Function `model`
* Function `act`
* Function `expReplay`

The `Constructor` is defined as `init` function and contains important parameters such as `discount factor` for reward function, `epsilon` for the _Œµ-greedy_ approach, `state size`, and `action size`. The number of actions is set at three (i.e., buy, sell, and hold). The `memory` variable defines the `replay memory` size. The input parameter of this function also consists of `is_eval` parameter, which defines whether training is ongoing. This variable is changed to `True` during the evaluation/testing phase. Also, if the pretrained model has to be used in the evaluation/training phase, it is passed using the `model_name` variable:

```
class Agent:
    def __init__(self, state_size, is_eval=False, model_name=""):
        self.state_size = state_size # normalized previous days
        self.action_size = 3 # hold, buy, sell
        self.memory = deque(maxlen=1000)
        self.inventory = []
        self.model_name = model_name
        self.is_eval = is_eval

        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995

        self.model = load_model("models/" + model_name) if is_eval \
         else self._model()
```

The function `model` is a deep learning model that maps the states to actions. This function takes in the state of the environment and returns a _Q-value_ table or a policy that refers to a probability distribution over actions. This function is built using the Keras Python library.[11](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864664958104) The architecture for the deep learning model used is:

* The model expects rows of data with number of variables equal to the _state size_, which comes as an input.
* The first, second, and third hidden layers have _64_, _32_, and _8_ nodes, respectively, and all of these layers use the ReLU activation function.
* The output layer has the number of nodes equal to the action size (i.e., three), and the node uses a linear activation function:[12](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864664864472)

```
    def _model(self):
        model = Sequential()
        model.add(Dense(units=64, input_dim=self.state_size, activation="relu"))
        model.add(Dense(units=32, activation="relu"))
        model.add(Dense(units=8, activation="relu"))
        model.add(Dense(self.action_size, activation="linear"))
        model.compile(loss="mse", optimizer=Adam(lr=0.001))

        return model
```

The function `act` returns an action given a state. The function uses the `model` function and returns a buy, sell, or hold action:

```
    def act(self, state):
        if not self.is_eval and random.random() <= self.epsilon:
            return random.randrange(self.action_size)

        options = self.model.predict(state)
        return np.argmax(options[0])
```

The function `expReplay` is the key function, where the neural network is trained based on the observed experience. This function implements the _Experience replay_ mechanism as previously discussed. Experience replay stores a history of state, action, reward, and next state transitions that are experienced by the agent. It takes a minibatch of the observations (_replay memory_) as an input and updates the deep learning‚Äìbased Q-learning model weights by minimizing the loss function. The _epsilon greedy_ approach implemented in this function prevents overfitting. In order to explain the function, different steps are numbered in the comments of the following Python code, along with an outline of the steps:

1. Prepare the replay buffer memory, which is the set of observation used for training. New experiences are added to the replay buffer memory using a for loop.
2. _Loop_ across all the observations of state, action, reward, and next state transitions in the mini-batch.
3. The target variable for the Q-table is updated based on the Bellman equation. The update happens if the current state is the terminal state or the end of the episode. This is represented by the variable `done` and is defined further in the training function. If it is not `done`, the target is just set to reward.
4. Predict the Q-value of the next state using a deep learning model.
5. The Q-value of this state for the action in the current replay buffer is set to the target.
6. The deep learning model weights are updated by using the `model.fit` function.
7. The epsilon greedy approach is implemented. Recall that this approach selects an action randomly with a probability of _Œµ_ or the best action, according to the Q-value function, with probability 1‚Äì_Œµ_.

```
    def expReplay(self, batch_size):
        mini_batch = []
        l = len(self.memory)
        #1: prepare replay memory
        for i in range(l - batch_size + 1, l):
            mini_batch.append(self.memory[i])

        #2: Loop across the replay memory batch.
        for state, action, reward, next_state, done in mini_batch:
            target = reward # reward or Q at time t
            #3: update the target for Q table. table equation
            if not done:
                target = reward + self.gamma * \
                 np.amax(self.model.predict(next_state)[0])
            #set_trace()

            # 4: Q-value of the state currently from the table
            target_f = self.model.predict(state)
            # 5: Update the output Q table for the given action in the table
            target_f[0][action] = target
            # 6. train and fit the model.
            self.model.fit(state, target_f, epochs=1, verbose=0)

        #7. Implement epsilon greedy algorithm
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
```

**5.4. Helper functions**

In this module, we create additional functions that are helpful for training. Some of the important helper functions are discussed here. For details about other helper functions, refer to the Jupyter notebook in the GitHub repository for this book.

The function `getState` generates the states given the stock data, time _t_ (the day of prediction), and window _n_ (number of days to go back in time). First, the vector of price difference is computed, followed by scaling this vector from zero to one with a `sigmoid` function. This is returned as the state.

```
def getState(data, t, n):
    d = t - n + 1
    block = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1]
    res = []
    for i in range(n - 1):
        res.append(sigmoid(block[i + 1] - block[i]))
    return np.array([res])
```

The function `plot_behavior` returns the plot of the market price along with indicators for the buy and sell actions. It is used for the overall evaluation of the algorithm during the training and testing phase.

```
def plot_behavior(data_input, states_buy, states_sell, profit):
    fig = plt.figure(figsize = (15, 5))
    plt.plot(data_input, color='r', lw=2.)
    plt.plot(data_input, '^', markersize=10, color='m', label='Buying signal',\
     markevery=states_buy)
    plt.plot(data_input, 'v', markersize=10, color='k', label='Selling signal',\
     markevery = states_sell)
    plt.title('Total gains: %f'%(profit))
    plt.legend()
    plt.show()
```

**5.5. Training the model**

We will proceed to train the data. Based on our agent, we define the following variables and instantiate the stock agent:

Episode

The number of times the code is trained through the entire data. In this case study, we use 10 episodes.

Windows size

Number of market days to consider to evaluate the state.

Batch size

Size of the replay buffer or memory use during training.

Once these variables are defined, we train the model iterating through the episodes. [Figure 9-8](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#TraingStepsQTrd) provides a deep dive into the training steps and brings together all the elements discussed so far. The upper section showing steps 1 to 7 describes the steps in the _training_ module, and the lower section describes the steps in the `replay buffer` function (i.e., `exeReplay` function).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_0908.png" alt="mlbf 0908" height="677" width="1389"><figcaption></figcaption></figure>

**Figure 9-8. Training steps of Q-trading**

Steps 1 to 6 shown in [Figure 9-8](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#TraingStepsQTrd) are numbered in the following Python code and are described as follows:

1. Get the current state using the helper function `getState`. It returns a vector of states, where the length of the vector is defined by windows size and the values of the states are between zero and one.
2. Get the action for the given state using the `act` function of the agent class.
3. Get the reward for the given action. The mapping of the action and reward is described in the problem definition section of this case study.
4. Get the next state using the `getState` function. The detail of the next state is further used in the Bellman equation for updating the Q-function.
5.  The details of the state, next state, action, etc., are saved in the memory of the agent object, which is used further by the `exeReply` function. A sample mini-batch is as follows:

    <figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in04.png" alt="mlbf 09in04" height="443" width="591"><figcaption></figcaption></figure>


6. Check if the batch is complete. The size of a batch is defined by the batch size variable. If the batch is complete, then we move to the `Replay buffer` function and update the Q-function by minimizing the MSE between the Q-predicted and the Q-target. If not, then we move to the next time step.

The code produces the final results of each episode, along with the plot showing the buy and sell actions and the total profit for each episode of the training phase.

```
window_size = 1
agent = Agent(window_size)
l = len(data) - 1
batch_size = 10
states_sell = []
states_buy = []
episode_count = 3

for e in range(episode_count + 1):
    print("Episode " + str(e) + "/" + str(episode_count))
    # 1-get state
    state = getState(data, 0, window_size + 1)

    total_profit = 0
    agent.inventory = []

    for t in range(l):
        # 2-apply best action
        action = agent.act(state)

        # sit
        next_state = getState(data, t + 1, window_size + 1)
        reward = 0

        if action == 1: # buy
            agent.inventory.append(data[t])
            states_buy.append(t)
            print("Buy: " + formatPrice(data[t]))

        elif action == 2 and len(agent.inventory) > 0: # sell
            bought_price = agent.inventory.pop(0)
             #3: Get Reward

            reward = max(data[t] - bought_price, 0)
            total_profit += data[t] - bought_price
            states_sell.append(t)
            print("Sell: " + formatPrice(data[t]) + " | Profit: " \
            + formatPrice(data[t] - bought_price))

        done = True if t == l - 1 else False
        # 4: get next state to be used in bellman's equation
        next_state = getState(data, t + 1, window_size + 1)

        # 5: add to the memory
        agent.memory.append((state, action, reward, next_state, done))
        state = next_state

        if done:

            print("--------------------------------")
            print("Total Profit: " + formatPrice(total_profit))
            print("--------------------------------")

        # 6: Run replay buffer function
        if len(agent.memory) > batch_size:
            agent.expReplay(batch_size)

    if e % 10 == 0:
        agent.model.save("models/model_ep" + str(e))
```

`Output`

```
Running episode 0/10
Total Profit: $6738.87
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in05.png" alt="mlbf 09in05" height="303" width="870"><figcaption></figcaption></figure>



```
Running episode 1/10
Total Profit: ‚Äì$45.07
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in06.png" alt="mlbf 09in06" height="303" width="870"><figcaption></figcaption></figure>



```
Running episode 9/10
Total Profit: $1971.54
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in07.png" alt="mlbf 09in07" height="303" width="870"><figcaption></figcaption></figure>



```
Running episode 10/10
Total Profit: $1926.84
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in08.png" alt="mlbf 09in08" height="303" width="870"><figcaption></figcaption></figure>



The charts show the details of the buy/sell pattern and the total gains of the first two (zero and one) and last two (9 and 10) episodes. The details of other episodes can be seen in Jupyter notebook under the GitHub repository for this book.

As we can see, in the beginning of episodes 0 and 1, since the agent has no preconception of the consequences of its actions, it takes randomized actions to observe the rewards associated with it. In episode zero, there is an overall profit of $6,738, a strong result indeed, but in episode one we experience an overall loss of $45. The fact that the cumulative reward per episode fluctuates substantially in the beginning illustrates the exploration process the algorithm is going through. Looking at episodes 9 and 10, it seems as though the agent begins learning from its training. It discovers the strategy and starts to exploit it consistently. The buy and sell actions of these last two episodes lead a PnL that is perhaps less than that of episode zero, but far more robust. The buy and sell actions in the later episodes have been performed uniformly over the entire time period, and the overall profit is stable.

Ideally, the number of training episodes should be higher than the number used in this case study. A higher number of training episodes will lead to a better training performance. Before we move on to the testing, let us go through the details about model tuning.

**5.6. Model tuning**

Similar to other machine learning techniques, we can find the best combination of model hyperparameters in RL by using techniques such as grid search. The grid search for RL-based problems are computationally intensive. Hence, in this section, rather than performing the grid search, we present the key hyperparameters to consider, along with their intuition and potential impact on the model output.

Gamma (discount factor)

Decaying gamma will have the agent prioritize short-term rewards as it learns what those rewards are, and place less emphasis on long-term rewards. Lowering the discount factor in this case study may cause the algorithm to focus on the long-term rewards.

Epsilon

The epsilon variable drives the _exploration versus exploitation_ property of the model. The more we get to know our environment, the less random exploration we want to do. When we reduce epsilon, the likelihood of a random action becomes smaller, and we take more opportunities to benefit from the high-valued actions that we already discovered. However, in the trading setup, we do not want the algorithm to _overfit_ to the training data, and the epsilon should be modified accordingly.

Episodes and batch size

A higher number of episodes and larger batch size in the training set will lead to better training and a more optimal Q-value. However, there is a trade-off, as increasing the number of episodes and batch size increases the total training time.

Window size

Window size determines the number of market days to consider to evaluate the state. This can be increased in case we want the state to be determined by a greater number of days in the past.

Number of layers and nodes of the deep learning model

This can be modified for better training and a more optimal Q-value. The details about the impact of changing the layers and nodes of ANN models are discussed in [Chapter 3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#Chapter3), and the grid search for a deep learning model is discussed in [Chapter 5](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch05.html#Chapter5).

#### 6. Testing the data

After training the data, it is evaluated against the test dataset. This is an important step, especially for reinforcement learning, as the agent may mistakenly correlate reward with certain spurious features from the data, or it may overfit a particular chart pattern. In the testing step, we look at the performance of the already trained model (_model\_ep10_) from the training step on the test data. The Python code looks similar to the training set we saw before. However, the `is_eval` flag is set to `true`, the `reply buffer` function is not called, and there is no training. Let us look at the results:

```
#agent is already defined in the training set above.
test_data = X_test
l_test = len(test_data) - 1
state = getState(test_data, 0, window_size + 1)
total_profit = 0
is_eval = True
done = False
states_sell_test = []
states_buy_test = []
model_name = "model_ep10"
agent = Agent(window_size, is_eval, model_name)
state = getState(data, 0, window_size + 1)
total_profit = 0
agent.inventory = []

for t in range(l_test):
    action = agent.act(state)

    next_state = getState(test_data, t + 1, window_size + 1)
    reward = 0

    if action == 1:

        agent.inventory.append(test_data[t])
        print("Buy: " + formatPrice(test_data[t]))

    elif action == 2 and len(agent.inventory) > 0:
        bought_price = agent.inventory.pop(0)
        reward = max(test_data[t] - bought_price, 0)
        total_profit += test_data[t] - bought_price
        print("Sell: " + formatPrice(test_data[t]) + " | profit: " +\
         formatPrice(test_data[t] - bought_price))

    if t == l_test - 1:
        done = True
    agent.memory.append((state, action, reward, next_state, done))
    state = next_state

    if done:
        print("------------------------------------------")
        print("Total Profit: " + formatPrice(total_profit))
        print("------------------------------------------")
```

`Output`

```
Total Profit: $1280.40
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in09.png" alt="mlbf 09in09" height="303" width="869"><figcaption></figcaption></figure>



Looking at the results above, our model resulted in an overall profit of $1,280, and we can say that our DQN agent performs quite well on the test set.

#### Conclusion

In this case study, we created an automated trading strategy, or a _trading bot_, that simply needs to be fed running stock market data to produce a trading signal. We saw that the algorithm decides the policy by itself, and the overall approach is much simpler and more principled than the supervised learning‚Äìbased approach. The trained model was profitable in the test set, corroborating the effectiveness of the RL-based trading strategy.

In using a reinforcement learning model such as DQN, which is based on a deep neural network, we can learn policies that are more complex and powerful than what a human trader could learn.

Given the high complexity and low interpretability of the RL-based model, visualization and testing steps become quite important. For interpretability, we used the plots of the training episodes of the training algorithm and found that the model starts to learn over a period of time, discovers the strategy, and starts to exploit it. A sufficient number of tests should be conducted on different time periods before deploying the model for live trading.

While using RL-based models, we should carefully select the RL components, such as the reward function and state, and ensure understanding of their impact on the overall model results. Before implementing or training the model, it is important to think of questions, such as ‚ÄúHow can we engineer the reward function or the state so that the RL algorithm has the potential to learn to optimize the right metric?‚Äù

Overall, these RL-based models can enable financial practitioners to create trading strategies with a very flexible approach. The framework provided in this case study can be a great starting point to develop more powerful models for algorithmic trading.

## Case Study 2: Derivatives Hedging

Much of traditional finance theory for handling derivatives pricing and risk management is based on the idealized complete markets assumption of perfect hedgability, without trading restrictions, transaction costs, market impact, or liquidity constraints. In practice, however, these frictions are very real. As a consequence, practical risk management using derivatives requires human oversight and maintenance; the models themselves are insufficient. Implementation is still partially driven by the trader‚Äôs intuitive understanding of the shortcomings of the existing tools.

Reinforcement learning algorithms, with their ability to tackle more nuances and parameters within the operational environment, are inherently aligned with the objective of hedging. These models can produce dynamic strategies that are optimal, even in a world with frictions. The model-free RL approaches demand very few theoretical assumptions. This allows for automation of hedging without requiring frequent human intervention, making the overall hedging process significantly faster. These models can learn from large amounts of historical data and can consider many variables to make more precise and accurate hedging decisions. Moreover, the availability of vast amounts of data makes RL-based models more useful and effective than ever before.

In this case study, we implement a reinforcement learning‚Äìbased hedging strategy that adopts the ideas presented in the paper [‚ÄúDeep Hedging‚Äù](https://oreil.ly/6\_Qvz) by Hans B√ºhler et al. We will build an optimal hedging strategy for a specific type of derivative (call options) by minimizing the risk-adjusted PnL. We use the measure _CVaR_ (conditional value at risk), which quantifies the amount of tail risk of a position or portfolio as a risk assessment measure.



In this case study, we will focus on:

* Using policy-based (or direct policy search‚Äìbased) reinforcement learning and implementing it using a deep neural network.
* Comparing the effectiveness of an RL-based trading strategy to the traditional Black-Scholes model.
* Setting up an agent for an RL problem using class structure in Python.
* Implementing and evaluating a policy gradient‚Äìbased RL method.
* Introducing the basic concept of functions in the TensorFlow Python package.
* Implementing a Monte Carlo simulation of stock price and the Black-Scholes pricing model, and computing option Greeks.

### Blueprint for Implementing a Reinforcement Learning‚ÄìBased Hedging Strategy

#### 1. Problem definition

In the reinforcement learning framework for this case study, the algorithm decides the best hedging strategy for call options using market prices of the underlying asset. A direct policy search reinforcement learning strategy is used. The overall idea, derived from the ‚ÄúDeep Hedging‚Äù paper, is based on minimizing the hedge error under a risk assessment measure. The overall PnL of a call option hedging strategy over a period of time, from _t_=1 to _t_=T, can be written as:

ÔøΩÔøΩÔøΩÔøΩ(ÔøΩ,ÔøΩ)=‚ÄìÔøΩÔøΩ+‚àëÔøΩ=1ÔøΩÔøΩÔøΩ‚Äì1(ÔøΩÔøΩ‚ÄìÔøΩÔøΩ‚Äì1)‚Äì‚àëÔøΩ=1ÔøΩÔøΩÔøΩ

where

* ÔøΩÔøΩ is the payoff of a call option at maturity.
* ÔøΩÔøΩ‚Äì1(ÔøΩÔøΩ‚ÄìÔøΩÔøΩ‚Äì1) is the cash flow from the hedging instruments on day ÔøΩ, where ÔøΩ is the hedge and ÔøΩÔøΩ is the spot price on day ÔøΩ.
* ÔøΩÔøΩ is the transaction cost at time ÔøΩ and may be constant or proportional to the hedge size.

The individual components in the equation are the components of the cash flow. However, it would be preferable to take into account the risk arising from any position while designing the reward function. We use the measure CVaR as the risk assessment measure. CVaR quantifies the amount of tail risk and is the `expected shortfall` (risk aversion parameter)[13](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864663276952) for the confidence level ÔøΩ. Now the reward function is modified to the following:

ÔøΩÔøΩ=ÔøΩ(‚ÄìÔøΩÔøΩ+‚àëÔøΩ=1ÔøΩÔøΩÔøΩ‚Äì1(ÔøΩÔøΩ‚ÄìÔøΩÔøΩ‚Äì1)‚Äì‚àëÔøΩ=1ÔøΩÔøΩÔøΩ)

where ÔøΩ represents the CVaR.

We will train an _RNN-based_ network to learn the optimal hedging strategy (i.e., ÔøΩ1,ÔøΩ2...,ÔøΩÔøΩ) given the stock price, strike price, and risk aversion parameter, (ÔøΩ), by minimizing CVaR. We assume transaction costs to be zero for simplicity. The model can easily be extended to incorporate transaction costs and other market frictions.

The data used for the synthetic underlying stock price is generated using Monte Carlo simulation, assuming a lognormal price distribution. We assume an interest rate of 0% and annual volatility of 20%.

The key components of the model are:

Agent

Trader or trading agent.

Action

Hedging strategy (i.e., ÔøΩ1,ÔøΩ2...,ÔøΩÔøΩ).

Reward function

CVaR‚Äîthis is a convex function and is minimized during the model training.

State

State is the representation of the current market and relevant product variables. The state represents the model inputs, which include the simulated stock price path (i.e., ÔøΩ1,ÔøΩ2...,ÔøΩÔøΩ), strike, and risk aversion parameter (ÔøΩ).

Environment

Stock exchange or stock market.

#### 2. Getting started

**2.1. Loading the Python packages**

The loading of Python packages is similar to the previous case studies. Please refer to the Jupyter notebook for this case study for more details.

**2.2. Generating the data**

In this step we generate the data for this case study using a Black-Scholes simulation.

This function generates the Monte Carlo paths for the stock price and gets the option price on each of the Monte Carlo paths. The calculation as shown is based on the lognormal assumption of stock prices:

ÔøΩÔøΩ+1=ÔøΩÔøΩÔøΩÔøΩ‚Äì12ÔøΩ2ÔøΩÔøΩ+ÔøΩÔøΩÔøΩÔøΩ

where ÔøΩ is stock price, ÔøΩ is volatility, ÔøΩ is the drift, ÔøΩ is time, and ÔøΩ is a standard normal variable.

```
def monte_carlo_paths(S_0, time_to_expiry, sigma, drift, seed, n_sims, \
  n_timesteps):
    """
    Create random paths of a stock price following a brownian geometric motion
    return:

    a (n_timesteps x n_sims x 1) matrix
    """
    if seed > 0:
            np.random.seed(seed)
    stdnorm_random_variates = np.random.randn(n_sims, n_timesteps)
    S = S_0
    dt = time_to_expiry / stdnorm_random_variates.shape[1]
    r = drift
    S_T = S * np.cumprod(np.exp((r-sigma**2/2)*dt+sigma*np.sqrt(dt)*\
    stdnorm_random_variates), axis=1)
    return np.reshape(np.transpose(np.c_[np.ones(n_sims)*S_0, S_T]), \
    (n_timesteps+1, n_sims, 1))
```

We generate 50,000 simulations of the spot price over a period of one month. The total number of time steps is 30. Hence, for each Monte Carlo scenario, there is one observation per day. The parameters needed for the simulation are defined below:

```
S_0 = 100; K = 100; r = 0; vol = 0.2; T = 1/12
timesteps = 30; seed = 42; n_sims = 5000

# Generate the monte carlo paths
paths_train = monte_carlo_paths(S_0, T, vol, r, seed, n_sims, timesteps)
```

#### 3. Exploratory data analysis

We will look at descriptive statistics and data visualization in this section. Given that the data was generated by the simulation, we simply inspect one path as a sanity check of the simulation algorithm:

```
#Plot Paths for one simulation
plt.figure(figsize=(20, 10))
plt.plot(paths_train[1])
plt.xlabel('Time Steps')
plt.title('Stock Price Sample Paths')
plt.show()
```

`Output`

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in10.png" alt="mlbf 09in10" height="468" width="748"><figcaption></figcaption></figure>



#### 4. Evaluate algorithms and models

In this direct policy search approach, we use an artificial neural network (ANN) to map the state to action. In a traditional ANN, we assume that all inputs (and outputs) are independent of each other. However, the hedging decision at time _t_ (represented by _Œ¥t_) is _path dependent_ and is influenced by the stock price and hedging decisions at previous time steps. Hence, using a traditional ANN is not feasible. _RNN_ is a type of ANN that can capture the time-varying dynamics of the underlying system and is more appropriate in this context. RNNs have a memory, which captures information about what has been calculated so far. We used this property of the RNN model for time series modeling in [Chapter 5](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch05.html#Chapter5). _LSTM_ (also discussed in [Chapter 5](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch05.html#Chapter5)) is a special kind of RNN capable of learning long-term dependencies. Past state information is made available to the network when mapping to an action; the extraction of relevant past data is then learned as part of the training process. We will use an LSTM model to map the state to action and get the hedging strategy (i.e., Œ¥1, Œ¥2,‚Ä¶Œ¥T).

**4.1. Policy gradient script**

We will cover the implementation steps and model training in this section. We provide the input variables‚Äîstock price path (ÔøΩ1,ÔøΩ2,...ÔøΩÔøΩ), strike, and risk aversion parameter, ÔøΩ‚Äîto the trained model and receive the hedging strategy (i.e., ÔøΩ1,ÔøΩ2,...ÔøΩÔøΩ) as the output. [Figure 9-9](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#PGTraining) provides an overview of the training of the policy gradient for this case study.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_0909.png" alt="mlbf 0909" height="532" width="1408"><figcaption></figcaption></figure>

**Figure 9-9. Policy gradient training for derivatives hedging**

We already performed step 1 in section 2 of this case study. Steps 2 to 5 are self-explanatory and are implemented in the `agent` class defined later. The `agent` holds the variables and member functions that perform the training. An `object` of the `agent` class is created using the training phase and is used for training the model. After a sufficient number of iterations of steps 2 to 5, an optimal policy gradient model is generated.

The class consists of the following modules:

* `Constructor`
* The function `execute_graph_batchwise`
* The functions `training`, `predict`, and `restore`

Let us dig deeper into the Python code for each of the functions.

The `Constructor` is defined as an `init` function, where we define the model parameters. We can pass the `timesteps`, `batch_size`, and `number of nodes` in each layer of the LSTM model to the constructor. We define the input variables of the model (i.e., stock price path, strike, and risk aversion parameter) as _TensorFlow placeholders_. Placeholders are used to feed in data from outside the computational graph, and we feed the data of these input variables during the training phase. We implement an LSTM network in TensorFlow by using the `tf.MultiRNNCell` function. The LSTM model uses four layers with 62, 46, 46, and 1 nodes. The loss function is the CVaR, which is minimized when `tf.train` is called during the training step. We sort the negative realized PnLs of the trading strategy and calculate the mean of the (1‚àí_Œ±_) top losses:

```
class Agent(object):
    def __init__(self, time_steps, batch_size, features,\
       nodes = [62, 46, 46, 1], name='model'):

        #1. Initialize the variables
        tf.reset_default_graph()
        self.batch_size = batch_size # Number of options in a batch
        self.S_t_input = tf.placeholder(tf.float32, [time_steps, batch_size, \
          features]) #Spot
        self.K = tf.placeholder(tf.float32, batch_size) #Strike
        self.alpha = tf.placeholder(tf.float32) #alpha for cVaR

        S_T = self.S_t_input[-1,:,0] #Spot at time T
        # Change in the Spot
        dS = self.S_t_input[1:, :, 0] - self.S_t_input[0:-1, :, 0]
        #dS = tf.reshape(dS, (time_steps, batch_size))


        #2. Prepare S_t for use in the RNN remove the \
        #last time step (at T the portfolio is zero)
        S_t = tf.unstack(self.S_t_input[:-1, :,:], axis=0)

        # Build the lstm
        lstm = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(n) \
        for n in nodes])

        #3. So the state is a convenient tensor that holds the last
        #actual RNN state,ignoring the zeros.
        #The strategy tensor holds the outputs of all cells.
        self.strategy, state = tf.nn.static_rnn(lstm, S_t, initial_state=\
          lstm.zero_state(batch_size, tf.float32), dtype=tf.float32)

        self.strategy = tf.reshape(self.strategy, (time_steps-1, batch_size))

        #4. Option Price
        self.option = tf.maximum(S_T-self.K, 0)

        self.Hedging_PnL = - self.option + tf.reduce_sum(dS*self.strategy, \
          axis=0)

        #5. Total Hedged PnL of each path
        self.Hedging_PnL_Paths = - self.option + dS*self.strategy

        # 6. Calculate the CVaR for a given confidence level alpha
        # Take the 1-alpha largest losses (top 1-alpha negative PnLs)
        #and calculate the mean
        CVaR, idx = tf.nn.top_k(-self.Hedging_PnL, tf.cast((1-self.alpha)*\
        batch_size, tf.int32))
        CVaR = tf.reduce_mean(CVaR)
        #7. Minimize the CVaR
        self.train = tf.train.AdamOptimizer().minimize(CVaR)
        self.saver = tf.train.Saver()
        self.modelname = name
```

The function `execute_graph_batchwise` is the key function of the program, in which we train the neural network based on the observed experience. It takes a batch of the states as input and updates the policy gradient‚Äìbased LSTM model weights by minimizing CVaR. This function trains the LSTM model to predict a hedging strategy by looping across the epochs and batches. First, it prepares a batch of market variables (stock price, strike, and risk aversion) and uses `sess.run` function for training. This `sess.run` is a TensorFlow function to run any operation defined within it. Here, it takes the inputs and runs the `tf.train` function that was defined in the constructor. After a sufficient number of iterations, an optimal policy gradient model is generated:

```
    def _execute_graph_batchwise(self, paths, strikes, riskaversion, sess, \
      epochs=1, train_flag=False):
        #1: Initialize the variables.
        sample_size = paths.shape[1]
        batch_size=self.batch_size
        idx = np.arange(sample_size)
        start = dt.datetime.now()
        #2:Loop across all the epochs
        for epoch in range(epochs):
            # Save the hedging Pnl for each batch
            pnls = []
            strategies = []
            if train_flag:
                np.random.shuffle(idx)
            #3. Loop across the observations
            for i in range(int(sample_size/batch_size)):
                indices = idx[i*batch_size : (i+1)*batch_size]
                batch = paths[:,indices,:]

                #4. Train the LSTM
                if train_flag:#runs the train, hedging PnL and strategy.
                    _, pnl, strategy = sess.run([self.train, self.Hedging_PnL, \
                      self.strategy], {self.S_t_input: batch,\
                        self.K : strikes[indices],\
                        self.alpha: riskaversion})
                        #5. Evaluation and no training
                else:
                    pnl, strategy = sess.run([self.Hedging_PnL, self.strategy], \
                      {self.S_t_input: batch,\
                      self.K : strikes[indices],
                      self.alpha: riskaversion})\

                pnls.append(pnl)
                strategies.append(strategy)
            #6. Calculate the option price # given the risk aversion level alpha

            CVaR = np.mean(-np.sort(np.concatenate(pnls))\
            [:int((1-riskaversion)*sample_size)])
            #7. Return training metrics, \
            #if it is in the training phase
            if train_flag:
                if epoch % 10 == 0:
                    print('Time elapsed:', dt.datetime.now()-start)
                    print('Epoch', epoch, 'CVaR', CVaR)
                    #Saving the model
                    self.saver.save(sess, "model.ckpt")
        self.saver.save(sess, "model.ckpt")

        #8. return CVaR and other parameters
        return CVaR, np.concatenate(pnls), np.concatenate(strategies,axis=1)
```

The `training` function simply triggers the `execute_graph_batchwise` function and provides all the inputs required for training to this function. The `predict` function returns the action (hedging strategy) given a state (market variables). The `restore` function restores the saved trained model, to be used further for training or prediction:

```
    def training(self, paths, strikes, riskaversion, epochs, session, init=True):
        if init:
            sess.run(tf.global_variables_initializer())
        self._execute_graph_batchwise(paths, strikes, riskaversion, session, \
          epochs, train_flag=True)


    def predict(self, paths, strikes, riskaversion, session):
        return self._execute_graph_batchwise(paths, strikes, riskaversion,\
          session,1, train_flag=False)

    def restore(self, session, checkpoint):
        self.saver.restore(session, checkpoint)
```

**4.2. Training the data**

The steps of training our policy-based model are:

1. Define the risk aversion parameter for CVaR, number of features (this is total number of stocks, and in this case we just have one), strike price, and batch size. The CVaR represents the amount of loss we want to minimize. For example, a CVaR of 99% means that we want to avoid extreme loss, while a CVaR of 50% minimizes average loss. We train with a CVaR of 50% to have smaller mean loss.
2. Instantiate the policy gradient agent, which has the RNN based-policy with the loss function based on the CVaR.
3. Iterate through the batches; the strategy is defined by the policy output of the LSTM-based network.
4. Finally, the trained model is saved.

```
batch_size = 1000
features = 1
K = 100
alpha = 0.50 #risk aversion parameter for CVaR
epoch = 101 #It is set to 11, but should ideally be a high number
model_1 = Agent(paths_train.shape[0], batch_size, features, name='rnn_final')
# Training the model takes a few minutes
start = dt.datetime.now()
with tf.Session() as sess:
    # Train Model
    model_1.training(paths_train, np.ones(paths_train.shape[1])*K, alpha,\
     epoch, sess)
print('Training finished, Time elapsed:', dt.datetime.now()-start)
```

`Output`

```
Time elapsed: 0:00:03.326560
Epoch 0 CVaR 4.0718956
Epoch 100 CVaR 2.853285
Training finished, Time elapsed: 0:01:56.299444
```

#### 5. Testing the data

Testing is an important step, especially for RL, as it is difficult for a model to provide any meaningful, intuitive relationships between input and their corresponding output that is easily understood. In the testing step, we will compare the effectiveness of the hedging strategy and compare it to a delta hedging strategy based on the Black-Scholes model. We first define the helper functions used in this step.

**5.1. Helper functions for comparison against Black-Scholes**

In this module, we create additional functions that are used for comparison against the traditional Black-Scholes model.

**5.1.1. Black-Scholes price and delta**

The function `BlackScholes_price` implements the analytical formula for the call option price, and `BS_delta` implements the analytical formula for the delta of a call option:

```
def BS_d1(S, dt, r, sigma, K):
    return (np.log(S/K) + (r+sigma**2/2)*dt) / (sigma*np.sqrt(dt))

def BlackScholes_price(S, T, r, sigma, K, t=0):
    dt = T-t
    Phi = stats.norm(loc=0, scale=1).cdf
    d1 = BS_d1(S, dt, r, sigma, K)
    d2 = d1 - sigma*np.sqrt(dt)
    return S*Phi(d1) - K*np.exp(-r*dt)*Phi(d2)

def BS_delta(S, T, r, sigma, K, t=0):
    dt = T-t
    d1 = BS_d1(S, dt, r, sigma, K)
    Phi = stats.norm(loc=0, scale=1).cdf
    return Phi(d1)
```

**5.1.2. Test results and plotting**

The following functions are used to compute the key metrics and related plots for evaluating the effectiveness of the hedge. The function `test_hedging_strategy` computes different types of PnL, including CVaR, PnL, and Hedge PnL. The function `plot_deltas` plots the comparison of the RL delta versus Black-Scholes hedging at different time points. The function `plot_strategy_pnl` is used to plot the total PnL of the RL-based strategy versus Black-Scholes hedging:

```
def test_hedging_strategy(deltas, paths, K, price, alpha, output=True):
    S_returns = paths[1:,:,0]-paths[:-1,:,0]
    hedge_pnl = np.sum(deltas * S_returns, axis=0)
    option_payoff = np.maximum(paths[-1,:,0] - K, 0)
    replication_portfolio_pnls = -option_payoff + hedge_pnl + price
    mean_pnl = np.mean(replication_portfolio_pnls)
    cvar_pnl = -np.mean(np.sort(replication_portfolio_pnls)\
    [:int((1-alpha)*replication_portfolio_pnls.shape[0])])
    if output:
        plt.hist(replication_portfolio_pnls)
        print('BS price at t0:', price)
        print('Mean Hedging PnL:', mean_pnl)
        print('CVaR Hedging PnL:', cvar_pnl)
    return (mean_pnl, cvar_pnl, hedge_pnl, replication_portfolio_pnls, deltas)

def plot_deltas(paths, deltas_bs, deltas_rnn, times=[0, 1, 5, 10, 15, 29]):
    fig = plt.figure(figsize=(10,6))
    for i, t in enumerate(times):
        plt.subplot(2,3,i+1)
        xs =  paths[t,:,0]
        ys_bs = deltas_bs[t,:]
        ys_rnn = deltas_rnn[t,:]
        df = pd.DataFrame([xs, ys_bs, ys_rnn]).T

        plt.plot(df[0], df[1], df[0], df[2], linestyle='', marker='x' )
        plt.legend(['BS delta', 'RNN Delta'])
        plt.title('Delta at Time %i' % t)
        plt.xlabel('Spot')
        plt.ylabel('$\Delta$')
    plt.tight_layout()

def plot_strategy_pnl(portfolio_pnl_bs, portfolio_pnl_rnn):
    fig = plt.figure(figsize=(10,6))
    sns.boxplot(x=['Black-Scholes', 'RNN-LSTM-v1 '], y=[portfolio_pnl_bs, \
    portfolio_pnl_rnn])
    plt.title('Compare PnL Replication Strategy')
    plt.ylabel('PnL')
```

**5.1.3. Hedging error for Black-Scholes replication**

The following function is used to get the hedging strategy based on the traditional Black-Scholes model, which is further used for comparison against the RL-based hedging strategy:

```
def black_scholes_hedge_strategy(S_0, K, r, vol, T, paths, alpha, output):
    bs_price = BlackScholes_price(S_0, T, r, vol, K, 0)
    times = np.zeros(paths.shape[0])
    times[1:] = T / (paths.shape[0]-1)
    times = np.cumsum(times)
    bs_deltas = np.zeros((paths.shape[0]-1, paths.shape[1]))
    for i in range(paths.shape[0]-1):
        t = times[i]
        bs_deltas[i,:] = BS_delta(paths[i,:,0], T, r, vol, K, t)
    return test_hedging_strategy(bs_deltas, paths, K, bs_price, alpha, output)
```

**5.2. Comparison between Black-Scholes and reinforcement learning**

We will compare the effectiveness of the hedging strategy by looking at the influence of the CVaR risk aversion parameter and inspect how well the RL-based model can generalize the hedging strategy if we change the moneyness of the option, the drift, and the volatility of the underlying process.

**5.2.1. Test at 99% risk aversion**

As mentioned before, the CVaR represents the amount of loss we want to minimize. We trained the model using a risk aversion of 50% to minimize average loss. However, for testing purposes we increase the risk aversion to 99%, meaning that we want to avoid extreme loss. These results are compared against the Black-Scholes model:

```
n_sims_test = 1000
# Monte Carlo Path for the test set
alpha = 0.99
paths_test =  monte_carlo_paths(S_0, T, vol, r, seed_test, n_sims_test, \
  timesteps)
```

We use the trained function and compare the Black-Scholes and RL models in the following code:

```
with tf.Session() as sess:
    model_1.restore(sess, 'model.ckpt')
    #Using the model_1 trained in the section above
    test1_results = model_1.predict(paths_test, np.ones(paths_test.shape[1])*K, \
    alpha, sess)

_,_,_,portfolio_pnl_bs, deltas_bs = black_scholes_hedge_strategy\
(S_0,K, r, vol, T, paths_test, alpha, True)
plt.figure()
_,_,_,portfolio_pnl_rnn, deltas_rnn = test_hedging_strategy\
(test1_results[2], paths_test, K, 2.302974467802428, alpha, True)
plot_deltas(paths_test, deltas_bs, deltas_rnn)
plot_strategy_pnl(portfolio_pnl_bs, portfolio_pnl_rnn)
```

`Output`

```
BS price at t0: 2.3029744678024286
Mean Hedging PnL: -0.0010458505607415178
CVaR Hedging PnL: 1.2447953011695538
RL based BS price at t0: 2.302974467802428
RL based Mean Hedging PnL: -0.0019250998451393934
RL based CVaR Hedging PnL: 1.3832611348053374
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in11.png" alt="mlbf 09in11" height="411" width="695"><figcaption></figcaption></figure>



<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in12.png" alt="mlbf 09in12" height="357" width="595"><figcaption></figcaption></figure>



For the first test set (strike 100, same drift, same vol) with a risk aversion of 99%, the results look quite good. We see that the delta from both Black-Scholes and the RL-based approach converge over time from day 1 to 30. The CVaRs of both strategies are similar and lower in magnitude, with values of 1.24 and 1.38 for Black-Scholes and RL, respectively. Also, the volatility of the two strategies is similar, as illustrated in the second chart.

**5.2.2. Changing moneyness**

Let us now look at the comparison of the strategies, when the moneyness, defined as the ratio of strike to spot price, is changed. In order to change the moneyness, we decrease the strike price by 10. The code snippet is similar to the previous case, and the output is shown below:

```
BS price at t0: 10.07339936955367
Mean Hedging PnL: 0.0007508571761945107
CVaR Hedging PnL: 0.6977526775080665
RL based BS price at t0: 10.073
RL based Mean Hedging PnL: -0.038571546628968216
RL based CVaR Hedging PnL: 3.4732447615593975
```

With the change in the moneyness, we see that the PnL of the RL strategy is significantly worse than that of the Black-Scholes strategy. We see a significant deviation of the delta between the two across all the days. The CVaR and the volatility of the RL-based strategy is much higher. The results indicate that we should be careful while generalizing the model to different levels of moneyness and should train the model with the option of using a variety of strikes before implementing it in a production environment.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in13.png" alt="mlbf 09in13" height="411" width="695"><figcaption></figcaption></figure>



<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in14.png" alt="mlbf 09in14" height="357" width="595"><figcaption></figcaption></figure>



**5.2.3. Changing drift**

Let us now look at the comparison of the strategies when the drift is changed. In order to change the drift, we assume the drift of the stock price is 4% per month, or 48% annualized. The output is shown below:

`Output`

```
BS price at t0: 2.3029744678024286
Mean Hedging PnL: -0.01723902964827388
CVaR Hedging PnL: 1.2141220199385756
RL based BS price at t0: 2.3029
RL based Mean Hedging PnL: -0.037668804359885316
RL based CVaR Hedging PnL: 1.357201635552361
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in15.png" alt="mlbf 09in15" height="411" width="697"><figcaption></figcaption></figure>



<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in16.png" alt="mlbf 09in16" height="357" width="595"><figcaption></figcaption></figure>



The overall results look good for the change in drift. The conclusion is similar to results when the risk aversion was changed, with the deltas for the two approaches converging over time. Again, the CVaRs are similar in magnitude, with Black-Scholes producing a value of 1.21, and RL a value of 1.357.

**5.2.4. Shifted volatility**

Finally, we look at the impact of shifting the volatility. In order to change the volatility, we increase it by 5%:

`Output`

```
BS price at t0: 2.3029744678024286
Mean Hedging PnL: -0.5787493248269506
CVaR Hedging PnL: 2.5583922824407566
RL based BS price at t0: 2.309
RL based Mean Hedging PnL: -0.5735181045192523
RL based CVaR Hedging PnL: 2.835487824499669
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in17.png" alt="mlbf 09in17" height="411" width="695"><figcaption></figcaption></figure>



Looking at the results, the delta, CVaR, and overall volatility of both models are similar. Hence looking at the different comparisons overall, the performance of this RL-based hedging is on par with Black-Scholes‚Äìbased hedging.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in18.png" alt="mlbf 09in18" height="357" width="595"><figcaption></figcaption></figure>



#### Conclusion

In this case study, we compared the effectiveness of a call option hedging strategy using RL. The RL-based hedging strategy did quite well even when certain input parameters were modified. However, this strategy was not able to generalize the strategy for options at different moneyness levels. It underscores the fact that RL is a data-intensive approach, and it is important to train the model with different scenarios, which becomes more important if the model is intended to be used across a wide variety of derivatives.

Although we found the RL and traditional Black-Scholes strategies comparable, the RL approach offers a much higher ceiling for improvement. The RL model can be further trained using a wide variety of instruments with different hyperparameters, leading to performance enhancements. It would be interesting to explore the comparison of these two hedging models for more exotic derivatives, given the trade-off between these approaches.

Overall, the RL-based approach is model independent and scalable, and it offers efficiency boosts for many classical problems.

## Case Study 3: Portfolio Allocation

As discussed in prior case studies, the most commonly used technique for portfolio allocation, _mean-variance portfolio optimization_, suffers from several weaknesses, including:

* Estimation errors in the expected returns and covariance matrix caused by the erratic nature of financial returns.
* Unstable quadratic optimization that greatly jeopardizes the optimality of the resulting portfolios.

We addressed some of these weaknesses in [‚ÄúCase Study 1: Portfolio Management: Finding an Eigen Portfolio‚Äù](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch07.html#CaseStudy1DR) in [Chapter 7](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch07.html#Chapter7), and in [‚ÄúCase Study 3: Hierarchical Risk Parity‚Äù](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch08.html#CaseStudy3CL) in [Chapter 8](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch08.html#Chapter8). Here, we approach this problem from an RL perspective.

Reinforcement learning algorithms, with the ability to decide the policy on their own, are strong models for performing portfolio allocation in an automated manner, without the need for continuous supervision. Automation of the manual steps involved in portfolio allocation can prove to be immensely useful, specifically for robo-advisors.

In an RL-based framework, we treat portfolio allocation not just as a one-step optimization problem but as _continuous control_ of the portfolio with delayed rewards. We move from discrete optimal allocation to continuous control territory, and in the environment of a continuously changing market, RL algorithms can be leveraged to solve complex and dynamic portfolio allocation problems.

In this case study, we will use a Q-learning-based approach and DQN to come up with a policy for optimal portfolio allocation among a set of cryptocurrencies. Overall, the approach and framework in terms of the Python-based implementation is similar to that in case study 1. Therefore, some repetitive sections or code explanation is skipped in this case study.



In this case study, we will focus on:

* Defining the components of RL in a portfolio allocation problem.
* Evaluating Q-learning in the context of portfolio allocation.
* Creating a simulation environment to be used in the RL framework.
* Extending the Q-learning framework used for trading strategy development to portfolio management.

### Blueprint for Creating a Reinforcement Learning‚ÄìBased Algorithm for Portfolio Allocation

#### 1. Problem definition

In the reinforcement learning framework defined for this case study, the algorithm performs an action, which is _optimal portfolio allocation_, depending on the current state of the portfolio. The algorithm is trained using a deep Q-learning framework, and the components of the model are as follows:

Agent

A portfolio manager, a robo-advisor, or an individual investor.

Action

Assignment and rebalancing of the portfolio weights. The DQN model provides the Q-values, which are converted into portfolio weights.

Reward function

The Sharpe ratio. Although there can be a wide range of complex reward functions that provide a trade-off between profit and risk, such as percentage return or maximum drawdown.

State

The state is the correlation matrix of the instruments based on a specific time window. The correlation matrix is a suitable state variable for the portfolio allocation, as it contains the information about the relationships between different instruments and can be useful in performing portfolio allocation.

Environment

The cryptocurrency exchange.

The dataset used in this case study is from the [Kaggle](https://oreil.ly/613O2) platform. It contains the daily prices of cryptocurrencies in 2018. The data contains some of the most liquid cryptocurrencies, including Bitcoin, Ethereum, Ripple, Litecoin, and Dash.

#### 2. Getting started‚Äîloading the data and Python packages

**2.1. Loading the Python packages**

The standard Python packages are loaded in this step. The details have already been presented in the previous case studies. Refer to the Jupyter notebook for this case study for more details.

**2.2. Loading the data**

The fetched data is loaded in this step:

```
dataset = read_csv('data/crypto_portfolio.csv',index_col=0)
```

#### 3. Exploratory data analysis

**3.1. Descriptive statistics**

We will look at descriptive statistics and data visualizations of the data in this section:

```
# shape
dataset.shape
```

`Output`

```
(375, 15)
```

```
# peek at data
set_option('display.width', 100)
dataset.head(5)
```

`Output`

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in19.png" alt="mlbf 09in19" height="257" width="1074"><figcaption></figcaption></figure>



The data has a total of 375 rows and 15 columns. These columns hold the daily prices of 15 different cryptocurrencies in 2018.

#### 4. Evaluate algorithms and models

This is the key step of the reinforcement learning model development, where we will define all the functions and classes and train the algorithm.

**4.1. Agent and cryptocurrency environment script**

We have an `Agent` class that holds the variables and member functions that perform the Q-learning. This is similar to the `Agent` class defined in case study 1, with an additional function to convert the Q-value output from the deep neural network to portfolio weights and vice versa. The training module implements iteration through several episodes and batches and saves the information of the state, action, reward, and next state to be used in training. We skip the detailed description of the Python code of `Agent` class and the training module in this case study. Readers can refer to the Jupyter notebook in the code repository for this book for more details.

We implement a simulation environment for cryptocurrencies using a class called `CryptoEnvironment`. The concept of a simulation environment, or _gym_, is quite common in RL problems. One of the challenges of reinforcement learning is the lack of available simulation environments on which to experiment. _OpenAI gym_ is a toolkit that provides a wide variety of simulated environments (e.g., Atari games, 2D/3D physical simulations), so we can train agents, compare them, or develop new RL algorithms. Additionally, it was developed with the aim of becoming a standardized environment and benchmark for RL research. We introduce a similar concept in the `CryptoEnvironment` class, where we create a simulation environment for cryptocurrencies. This class has the following key functions:

`getState`

This function returns the state as well as the historical return or raw historical data depending on the `is_cov_matrix` or `is_raw_time_series` flag

`getReward`

This function returns the reward (i.e., Sharpe ratio) of the portfolio, given the portfolio weights and lookback period

```
class CryptoEnvironment:

    def __init__(self, prices = './data/crypto_portfolio.csv', capital = 1e6):
        self.prices = prices
        self.capital = capital
        self.data = self.load_data()

    def load_data(self):
        data =  pd.read_csv(self.prices)
        try:
            data.index = data['Date']
            data = data.drop(columns = ['Date'])
        except:
            data.index = data['date']
            data = data.drop(columns = ['date'])
        return data

    def preprocess_state(self, state):
        return state

    def get_state(self, t, lookback, is_cov_matrix=True\
       is_raw_time_series=False):

        assert lookback <= t

        decision_making_state = self.data.iloc[t-lookback:t]
        decision_making_state = decision_making_state.pct_change().dropna()

        if is_cov_matrix:
            x = decision_making_state.cov()
            return x
        else:
            if is_raw_time_series:
                decision_making_state = self.data.iloc[t-lookback:t]
            return self.preprocess_state(decision_making_state)

    def get_reward(self, action, action_t, reward_t, alpha = 0.01):

        def local_portfolio(returns, weights):
            weights = np.array(weights)
            rets = returns.mean() # * 252
            covs = returns.cov() # * 252
            P_ret = np.sum(rets * weights)
            P_vol = np.sqrt(np.dot(weights.T, np.dot(covs, weights)))
            P_sharpe = P_ret / P_vol
            return np.array([P_ret, P_vol, P_sharpe])

        data_period = self.data[action_t:reward_t]
        weights = action
        returns = data_period.pct_change().dropna()

        sharpe = local_portfolio(returns, weights)[-1]
        sharpe = np.array([sharpe] * len(self.data.columns))
        ret = (data_period.values[-1] - data_period.values[0]) / \
        data_period.values[0]

        return np.dot(returns, weights), ret
```

Let‚Äôs explore the training of the RL model in the next step.

**4.3. Training the data**

As a first step, we initialize the `Agent` class and `CryptoEnvironment` class. Then, we set the `number of episodes` and `batch size` for the training purpose. Given the volatility of cryptocurrencies, we set the state `window size` to 180 and `rebalancing frequency` to 90 days:

```
N_ASSETS = 15
agent = Agent(N_ASSETS)
env = CryptoEnvironment()
window_size = 180
episode_count = 50
batch_size = 32
rebalance_period = 90
```

[Figure 9-10](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#QLearnPort) provides a deep dive into the training of the DQN algorithm used for developing the RL-based portfolio allocation strategy. If we look carefully, the chart is similar to the steps defined in [Figure 9-8](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#TraingStepsQTrd) in case study 1, with minor differences in the _Q-Matrix_, _reward function_, and _action_. Steps 1 to 7 describe the training and `CryptoEnvironment` module; steps 8 to 10 show what happens in the `replay buffer` function (i.e., `exeReplay` function) in the `Agent` module.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_0910.png" alt="mlbf 0910" height="723" width="1389"><figcaption></figcaption></figure>

**Figure 9-10. DQN training for portfolio optimization**

The details of steps 1 to 6 are:

1. Get the _current state_ using the helper function `getState` defined in the `CryptoEnvironment` module. It returns a correlation matrix of the cryptocurrencies based on the window size.
2. Get the _action_ for the given state using the `act` function of the `Agent` class. The action is the weight of the cryptocurrency portfolio.
3. Get the _reward_ for the given action using the `getReward` function in the `CryptoEnvironment` module.
4. Get the next state using the `getState` function. The detail of the next state is further used in the Bellman equation for updating the Q-function.
5. The details of the state, next state, and action are saved in the memory of the `Agent` object. This memory is used further by the `exeReply` function.
6. Check if the batch is complete. The size of a batch is defined by the batch size variable. If the batch is not complete, we move to the next time iteration. If the batch is complete, then we move to the `Replay buffer` function and update the Q-function by minimizing the MSE between the Q-predicted and the Q-target in steps 8, 9, and 10.

As shown in the following charts, the code produces the final results along with two charts for each episode. The first chart shows the total cumulative return over time, while the second chart shows the percentage of each cryptocurrency in the portfolio.

`Output`

`Episode 0/50 epsilon 1.0`

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in20.png" alt="mlbf 09in20" height="124" width="692"><figcaption></figcaption></figure>



<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in21.png" alt="mlbf 09in21" height="146" width="692"><figcaption></figcaption></figure>



`Episode 1/50 epsilon 1.0`

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in22.png" alt="mlbf 09in22" height="124" width="692"><figcaption></figcaption></figure>



<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in23.png" alt="mlbf 09in23" height="146" width="692"><figcaption></figcaption></figure>



`Episode 48/50 epsilon 1.0`

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in24.png" alt="mlbf 09in24" height="124" width="701"><figcaption></figcaption></figure>



<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in25.png" alt="mlbf 09in25" height="146" width="701"><figcaption></figcaption></figure>



`Episode 49/50 epsilon 1.0`

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in26.png" alt="mlbf 09in26" height="124" width="701"><figcaption></figcaption></figure>



<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in27.png" alt="mlbf 09in27" height="146" width="692"><figcaption></figcaption></figure>



The charts outline the details of the portfolio allocation of the first two and last two episodes. The details of other episodes can be seen in the Jupyter notebook under the GitHub repository for this book. The black line shows the performance of the portfolio, and the dotted grey line shows the performance of the benchmark, which is an equally weighted portfolio of cryptocurrencies.

In the beginning of episodes zero and one, the agent has no preconception of the consequences of its actions, and it takes randomized actions to observe the returns, which are quite volatile. Episode zero shows a clear example of erratic performance behavior. Episode one displays more stable movement but ultimately underperforms the benchmark. This is evidence that the cumulative reward per episode fluctuates significantly in the beginning of training.

The last two charts of episodes 48 and 49 show the agent starting to learn from its training and discovering the optimal strategy. Overall returns are relatively stable and outperform the benchmark. However, the overall portfolio weights are still quite volatile due to the short time series and high volatility of the underlying cryptocurrency assets. Ideally, we would be able to increase the number of training episodes and the length of historical data to enhance the training performance.

Let us look at the testing results.

#### 5. Testing the data

Recall that the black line shows the performance of the portfolio, and the dotted grey line is that of an equally weighted portfolio of cryptocurrencies:

```
agent.is_eval = True

actions_equal, actions_rl = [], []
result_equal, result_rl = [], []

for t in range(window_size, len(env.data), rebalance_period):

    date1 = t-rebalance_period
    s_ = env.get_state(t, window_size)
    action = agent.act(s_)

    weighted_returns, reward = env.get_reward(action[0], date1, t)
    weighted_returns_equal, reward_equal = env.get_reward(
        np.ones(agent.portfolio_size) / agent.portfolio_size, date1, t)

    result_equal.append(weighted_returns_equal.tolist())
    actions_equal.append(np.ones(agent.portfolio_size) / agent.portfolio_size)

    result_rl.append(weighted_returns.tolist())
    actions_rl.append(action[0])

result_equal_vis = [item for sublist in result_equal for item in sublist]
result_rl_vis = [item for sublist in result_rl for item in sublist]

plt.figure()
plt.plot(np.array(result_equal_vis).cumsum(), label = 'Benchmark', \
color = 'grey',ls = '--')
plt.plot(np.array(result_rl_vis).cumsum(), label = 'Deep RL portfolio', \
color = 'black',ls = '-')
plt.xlabel('Time Period')
plt.ylabel('Cumulative Returnimage::images\Chapter9-b82b2.png[]')
plt.show()
```

Despite underperforming during the initial period, the model performance was better overall, primarily due to avoiding the steep decline that the benchmark portfolio experienced in the latter part of the test window. The returns appear very stable, perhaps due to rotating away from the most volatile cryptocurrencies.

`Output`

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_09in28.png" alt="mlbf 09in28" height="440" width="667"><figcaption></figcaption></figure>



Let us inspect the return, volatility, Sharpe ratio, alpha, and beta of the portfolio and benchmark:

```
import statsmodels.api as sm
from statsmodels import regression
def sharpe(R):
    r = np.diff(R)
    sr = r.mean()/r.std() * np.sqrt(252)
    return sr

def print_stats(result, benchmark):

    sharpe_ratio = sharpe(np.array(result).cumsum())
    returns = np.mean(np.array(result))
    volatility = np.std(np.array(result))

    X = benchmark
    y = result
    x = sm.add_constant(X)
    model = regression.linear_model.OLS(y, x).fit()
    alpha = model.params[0]
    beta = model.params[1]

    return np.round(np.array([returns, volatility, sharpe_ratio, \
      alpha, beta]), 4).tolist()
```

```
print('EQUAL', print_stats(result_equal_vis, result_equal_vis))
print('RL AGENT', print_stats(result_rl_vis, result_equal_vis))
```

`Output`

```
EQUAL [-0.0013, 0.0468, -0.5016, 0.0, 1.0]
RL AGENT [0.0004, 0.0231, 0.4445, 0.0002, -0.1202]
```

Overall, the RL portfolio performs better across the board, with a higher return, higher Sharpe ratio, lower volatility, slight alpha, and negative correlation to the benchmark.

#### Conclusion

In this case study, we went beyond the classic efficient frontier for portfolio optimization and directly learned a policy of dynamically changing portfolio weights. We trained an RL-based model by setting up a standardized simulation environment. This approach facilitated the training process and can be explored further for general RL-based model training.

The trained RL-based model outperformed an equal-weight benchmark in the test set. The performance of the RL-based model can be further improved by optimizing the hyperparameters or using a longer time series for training. However, given the high complexity and low interpretability of an RL-based model, testing should occur across different time periods and market cycles before deploying the model for live trading. Also, as discussed in case study 1, we should carefully select the RL components, such as the reward function and state, and ensure we understand their impact on the overall model results.

The framework provided in this case study can enable financial practitioners to perform portfolio allocation and rebalancing with a very flexible and automated approach.

## Chapter Summary

Reward maximization is one of the key principles that drives algorithmic trading, portfolio management, derivative pricing, hedging, and trade execution. In this chapter, we saw that when we use RL-based approaches, explicitly defining the strategy or policy for trading, derivative hedging, or portfolio management is unnecessary. The algorithm determines the policy itself, which can lead to a much simpler and more principled approach than other machine learning techniques.

In [‚ÄúCase Study 1: Reinforcement Learning‚ÄìBased Trading Strategy‚Äù](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#CaseStudy1RL), we saw that RL makes algorithmic trading a simple game, which may or may not involve understanding fundamental information. In [‚ÄúCase Study 2: Derivatives Hedging‚Äù](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#CaseStudy2RL), we explored the use of reinforcement learning for a traditional derivative hedging problem. This exercise demonstrated that we can leverage the efficient numerical calculation of RL in derivatives hedging to address some of the drawbacks of the more traditional models. In [‚ÄúCase Study 3: Portfolio Allocation‚Äù](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#CaseStudy3RL), we performed portfolio allocation by learning a policy of changing portfolio weights dynamically in a continuously changing market environment, leading to further automation of the portfolio management process.

Although RL comes with some challenges, such as being computationally expensive and data intensive and lacking interpretability, it aligns perfectly with some areas in finance that are suited for policy frameworks based on reward maximization. Reinforcement learning has managed to achieve superhuman performance in finite action spaces, such as those in the games of Go, chess, and Atari. Looking ahead, with the availability of more data, refined RL algorithms, and superior infrastructure, RL will continue to prove to be immensely useful in finance.

## Exercises

* Using the ideas and concepts presented in case studies 1 and 2, implement a trading strategy based on a policy gradient algorithm for FX. Vary the key components (i.e., reward function, state, etc.) for this implementation.
* Implement the hedging of a fixed income derivative using the concepts presented in case study 2.
* Incorporate a transaction cost in case study 2 and see the impact on the overall results.
* Based on the ideas presented in case study 3, implement a Q-learning-based portfolio allocation strategy on a portfolio of stocks, FX, or fixed income instruments.

[1](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864666585896-marker) Reinforcement learning is also referred to as RL throughout this chapter.

[2](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864666519384-marker) For more details, be sure to check out _Reinforcement Learning: An Introduction_ by Richard Sutton and Andrew Barto (MIT Press), or David Silver‚Äôs free online [RL course at University College London](https://oreil.ly/niRu-).

[3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864666357976-marker) See [‚ÄúReinforcement Learning Models‚Äù](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#reinforcement\_learning\_models) for more details on model-based and model-free approaches.

[4](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864666341672-marker) A maximum drawdown is the maximum observed loss from peak to trough of a portfolio before a new peak is attained; it is an indicator of downside risk over a specified time period.

[5](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864665781608-marker) If the state and action spaces of MDP are finite, then it is called a finite Markov decision process.

[6](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864665772264-marker) The MDP example based on dynamic programming that was discussed in the previous section was an example of a model-based algorithm. As seen there, example rewards and transition probabilities are needed for such algorithms.

[7](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864665764040-marker) There are some models, such as the actor-critic model, that leverage both policy-based and value-based methods.

[8](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864665739960-marker) _Off-policy_, _Œµ-greedy_, _exploration_, and _exploitation_ are commonly used terms in RL and will be used in other sections and case studies as well.

[9](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864665665352-marker) Refer to [Chapter 3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#Chapter3) for more details on gradient descent.

[10](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864665424200-marker) Refer to [Chapter 3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#Chapter3) for more details on the sigmoid function.

[11](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864664958104-marker) The details of the Keras-based implementation of deep learning models are shown in [Chapter 3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#Chapter3).

[12](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864664864472-marker) Refer to [Chapter 3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#Chapter3) for more details on the linear and ReLU activation functions.

[13](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#idm45864663276952-marker) The expected shortfall is the expected value of an investment in the tail scenario.
