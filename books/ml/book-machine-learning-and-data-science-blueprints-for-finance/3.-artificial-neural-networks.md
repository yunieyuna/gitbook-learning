# 3. Artificial Neural Networks

## Chapter 3. Artificial Neural Networks

There are many different types of models used in machine learning. However, one class of machine learning models that stands out is artificial neural networks (ANNs). Given that artificial neural networks are used across all machine learning types, this chapter will cover the basics of ANNs.

ANNs are computing systems based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it.

_Deep learning_ involves the study of complex ANN-related algorithms. The complexity is attributed to elaborate patterns of how information flows throughout the model. Deep learning has the ability to represent the world as a nested hierarchy of concepts, with each concept defined in relation to a simpler concept. Deep learning techniques are extensively used in reinforcement learning and natural language processing applications that we will look at in Chapters [9](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch09.html#Chapter9) and [10](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch10.html#Chapter10).



We will review detailed terminology and processes used in the field of ANNs[1](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#idm45864692569272) and cover the following topics:

* Architecture of ANNs: Neurons and layers
* Training an ANN: Forward propagation, backpropagation and gradient descent
* Hyperparameters of ANNs: Number of layers and nodes, activation function, loss function, learning rate, etc.
* Defining and training a deep neural network–based model in Python
* Improving the training speed of ANNs and deep learning models

## ANNs: Architecture, Training, and Hyperparameters

ANNs contain multiple neurons arranged in layers. An ANN goes through a training phase by comparing the modeled output to the desired output, where it learns to recognize patterns in data. Let us go through the components of ANNs.

### Architecture

An ANN architecture comprises neurons, layers, and weights.

#### Neurons

The building blocks for ANNs are neurons (also known as artificial neurons, nodes, or perceptrons). Neurons have one or more inputs and one output. It is possible to build a network of neurons to compute complex logical propositions. Activation functions in these neurons create complicated, nonlinear functional mappings between the inputs and the output.[2](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#idm45864692554632)

As shown in [Figure 3-1](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#SingleNeuron), a neuron takes an input (_x1_, _x2_…_xn_), applies the learning parameters to generate a weighted sum (_z_), and then passes that sum to an activation function (_f_) that computes the output _f(z)_.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_0301.png" alt="mlbf 0301" height="668" width="747"><figcaption></figcaption></figure>

**Figure 3-1. An artificial neuron**

#### Layers

The output _f(z)_ from a single neuron (as shown in [Figure 3-1](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#SingleNeuron)) will not be able to model complex tasks. So, in order to handle more complex structures, we have multiple layers of such neurons. As we keep stacking neurons horizontally and vertically, the class of functions we can get becomes increasing complex. [Figure 3-2](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#Layers) shows an architecture of an ANN with an input layer, an output layer, and a hidden layer.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_0302.png" alt="mlbf 0302" height="507" width="743"><figcaption></figcaption></figure>

**Figure 3-2. Neural network architecture**

**Input layer**

The input layer takes input from the dataset and is the exposed part of the network. A neural network is often drawn with an input layer of one neuron per input value (or column) in the dataset. The neurons in the input layer simply pass the input value though to the next layer.

**Hidden layers**

Layers after the input layer are called hidden layers because they are not directly exposed to the input. The simplest network structure is to have a single neuron in the hidden layer that directly outputs the value.

A multilayer ANN is capable of solving more complex machine learning–related tasks due to its hidden layer(s). Given increases in computing power and efficient libraries, neural networks with many layers can be constructed. ANNs with many hidden layers (more than three) are known as _deep neural networks_. Multiple hidden layers allow deep neural networks to learn features of the data in a so-called feature hierarchy, because simple features recombine from one layer to the next to form more complex features. ANNs with many layers pass input data (features) through more mathematical operations than do ANNs with few layers and are therefore more computationally intensive to train.

**Output layer**

The final layer is called the output layer; it is responsible for outputting a value or vector of values that correspond to the format required to solve the problem.

#### Neuron weights

A neuron weight represents the strength of the connection between units and measures the influence the input will have on the output. If the weight from neuron one to neuron two has greater magnitude, it means that neuron one has a greater influence over neuron two. Weights near zero mean changing this input will not change the output. Negative weights mean increasing this input will decrease the output.

### Training

Training a neural network basically means calibrating all of the weights in the ANN. This optimization is performed using an iterative approach involving forward propagation and backpropagation steps.

#### Forward propagation

Forward propagation is a process of feeding input values to the neural network and getting an output, which we call _predicted value_. When we feed the input values to the neural network’s first layer, it goes without any operations. The second layer takes values from the first layer and applies multiplication, addition, and activation operations before passing this value to the next layer. The same process repeats for any subsequent layers until an output value from the last layer is received.

#### Backpropagation

After forward propagation, we get a predicted value from the ANN. Suppose the desired output of a network is _Y_ and the predicted value of the network from forward propagation is _Y′_. The difference between the predicted output and the desired output (_Y_–_Y′_ ) is converted into the loss (or cost) function _J(w)_, where _w_ represents the weights in ANN.[3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#idm45864692422888) The goal is to optimize the loss function (i.e., make the loss as small as possible) over the training set.

The optimization method used is _gradient descent_. The goal of the gradient descent method is to find the gradient of _J(w)_ with respect to _w_ at the current point and take a small step in the direction of the negative gradient until the minimum value is reached, as shown in [Figure 3-3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#GradDesc).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_0303.png" alt="mlbf 0303" height="520" width="941"><figcaption></figcaption></figure>

**Figure 3-3. Gradient descent**

In an ANN, the function _J(w)_ is essentially a composition of multiple layers, as explained in the preceding text. So, if layer one is represented as function _p()_, layer two as _q()_, and layer three as _r()_, then the overall function is _J(w)=r(q(p()))._ _w_ consists of all weights in all three layers. We want to find the gradient of _J(w)_ with respect to each component of _w_.

Skipping the mathematical details, the above essentially implies that the gradient of a component _w_ in the first layer would depend on the gradients in the second and third layers. Similarly, the gradients in the second layer will depend on the gradients in the third layer. Therefore, we start computing the derivatives in the reverse direction, starting with the last layer, and use backpropagation to compute gradients of the previous layer.

Overall, in the process of backpropagation, the model error (difference between predicted and desired output) is propagated back through the network, one layer at a time, and the weights are updated according to the amount they contributed to the error.

Almost all ANNs use gradient descent and backpropagation. Backpropagation is one of the cleanest and most efficient ways to find the gradient.

### Hyperparameters

_Hyperparameters_ are the variables that are set before the training process, and they cannot be learned during training. ANNs have a large number of hyperparameters, which makes them quite flexible. However, this flexibility makes the model tuning process difficult. Understanding the hyperparameters and the intuition behind them helps give an idea of what values are reasonable for each hyperparameter so we can restrict the search space. Let’s start with the number of hidden layers and nodes.

#### Number of hidden layers and nodes

More hidden layers or nodes per layer means more parameters in the ANN, allowing the model to fit more complex functions. To have a trained network that generalizes well, we need to pick an optimal number of hidden layers, as well as of the nodes in each hidden layer. Too few nodes and layers will lead to high errors for the system, as the predictive factors might be too complex for a small number of nodes to capture. Too many nodes and layers will overfit to the training data and not generalize well.

There is no hard-and-fast rule to decide the number of layers and nodes.

The number of hidden layers primarily depends on the complexity of the task. Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers and a huge amount of training data. For the majority of the problems, we can start with just one or two hidden layers and then gradually ramp up the number of hidden layers until we start overfitting the training set.

The number of hidden nodes should have a relationship to the number of input and output nodes, the amount of training data available, and the complexity of the function being modeled. As a rule of thumb, the number of hidden nodes in each layer should be somewhere between the size of the input layer and the size of the output layer, ideally the mean. The number of hidden nodes shouldn’t exceed twice the number of input nodes in order to avoid overfitting.

#### Learning rate

When we train ANNs, we use many iterations of forward propagation and backpropagation to optimize the weights. At each iteration we calculate the derivative of the loss function with respect to each weight and subtract it from that weight. The learning rate determines how quickly or slowly we want to update our weight (parameter) values. This learning rate should be high enough so that it converges in a reasonable amount of time. Yet it should be low enough so that it finds the minimum value of the loss function.

#### Activation functions

Activation functions (as shown in [Figure 3-1](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#SingleNeuron)) refer to the functions used over the weighted sum of inputs in ANNs to get the desired output. Activation functions allow the network to combine the inputs in more complex ways, and they provide a richer capability in the relationship they can model and the output they can produce. They decide which neurons will be activated—that is, what information is passed to further layers.

Without activation functions, ANNs lose a bulk of their representation learning power. There are several activation functions. The most widely used are as follows:

Linear (identity) function

Represented by the equation of a straight line (i.e., �(�)=��+�), where activation is proportional to the input. If we have many layers, and all the layers are linear in nature, then the final activation function of the last layer is the same as the linear function of the first layer. The range of a linear function is _–inf_ to _+inf_.

Sigmoid function

Refers to a function that is projected as an S-shaped graph (as shown in [Figure 3-4](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#ActFunc)). It is represented by the mathematical equation �(�)=1/(1+�–�) and ranges from 0 to 1. A large positive input results in a large positive output; a large negative input results in a large negative output. It is also referred to as logistic activation function.

Tanh function

Similar to sigmoid activation function with a mathematical equation ���ℎ(�)=2�������(2�)–1, where _Sigmoid_ represents the `sigmoid` function discussed above. The output of this function ranges from –1 to 1, with an equal mass on both sides of the zero-axis, as shown in [Figure 3-4](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#ActFunc).

ReLU function

ReLU stands for the Rectified Linear Unit and is represented as �(�)=���(�,0). So, if the input is a positive number, the function returns the number itself, and if the input is a negative number, then the function returns zero. It is the most commonly used function because of its simplicity.

[Figure 3-4](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#ActFunc) shows a summary of the activation functions discussed in this section.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_0304.png" alt="mlbf 0304" height="993" width="1438"><figcaption></figcaption></figure>

**Figure 3-4. Activation functions**

There is no hard-and-fast rule for activation function selection. The decision completely relies on the properties of the problem and the relationships being modeled. We can try different activation functions and select the one that helps provide faster convergence and a more efficient training process. The choice of activation function in the output layer is strongly constrained by the type of problem that is modeled.[4](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#idm45864692337496)

#### Cost functions

Cost functions (also known as loss functions) are a measure of the ANN performance, measuring how well the ANN fits empirical data. The two most common cost functions are:

Mean squared error (MSE)

This is the cost function used primarily for regression problems, where output is a continuous value. MSE is measured as the average of the squared difference between predictions and actual observation. MSE is described further in [Chapter 4](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch04.html#Chapter4).

Cross-entropy (or _log loss_)

This cost function is used primarily for classification problems, where output is a probability value between zero and one. Cross-entropy loss increases as the predicted probability diverges from the actual label. A perfect model would have a cross-entropy of zero.

#### Optimizers

Optimizers update the weight parameters to minimize the loss function.[5](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#idm45864692320696) Cost function acts as a guide to the terrain, telling the optimizer if it is moving in the right direction to reach the global minimum. Some of the common optimizers are as follows:

Momentum

The _momentum optimizer_ looks at previous gradients in addition to the current step. It will take larger steps if the previous updates and the current update move the weights in the same direction (gaining momentum). It will take smaller steps if the direction of the gradient is opposite. A clever way to visualize this is to think of a ball rolling down a valley—it will gain momentum as it approaches the valley bottom.

AdaGrad (Adaptive Gradient Algorithm)

_AdaGrad_ adapts the learning rate to the parameters, performing smaller updates for parameters associated with frequently occurring features, and larger updates for parameters associated with infrequent features.

RMSProp

_RMSProp_ stands for Root Mean Square Propagation. In RMSProp, the learning rate gets adjusted automatically, and it chooses a different learning rate for each parameter.

Adam (Adaptive Moment Estimation)

_Adam_ combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization and is one of the most popular gradient descent optimization algorithms.

#### Epoch

One round of updating the network for the entire training dataset is called an _epoch_. A network may be trained for tens, hundreds, or many thousands of epochs depending on the data size and computational constraints.

#### Batch size

The batch size is the number of training examples in one forward/backward pass. A batch size of 32 means that 32 samples from the training dataset will be used to estimate the error gradient before the model weights are updated. The higher the batch size, the more memory space is needed.

## Creating an Artificial Neural Network Model in Python

In [Chapter 2](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch02.html#Chapter2) we discussed the steps for end-to-end model development in Python. In this section, we dig deeper into the steps involved in building an ANN-based model in Python.

Our first step will be to look at Keras, the Python package specifically built for ANN and deep learning.

### Installing Keras and Machine Learning Packages

There are several Python libraries that allow building ANN and deep learning models easily and quickly without getting into the details of underlying algorithms. Keras is one of the most user-friendly packages that enables an efficient numerical computation related to ANNs. Using Keras, complex deep learning models can be defined and implemented in a few lines of code. We will primarily be using Keras packages for implementing deep learning models in several of the book’s case studies.

[Keras](https://keras.io/) is simply a wrapper around more complex numerical computation engines such as [TensorFlow](https://www.tensorflow.org/) and [Theano](https://oreil.ly/-XFJP). In order to install Keras, TensorFlow or Theano needs to be installed first.

This section describes the steps to define and compile an ANN-based model in Keras, with a focus on the following steps.[6](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#idm45864692283160)

#### Importing the packages

Before you can start to build an ANN model, you need to import two modules from the Keras package: `Sequential` and `Dense`:

```
from Keras.models import Sequential
from Keras.layers import Dense
import numpy as np
```

#### Loading data

This example makes use of the `random` module of NumPy to quickly generate some data and labels to be used by ANN that we build in the next step. Specifically, an array with size _(1000,10)_ is first constructed. Next, we create a labels array that consists of zeros and ones with a size _(1000,1)_:

```
data = np.random.random((1000,10))
Y = np.random.randint(2,size= (1000,1))
model = Sequential()
```

#### Model construction—defining the neural network architecture

A quick way to get started is to use the Keras Sequential model, which is a linear stack of layers. We create a Sequential model and add layers one at a time until the network topology is finalized. The first thing to get right is to ensure the input layer has the right number of inputs. We can specify this when creating the first layer. We then select a dense or fully connected layer to indicate that we are dealing with an input layer by using the argument `input_dim`.

We add a layer to the model with the `add()` function, and the number of nodes in each layer is specified. Finally, another dense layer is added as an output layer.

The architecture for the model shown in [Figure 3-5](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#ANNArchitecture) is as follows:

* The model expects rows of data with 10 variables (`input_dim_=10` argument).
* The first hidden layer has 32 nodes and uses the `relu` activation function.
* The second hidden layer has 32 nodes and uses the `relu` activation function.
* The output layer has one node and uses the `sigmoid` activation function.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492073048/files/assets/mlbf_0305.png" alt="mlbf 0305" height="777" width="945"><figcaption></figcaption></figure>

**Figure 3-5. An ANN architecture**

The Python code for the network in [Figure 3-5](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#ANNArchitecture) is shown below:

```
model = Sequential()
model.add(Dense(32, input_dim=10, activation= 'relu' ))
model.add(Dense(32, activation= 'relu' ))
model.add(Dense(1, activation= 'sigmoid'))
```

#### Compiling the model

With the model constructed, it can be compiled with the help of the `compile()` function. Compiling the model leverages the efficient numerical libraries in the Theano or TensorFlow packages. When compiling, it is important to specify the additional properties required when training the network. Training a network means finding the best set of weights to make predictions for the problem at hand. So we must specify the loss function used to evaluate a set of weights, the optimizer used to search through different weights for the network, and any optional metrics we would like to collect and report during training.

In the following example, we use `cross-entropy` loss function, which is defined in Keras as `binary_crossentropy`. We will also use the adam optimizer, which is the default option. Finally, because it is a classification problem, we will collect and report the classification accuracy as the metric.[7](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#idm45864692114968) The Python code follows:

```
model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , \
  metrics=[ 'accuracy' ])
```

#### Fitting the model

With our model defined and compiled, it is time to execute it on data. We can train or fit our model on our loaded data by calling the `fit()` function on the model.

The training process will run for a fixed number of iterations (epochs) through the dataset, specified using the `nb_epoch` argument. We can also set the number of instances that are evaluated before a weight update in the network is performed. This is set using the `batch_size` argument. For this problem we will run a small number of epochs (10) and use a relatively small batch size of 32. Again, these can be chosen experimentally through trial and error. The Python code follows:

```
model.fit(data, Y, nb_epoch=10, batch_size=32)
```

#### Evaluating the model

We have trained our neural network on the entire dataset and can evaluate the performance of the network on the same dataset. This will give us an idea of how well we have modeled the dataset (e.g., training accuracy) but will not provide insight on how well the algorithm will perform on new data. For this, we separate the data into training and test datasets. The model is evaluated on the training dataset using the `evaluation()` function. This will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics configured, such as accuracy. The Python code follows:

```
scores = model.evaluate(X_test, Y_test)
print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
```

### Running an ANN Model Faster: GPU and Cloud Services

For training ANNs (especially deep neural networks with many layers), a large amount of computation power is required. Available CPUs, or Central Processing Units, are responsible for processing and executing instructions on a local machine. Since CPUs are limited in the number of cores and take up the job sequentially, they cannot do rapid matrix computations for the large number of matrices required for training deep learning models. Hence, the training of deep learning models can be extremely slow on the CPUs.

The following alternatives are useful for running ANNs that generally require a significant amount of time to run on a CPU:

* Running notebooks locally on a GPU.
* Running notebooks on Kaggle Kernels or Google Colaboratory.
* Using Amazon Web Services.

#### GPU

A GPU is composed of hundreds of cores that can handle thousands of threads simultaneously. Running ANNs and deep learning models can be accelerated by the use of GPUs.

GPUs are particularly adept at processing complex matrix operations. The GPU cores are highly specialized, and they massively accelerate processes such as deep learning training by offloading the processing from CPUs to the cores in the GPU subsystem.

All the Python packages related to machine learning, including Tensorflow, Theano, and Keras, can be configured for the use of GPUs.

#### Cloud services such as Kaggle and Google Colab

If you have a GPU-enabled computer, you can run ANNs locally. If you do not, we recommend you use a service such as Kaggle Kernels, Google Colab, or AWS:

Kaggle

A popular data science website owned by Google that hosts Jupyter service and is also referred to as [Kaggle Kernels](https://www.kaggle.com/). Kaggle Kernels are free to use and come with the most frequently used packages preinstalled. You can connect a kernel to any dataset hosted on Kaggle, or alternatively, you can just upload a new dataset on the fly.

Google Colaboratory

A free Jupyter Notebook environment provided by Google where you can use free GPUs. The features of [Google Colaboratory](https://oreil.ly/keqHk) are the same as Kaggle.

Amazon Web Services (AWS)

[AWS Deep Learning](https://oreil.ly/gU84O) provides an infrastructure to accelerate deep learning in the cloud, at any scale. You can quickly launch AWS server instances preinstalled with popular deep learning frameworks and interfaces to train sophisticated, custom AI models, experiment with new algorithms, or learn new skills and techniques. These web servers can run longer than Kaggle Kernels. So for big projects, it might be worth using an AWS instead of a kernel.

## Chapter Summary

ANNs comprise a family of algorithms used across all types of machine learning. These models are inspired by the biological neural networks containing neurons and layers of neurons that constitute animal brains. ANNs with many layers are referred to as deep neural networks. Several steps, including forward propagation and backpropagation, are required for training these ANNs. Python packages such as Keras make the training of these ANNs possible in a few lines of code. The training of these deep neural networks require more computational power, and CPUs alone might not be enough. Alternatives include using a GPU or cloud service such as Kaggle Kernels, Google Colaboratory, or Amazon Web Services for training deep neural networks.

### Next Steps

As a next step, we will be going into the details of the machine learning concepts for supervised learning, followed by case studies using the concepts covered in this chapter.

[1](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#idm45864692569272-marker) Readers are encouraged to refer to the book _Deep Learning_ by Aaron Courville, Ian Goodfellow, and Yoshua Bengio (MIT Press) for more details on ANN and deep learning.

[2](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#idm45864692554632-marker) Activation functions are described in detail later in this chapter.

[3](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#idm45864692422888-marker) There are many available loss functions discussed in the next section. The nature of our problem dictates our choice of loss function.

[4](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#idm45864692337496-marker) Deriving a regression or classification output by changing the activation function of the output layer is described further in [Chapter 4](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch04.html#Chapter4).

[5](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#idm45864692320696-marker) Refer to [_https://oreil.ly/FSt-8_](https://oreil.ly/FSt-8) for more details on optimization.

[6](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#idm45864692283160-marker) The steps and Python code related to implementing deep learning models using Keras, as demonstrated in this section, are used in several case studies in the subsequent chapters.

[7](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch03.html#idm45864692114968-marker) A detailed discussion of the evaluation metrics for classification models is presented in [Chapter 4](https://learning.oreilly.com/library/view/machine-learning-and/9781492073048/ch04.html#Chapter4).
