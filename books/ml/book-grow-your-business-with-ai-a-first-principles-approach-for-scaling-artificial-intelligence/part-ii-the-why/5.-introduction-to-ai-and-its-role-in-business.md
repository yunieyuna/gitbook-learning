# 5. Introduction to AI and Its Role in Business

This chapter is the first one in the second part of the book, “The WHY.” Over the next three chapters, we will explore in detail the underlying reasons why most companies should fully embrace AI. I will provide the reader with an overview of AI, its history, and how AI can be integrated within the business fabric of enterprises, all from a First Principles point of view. This will help business and tech executives develop a solid foundation to incorporate AI into their strategies and drive sustainable growth.

You will see why we have waited until this chapter for an in-depth definition of AI: it is complicated. On the one hand, at a high level, there is some agreement in the field that AI refers to the development of **computer systems capable of performing tasks that usually require human intelligence**, such as visual perception, speech recognition, decision-making, and natural language understanding. On the other hand, when you go deep down to define what is exactly AI and how it is different from other technologies, we see it is quite complicated, and for some advanced concepts (what is intelligence, consciousness, or what is exactly a “super-intelligence”), there are many opinions in the field, some opposing ones. One recent example is when the European Union started legislation on AI; they really struggled on the exact definition of what an AI system is. The EU regulation[1](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn1) tries to be very specific on the type of algorithms, which is an issue due to two key reasons, as follows:

1.  a.

    Listing exactly what AI algorithms are in use now will miss the new ones being continually being developed.

    &#x20;
2.  b.

    It also extended to some rules-based algorithms that, technically speaking, it is not really AI but “traditional” math/statistics algorithms, such as an algorithm programmed with a well-defined sequence of rules to decide, prediction, or recommendation, for example, the amount an individual should receive in their social security payment, their grades on a particular exam, etc.

    &#x20;

> _Definition of AI used throughout the book._

> _I have used the International Organization for Standardization (ISO) definition of AI_[2](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn2)_,() which is “an interdisciplinary field, usually regarded as a branch of computer science, dealing with models and systems for the performance of functions generally associated with human intelligence, such as reasoning and learning.”_

We will provide the core components from a First Principles point of view to go deeper in the different definitions and to also explain the historical evolution. The reason why it is very important to have a good intuition about how AI has evolved is to predict how AI will evolve over the next few decades.

The core components of AI are as follows:

1.  a.

    Machine learning algorithms

    &#x20;
2.  b.

    Data (structured and unstructured)

    &#x20;
3.  c.

    Computational power and hardware

    &#x20;
4.  d.

    Software and programming languages

    &#x20;
5.  e.

    Human-computer interaction

    &#x20;

### What Is AI and How Does It Differ from Other Technologies?

It is very helpful to understand how an AI program differs from a traditional software program. As we have seen in the introduction, artificial intelligence (AI) encompasses the study and creation of computer systems capable of executing tasks that previously demanded human intellect. AI is an extensive domain, with machine learning (ML) as a subfield.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484296691/files/images/605840_1_En_5_Chapter/605840_1_En_5_Fig1_HTML.jpg" alt="" height="705" width="1418"><figcaption><p>Figure 5-1 </p></figcaption></figure>

Machine learning involves computer programs that can adapt models or identify patterns from data without explicit programming and with minimal to no human intervention (Figure [5-1](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fig1)). This contrasts with “rules-based algorithms,” where human programmers explicitly dictate the decisions made under specific world states.

In addition to defining AI, it is crucial to examine the attributes of AI applications and how they distinguish themselves from non-AI applications that achieve similar outcomes.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484296691/files/images/605840_1_En_5_Chapter/605840_1_En_5_Fig2_HTML.jpg" alt="" height="968" width="1713"><figcaption><p>Figure 5-2 </p></figcaption></figure>

These attributes can encompass AI’s complexity, its iterative methodology, the employment of hyperparameters, and the utilization of unstructured datasets. For instance, the **Bundesbank and BaFin**[4](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn4) adopted this strategy in a recent publication (refer to Figure [5-2](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fig2)). Instead of relying on a specific definition of machine learning, the document outlines various machine learning characteristics that establish a demarcation between what could be considered in and out of scope. Given the absence of a universally accepted definition of AI, this approach might prove beneficial for the industry.

#### Intuition About Models and Algorithms

> _All models are wrong, but some are useful._
>
> —George Box

**Intelligence** in AI refers to the ability of an algorithm to autonomously learn, adapt, and make decisions or predictions based on data. It encompasses the system’s capacity to recognize patterns, identify relationships, and reason in ways that mimic or surpass human cognitive abilities.

At a fundamental level, the objective of AI is to establish a model or algorithm capable of predicting the right outcome for any given input, even if the system has never encountered that specific input before. This is achieved by developing algorithms that can generalize from the patterns and relationships they learn from the training data. The goal is to create systems that can adapt and make accurate predictions or decisions in novel, unseen situations.

Achieving this level of intelligence in AI allows for the creation of more robust and versatile solutions that can handle a wide range of problems and adapt to new information or changing environments. This is crucial for real-world applications, where the data is often dynamic, and the systems must be capable of adjusting to new circumstances and making informed decisions without requiring constant human intervention.

It is important to keep in mind that what we are trying to do at its fundamental level is to find the best model that, fitting whatever data we have seen so far (training dataset), will be able to predict the right outcome in new circumstances.

An interesting fact is the so-called **no free lunch principle**: there is no single algorithm that works best for every problem or dataset. In simpler terms, it means that each algorithm has its strengths and weaknesses, and its performance is dependent on the specific problem it is trying to solve. Consequently, selecting the optimal algorithm requires understanding the characteristics of the problem and tailoring the approach to the unique requirements of that problem.

The first step in using AI to solve a problem is to select the appropriate algorithm. There are many different algorithms available, each with its own strengths and weaknesses. The choice of algorithm will depend on the specific problem being solved.

For example, if the problem is to classify images, a convolutional neural network (CNN) would be a good choice. CNNs are a type of neural network that are well-suited for image classification. They can learn the spatial relationships between pixels in an image, which is important for identifying objects.

Selecting the right algorithm for a specific problem is a critical aspect of the AI implementation process. As we described AI in the last section, in ML, the humans specify an optimization criterion (or sometimes called “objective function”), and the machine will follow it when seeing new data. This subsection aims to provide a basic understanding of the intuition behind algorithm selection, the overall process, and key factors to consider. By grasping these fundamental concepts, business executives can better comprehend AI applications and make informed decisions for their organizations.

It is worth noticing that you do not need to understand all the details to take advantage of the AI/ML algorithms; however, understanding the fundamentals will provide the reader a better grasp of the concepts and where certain algorithms can work better than others.

The overall process of algorithm selection involves the following steps:

1.  1\.

    **Analyze the training dataset**: Start by examining the available training data, which contains input-output pairs. The goal is to understand the data’s structure, identify patterns, and determine the type of problem at hand (classification, regression, etc.).

    &#x20;
2.  2\.

    **Choose a loss function**: The loss function quantifies the difference between the algorithm’s predictions and the actual output. Common loss functions include the sum of squares (used in regression problems) and the sum of absolute values. The choice of loss function depends on the problem type and the desired properties of the model, such as robustness to outliers or sensitivity to small errors.

    &#x20;
3.  3\.

    **Determine the optimization method**: To minimize the loss function, various optimization methods can be employed, such as gradient descent or stochastic gradient descent. These methods dictate how far to move in the weight space at each step to find the optimal solution. The choice of optimization method depends on factors like the size of the dataset and the complexity of the problem.

    &#x20;
4.  4\.

    **Select the batch size**: During the optimization process, the dataset can be divided into smaller subsets, or “batches,” to speed up computation and improve convergence. The choice of batch size can impact the algorithm’s performance and should be chosen based on factors like the available computational resources and the desired balance between speed and accuracy.

    &#x20;
5.  5\.

    **Tune hyperparameters**: Hyperparameters are parameters that control the learning process and are not learned by the algorithm itself. They include factors like the learning rate, the number of hidden layers in a neural network, and the amount of regularization. Hyperparameter tuning involves finding the optimal values for these parameters to maximize the algorithm’s performance on out-of-sample predictions.

    &#x20;

We will go into the details of this process in later chapters (the WHAT and the HOW) or how one picks the right algorithm, and more importantly, the tools and methodologies to implement it successfully within enterprise.

#### Different Types of Algorithms

While Chapters [8](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_8\_Chapter.xhtml), [9](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_9\_Chapter.xhtml), and [10](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_10\_Chapter.xhtml) will go in detail of the algorithms themselves, here we will present here a few popular classifications of AI/ML algorithms. There are few different classifications, or “taxonomies.”

We will start with the most popular one, machine learning. ML is a subset of AI that focuses on creating algorithms that can learn and improve from experience. ML algorithms can be broadly categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning. These types of ML differ in their approach to learning and the kind of data they use.

**Supervised Learning**

Supervised learning is the most common form of ML. In this approach, algorithms are trained on labeled data, which consists of input-output pairs. The goal is to learn a mapping from input data to the correct output. Supervised learning is commonly used for tasks such as classification and regression.

**Unsupervised Learning**

Unsupervised learning involves training algorithms on data without labels. The goal is to find patterns or structures within the data, such as grouping similar data points together (clustering) or reducing the dimensionality of the data (dimensionality reduction). Unsupervised learning is often used for tasks like anomaly detection or data compression. Figure [5-3](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fig3) shows both algorithms applied to the same dataset. Supervised learning focuses on making predictions based on labeled data, while unsupervised learning aims to identify underlying patterns in the data without relying on labels, and both can offer complementary insights when applied to the same dataset.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484296691/files/images/605840_1_En_5_Chapter/605840_1_En_5_Fig3_HTML.jpg" alt="" height="739" width="1713"><figcaption><p>Figure 5-3 </p></figcaption></figure>

**Reinforcement Learning**

Reinforcement learning is a type of ML where an algorithm learns to make decisions by interacting with an environment. The learning agent receives feedback in the form of rewards or penalties and adjusts its behavior to maximize the cumulative reward over time. Reinforcement learning is particularly useful in scenarios where the optimal solution is not known beforehand, such as game playing or robotics.

Table [5-1](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Tab1) shows these three main popular ML types and their typical use cases.Table 5-1&#x20;

Taxonomy of machine learning types and their goals and use cases

| Type of ML             | Goal                                      | Example use case       |
| ---------------------- | ----------------------------------------- | ---------------------- |
| Supervised learning    | Learn a mapping from input data to output | Image classification   |
| Unsupervised learning  | Find patterns or structures within data   | Anomaly detection      |
| Reinforcement learning | Maximize cumulative reward over time      | Game playing, robotics |

As well as the above most current popular taxonomy for the most used models, it is worth mentioning there are many others. A good wide definition is given in the book _The Master Algorithm_[6](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn6) by Pedro Domingos – see Table [5-2](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Tab2). He describes five “tribes” of AI algorithms, each with its unique approach to learning from data. The five tribes are as follows:

* **Symbolists**: Symbolists focus on manipulating symbols and logic to form rules and representations of knowledge. They primarily use techniques like inverse deduction and inductive logic programming. While symbolist methods were popular in early AI research, they have been overshadowed by other approaches due to their limitations in handling uncertainty and processing large datasets.
* **Connectionists**: Connectionists model learning based on neural networks, inspired by the human brain’s structure and function. They have gained significant popularity due to the success of deep learning in various applications such as image recognition, natural language processing, and speech recognition. Connectionist methods currently dominate the field of AI research and commercial applications.
* **Evolutionaries**: Evolutionaries take inspiration from the natural process of evolution, using techniques like genetic algorithms and genetic programming to optimize solutions. While these methods have been successful in certain optimization problems, they tend to be less popular than other approaches due to their computational complexity and slower convergence.
* **Bayesians**: Bayesians focus on probabilistic inference and reasoning under uncertainty, using Bayesian networks and graphical models. These methods have been particularly useful in applications where uncertainty is inherent, such as medical diagnosis and natural language understanding. However, Bayesian methods can be computationally intensive, limiting their widespread adoption in some cases.
* **Analogizers**: Analogizers learn from data by identifying and extrapolating similarities and relationships between instances. Techniques like Support Vector Machines (SVM) and k Nearest Neighbor(kNN) algorithms fall under this category. While these methods have proven effective in various applications, they tend to be less popular than connectionist approaches, particularly in large-scale data-driven problems.

Connectionist methods are currently the most popular among the five tribes due to their success in handling large datasets and complex problems. However, each tribe has its strengths and weaknesses, and understanding these different approaches can help researchers and practitioners choose the most suitable techniques for their specific AI challenges.Table 5-2&#x20;

Different families of algorithms – as per Pedro Domingo’s book

| Family type/“tribe” | Strength              | Example algorithms                                 |
| ------------------- | --------------------- | -------------------------------------------------- |
| Symbolists          | Structure inference   | Inverse deduction, decision trees                  |
| Connectionists      | Estimating parameters | Backpropagation, deep neural networks, perceptrons |
| Evolutionaries      | Structure discovery   | Genetic programming                                |
| Bayesians           | Uncertainty reduction | Probabilistic inference, Naïve Bayes, LDA          |
| Analogizers         | Mapping to novelty    | Kernel machines: SVM, kNN                          |

#### AI vs ML vs Deep Learning?

There is also a bit of confusion about how AI relates to ML or deep learning.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484296691/files/images/605840_1_En_5_Chapter/605840_1_En_5_Fig4_HTML.jpg" alt="" height="536" width="1062"><figcaption><p>Figure 5-4 </p></figcaption></figure>

The differences between these terms can be visualized using concentric circles (Figure [5-4](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fig4)), with deep learning at the center and AI as the outermost circle.

* **AI (artificial intelligence)**: AI is the broadest concept, referring to the development of computer systems that can perform tasks typically requiring human intelligence. This includes expert systems, problem-solving, pattern recognition, decision-making, and natural language understanding. AI encompasses various techniques and approaches to achieve these intelligent behaviors.
* **ML (machine learning)**: As a subfield of AI, ML focuses on developing algorithms that enable computers to learn from and make predictions or decisions based on data. Rather than being explicitly programmed for specific tasks, ML models can “learn” from examples and improve their performance over time. ML covers a wide range of algorithms, including decision trees, support vector machines, and clustering algorithms.
* **Neural networks**: Neural networks are a specific type of machine learning model inspired by the structure and function of the human brain. They consist of interconnected nodes (neurons) that process and transmit information. Neural networks can be used for various tasks, such as image recognition, natural language processing, and game playing. They can learn complex patterns and generalize from training data to make predictions or decisions.
* **Deep learning**: At the center of the concentric circles, deep learning is a subset of neural networks that involves using multi-layered neural networks with many hidden layers. These deep architectures allow the model to learn hierarchical representations of data, enabling the extraction of high-level features and abstractions. Deep learning has been responsible for significant breakthroughs in AI, particularly in areas like computer vision, natural language processing, and speech recognition. Most current large language models such as ChatGPT or Bard use deep learning.

#### Artificial General Intelligence

One of the main challenges in the field is the usage of the same terms to mean very different things. Indeed, whenever people use the term “AI,” they could be referring to very different concepts.

Across the book, whenever I refer to AI, I am talking about the current generation, or “narrow AI,” also known as weak AI or artificial specific intelligence, refers to AI systems that excel at specific tasks or domains but lack the versatility and adaptability of human intelligence.

**AGI (artificial general intelligence)** represents the next level, where AI systems can understand, learn, and apply knowledge across a wide range of tasks, like human intelligence.

In contrast, **superintelligence** describes an AI that has undergone rapid self-improvement after reaching “_the Singularity_,” resulting in an entity with intellectual capabilities far surpassing any humans.

**Consciousness**, on the other hand, is a complex and elusive concept, often associated with self-awareness, perception, and subjective experience. Although researchers and philosophers have debated the nature of consciousness for centuries, a clear and universally agreed-upon definition remains elusive. This lack of consensus makes it challenging to determine if and how AI systems could achieve consciousness and raises questions about the ethical implications of developing such advanced technologies**.**

We will go deeper through all these fascinating concepts and potential future developments – at least inasmuch as it is possible at the moment! – in Chapter [23](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_23\_Chapter.xhtml), but for the time being it is good to be aware of these different concepts, so whenever in conversation you can frame an informed point of view.

### The History of AI: Its Evolution over Time

The story of AI would surprise the reader in many aspects. The AI field traces its roots to the **Dartmouth Conference**[7](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn7) held in the summer of 1956. This conference was organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, who are considered the founding fathers of AI. The Dartmouth Conference aimed to explore the potential of machines to simulate human intelligence and problem-solving abilities.

In the original proposal for the conference, the organizers stated:

> _We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it._

This quote highlights the optimism of the early AI pioneers, who believed that creating artificial general intelligence (AGI) could be accomplished within a single summer with a small group of researchers. While this ambitious goal was not realized during the conference, the event marked the beginning of formal AI research, setting the stage for decades of advancements and innovations in the field.

As you can see in Figure [5-5](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fig5), AI history has been marked by alternating periods of optimism and pessimism, often referred to as “AI summers” and “AI winters.” These fluctuations in the field have been influenced by various factors, including technological breakthroughs, funding availability, and public perception:

* **First AI Summer (late 1950s–1960s)**: The Dartmouth Conference in 1956 marked the beginning of AI research. This period saw early optimism and significant funding for AI projects, leading to the development of early AI systems, such as symbolic reasoning systems and rule-based expert systems.
* **First AI Winter (1970s)**: During this period, optimism faded due to the limitations of early AI systems and their inability to fulfill the initial high expectations. This led to reduced funding and a slowdown in AI research. The Lighthill Report in the UK[8](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn8) and the Mansfield Amendment[9](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn9) in the USA were key factors contributing to this decline.
* **Second AI Summer (1980s)**: AI regained momentum with the emergence of expert systems, which were commercialized and adopted by various industries. This period also saw the rise of funding for AI research, primarily driven by the Japanese Fifth Generation Computer Systems project, which aimed to develop advanced AI systems.
* **Second AI Winter (late 1980s–1990s)**: This winter occurred due to the limitations of expert systems, which were brittle, difficult to maintain, and costly. Additionally, the Japanese project[10](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn10) failed to meet its ambitious goals, leading to a decline in funding and public interest in AI.
* **Third AI Summer (late 1990s–present)**: The current AI summer began with the advent of the Internet, big data, and advances in computational power. This period has seen the development of machine learning and deep learning techniques that have led to breakthroughs in various AI applications, such as computer vision, natural language processing, and speech recognition. The success of these techniques has attracted substantial investment and interest in AI research and development.

I personally experienced the second AI winter. I started my Industrial Engineering degree in early 1990s, and when I asked my professor to do the AI specialization (or as it was called, “Automation and Robotics”), his honest advice was “don’t do it, really think about your future, do you want to end up in the middle of nowhere, in charge of an automated factory to produce obscure industrial equipment?” I do believe the professor had my interest at heart as AI funding at the time was limited and hence career opportunities; however, I decided to do the specialization and time proved me right!

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484296691/files/images/605840_1_En_5_Chapter/605840_1_En_5_Fig5_HTML.jpg" alt="" height="1173" width="1713"><figcaption><p>Figure 5-5 </p></figcaption></figure>

We will now go through the development of AI core components over time. They all developed at different speeds, but they are also interdependent on each other. For instance, some algorithms that were known decades ago only became popular once the hardware or the data was available. One of the most recent AI successes, deep learning, is based on neural networks. While the basic neural network math was developed in the 1950s and some even before (the chain rule was discovered in the 17th century), the backpropagation algorithm is probably the most fundamental building block. It was first introduced in the 1960s and almost 30 years later (1989) popularized by Rumelhart, Hinton, and Williams in a paper called “Learning representations by back-propagating errors.” But it was not until around 2010, where some tweaks to the original algorithm, a combination of higher computer speed, lower cost, and the large amount of data availability, made them very popular and hence started to get real traction in industry.

#### Machine Learning Algorithms

AI research began in the mid-20th century, focusing on developing algorithms that could learn from data and improve their performance over time. Key milestones include the following:

* **1950s**: Early AI research focused on symbolic reasoning and rule-based systems (e.g., Samuel’s Checkers-playing program).
* **1980s**: The advent of connectionism and the development of artificial neural networks (e.g., Rumelhart, Hinton, and Williams’ backpropagation algorithm).
* **1990s**: The rise of statistical learning methods, such as support vector machines and Bayesian networks.
* **2010s**: The resurgence of deep learning and the development of more advanced neural network architectures (e.g., CNNs, RNNs, LSTMs, and Transformers).

#### Data (Structured and Unstructured)

As AI evolved, the availability and importance of data grew significantly. Advances in data storage and retrieval, as well as the growth of the Internet, facilitated access to massive amounts of data, enabling the development of more sophisticated AI models.

* **1950s–1960s**: Early days of computing and data storage. Data storage was limited and costly, often relying on magnetic tapes or punch cards for input and output.
* **1970s–1980s**: Rise of relational databases and structured data. The use of relational databases and Structured Query Language (SQL) became the standard for organizing and querying structured data.
* **1990s**: Data warehousing and the growth of unstructured data. With the advent of the Internet, there was an explosion in the volume and variety of data available, including unstructured data such as text, images, and multimedia. Data warehousing emerged as a solution for consolidating, storing, and analyzing large volumes of structured data from different sources.
* **2000s**: Big data and the rise of machine learning. As the volume, variety, and velocity of data continued to grow, the concept of “big data” emerged, along with new technologies for processing and analyzing massive datasets. This period saw the rise of distributed computing frameworks like Hadoop and NoSQL databases for handling unstructured data.
* **2010s**: Cloud computing, data lakes, and deep learning. Cloud computing revolutionized the way data is stored and processed, making it more accessible and scalable. Data lakes emerged as a solution for storing and processing vast amounts of raw, unstructured data in its native format.
* **2020s**: Data lakehouses and the fusion of structured and unstructured data. Data lakehouses combine the best aspects of data lakes and data warehouses, enabling organizations to store and analyze both structured and unstructured data in a unified platform.

#### Computational Power and Hardware

The growth of AI has been closely tied to advances in computational power. Key milestones include the following:

* **1940s–1960s**: The development of early digital computers, such as the Electronic Numerical Integrator and Computer (ENIAC).
* **1970s–1980s**: The invention of microprocessors and the rise of personal computers.
* **1990s–2000s**: The growth of parallel processing and the development of Graphics Processing Units (GPUs), which greatly accelerated AI computations.
* **2010s**: The rise of specialized hardware for AI, such as Tensor Processing Units (TPUs) and neuromorphic chips.

#### Software and Programming Languages

AI research and development have relied on various programming languages and software frameworks. Some key milestones include the following:

* **1950s–1960s**: Early AI languages such as Lisp and Prolog.
* **1980s–1990s**: The rise of object-oriented programming languages, such as C++ and Java.
* **2000s–2010s**: The development of Python as a dominant language for AI research, and the emergence of machine learning frameworks like TensorFlow, PyTorch, and Keras.

#### Human-Computer Interaction

AI systems have become increasingly integrated with human lives, leading to advancements in human-computer interaction. Key milestones include the following:

* **1960s–1970s**: The development of early graphical user interfaces (GUIs) and natural language processing (NLP) techniques.
* **1980s–1990s**: The rise of personal computers, the Internet, and the World Wide Web, enabling more accessible and user-friendly AI applications.
* **2000s–2010s**: The growth of mobile devices, voice assistants, and AI-powered recommendation systems, further integrating AI into daily life.

The interesting fact of looking at the evolution of AI through the lenses of their core components is how breakthrough moments happen when different small increments from different components converge to create something more powerful that the individual parts. Similarly, and looking at the future – more in later chapters – but just imagine for a moment what will happen when the current Hardware meets the Quantum Computing developments, when, as the Metaverse (both Augmented Reality and Virtual Reality) continues to evolve and humans spend more time on it, the data that will be available or finally, as more and more genetic data is uncovered, together with the processes to manipulate genetic material (e.g., CRISPR[12](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn12)), what AI could accomplish to help humanity (end diseases, extend human lifespan, etc.).

### AI in Business: Opportunities and Applications

As per the early sections, we have seen how _**AI technologies can automate “human-like” decisions at a local level at massive scale**_. Taking these capabilities to the enterprise means that every business process that currently relies on human involvement can either be optimized or completely replaced by the machines.

Artificial intelligence has already been transforming the way businesses operate and compete in today’s rapidly evolving market landscape. By adopting AI in their strategic plans, companies can unlock unprecedented opportunities to enhance decision-making, improve customer experience, streamline operations, and enable innovation. This subsection will delve into the various ways AI can be integrated into business strategy, with each key point substantiated by relevant examples and data.

#### Enhancing Decision-Making

The growing volume of data generated by businesses and consumers provides a wealth of information that can be harnessed using AI algorithms. AI-powered analytics enable businesses to derive actionable insights and make data-driven decisions. Key applications include the following:

* **Data-driven insights**: AI-powered analytics tools can process vast amounts of structured and unstructured data, extracting valuable insights that help businesses make more informed decisions. For example, advanced data analysis can identify patterns and trends that might be overlooked by traditional methods, enabling companies to better understand customer behavior, optimize pricing strategies, and forecast demand.
* **Predictive analytics**: Predictive analytics, a key application of AI, involves using historical data to forecast future outcomes. Businesses can leverage predictive analytics to anticipate customer preferences, identify potential market shifts, and proactively mitigate risks. For instance, AI-powered algorithms can predict equipment failures, enabling companies to schedule maintenance and avoid costly downtime.

#### Improving Customer Experience

AI-driven personalization and customer experience enhancements can help businesses foster customer loyalty and increase revenue. Key applications include the following:

* **Personalization**: AI can enable hyper-personalization by analyzing individual customer behavior, preferences, and demographics. This granular understanding allows companies to tailor marketing messages, product recommendations, and user experiences for each customer. For example, e-commerce platforms like Amazon use AI to recommend relevant products, resulting in increased customer satisfaction and higher conversion rates.
* **Chatbots and virtual assistants**: AI-powered chatbots and virtual assistants can handle routine customer inquiries, reducing response times and freeing up human agents to focus on more complex tasks. With natural language processing capabilities, these tools can understand and respond to customer queries, providing efficient and personalized support.

#### Streamlining Operations

AI-powered automation can significantly enhance a company’s productivity by streamlining processes, reducing manual tasks, and minimizing errors. Some key applications of AI in automation include the following:

* **Automation**: AI can automate repetitive tasks and processes, increasing efficiency and reducing human error. Robotic process automation (RPA) tools can handle tasks such as data entry, invoice processing, and payroll management, allowing employees to focus on more strategic responsibilities. For some industries, _Industrial Automation_, AI-powered robots can perform tasks in manufacturing and assembly lines with high precision and speed, resulting in improved production efficiency and reduced operational costs.
* **Process optimization**: AI-driven analytics can identify inefficiencies and bottlenecks in existing processes, enabling businesses to optimize workflows, reduce costs, and improve overall performance. For example, AI can optimize supply chain management by predicting demand fluctuations and adjusting inventory levels accordingly.

#### Enabling Innovation

AI can accelerate the innovation process and drive new product development by identifying emerging trends, generating novel ideas, and optimizing design and manufacturing processes. Key applications include:

* **New product development**: AI can accelerate new product development by identifying market gaps, predicting customer preferences, and simulating product performance. By leveraging AI in the design phase, companies can optimize product features, reduce development time, and minimize costs. This includes Generative _Design_, where AI algorithms can explore a vast design space to generate innovative product concepts that meet specific requirements and constraints, such as weight reduction, cost optimization, and performance improvement.
* **Identifying untapped markets**: AI algorithms can analyze market data, competitor performance, and customer preferences to identify new growth opportunities and untapped markets. This strategic use of AI can provide businesses with a competitive edge, enabling them to seize opportunities before their rivals.

As we have seen, the opportunities and applications of AI in business are vast and transformative. We will go into the details of how to identify specific areas to implement AI in your enterprise in later chapters. From automating mundane tasks to driving strategic decision-making, AI has the potential to significantly improve efficiency, competitiveness, and innovation across various industries. Businesses can successfully integrate AI into their operations, ensuring sustainable growth and long-term success using First Principles to unlock this business potential. Table [5-3](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Tab3) provides some examples.Table 5-3&#x20;

Examples of AI applications and their benefits in business strategy

| Application                  | Benefit                        | Example                            |
| ---------------------------- | ------------------------------ | ---------------------------------- |
| Data-driven insights         | Informed decision-making       | Optimizing pricing strategies      |
| Predictive analytics         | Anticipating future outcomes   | Predicting equipment failures      |
| Personalization              | Enhanced customer experience   | Tailored product recommendations   |
| Chatbots                     | Efficient customer support     | AI-powered virtual assistants      |
| Automation                   | Increased efficiency           | Robotic process automation         |
| Process optimization         | Improved performance           | Optimizing supply chain management |
| New product development      | Accelerated innovation         | AI-assisted design                 |
| Data-driven insights         | Informed decision-making       | Optimizing pricing strategies      |
| Identifying untapped markets | Market expansion opportunities | Discovering new growth segments    |

### Key Takeaways

In this chapter, we explored the foundations of AI, its history, and the opportunities and applications it presents for businesses. Here are the three key takeaways to remember from this chapter:

* **Takeaway point 1: Understanding AI fundamentals**
  * It is essential for business executives to have a firm grasp of AI concepts, including their definition, types, and core components. This understanding enables informed decision-making and allows organizations to leverage AI’s full potential effectively.
  * **Do**: Invest time and resources in learning the basics of AI, ML, neural networks, and deep learning. Encourage continuous learning and training within the organization to stay updated on the latest AI developments.
  * **Don’t**: Overlook the importance of understanding AI fundamentals, as this knowledge gap can lead to unrealistic expectations and poor implementation of AI technologies.
* **Takeaway point 2: Learning from AI history**
  * _**AI is a rapidly developing field, and there are several best practices that businesses should follow when using AI.**_ The history of AI, marked by alternating periods of optimism and pessimism, provides valuable lessons for businesses venturing into AI adoption. Understanding the factors that led to AI winters and summers can help organizations navigate potential challenges and capitalize on emerging opportunities.
  * **Do**: Study past successes and failures in AI to identify best practices and avoid repeating past mistakes. Recognize the importance of setting realistic expectations and goals when implementing AI solutions.
  * **Don’t**: Ignore the lessons from AI history, as they can provide invaluable insights into managing expectations, securing funding and overcoming technological barriers.
* **Takeaway point 3: Leveraging AI in business**
  * _**AI is a powerful tool that can be used to improve business performance**_. AI presents a wide range of opportunities and applications for businesses, including enhancing decision-making, improving customer experience, streamlining operations, and enabling innovation. Companies must identify the most relevant AI applications for their specific needs and goals to maximize the benefits of AI adoption.
  * **Do**: Conduct a thorough analysis of your business processes and identify areas where AI can provide the most significant impact. Prioritize AI initiatives that align with your company’s strategic objectives and carefully monitor the implementation and outcomes.
  * **Don’t**: Adopt AI technologies without a clear understanding of their purpose and potential benefits. Avoid implementing AI solutions merely for the sake of following trends or competing with rivals, as this can result in wasted resources and disappointing outcomes.

As we have seen in the brief history of AI, it is still early days, especially compared to other sciences or technology fields. In the next chapter, we will explore the current trends on AI.

Footnotes[1](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn1\_source)

[https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai)

&#x20;[2](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn2\_source)

[ISO/​IEC 22989:​2022(en), Information technology — Artificial intelligence — Artificial intelligence concepts and terminology](https://www.iso.org/obp/ui/%23iso:std:iso-iec:22989:ed-1:v1:en)

&#x20;[3](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn3\_source)

[www.fca.org.uk/publication/research/research-note-on-machine-learning-in-uk-financial-services.pdf](http://www.fca.org.uk/publication/research/research-note-on-machine-learning-in-uk-financial-services.pdf)

&#x20;[4](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn4\_source)

[Consultation paper:​ Machine learning in risk models – Characteristics and supervisory priorities (bundesbank.​de)](https://www.bundesbank.de/resource/blob/793670/61532e24c3298d8b24d4d15a34f503a8/mL/2021-07-15-ml-konsultationspapier-data.pdf)

&#x20;[5](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn5\_source)

[www.researchgate.net/figure/Supervised-and-unsupervised-machine-learning\_fig2\_325867536](http://www.researchgate.net/figure/Supervised-and-unsupervised-machine-learning\_fig2\_325867536)

&#x20;[6](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn6\_source)

[www.amazon.co.uk/Master-Algorithm-Ultimate-Learning-Machine/dp/0241004543](http://www.amazon.co.uk/Master-Algorithm-Ultimate-Learning-Machine/dp/0241004543)

&#x20;[7](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn7\_source)

[https://home.dartmouth.edu/about/artificial-intelligence-ai-coined-dartmouth](https://home.dartmouth.edu/about/artificial-intelligence-ai-coined-dartmouth)

&#x20;[8](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn8\_source)

[https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100/10018.htm](https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100/10018.htm)

&#x20;[9](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn9\_source)

[www.nsf.gov/nsb/documents/2000/nsb00215/nsb50/1970/mansfield.html](http://www.nsf.gov/nsb/documents/2000/nsb00215/nsb50/1970/mansfield.html)

&#x20;[10](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn10\_source)

[www.nytimes.com/1992/06/05/business/fifth-generation-became-japan-s-lost-generation.html](http://www.nytimes.com/1992/06/05/business/fifth-generation-became-japan-s-lost-generation.html)

&#x20;[11](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn11\_source)

[www.researchgate.net/figure/Development-history-of-artificial-intelligence-AI\_fig8\_323591839](http://www.researchgate.net/figure/Development-history-of-artificial-intelligence-AI\_fig8\_323591839)

&#x20;[12](https://learning.oreilly.com/library/view/grow-your-business/9781484296691/html/605840\_1\_En\_5\_Chapter.xhtml#Fn12\_source)

[https://sitn.hms.harvard.edu/flash/2014/crispr-a-game-changing-genetic-engineering-technique/](https://sitn.hms.harvard.edu/flash/2014/crispr-a-game-changing-genetic-engineering-technique/)
