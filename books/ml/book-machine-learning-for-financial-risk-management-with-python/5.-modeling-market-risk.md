# 5. Modeling Market Risk

## Chapter 5. Modeling Market Risk

> A measure of risk driven by historical data assumes the future will follow the pattern of the past. You need to understand the limitations of that assumption. More importantly, you need to model scenarios in which that pattern breaks down.
>
> &#x20;Miles Kennedy

Risk is ubiquitous in finance, but it is hard to quantify. First and foremost, it’s important to know how to differentiate the sources of financial risks on the grounds that it might not be a wise move to use the same tools against risks arising from different sources.

Thus, treating the various sources of financial risk differently is crucial because the impacts of those different risks, as well as the tools used to mitigate them, are completely different. Assuming that firms are subject to large market fluctuations, then all assets in their portfolios are susceptible to risk originating from these fluctuations. However, a different tool should be developed to cope with a risk emanating from customer profiles. In addition, keep in mind that different risk factors contribute significantly to asset prices. All of these examples imply that treating risk factors needs careful consideration in finance.

As was briefly discussed previously, these risks are mainly market, credit, liquidity, and operational risks. It is evident that some other types can be added to this list, but they can be thought of as subbranches of these main four risk types, which will be our focus throughout this chapter.

_Market risk_ is the risk arising from changes in financial indicators, such as the exchange rate, interest rate, inflation, and so on. Market risk can be referred to as risk of losses in on- and off-balance-sheet positions arising from movements in market prices (BIS 2020). Let’s now see how these factors affect market risk. Suppose that a rise in inflation rates poses a threat to the current profitability of the financial institutions, since inflation creates pressures on interest rates. This, in turn, affects the cost of funds for borrowers. These instances can be amplified, but we should also note the interactions of these financial risk sources. That is, when a single source of financial risk changes, other risk sources cannot stay constant. Thus to some extent, financial indicators are interrelated, meaning that the interactions of these risk sources should be taken into account.

As you can imagine, there are different tools to manage market risk. Of them, the most prominent and widely accepted tools are value at risk (VaR) and expected shortfall (ES). The ultimate aim of this chapter is to augment these approaches using recent developments in ML. At this juncture, it would be tempting to ask the following questions:

* Do traditional models fail in finance?
* What makes the ML-based model different?

I will start by tackling the first question. The first and foremost challenge that traditional models are unable to address is the complexity of the financial system. Due either to some strong assumptions, or simply their inability to capture the complexity introduced by the data, long-standing traditional models are starting to be replaced by ML-based models.

This fact is well put by Prado (2020):

> Considering the complexity of modern financial systems, it is unlikely that a researcher will be able to uncover the ingredients of a theory by visual inspection of the data or by running a few regressions.

To address the second question, it would be wise to think about the working logic of ML models. ML models, as opposed to old statistical methods, try to unveil the associations between variables, identify key variables, and enable us to find out the impact of the variables on the dependent variable without the need for a well-established theory. This is, in fact, the beauty of ML models in the sense that they allow us to discover theories rather than require them:

> Many methods from statistics and machine learning (ML) may, in principle, be used for both prediction and inference. However, statistical methods have a long-standing focus on inference, which is achieved through the creation and fitting of a project-specific probability model...
>
> By contrast, ML concentrates on prediction by using general-purpose learning algorithms to find patterns in often rich and unwieldy data.
>
> Bzdok (2018, p. 232)

In the following section, we’ll start our discussion on the market risk models. First, we’ll talk about the application of the VaR and ES models. After discussing the traditional application of these models, we will learn how we can improve them by using an ML-based approach. Let’s jump in.

## Value at Risk (VaR)

The VaR model emerged from a request made by a J.P. Morgan executive who wanted to have a summary report showing possible losses as well as risks that J.P. Morgan was exposed to on a given day. This report would inform executives about the risks assumed by the institution in an aggregated manner. The method by which market risk is computed is known as VaR. This report was the starting point of VaR, and now it has become so widespread that not only institutions prefer using VaR, but its adoption has become required by regulators.

The adoption of VaR dates back to the 1990s, and despite numerous extensions to it and new proposed models, it is still in use. What makes it so appealing? The answer comes from Kevin Dowd (2002, p. 10):

> The VaR figure has two important characteristics. The first is that it provides a common consistent measure of risk across different positions and risk factors. It enables us to measure the risk associated with a fixed-income position, say, in a way that is comparable to and consistent with a measure of the risk associated with equity positions. VaR provides us with a common risk yardstick, and this yardstick makes it possible for institutions to manage their risks in new ways that were not possible before. The other characteristic of VaR is that it takes account of the correlations between different risk factors. If two risks offset each other, the VaR allows for this offset and tells us that the overall risk is fairly low.

In fact, VaR addresses one of the most common questions an investor has: _what is the maximum expected loss of my investment?_

VaR provides a very intuitive and practical answer to this question. In this regard, it is used to measure the worst expected loss for a company over a given period and a pre-defined confidence interval. Suppose that a daily VaR of an investment is $1 million with 95% confidence interval. This would read as there being a 5% chance that an investor might incur a loss greater than $1 million in a day.

Based on this definition, we can determine that the components of VaR are a confidence interval, a time period, the value of an asset or portfolio, and the standard deviation, as we are talking about risk.

In summary, there are some important points in VaR analysis that need to be highlighted:

* VaR needs an estimation of the probability of loss.
* VaR concentrates on the potential losses. We are not talking about actual or realized losses; rather, VaR is a kind of loss projection.
* VaR has three key ingredients:
  * Standard deviation that defines the level of loss.
  * Fixed time horizon over which risk is assessed.
  * Confidence interval.

VaR can be measured via three different approaches:

* Variance-covariance VaR
* Historical simulation VaR
* Monte Carlo VaR

### Variance-Covariance Method

The variance-covariance method is also known as the _parametric_ method, because observations are assumed to be normally distributed. The variance-covariance method is commonplace in that returns are deemed to follow normal distribution. The parametric form assumption makes the application of variance-covariance method easy.

As in all VaR approaches, we can either work with a single asset or a portfolio. However, working with a portfolio requires careful treatment in the sense that correlation structure and portfolio variance need to be estimated. At this point, correlation comes into the picture, and historical data is used to calculate correlation, mean, and standard deviation. When augmenting this with an ML-based approach, correlation structure will be our main focus.

Suppose that we have a portfolio consisting of a single asset, as shown in [Figure 5-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#VaR\_illustration). It is shown that the return of this asset is zero and standard deviation is 1, and if the holding period is 1, the corresponding VaR value can be computed from the value of the asset by the corresponding Z-value and standard deviation. Hence, the normality assumption makes things easier, but it is a strong assumption, as there is no guarantee that asset returns are normally distributed; rather, most asset returns do not follow a normal distribution. Moreover, due to the normality assumption, potential risk in tail might not be captured. Therefore the normality assumption comes with a cost. See the following:

```
In [1]: import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        import datetime
        import yfinance as yf
        from scipy.stats import norm
        import requests
        from io import StringIO
        import seaborn as sns; sns.set()
        import warnings
        warnings.filterwarnings('ignore')
        plt.rcParams['figure.figsize'] = (10,6)

In [2]: mean = 0
        std_dev = 1
        x = np.arange(-5, 5, 0.01)
        y = norm.pdf(x, mean, std_dev)
        pdf = plt.plot(x, y)
        min_ylim, max_ylim = plt.ylim()
        plt.text(np.percentile(x, 5), max_ylim * 0.9, '95%:${:.4f}'
                 .format(np.percentile(x, 5)))
        plt.axvline(np.percentile(x, 5), color='r', linestyle='dashed',
                    linewidth=4)
        plt.title('Value at Risk Illustration')
        plt.show()
In [3]: mean = 0
        std_dev = 1
        x = np.arange(-5, 5, 0.01)
        y = norm.pdf(x, mean, std_dev) 
        pdf = plt.plot(x, y)
        min_ylim, max_ylim = plt.ylim() 
        plt.text(np.percentile(x, 5), max_ylim * 0.9, '95%:${:.4f}'
                 .format(np.percentile(x, 5))) 
        plt.axvline(np.percentile(x, 5), color='r', linestyle='dashed',
                    linewidth=4)
        plt.title('Value at Risk Illustration')
        plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO1-1)

Generating probability density function based on given `x`, mean, and standard deviation

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO1-2)

Limiting the x-axis and y-axis

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO1-3)

Specifying the location of `x` at 5% percentile of the `x` data

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0501.png" alt="VaR_illustration" height="365" width="600"><figcaption></figcaption></figure>

**Figure 5-1. VaR illustration**

**NOTE**

Following Fama (1965), it was realized that stock price returns do not follow normal distribution due to fat tail and asymmetry. This empirical observation implies that stock returns have higher kurtosis than that of a normal distribution.

Having high kurtosis amounts to fat tail, and this is able to capture the extreme negative returns. As the variance-covariance method is unable to capture fat tail, it cannot, therefore, estimate extreme negative returns that are likely to occur, especially in periods of crisis.

Let’s see how we apply the variance-covariance VaR in Python. To illustrate, let’s consider a two-asset portfolio. The formula of the variance-covariance VaR is as follows:

VaR=��������=�12�12+�22�22+��1�2�1�2��=�1�1+�2+�+2�1�2∑1,2

To apply this in code, we start with the following:

```
In [4]: def getDailyData(symbol):
                parameters = {'function': 'TIME_SERIES_DAILY_ADJUSTED',
                              'symbol': symbol,
                               'outputsize':'full',
                               'datatype': 'csv',
                               'apikey': 'insert your api key here'} 

                response = requests.get('https://www.alphavantage.co/query',
                                        params=parameters) 

                csvText = StringIO(response.text) 
                data = pd.read_csv(csvText, index_col='timestamp')
                return data

In [5]: symbols = ["IBM", "MSFT", "INTC"]
        stock3 = []
        for symbol in symbols:
            stock3.append(getDailyData(symbol)[::-1]['close']
                          ['2020-01-01': '2020-12-31']) 
        stocks = pd.DataFrame(stock3).T
        stocks.columns = symbols

In [6]: stocks.head()
Out[6]:                IBM    MSFT   INTC
        timestamp
        2020-01-02  135.42  160.62  60.84
        2020-01-03  134.34  158.62  60.10
        2020-01-06  134.10  159.03  59.93
        2020-01-07  134.19  157.58  58.93
        2020-01-08  135.31  160.09  58.97
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO2-1)

Identifying the parameters to be used in extracting data from Alpha Vantage

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO2-2)

Making a request to the Alpha Vantage website

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO2-3)

Opening the response file, which is in a text format

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO2-4)

Reversing the data that covers the period of 2019-01 to 2019-12 and appending the daily stock prices of IBM, MSFT, and INTC

**NOTE**

Alpha Vantage is a data-providing company that partners with major exchanges and institutions. Using Alpha Vantage’s API, it is possible to access stock prices with various time intervals (intraday, daily, weekly, and so on), stock fundamentals, and foreign exchange information. For more information, please see [Alpha Vantage’s website](https://oreil.ly/ByZYD).

We then perform our calculations:

```
In [7]: stocks_returns = (np.log(stocks) - np.log(stocks.shift(1))).dropna() 
        stocks_returns
Out[7]:                  IBM      MSFT      INTC
        timestamp
        2020-01-03 -0.008007 -0.012530 -0.012238
        2020-01-06 -0.001788  0.002581 -0.002833
        2020-01-07  0.000671 -0.009160 -0.016827
        2020-01-08  0.008312  0.015803  0.000679
        2020-01-09  0.010513  0.012416  0.005580
        ...              ...       ...       ...
        2020-12-24  0.006356  0.007797  0.010679
        2020-12-28  0.001042  0.009873  0.000000
        2020-12-29 -0.008205 -0.003607  0.048112
        2020-12-30  0.004352 -0.011081 -0.013043
        2020-12-31  0.012309  0.003333  0.021711

        [252 rows x 3 columns]

In [8]: stocks_returns_mean = stocks_returns.mean()
        weights  = np.random.random(len(stocks_returns.columns)) 
        weights /= np.sum(weights) 
        cov_var = stocks_returns.cov() 
        port_std = np.sqrt(weights.T.dot(cov_var).dot(weights)) 

In [9]: initial_investment = 1e6
        conf_level = 0.95

In [10]: def VaR_parametric(initial_investment, conf_level):
             alpha = norm.ppf(1 - conf_level, stocks_returns_mean, port_std) 
             for i, j in zip(stocks.columns, range(len(stocks.columns))):
                 VaR_param = (initial_investment - initial_investment *
                              (1 + alpha))[j] 
                 print("Parametric VaR result for {} is {} "
                       .format(i, VaR_param))
             VaR_param = (initial_investment - initial_investment * (1 + alpha))
             print('--' * 25)
             return VaR_param

In [11]: VaR_param = VaR_parametric(initial_investment, conf_level)
         VaR_param
         Parametric VaR result for IBM is 42606.16125893139
         Parametric VaR result for MSFT is 41024.50194348814
         Parametric VaR result for INTC is 43109.25240851776
         --------------------------------------------------

Out[11]: array([42606.16125893, 41024.50194349, 43109.25240852])
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO3-1)

Calculating logarithmic return

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO3-2)

Drawing random numbers for weights

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO3-3)

Generating weights

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO3-4)

Calculating covariance matrix

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO3-5)

Finding the portfolio standard deviation

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO3-6)

Computing the Z-score for a specific value using the percent point function (`ppf`)

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/7.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO3-7)

Estimating the variance-covariance VaR model

VaR changes depending on the time horizon in the sense that holding assets for a longer period makes an investor more susceptible to risk. As shown in [Figure 5-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#VaR\_horizon), VaR increases in relation to holding time by the amount of �. Additionally, the holding period is the longest period for portfolio liquidation. Taking into account the reporting purpose, a 30-day period may be a more suitable one for an investor. Therefore, we’ll illustrate that period in the following code, in which we generate [Figure 5-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#VaR\_horizon).

```
In [12]: var_horizon = []
         time_horizon = 30
         for j in range(len(stocks_returns.columns)):
             for i in range(1, time_horizon + 1):
                 var_horizon.append(VaR_param[j] * np.sqrt(i))
         plt.plot(var_horizon[:time_horizon], "o",
                  c='blue', marker='*', label='IBM')
         plt.plot(var_horizon[time_horizon:time_horizon + 30], "o",
                  c='green', marker='o', label='MSFT')
         plt.plot(var_horizon[time_horizon + 30:time_horizon + 60], "o",
                  c='red', marker='v', label='INTC')
         plt.xlabel("Days")
         plt.ylabel("USD")
         plt.title("VaR over 30-day period")
         plt.legend()
         plt.show()
```

The pros and cons of the variance-covariance method are as follows:

Pros

* Easy to calculate
* Does not require a large number of samples

Cons

* Observations are normally distributed
* Does not work well with nonlinear structures
* Requires the computation of the covariance matrix

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0502.png" alt="VaR_horizon" height="364" width="600"><figcaption></figcaption></figure>

**Figure 5-2. VaR over different horizons**

So, even though assuming normality sounds appealing, it may not be the best way to estimate VaR, especially in the case where the asset returns do not have a normal distribution. Luckily, there is another method that does not have a normality assumption, namely the historical simulation VaR model.

### The Historical Simulation Method

Having strong assumptions, such as a normal distribution, might be the cause of inaccurate estimations. A solution to this issue is the historical simulation VaR. This is an empirical method: instead of using a parametric approach, we find the percentile, which is the Z-table equivalent of variance-covariance method. Suppose that the confidence interval is 95%; 5% will be used in lieu of the Z-table values, and all we need to do is to multiply this percentile by the initial investment.

The following are the steps taken in the historical simulation VaR:

1. Obtain the asset returns of the portfolio (or individual asset)
2. Find the corresponding return percentile based on confidence interval
3. Multiply this percentile by initial investment

To do this in code, we can define the following function:

```
In [13]: def VaR_historical(initial_investment, conf_level): 
             Hist_percentile95 = []
             for i, j in zip(stocks_returns.columns,
                             range(len(stocks_returns.columns))):
                 Hist_percentile95.append(np.percentile(stocks_returns.loc[:, i],
                                                        5))
                 print("Based on historical values 95% of {}'s return is {:.4f}"
                       .format(i, Hist_percentile95[j]))
                 VaR_historical = (initial_investment - initial_investment *
                                   (1 + Hist_percentile95[j]))
                 print("Historical VaR result for {} is {:.2f} "
                       .format(i, VaR_historical))
                 print('--' * 35)

In [14]: VaR_historical(initial_investment,conf_level) 
         Based on historical values 95% of IBM's return is -0.0371
         Historical VaR result for IBM is 37081.53
         ----------------------------------------------------------------------
         Based on historical values 95% of MSFT's return is -0.0426
         Historical VaR result for MSFT is 42583.68
         ----------------------------------------------------------------------
         Based on historical values 95% of INTC's return is -0.0425
         Historical VaR result for INTC is 42485.39
         ----------------------------------------------------------------------
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO4-1)

Calculating the 95% percentile of stock returns

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO4-2)

Estimating the historical simulation VaR

The historical simulation VaR method implicitly assumes that historical price changes have a similar pattern, i.e., that there is no structural break. The pros and cons of this method are as follows:

Pros

* No distributional assumption
* Works well with nonlinear structures
* Easy to calculate

Cons

* Requires a large sample
* Needs high computing power

### The Monte Carlo Simulation VaR

Before delving into the Monte Carlo simulation VaR estimation, it would be good to briefly introduce the Monte Carlo simulation. Monte Carlo is a computerized mathematical method used to make an estimation in cases where there is no closed-form solution, so it is a highly efficient tool for numerical approximation. Monte Carlo relies on repeated random samples from a given distribution.

The logic behind Monte Carlo is well defined by Glasserman (2003, p. 11):

> Monte Carlo methods are based on the analogy between probability and volume. The mathematics of measure formalizes the intuitive notion of probability, associating an event with a set of outcomes and defining the probability of the event to be its volume or measure relative to that of a universe of possible outcomes. Monte Carlo uses this identity in reverse, calculating the volume of a set by interpreting the volume as a probability.

From the application standpoint, Monte Carlo is very similar to the historical simulation VaR, but it does not use historical observations. Rather, it generates random samples from a given distribution. Monte Carlo helps decision makers by providing links between possible outcomes and probabilities, which makes it an efficient and applicable tool in finance.

Mathematical Monte Carlo can be defined in the following way:

Let �1,�2,⋯,�� be independent and identically distributed random variables, and f(x) be a real-valued function. The law of large numbers states that:

𝖤(�(�))≈1�∑���(��)

So in a nutshell, a Monte Carlo simulation is doing nothing but generating random samples and calculating their mean. Computationally, it follows these steps:

1. Define the domain
2. Generate random numbers
3. Iterate and aggregate the result

The determination of mathematical � is a simple but illustrative example of Monte Carlo application.

Suppose we have a circle with radius _r_ = 1 and an area of 4. The area of a circle is �, and area of a square in which we try to fit the circle is 4. The ratio turns out to be:

�4

To leave � alone, the proportion between a circle and area can be defined as:

�����������������������������=��

Once we equalize these equations, it turns out that:

�=4���

If we go step by step, the first is to define domain, which is \[-1, 1]. So the numbers inside the circle satisfy �2+�2≤1.

The second step is to generate random numbers to meet this given condition. That is to say, we need to have uniformly distributed random samples, which is a rather easy task in Python. For the sake of practice, I will generate 100 uniformly distributed random numbers using the NumPy library:

```
In [15]: x = np.random.uniform(-1, 1, 100) 
         y = np.random.uniform(-1, 1, 100)

In [16]: sample = 100
         def pi_calc(x, y):
             point_inside_circle = 0
             for i in range(sample):
                 if np.sqrt(x[i] ** 2 + y[i] ** 2) <= 1: 
                     point_inside_circle += 1
             print('pi value is {}'.format(4 * point_inside_circle/sample))

In [17]: pi_calc(x,y)
         pi value is 3.2

In [18]: x = np.random.uniform(-1, 1, 1000000)
         y = np.random.uniform(-1, 1, 1000000)

In [19]: sample = 1000000

         def pi_calc(x, y):
             point_inside_circle = 0
             for i in range(sample):
                 if np.sqrt(x[i] ** 2 + y[i] ** 2) < 1:
                     point_inside_circle += 1
             print('pi value is {:.2f}'.format(4 * point_inside_circle/sample))

In [20]: pi_calc(x,y)
         pi value is 3.14

In [21]: sim_data = pd.DataFrame([])
         num_reps = 1000
         n = 100
         for i in range(len(stocks.columns)):
             mean = np.random.randn(n).mean()
             std = np.random.randn(n).std()
             temp = pd.DataFrame(np.random.normal(mean, std, num_reps))
             sim_data = pd.concat([sim_data, temp], axis=1)
         sim_data.columns = ['Simulation 1', 'Simulation 2', 'Simulation 3']

In [22]: sim_data
Out[22]:      Simulation 1  Simulation 2  Simulation 3
         0        1.587297     -0.256668      1.137718
         1        0.053628     -0.177641     -1.642747
         2       -1.636260     -0.626633      0.393466
         3        1.088207      0.847237      0.453473
         4       -0.479977     -0.114377     -2.108050
         ..            ...           ...           ...
         995      1.615190      0.940931      0.172129
         996     -0.015111     -1.149821     -0.279746
         997     -0.806576     -0.141932     -1.246538
         998      1.609327      0.582967     -1.879237
         999     -0.943749     -0.286847      0.777052

         [1000 rows x 3 columns]

In [23]: def MC_VaR(initial_investment, conf_level):
             MC_percentile95 = []
             for i, j in zip(sim_data.columns, range(len(sim_data.columns))):
                 MC_percentile95.append(np.percentile(sim_data.loc[:, i], 5)) 
                 print("Based on simulation 95% of {}'s return is {:.4f}"
                       .format(i, MC_percentile95[j]))
                 VaR_MC = (initial_investment - initial_investment *
                           (1 + MC_percentile95[j])) 
                 print("Simulation VaR result for {} is {:.2f} "
                       .format(i, VaR_MC))
                 print('--' * 35)

In [24]: MC_VaR(initial_investment, conf_level)
         Based on simulation 95% of Simulation 1's return is -1.7880
         Simulation VaR result for Simulation 1 is 1787990.69
         ----------------------------------------------------------------------
         Based on simulation 95% of Simulation 2's return is -1.6290
         Simulation VaR result for Simulation 2 is 1628976.68
         ----------------------------------------------------------------------
         Based on simulation 95% of Simulation 3's return is -1.5156
         Simulation VaR result for Simulation 3 is 1515623.93
         ----------------------------------------------------------------------
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO5-1)

Generating random numbers from uniform distribution

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO5-2)

Checking if points are inside the circle, which has a radius of 1

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO5-3)

Calculating 95% of every stock return and appending the result in the list named `MC_percentile95`

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO5-4)

Estimating Monte Carlo VaR

## Denoising

Volatility is everywhere, but it is a formidable task to find out what kind of volatility is most valuable. In general, there are two types of information in the market: _noise_ and _signal_. The former generates nothing but random information, but the latter equips us with valuable information by which an investor can make money. To illustrate, consider that there are two main players in the market: one using noisy information called a noise trader, and an informed trader who exploits signal or insider information. The noise trader’s trading motivation is driven by random behavior. So information flow in the market is considered a buying signal for some noise traders and a selling signal for others.

However, informed traders are considered to be rational ones in the sense that they are able to assess a signal because they know that it is private information.

Consequently, continuous flow of information should be treated with caution. In short, information coming from noise traders can be considered as noise, and information coming from insiders can be taken as signal, and this is the sort of information that matters. Investors who cannot distinguish between noise and signal can fail to gain profit and/or assess risk properly.

Now the problem turns out to be differentiating the flow of information in the financial markets. How can we differentiate noise from signal? And how can we use this information?

It is now worthwhile to discuss the Marchenko–Pastur theorem, which helps have homogenous covariance matrices. The Marchenko–Pastur theorem allows us to extract signal from noise using eigenvalues of covariance matrices.

**NOTE**

Let �∈ℝ��� be a square matrix. Then, �∈ℝ is an eigenvalue of _A_ and �∈ℝ� is the corresponding eigenvector of _A_ if

��=��

where �∈ℝ�≠0.

_Eigenvalue_ and _eigenvector_ have special meanings in a financial context. Eigenvectors represent the variance in covariance matrix, while an eigenvalue shows the magnitude of an eigenvector. Specifically, the largest eigenvector corresponds to largest variance, and the magnitude of this is equal to the corresponding eigenvalue. Due to noise in the data, some eigenvalues can be thought of as random, and it makes sense to detect and filter out these eigenvalues to retain only signals.

To differentiate noise and signal, we fit the Marchenko–Pastur theorem probability density function (PDF) to the noisy covariance. The PDF the of Marchenko–Pastur theorem takes the following form (Prado 2020):

�(�)=��(��-�)(�-�-)if�∈\[�-�-]0,if�∉\[�-�-]

where �+and�- are the maximum and minimum eigenvalues, respectively.

In the following code block, which is a slight modification of the code provided by Prado (2020), we will generate the probability density function of a Marchenko–Pastur distribution and kernel density, which will allow us to model a random variable in a nonparametric approach. Then, the Marchenko–Pastur distribution will be fitted to the data:

```
In [25]: def mp_pdf(sigma2, q, obs):
             lambda_plus = sigma2 * (1 + q ** 0.5) ** 2 
             lambda_minus = sigma2 * (1 - q ** 0.5) ** 2 
             l = np.linspace(lambda_minus, lambda_plus, obs)
             pdf_mp = 1 / (2 * np.pi * sigma2 * q * l) \
                      * np.sqrt((lambda_plus  - l)
                      *  (l - lambda_minus)) 
             pdf_mp = pd.Series(pdf_mp, index=l)
             return pdf_mp

In [26]: from sklearn.neighbors import KernelDensity

         def kde_fit(bandwidth,obs,x=None):
             kde = KernelDensity(bandwidth, kernel='gaussian') 
             if len(obs.shape) == 1:
                 kde_fit=kde.fit(np.array(obs).reshape(-1, 1)) 
             if x is None:
                 x=np.unique(obs).reshape(-1, 1)
             if len(x.shape) == 1:
                 x = x.reshape(-1, 1)
             logprob = kde_fit.score_samples(x) 
             pdf_kde = pd.Series(np.exp(logprob), index=x.flatten())
             return pdf_kde

In [27]: corr_mat = np.random.normal(size=(10000, 1000)) 
         corr_coef = np.corrcoef(corr_mat, rowvar=0) 
         sigma2 = 1
         obs = corr_mat.shape[0]
         q = corr_mat.shape[0] / corr_mat.shape[1]

         def plotting(corr_coef, q):
             ev, _ = np.linalg.eigh(corr_coef) 
             idx = ev.argsort()[::-1]
             eigen_val = np.diagflat(ev[idx]) 
             pdf_mp = mp_pdf(1., q=corr_mat.shape[1] / corr_mat.shape[0],
                             obs=1000) 
             kde_pdf = kde_fit(0.01, np.diag(eigen_val)) 
             ax = pdf_mp.plot(title="Marchenko-Pastur Theorem",
                              label="M-P", style='r--')
             kde_pdf.plot(label="Empirical Density", style='o-', alpha=0.3)
             ax.set(xlabel="Eigenvalue", ylabel="Frequency")
             ax.legend(loc="upper right")
             plt.show()
             return plt

In [28]: plotting(corr_coef, q);
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO6-1)

Calculating maximum expected eigenvalue

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO6-2)

Calculating minimum expected eigenvalue

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO6-3)

Generating probability density function of Marchenko-Pastur distribution

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO6-4)

Initiating kernel density estimation

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO6-5)

Fitting kernel density to the observations

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO6-6)

Assessing the log density model on observations

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/7.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO6-7)

Generating random samples from normal distribution

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/8.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO6-8)

Converting covariance matrix into correlation matrix

[![9](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/9.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO6-9)

Calculating eigenvalues of the correlation matrix

[![10](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/10.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO6-10)

Turning the NumPy array into diagonal matrix

[![11](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/11.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO6-11)

Calling `mp_pdf` to estimate the probability density function of the Marchenko–Pastur distribution

[![12](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/12.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO6-12)

Calling `kde_fit` to fit kernel distribution to the data

The resulting [Figure 5-3](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#Marchenko\_Pastur) shows that the Marchenko–Pastur distribution fits the data well. Thanks to the Marchenko–Pastur theorem, we are able to differentiate the noise and signal; we can now refer to data for which the noise has filtered as _denoised_.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0503.png" alt="M_P" height="377" width="600"><figcaption></figcaption></figure>

**Figure 5-3. Fitting Marchenko–Pastur distribution**

So far, we have discussed the main steps to take to denoising the covariance matrix so that we can plug it into the VaR model, which is called the _denoised VaR_ estimation. Denoising the covariance matrix is nothing but taking unnecessary information (noise) out of the data. So we can then make use of the signal from the market, focusing our attention on the important events only.

Denoising the covariance matrix includes the following stages:[1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#idm45737228731696)

1. Calculate the eigenvalues and eigenvectors based on correlation matrix.
2. Use kernel density estimation, find the eigenvector for a specific eigenvalue.
3. Fit the Marchenko–Pastur distribution to the kernel density estimation.
4. Find the maximum theoretical eigenvalue using the Marchenko–Pastur distribution.
5. Calculate the average of eigenvalues greater than the theoretical value.
6. Use these new eigenvalues and eigenvectors to calculate the denoised correlation matrix.
7. Calculate the denoised covariance matrix by the new correlation matrix.

Let’s take a look at how easy it is to apply finding the denoised covariance matrix with a few lines of code using the `portfoliolab` library in Python:

```
In [29]: import portfoliolab as pl

In [30]: risk_estimators = pl.estimators.RiskEstimators()

In [31]: stock_prices = stocks.copy()

In [32]: cov_matrix = stocks_returns.cov()
         cov_matrix
Out[32]:            IBM      MSFT      INTC
         IBM   0.000672  0.000465  0.000569
         MSFT  0.000465  0.000770  0.000679
         INTC  0.000569  0.000679  0.001158

In [33]: tn_relation = stock_prices.shape[0] / stock_prices.shape[1] 
         kde_bwidth = 0.25 
         cov_matrix_denoised = risk_estimators.denoise_covariance(cov_matrix,
                                                                  tn_relation,
                                                                  kde_bwidth) 
         cov_matrix_denoised = pd.DataFrame(cov_matrix_denoised,
                                            index=cov_matrix.index,
                                            columns=cov_matrix.columns)
         cov_matrix_denoised
Out[33]:            IBM      MSFT      INTC
         IBM   0.000672  0.000480  0.000589
         MSFT  0.000480  0.000770  0.000638
         INTC  0.000589  0.000638  0.001158

In [34]: def VaR_parametric_denoised(initial_investment, conf_level):
             port_std = np.sqrt(weights.T.dot(cov_matrix_denoised)
                                .dot(weights)) 
             alpha = norm.ppf(1 - conf_level, stocks_returns_mean, port_std)
             for i, j in zip(stocks.columns,range(len(stocks.columns))):
                 print("Parametric VaR result for {} is {} ".format(i,VaR_param))
             VaR_params = (initial_investment - initial_investment * (1 + alpha))
             print('--' * 25)
             return VaR_params

In [35]: VaR_parametric_denoised(initial_investment, conf_level)
         Parametric VaR result for IBM is [42606.16125893 41024.50194349
          43109.25240852]
         Parametric VaR result for MSFT is [42606.16125893 41024.50194349
          43109.25240852]
         Parametric VaR result for INTC is [42606.16125893 41024.50194349
          43109.25240852]
         --------------------------------------------------

Out[35]: array([42519.03744155, 40937.37812611, 43022.12859114])

In [36]: symbols = ["IBM", "MSFT", "INTC"]
         stock3 = []
         for symbol in symbols:
             stock3.append(getDailyData(symbol)[::-1]['close']
                           ['2007-04-01': '2009-02-01'])
         stocks_crisis = pd.DataFrame(stock3).T
         stocks_crisis.columns = symbols

In [37]: stocks_crisis
Out[37]:               IBM   MSFT   INTC
         timestamp
         2007-04-02  95.21  27.74  19.13
         2007-04-03  96.10  27.87  19.31
         2007-04-04  96.21  28.50  19.38
         2007-04-05  96.52  28.55  19.58
         2007-04-09  96.62  28.57  20.10
         ...           ...    ...    ...
         2009-01-26  91.60  17.63  13.38
         2009-01-27  91.66  17.66  13.81
         2009-01-28  94.82  18.04  14.01
         2009-01-29  92.51  17.59  13.37
         2009-01-30  91.65  17.10  12.90

         [463 rows x 3 columns]

In [38]: stock_prices = stocks_crisis.copy()

In [39]: stocks_returns = (np.log(stocks) - np.log(stocks.shift(1))).dropna()

In [40]: cov_matrix = stocks_returns.cov()

In [41]: VaR_parametric(initial_investment, conf_level)
         Parametric VaR result for IBM is 42606.16125893139
         Parametric VaR result for MSFT is 41024.50194348814
         Parametric VaR result for INTC is 43109.25240851776
         --------------------------------------------------

Out[41]: array([42606.16125893, 41024.50194349, 43109.25240852])

In [42]: VaR_parametric_denoised(initial_investment, conf_level)
         Parametric VaR result for IBM is [42606.16125893 41024.50194349
          43109.25240852]
         Parametric VaR result for MSFT is [42606.16125893 41024.50194349
          43109.25240852]
         Parametric VaR result for INTC is [42606.16125893 41024.50194349
          43109.25240852]
         --------------------------------------------------

Out[42]: array([42519.03744155, 40937.37812611, 43022.12859114])
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO7-1)

Relating the number of observations `T` to the number of variables `N`

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO7-2)

Identifying the bandwidth for kernel density estimation

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO7-3)

Generating the denoised covariance matrix

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO7-4)

Incorporating the denoised covariance matrix into the VaR formula

The difference between the traditionally applied VaR and the denoised VaR is even more pronounced in a crisis period. During a crisis period, correlation among assets becomes higher, which is sometimes referred to as _correlation breakdown_. We will evaluate the effect of a crisis to check this phenomenon, and to do that, we will use the 2017–2018 crisis. However, the exact beginning and ending date of the crisis is necessary to run this analysis; we’ll get this information from the National Bureau of Economic Research (NBER), which announces business cycles.[2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#idm45737230084272)

The result confirms that the correlation, and thereby VaRs, become higher during crisis periods.

Now, we managed to obtain a ML-based VaR using a denoised covariance matrix in lieu of an empirical matrix that we calculate directly from the data. Despite its appeal and ease of use, VaR is not a coherent risk measure, which requires satisfying certain conditions or axioms. You can think of these axioms as technical requirements for a risk measure.

Let �∈(0,1) be a fixed confidence level and (�, ℱ, 𝖯) be a probability space in which � represents a sample space, ℱ denotes a subset of sample space, and 𝖯 is probability measure.

**NOTE**

To illustrate, say � is the set of all possible outcomes in the event of tossing a coin, � = {H, T}. ℱ can be treated as tossing a coin twice, ℱ=2� = 22. Finally, probability measure, 𝖯, is the odds of getting tails (0.5).

Here are the four axioms of a coherent risk measure:

Translation invariance

For all outcomes _Y_ and a constant �∈ℝ, we have

���(�+�)=���(�)+�

which means that if a riskless amount _a_ is added to the portfolio, it results in lowering VaR by _a_.

Subadditivity

For all �1 and �2, we have

���(�1+�2)≤���(�1)+���(�2)

This axiom stresses the importance of diversification in risk management. Take �1 and �2 as two assets: if they are both included in the portfolio, then that results in lower VaR than having them separately. Let’s check whether VaR satisfies the subadditivity assumption:

```
In [43]: asset1 = [-0.5, 0, 0.1, 0.4] 
         VaR1 = np.percentile(asset1, 90)
         print('VaR for the Asset 1 is {:.4f}'.format(VaR1))
         asset2 = [0, -0.5, 0.01, 0.4] 
         VaR2 = np.percentile(asset2, 90)
         print('VaR for the Asset 2 is {:.4f}'.format(VaR2))
         VaR_all = np.percentile(asset1 + asset2, 90)
         print('VaR for the portfolio is {:.4f}'.format(VaR_all))
         VaR for the Asset 1 is 0.3100
         VaR for the Asset 2 is 0.2830
         VaR for the portfolio is 0.4000

In [44]: asset1 = [-0.5, 0, 0.05, 0.03] 
         VaR1 = np.percentile(asset1, 90)
         print('VaR for the Asset 1 is {:.4f}'.format(VaR1))
         asset2 = [0, -0.5, 0.02, 0.8] 
         VaR2 = np.percentile(asset2,90)
         print('VaR for the Asset 2 is {:.4f}'.format(VaR2))
         VaR_all = np.percentile(asset1 + asset2 , 90)
         print('VaR for the portfolio is {:.4f}'.format(VaR_all))
         VaR for the Asset 1 is 0.0440
         VaR for the Asset 2 is 0.5660
         VaR for the portfolio is 0.2750
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO8-1)

Asset return for the first asset

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO8-2)

Asset return for the second asset

It turns out that portfolio VaR is less that the sum of the individual VaRs, which makes no sense due to the risk mitigation through diversification. More elaborately, portfolio VaR should be lower than the sum of individual VaRs via diversification, as diversification mitigates risk, which in turn reduces the portfolio VaR.

Positive homogeneity

For all outcomes _Y_ and _a_ > 0, we have

���(��)=����(�)

which implies that the risk and value of the portfolio go in tandem—that is, if the value of a portfolio increases by an amount _a_, the risk goes up by _a_.

Monotonicity

For any two outcomes, �1 and �2, if �1≤�2, then:

���(�2)≤���(�1)

At first, this may seem puzzling, but it is intuitive in the sense that monotonicity implies a lower VaR in the case of higher asset returns.

We now know that VaR is not a coherent risk measure. However, VaR is not the only tool by which we estimate market risk. Expected shortfall is another, and coherent, market risk measure.

## Expected Shortfall

Unlike VaR, ES focuses on the tail of the distribution. More specifically, ES enables us to take into account unexpected risks in the market. However, this doesn’t mean that ES and VaR are two entirely different concepts. Rather, they are related—that is, it is possible to express ES _using_ VaR.

Let’s assume that loss distribution is continuous; then ES can be mathematically defined as:

���=11-�∫�1����

where _q_ denotes the quantile of the loss distribution. The ES formula suggests that it is nothing but a probability weighted average of (1-�)% of losses.

Let’s substitute �� and VaR, which gives us the following equation:

���=11-�∫�1������

Alternatively, it is the mean of losses exceeding VaR:

���=𝖤(�|�>����)

Loss distribution can be continuous or discrete and, as you can imagine, if it takes the discrete form, the ES is different such that

���=11-�∑�=01max(��)Pr(��)

where ���(��) shows the highest ��ℎ loss, and Pr(��) indicates probability of ��ℎ highest loss. In code, we can formulate this as:

```
In [45]: def ES_parametric(initial_investment , conf_level):
             alpha = - norm.ppf(1 - conf_level,stocks_returns_mean,port_std)
             for i, j in zip(stocks.columns, range(len(stocks.columns))):
                 VaR_param = (initial_investment * alpha)[j] 
                 ES_param = (1 / (1 - conf_level)) \
                            * initial_investment \
                            * norm.expect(lambda x: x,
                                          lb = norm.ppf(conf_level,
                                                        stocks_returns_mean[j],
                                                        port_std),
                                          loc = stocks_returns_mean[j],
                                          scale = port_std) 
                 print(f"Parametric ES result for {i} is {ES_param}")

In [46]: ES_parametric(initial_investment, conf_level)
         Parametric ES result for IBM is 52776.42396231898
         Parametric ES result for MSFT is 54358.083277762125
         Parametric ES result for INTC is 52273.33281273264
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO9-1)

Estimating the variance-covariance VaR

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO9-2)

Given the confidence interval, estimating the ES based on VaR

ES can also be computed based on the historical observations. Like the historical simulation VaR method, parametric assumption can be relaxed. To do that, the first return (or loss) corresponding to the 95% is found, and then the mean of the observations greater than the 95% gives us the result.

Here is what we do in code:

```
In [47]: def ES_historical(initial_investment, conf_level):
             for i, j in zip(stocks_returns.columns,
                             range(len(stocks_returns.columns))):
                 ES_hist_percentile95 = np.percentile(stocks_returns.loc[:, i],
                                                      5) 
                 ES_historical = stocks_returns[str(i)][stocks_returns[str(i)] <=
                                                        ES_hist_percentile95]\
                                                        .mean() 
                 print("Historical ES result for {} is {:.4f} "
                       .format(i, initial_investment * ES_historical))

In [48]: ES_historical(initial_investment, conf_level)
         Historical ES result for IBM is -64802.3898
         Historical ES result for MSFT is -65765.0848
         Historical ES result for INTC is -88462.7404
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO10-1)

Calculating the 95% of the returns

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO10-2)

Estimating the ES based on the historical observations

Thus far, we have seen how to model the expected shortfall in a traditional way. Now, it is time to introduce an ML-based approach to further enhance the estimation performance and reliability of the ES model.

## Liquidity-Augmented Expected Shortfall

As discussed, ES provides us with a coherent risk measure to gauge market risk. However, though we differentiate financial risks as market, credit, liquidity, and operational risks, that does not necessarily mean that these risks are entirely unrelated to one another. Rather, they are, to some extent, correlated. That is, once a financial crisis hit the market, market risk surges along with the drawdown on lines of credit, which in turn increases liquidity risk.

This fact is supported by Antoniades (2014, p. 6) stating that:

> Common pool of liquid assets is the resource constraint through which liquidity risk can affect the supply of mortgage credit.
>
> During the financial crisis of 2007–2008 the primary source of stresses to bank funding conditions arose from the funding illiquidity experienced in the markets for wholesale funding.

Ignoring the liqudity dimension of risk may result in underestimating the market risk. Therefore, augmenting ES with liquidity risk may make a more accurate and reliable estimation. Well, it sounds appealing, but how can we find a proxy for liquidity?

In the literature, bid-ask spread measures are commonly used for modeling liquidity. Shortly, _bid-ask spread_ is the difference between the highest available price (bid price) that a buyer is willing to pay and the lowest price (ask price) that a seller is willing to get. So bid-ask spread gives a tool to measure the transaction cost.

**NOTE**

_Liquidity_ can be defined as the ease of making a transaction in which assets are sold in a very short time period without a significant impact on market price. There are two main measures of liquidity:

Market liquidity

The ease with which an asset is traded.

Funding liquidity

The ease with which an investor can obtain funding.

Liquidity and the risk arising from it will be discussed in greater detail in [Chapter 7](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#chapter\_7).

To the extent that bid-ask spread is a good indicator of transaction cost, it is also a good proxy of liquidity in the sense that transaction cost is one of the components of liquidity. Spreads can be defined various ways depending on their focus. Here are the bid-ask spreads that we will use to incorporate liquidity risk into the ES model:

Effective spreadEffectivespread=2|(��-����)|

where �� is the price of trade at time _t_ and ���� is the midpoint of the bid-ask offer ((����-����)/2) prevailing at the time of the �.

Proportional quoted spreadProportionalquotedspread=(����-����)/����

where ���� is the ask price and ���� and ���� are bid price and mid price, respectively.

Quoted spreadQuotedspread=����-����Proportional effective spreadProportionaleffectivespread=2(|��-����|)/����

## Effective Cost

A buyer-initiated trade occurs when a trade is executed at a price above the quoted mid price. Similarly, a seller-initiated trade occurs when a trade is executed at a price below the quoted mid price. We can then describe the _effective cost_ as follows:

Effectivecost=(��-����)/����forbuyer-initiated(����/��)/����forseller-initiated

Now we need to find a way to incorporate these bid-ask spreads into the ES model so that we are able to account for the liquidity risk as well as market risk. We will employ two different methods to accomplish this task. The first method we’ll use is to take the cross-sectional mean of the bid-ask spread, as suggested by Chordia et al., (2000) and Pástor and Stambaugh (2003). The second method is to apply principal component analysis (PCA) as proposed by Mancini et al. (2013).

The cross-sectional mean is nothing but a row-wise averaging of the bid-ask spread. Using this method, we are able to generate a measure for market-wide liquidity. The averaging formula is as follows:

��,�=1�∑����,�

where ��,� is the market liquidity and ��,� is the individual liquidity measure, namely bid-ask spread in our case. Then we can calculate

���=��+Liquiditycost���=11-�∫�1������+12�����(�+��)

where

* ����� is the closing stock price
* � is the mean of spread
* _k_ is the scaling factor to accommodate fat tail
* � is the standard deviation of the spread

To convert these methods to code, we’ll do the following:

```
In [49]: bid_ask = pd.read_csv('bid_ask.csv') 

In [50]: bid_ask['mid_price'] = (bid_ask['ASKHI'] + bid_ask['BIDLO']) / 2 
         buyer_seller_initiated = []
         for i in range(len(bid_ask)):
             if bid_ask['PRC'][i] > bid_ask['mid_price'][i]: 
                 buyer_seller_initiated.append(1) 
             else:
                 buyer_seller_initiated.append(0) 

         bid_ask['buyer_seller_init'] = buyer_seller_initiated

In [51]: effective_cost = []
         for i in range(len(bid_ask)):
             if bid_ask['buyer_seller_init'][i] == 1:
                 effective_cost.append((bid_ask['PRC'][i] -
                                        bid_ask['mid_price'][i]) /
                                        bid_ask['mid_price'][i]) 
             else:
                 effective_cost.append((bid_ask['mid_price'][i] -
                                        bid_ask['PRC'][i])/
                                        bid_ask['mid_price'][i]) 
         bid_ask['effective_cost'] = effective_cost

In [52]: bid_ask['quoted'] = bid_ask['ASKHI'] - bid_ask['BIDLO'] 
         bid_ask['prop_quoted'] = (bid_ask['ASKHI'] - bid_ask['BIDLO']) /\
                                  bid_ask['mid_price'] 
         bid_ask['effective'] = 2 * abs(bid_ask['PRC'] - bid_ask['mid_price']) 
         bid_ask['prop_effective'] = 2 * abs(bid_ask['PRC'] -
                                             bid_ask['mid_price']) /\
                                             bid_ask['PRC'] 

In [53]: spread_meas = bid_ask.iloc[:, -5:]
         spread_meas.corr()
Out[53]:                 effective_cost    quoted  prop_quoted  effective  \
         effective_cost        1.000000  0.441290     0.727917   0.800894
         quoted                0.441290  1.000000     0.628526   0.717246
         prop_quoted           0.727917  0.628526     1.000000   0.514979
         effective             0.800894  0.717246     0.514979   1.000000
         prop_effective        0.999847  0.442053     0.728687   0.800713

                         prop_effective
         effective_cost        0.999847
         quoted                0.442053
         prop_quoted           0.728687
         effective             0.800713
         prop_effective        1.000000

In [54]: spread_meas.describe()
Out[54]:     effective_cost      quoted  prop_quoted   effective  prop_effective
      count      756.000000  756.000000   756.000000  756.000000      756.000000
      mean         0.004247    1.592583     0.015869    0.844314        0.008484
      std          0.003633    0.921321     0.007791    0.768363        0.007257
      min          0.000000    0.320000     0.003780    0.000000        0.000000
      25%          0.001517    0.979975     0.010530    0.300007        0.003029
      50%          0.003438    1.400000     0.013943    0.610000        0.006874
      75%          0.005854    1.962508     0.019133    1.180005        0.011646
      max          0.023283    8.110000     0.055451    6.750000        0.047677

In [55]: high_corr = spread_meas.corr().unstack()\
                     .sort_values(ascending=False).drop_duplicates() 
         high_corr[(high_corr > 0.80) & (high_corr != 1)] 
Out[55]: effective_cost  prop_effective    0.999847
         effective       effective_cost    0.800894
         prop_effective  effective         0.800713
         dtype: float64

In [56]: sorted_spread_measures = bid_ask.iloc[:, -5:-2]

In [57]: cross_sec_mean_corr = sorted_spread_measures.mean(axis=1).mean() 
         std_corr = sorted_spread_measures.std().sum() / 3 

In [58]: df = pd.DataFrame(index=stocks.columns)
         last_prices = []
         for i in symbols:
             last_prices.append(stocks[i].iloc[-1]) 
         df['last_prices'] = last_prices

In [59]: def ES_parametric(initial_investment, conf_level):
             ES_params = [ ]
             alpha = - norm.ppf(1 - conf_level, stocks_returns_mean, port_std)
             for i,j in zip(stocks.columns,range(len(stocks.columns))):
                 VaR_param = (initial_investment * alpha)[j]
                 ES_param = (1 / (1 - conf_level)) \
                            * norm.expect(lambda x: VaR_param, lb = conf_level)
                 ES_params.append(ES_param)
             return ES_params

In [60]: ES_params = ES_parametric(initial_investment, conf_level)
         for i in range(len(symbols)):
             print(f'The ES result for {symbols[i]} is {ES_params[i]}')
         The ES result for IBM is 145760.89803654602
         The ES result for MSFT is 140349.84772375744
         The ES result for INTC is 147482.03450111256

In [61]: k = 1.96

         for i, j in zip(range(len(symbols)), symbols):
             print('The liquidity Adjusted ES of {} is {}'
                   .format(j, ES_params[i] + (df.loc[j].values[0] / 2) *
                           (cross_sec_mean_corr + k * std_corr))) 
         The liquidity Adjusted ES of IBM is 145833.08767607837
         The liquidity Adjusted ES of MSFT is 140477.40110495212
         The liquidity Adjusted ES of INTC is 147510.60526566216
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO11-1)

Importing the `bid_ask` data

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO11-2)

Calculating the mid price

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO11-3)

Defining conditions for buyer- and seller-initiated trade

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO11-4)

If the above-given condition holds, it returns 1, and it is appended into the `buyer_seller_initiated` list

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO11-5)

If the above-given condition does not hold, it returns 0, and it is appended into the `buyer_seller_initiated` list

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO11-6)

If the `buyer_seller_initiated` variable takes a value of 1, the corresponding effective cost formula is run

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/7.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO11-7)

If the `buyer_seller_initiated` variable takes a value of 0, the corresponding effective cost formula is run

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/8.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO11-8)

Calculating the quoted, proportional quoted, effective, and proportional effective spreads

[![9](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/9.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO11-12)

Obtaining the correlation matrices and listing them column-wise

[![10](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/10.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO11-13)

Sorting out the correlation greater than 80%

[![11](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/11.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO11-14)

Calculating the cross-sectional mean of spread measures

[![12](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/12.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO11-15)

Obtaining the standard deviation of spreads

[![13](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/13.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO11-16)

Filtering the last observed stock prices from the `stocks` data

[![14](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/14.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO11-17)

Estimating the liquidity-adjusted ES

The PCA is a method used to reduce dimensionality. It is used to extract as much information as possible using as few components as possible. If we were to take [Figure 5-4](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#scree\_plot) as an example, out of five features, we might pick two components. So we reduce dimensionality at the expense of losing information because, depending on our chosen cut-off point, we pick the number of components and lose as much information as how many components we left off.

To be more specific, the point at which [Figure 5-4](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#scree\_plot) gets flatter implies that we retain less information and this is the cut-off point for the PCA. However, it is not an easy call in that there is a trade-off between the cutoff point and information retained. On the one hand, the higher the cut-off point (the higher number of components we have), the more information we retain (the less dimensionality we reduce). On the other hand, the lower the cut-off point (the fewer number of components we have), the less information we retain (the higher dimensionality we reduce). Getting a flatter scree plot is not the only criteria for selecting a suitable number of components, so what would be the possible criteria for picking the proper number of components? Here are the possible cut-off criteria for PCA:

* Greater than 80% explained variance
* More than one eigenvalue
* The point at which the scree plot gets flatter

**NOTE**

Please note that liquidity adjustment can be applied to VaR, too. The same procedure applies to VaR. Mathematically,

����=�����+12�����(�+��)

This application is left to the reader.

However, dimensionality reduction is not the only thing that we can take advantage of. In this example, we apply PCA for the benefit of getting the peculiar features of liquidity, because PCA filters the most important information from the data for us:

```
In [62]: from sklearn.decomposition import PCA
         from sklearn.preprocessing import StandardScaler

In [63]: scaler = StandardScaler()
         spread_meas_scaled = scaler.fit_transform(np.abs(spread_meas)) 
         pca = PCA(n_components=5) 
         prin_comp = pca.fit_transform(spread_meas_scaled) 

In [64]: var_expl = np.round(pca.explained_variance_ratio_, decimals=4) 
         cum_var = np.cumsum(np.round(pca.explained_variance_ratio_,
                                      decimals=4)) 
         print('Individually Explained Variances are:\n{}'.format(var_expl))
         print('=='*30)
         print('Cumulative Explained Variances are: {}'.format(cum_var))
         Individually Explained Variances are:
         [0.7494 0.1461 0.0983 0.0062 0.    ]
         ============================================================
         Cumulative Explained Variances are: [0.7494 0.8955 0.9938 1.     1.    ]

In [65]: plt.plot(pca.explained_variance_ratio_) 
         plt.xlabel('Number of Components')
         plt.ylabel('Variance Explained')
         plt.title('Scree Plot')
         plt.show()
In [66]: pca = PCA(n_components=2) 
         pca.fit(np.abs(spread_meas_scaled))
         prin_comp = pca.transform(np.abs(spread_meas_scaled))
         prin_comp = pd.DataFrame(np.abs(prin_comp), columns = ['Component 1',
                                                                'Component 2'])
         print(pca.explained_variance_ratio_*100)
         [65.65640435 19.29704671]

In [67]: def myplot(score, coeff, labels=None):
             xs = score[:, 0]
             ys = score[:, 1]
             n = coeff.shape[0]
             scalex = 1.0 / (xs.max() - xs.min())
             scaley = 1.0 / (ys.max() - ys.min())
             plt.scatter(xs * scalex * 4, ys * scaley * 4, s=5)
             for i in range(n):
                 plt.arrow(0, 0, coeff[i, 0], coeff[i, 1], color = 'r',
                           alpha=0.5)
                 if labels is None:
                     plt.text(coeff[i, 0], coeff[i, 1], "Var"+str(i),
                              color='black')
                 else:
                     plt.text(coeff[i,0 ], coeff[i, 1], labels[i],
                              color='black')

             plt.xlabel("PC{}".format(1))
             plt.ylabel("PC{}".format(2))
             plt.grid()

In [68]: spread_measures_scaled_df = pd.DataFrame(spread_meas_scaled,
                                                  columns=spread_meas.columns)

In [69]: myplot(np.array(spread_measures_scaled_df)[:, 0:2],
                np.transpose(pca.components_[0:2,:]),
                list(spread_measures_scaled_df.columns)) 
         plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO12-1)

Standardizing the spread measures

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO12-2)

Identifying the number of principal components as 5

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO12-3)

Applying the principal component to the _spread\_measures\_scaled_

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO12-4)

Observing the explained variance of the five principal components

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO12-5)

Observing the cumulative explained variance of the five principal components

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO12-6)

Drawing the _scree plot_ ([Figure 5-4](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#scree\_plot))

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/7.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO12-7)

Based on scree plot, determining two to be the number of components to be used in our PCA analysis

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/8.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO12-8)

Drawing the _biplot_ ([Figure 5-5](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#bi\_plot)) to observe the relationship between components and features

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0504.png" alt="mlfr 0504" height="377" width="600"><figcaption></figcaption></figure>

**Figure 5-4. PCA scree plot**

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0505.png" alt="mlfr 0505" height="357" width="600"><figcaption></figcaption></figure>

**Figure 5-5. PCA biplot**

We now have all the necessary information, and by incorporating this information, we are able to calculate the liquidity-adjusted ES. Unsurprisingly, the following code reveals that the liquidity-adjusted ES provides larger values compared to the standard ES application. This implies that including a liquidity dimension in our ES estimation results in higher risk:

```
In [70]: prin_comp1_rescaled = prin_comp.iloc[:,0] * prin_comp.iloc[:,0].std()\
                               + prin_comp.iloc[:, 0].mean() 
         prin_comp2_rescaled = prin_comp.iloc[:,1] * prin_comp.iloc[:,1].std()\
                               + prin_comp.iloc[:, 1].mean() 
         prin_comp_rescaled = pd.concat([prin_comp1_rescaled,
                                         prin_comp2_rescaled],
                                        axis=1)
         prin_comp_rescaled.head()
Out[70]:    Component 1  Component 2
         0     1.766661     1.256192
         1     4.835170     1.939466
         2     3.611486     1.551059
         3     0.962666     0.601529
         4     0.831065     0.734612

In [71]: mean_pca_liq = prin_comp_rescaled.mean(axis=1).mean() 
         mean_pca_liq
Out[71]: 1.0647130086973815

In [72]: k = 1.96
         for i, j in zip(range(len(symbols)), symbols):
             print('The liquidity Adjusted ES of {} is {}'
                   .format(j, ES_params[i] + (df.loc[j].values[0] / 2) *
                           (mean_pca_liq + k * std_corr))) 
         The liquidity Adjusted ES of IBM is 145866.2662997893
         The liquidity Adjusted ES of MSFT is 140536.02510785797
         The liquidity Adjusted ES of INTC is 147523.7364940803
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO13-1)

Calculating the liquidity part of the liquidity-adjusted ES formula for the first principal component

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO13-2)

Calculating the liquidity part of the liquidity-adjusted ES formula for the second principal component

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO13-3)

Calculating cross-sectional mean of the two principal components

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#co\_modeling\_market\_risk\_CO13-4)

Estimating the liquidity-adjusted ES

## Conclusion

Market risk has been always under scrutiny as it gives us the extent to which a company is vulnerable to risk emanating from market events. In a financial risk management textbook, it is customary to find a VaR and an ES model, which are two prominent and commonly applied models in theory and practice. In this chapter, after providing an introduction to these models, models were introduced to revisit and improve model estimation. To this end, we first tried to differentiate information flows in the form of noise and signal, which is called denoising. Then, we employed a denoised covariance matrix to improve the VaR estimation.

Next, we discussed an ES model as a coherent risk measure. The method that we applied to improve this model was a liquidity-based approach, by which we revisited the ES model and augmented it using a liquidity component so that it was possible to consider liquidity risk in estimating ES.

Further improvements in market risk estimation are possible, but our aim here is to give a general idea and the requisite tooling to provide a decent foundation for ML-based market risk approaches. However, you can go further and apply different tools. In the next chapter, we will discuss credit risk modeling as suggested by regulatory bodies like the Basel Committee on Banking Supervision (BCBS) and then enrich this model using an ML-based approach.

## References

Articles cited in this chapter:

* Antoniades, Adonis. 2016. “Liquidity Risk and the Credit Crunch of 2007-2008: Evidence from Micro-Level Data on Mortgage Loan Applications.” _Journal of Financial and Quantitative Analysis_ 51 (6): 1795-1822.
* Bzdok, D., N. Altman, and M. Krzywinski. 2018. “Points of Significance: Statistics Versus Machine Learning.” _Nature Methods_ 15 (4): 233-234.
* BIS, Calculation of RWA for Market Risk, 2020.
* Chordia, Tarun, Richard Roll, and Avanidhar Subrahmanyam. 2000. “Commonality in Liquidity.” Journal of Financial Economics 56 (1): 3-28.
* Mancini, Loriano, Angelo Ranaldo, and Jan Wrampelmeyer. 2013. “Liquidity in the Foreign Exchange Market: Measurement, Commonality, and Risk Premiums.” _The Journal of Finance_ 68 (5): 1805-1841.
* Pástor, Ľuboš, and Robert F. Stambaugh. 2003. “Liquidity Risk and Expected Stock Returns.” _Journal of Political Economy_ 111 (3): 642-685.

Books cited in this chapter:

* Dowd, Kevin. 2003. _An Introduction to Market Risk Measurement_. Hoboken, NJ: John Wiley and Sons.
* Glasserman, Paul. _Monte Carlo Methods in Financial Engineering_. 2013. Stochastic Modelling and Applied Probability Series, Volume 53. New York: Springer Science & Business Media.
* M. López De Prado. 2020. _Machine Learning for Asset Managers_. Cambridge: Cambridge University Press.

[1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#idm45737228731696-marker) The details of the procedure can be found at [Hudson and Thames](https://oreil.ly/gkQjX).

[2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch05.html#idm45737230084272-marker) See [NBER’s website](https://oreil.ly/07s71) for further information.
