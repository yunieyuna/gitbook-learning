# 6. Credit Risk Estimation

## Chapter 6. Credit Risk Estimation

> Although market risk is much better researched, the larger part of banks’ economic capital is generally used for credit risk. The sophistication of traditional standard methods of measurement, analysis, and management of credit risk might, therefore, not be in line with its significance.
>
> &#x20;Uwe Wehrspohn (2002)

The primary role of financial institutions is to create a channel by which funds move from entities with surplus into ones with deficit. Thereby, financial institutions ensure the capital allocation in the financial system as well as gain profit in exchange for these transactions.

However, there is an important risk for financial institutions to handle, which is credit risk. This is such a big risk that without it capital allocation might be less costly and more efficient. _Credit risk_ is the risk that arises when a borrower is not able to honor their debt. In other words, when a borrower defaults, they fail to pay back their debt, which causes losses for financial institutions.

Credit risk and its goal can be defined in a more formal way (BCBS and BIS 2000):

> Credit risk is most simply defined as the potential that a bank borrower or counterparty will fail to meet its obligations in accordance with agreed terms. The goal of credit risk management is to maximise a bank’s risk-adjusted rate of return by maintaining credit risk exposure within acceptable parameters.

Estimating credit risk is so formidable a task that a regulatory body, Basel, closely monitors recent developments in the financial markets and sets regulations to strengthen bank capital requirements. The importance of having strong capital requirements for a bank rests on the idea that banks should have a capital buffer in turbulent times.

There is a consensus among policy makers that financial institutions should have a minimum capital requirement to ensure the stability of the financial system because a series of defaults may result in a collapse in financial markets, as financial institutions provide collateral to one another. Those looking for a workaround for this capital requirement learned their lessons the hard way during the [2007—2008 mortgage crisis](https://oreil.ly/OjDw9).

Of course, ensuring at least a minimum capital requirement is a burden for financial institutions in the sense that capital is an asset they cannot channel to deficit entities to make a profit. Consequently, managing credit risk amounts to profitable and efficient transactions.

In this respect, this chapter shows how credit risk can be estimated using cutting-edge ML models. We start our discussion with a theoretical background of credit risk. Needless to say, there are many topics in credit risk analysis, but we confine our focus on probability of default and how we can introduce ML approaches for estimating it. For this purpose, customers are segmented via a clustering method so that models can be separately fitted to this data. This provides a better fit in the sense that the distribution of credit risk data changes across different customer segments. Given the clusters obtained, ML and deep learning models, including the Bayesian approach, are introduced to model the credit risk.

## Estimating the Credit Risk

Aside from the probability of default (which is the likelihood that a borrower fails to cover their debt), credit risk has three defining characteristics:

Exposure

This refers to a party that may possibly default or suffer an adverse change in its ability to perform.

Likelihood

The likelihood that this party will default on its obligations.

Recovery rate

How much can be retrieved if a default takes place.

The BCBS put forth the global financial credit management standards, which are known as the _Basel Accord_. There are currently three Basel Accords. The most distinctive rule set by Basel I in 1988 was the requirement to hold capital equating to at least 8% of risk-weighted assets.

Basel I includes the very first capital measurement system, which was created following the onset of the [Latin American debt crisis](https://oreil.ly/KI5vs). In Basel I, assets are classified as follows:

* 0% for risk-free assets
* 20% for loans to other banks
* 50% for residential mortgages
* 100% for corporate debt

In 1999, Basel II issued a revision to Basel I based on three main pillars:

* Minimum capital requirements, which sought to develop and expand the standardized rules set out in the 1988 Accord
* Supervisory review of an institution’s capital adequacy and internal assessment process
* Effective use of disclosure as a lever to strengthen market discipline and encourage sound banking practices

The last accord, Basel III in 2010, was inevitable. as the 2007–2008 mortgage crisis heightened. It introduced a new set of measures to further strengthened liquidity and poor governance practices. For instance, equity requirements were introduced to prevent a serial failure in the financial system, known as _domino effect_, during times of financial turbulence and crises. Accordingly, Basel III requires the financial ratios for banks listed in [Table 6-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#table6-1).

| Financial ratio          | Formula                                                                           |
| ------------------------ | --------------------------------------------------------------------------------- |
| Tier 1 capital ratio     | EquitycapitalRiskweightedassets>=4.5%                                             |
| Leverage ratio           | Tier1capitalAveragetotalassets>=3%                                                |
| Liquidity coverage ratio | StockofhighqualityliquidassetsTotalnetcashoutflowsoverthenext30calendardays>=100% |

Basel II suggests banks implement either a standardized approach or an internal ratings–based (IRB) approach to estimate the credit risk. The standardized approach is out of the scope of this book, but interested readers can refer to the “Standardized Approach to Credit Risk” [consultative document from the BIS](https://oreil.ly/0Mj7J).

Let’s now focus on the IRB approach; the key parameters of this internal assessment are:

Expectedloss=EAD×LGD×PD

where _PD_ is the probability of default, _LGD_ is the expected loss given default taking a value between 0 and 1, and _EAD_ is the exposure at default.

The most important and challenging part of estimating credit risk is to model the probability of default, and the aim of this chapter is mainly to come up with an ML model to address this issue. Before moving forward, there is one more important issue in estimating credit risk that is sometimes neglected or overlooked: _risk_ _bucketing_.

## Risk Bucketing

Risk bucketing is nothing but grouping borrowers with similar creditworthiness. The behind-the-scenes story of risk bucketing is to obtain homogenous groups or clusters so that we can better estimate the credit risk. Treating different risky borrowers equally may result in poor predictions because the model cannot capture entirely different characteristics of the data at once. Thus, by dividing the borrowers into different groups based on riskiness, risk bucketing enables us to make accurate predictions.

Risk bucketing can be accomplished via different statistical methods, but we will apply a clustering technique to end up with homogeneous clusters using K-means.

We live in the age of data, but that does not necessarily mean that we always find the data we are searching for. Rather, it is rare to find it without applying data-wrangling and cleaning techniques.

Data with dependent variables is, of course, easy to work with and also helps us get more accurate results. However, sometimes we need to unveil the hidden characteristics of the data—that is, if the riskiness of the borrowers is not known, we are supposed to come up with a solution for grouping them based on their riskiness.

Clustering is the method proposed to create these groups or _clusters_. Optimal clustering has clusters located far away from one another spatially:

> Clustering groups data instances into subsets in such a manner that similar instances are grouped together, while different instances belong to different groups. The instances are thereby organized into an efficient representation that characterizes the population being sampled.
>
> &#x20;Rokach and Maimon (2005)

Different clustering methods are available, but the K-means algorithm serves our purpose, which is to create risk bucketing for credit risk analysis. In K-means, the distance of observations within the cluster is calculated based on the cluster center, the _centroid_. Depending on the distance to the centroid, observations are clustered. This distance can be measured via different methods. Of them, the following are the most well-known metrics:

Euclidean

∑�=1�(��-��)2

Minkowski

(∑�=1�|��-��|�)1/�

Manhattan

∑�=1�|��-��|

The aim in clustering is to minimize the distance between the centroid and observations so that similar observations will be on the same cluster. This logic rests on the intuition that the more similar observations are, the smaller the distance between them. So we are seeking to minimize the distance between observations and the centroid, which is another way of saying that we are minimizing the sum of the squared error between the centroid and the observations:

∑�=1�∑�∈��(��-�)2

where _x_ is observation and �� is the centroid of ��ℎ cluster. However, considering the number of observations and the combinations of clusters, the search area might be too big to handle. It may sound intimidating, but don’t worry: we have the _expectation-maximization_ _(E-M)_ algorithm behind our clustering. As K-means does not have a closed-form solution, we are searching for an approximate one, and E-M provides this solution. In the E-M algorithm, _E_ refers to assigning observations to the nearest centroid, and _M_ denotes completion of the data generation process by updating the parameters.

In the E-M algorithm, the distances between observations and the centroid is iteratively minimized. The algorithm works as follows:

1. Pick _k_ random points to be centroids.
2. Based on the distance metric chosen, calculate the distances between observations and _n_ centroids. Based on these distances, assign each observation to the closest cluster.
3. Update cluster centers based on the assignment.
4. Repeat the process from step 2 until the centroid does not change.

Now, we apply risk bucketing using K-means clustering. To decide the optimal number of clusters, different techniques will be employed. First, we use the _elbow method_, which is based on the _inertia_.

Inertia is computed as the sum of the squared distances of observations to their closest centroid. Second, the _Silhouette score_ is introduced as a tool to decide the optimal number of clusters. This takes a value between 1 and -1. A value of 1 indicates that an observation is close to the correct centroid and correctly classified. However, -1 shows that an observation is not correctly clustered. The strength of the Silhouette score rests on taking into account both the intracluster distance and the intercluster distance. The formula for Silhouette score is as follows:

Silhouettescore=�-�max(�,�)

where _x_ is the average intercluster distance between clusters, and _y_ is the mean intracluster distance.

The third method is _Calinski-Harabasz_ _(CH)_, which is known as the _variance ratio criterion_. The formula for the CH method is as follows:

CH=������×�-��-1

where ��� denotes between-cluster variance, ��� represents within cluster variance, _N_ is number of observations, and _k_ is the number of clusters. Given this information, we are seeking a high CH score, as the larger (lower) the between-cluster variance (within cluster variance), the better it is for finding the optimal number of clusters.

The final approach is _gap analysis_. Tibshirani et al. (2001) came up with a unique idea by which we are able to find the optimal number of clusters based on reference distribution. Following the similar notations of Tibshirani et al., let ���� be a Euclidean distance between ��� and ���� and let �� be the ��ℎ cluster denoting the number of observations in cluster _r_:

∑�(���-����)2

The sum of pairwise distances for all observations in cluster _r_ is:

��=∑�,��∈����,��

The within-cluster sum of squares, ��, is:

��=∑�=1�12����

where _n_ is the sample size and expectation of �� is:

��=���(��/12)-(2/�)���(�)+��������

where _p_ and _k_ are dimension and centroids, respectively. Let’s create a practice exercise using German credit risk data. The data is gathered from the [Kaggle platform](https://oreil.ly/4NgIy), and the explanations of the variables are shown here:

* Age: Numerical
* Sex: Male, female
* Job: 0—unskilled and non-resident, 1—unskilled and resident, 2—skilled, 3—highly skilled
* Housing: Own, rent, free
* Saving accounts: Little, moderate, quite rich, rich
* Checking account: Numerical
* Credit amount: Numerical
* Duration: Numerical
* Purpose: Car, furniture/equipment, radio/TV, domestic appliances, repairs, education, business, vacation/others

The estimate of the optimal clusters will be the value that maximizes the gap statistic, as the gap statistic is the difference between the total within-intracluster variation for different values of _k_ and their expected values under null reference distribution of the respective data. The decision is made when we get the highest gap value.

In the following code block, we import the German credit dataset and drop the unnecessary columns. The dataset includes both categorical and numerical values, which need to be treated differently, and we will do this soon:

```
In [1]: import pandas as pd

In [2]: credit = pd.read_csv('credit_data_risk.csv')

In [3]: credit.head()
Out[3]: Unnamed: 0  Age     Sex  Job Housing Saving accounts Checking account  \
        0           0   67    male    2     own             NaN           little

        1           1   22  female    2     own          little         moderate

        2           2   49    male    1     own          little              NaN

        3           3   45    male    2    free          little           little

        4           4   53    male    2    free          little           little



           Credit amount  Duration              Purpose  Risk

        0           1169         6             radio/TV  good

        1           5951        48             radio/TV   bad

        2           2096        12            education  good

        3           7882        42  furniture/equipment  good

        4           4870        24                  car   bad


In [4]: del credit['Unnamed: 0'] 
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO1-1)

Dropping unnecessary column named `Unnamed: 0`

The summary statistics are given in the following code. Accordingly, the average age of the customers is roughly 35, average job type is skilled, average credit amount and duration are nearly 3,271 and 21, respectively. Additionally, the summary statistics tell us that the `credit amount` variable shows a relatively high standard deviation as expected. The `duration` and `age` variables have a very similar standard deviation, but the duration moves within a narrower interval as its minimum and maximum values are 4 and 72, respectively. As `job` is a discrete variable, it is natural to expect low dispersion and we have it:

```
In [5]: credit.describe()
Out[5]:                Age          Job  Credit amount     Duration
        count  1000.000000  1000.000000    1000.000000  1000.000000
        mean     35.546000     1.904000    3271.258000    20.903000
        std      11.375469     0.653614    2822.736876    12.058814
        min      19.000000     0.000000     250.000000     4.000000
        25%      27.000000     2.000000    1365.500000    12.000000
        50%      33.000000     2.000000    2319.500000    18.000000
        75%      42.000000     2.000000    3972.250000    24.000000
        max      75.000000     3.000000   18424.000000    72.000000
```

In what follows, the distribution of the numerical variables in the dataset are examined via histogram and it turns out none of the variables follow a normal distribution. The `age`, `credit amount`, and `duration` variables are positively skewed as we can see in [Figure 6-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#credit\_risk\_hist), generated by the following:

```
In [6]: import matplotlib.pyplot as plt
        import seaborn as sns; sns.set()
        plt.rcParams["figure.figsize"] = (10,6) 

In [7]: numerical_credit = credit.select_dtypes(exclude='O') 

In [8]: plt.figure(figsize=(10, 8))
        k = 0
        cols = numerical_credit.columns
        for i, j in zip(range(len(cols)), cols):
            k +=1
            plt.subplot(2, 2, k)
            plt.hist(numerical_credit.iloc[:, i])
            plt.title(j)
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO2-1)

Setting a fix figure size

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO2-2)

Dropping the object type variables to obtain all numerical variables

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0601.png" alt="credit_risk" height="478" width="600"><figcaption></figcaption></figure>

**Figure 6-1. Credit risk data histogram**

[Figure 6-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#credit\_risk\_hist) shows the distribution of age, job, credit amount, and duration variables. Aside from the `job` variable, which is a discrete variable, all other variables have skewed distributions.

The elbow method, as a first method, is introduced in the following code snippet and the resulting [Figure 6-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#elbow\_kmeans). To find the optimal number of clusters, we observe the slope of the curve and decide the cut-off point at which the curve gets flatter—that is, the slope of the curve gets lower. As it gets flatter, the inertia, telling us how far away the points within a cluster are located, decreases, which is nice for the purpose of clustering. On the other hand, as we allow inertia to decrease, the number of clusters increases, which makes the analysis more complicated. Given that trade-off, the stopping criteria is the point where the curve gets flatter. In code:

```
In [9]: from sklearn.preprocessing import StandardScaler
        from sklearn.cluster import KMeans
        import numpy as np

In [10]: scaler = StandardScaler()
         scaled_credit = scaler.fit_transform(numerical_credit) 

In [11]: distance = []
         for k in range(1, 10):
             kmeans = KMeans(n_clusters=k) 
             kmeans.fit(scaled_credit)
             distance.append(kmeans.inertia_) 

In [12]: plt.plot(range(1, 10), distance, 'bx-')
         plt.xlabel('k')
         plt.ylabel('Inertia')
         plt.title('The Elbow Method')
         plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO3-1)

Applying standardization for scaling purpose

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO3-2)

Running K-means algorithm

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO3-3)

Calculating `inertia` and storing into a list named `distance`

[Figure 6-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#elbow\_kmeans) shows that the curve gets flatter after four clusters. Thus, the elbow method suggests that we stop at four clusters.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0602.png" alt="elbow" height="369" width="600"><figcaption></figcaption></figure>

**Figure 6-2. Elbow method**

The following code, resulting in [Figure 6-3](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#silhouette\_kmeans), presents Silhouette scores on the x-axis for clusters 2 to 10. Given the average Silhouette score represented by the dashed line, the optimal number of clusters can be two:

```
In [13]: from sklearn.metrics import silhouette_score 
         from yellowbrick.cluster import SilhouetteVisualizer 

In [14]: fig, ax = plt.subplots(4, 2, figsize=(25, 20))
         for i in range(2, 10):
             km = KMeans(n_clusters=i)
             q, r = divmod(i, 2) 
             visualizer = SilhouetteVisualizer(km, colors='yellowbrick',
                                               ax=ax[q - 1][r]) 
             visualizer.fit(scaled_credit)
             ax[q - 1][r].set_title("For Cluster_"+str(i))
             ax[q - 1][r].set_xlabel("Silhouette Score")
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO4-1)

Importing the `silhouette_score` module to calculate Silhouette score

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO4-2)

Importing the `SilhouetteVisualizer` module to draw Silhouette plots

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO4-3)

Using `divmod` for configuring labels, as it returns the quotient (`q`) and remainder (`r`)

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO4-4)

Plotting the Silhouette scores

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0603.png" alt="silhouette" height="476" width="600"><figcaption></figcaption></figure>

**Figure 6-3. Silhouette score**

As mentioned, the CH method is a convenient tool for finding optimal clustering, and the following code shows how we can use this method in Python, resulting in [Figure 6-4](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#CH\_analysis). We are looking for the highest CH score, and we’ll see that it is obtained at cluster 2:

```
In [15]: from yellowbrick.cluster import KElbowVisualizer 
         model = KMeans()
         visualizer = KElbowVisualizer(model, k=(2, 10),
                                       metric='calinski_harabasz',
                                       timings=False) 
         visualizer.fit(scaled_credit)
         visualizer.show()
Out[]: <Figure size 576x396 with 0 Axes>
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO5-1)

Importing `KElbowVisualizer` to draw the CH score

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO5-2)

Visualizing the CH metric

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0604.png" alt="CH_analysis" height="400" width="600"><figcaption></figcaption></figure>

**Figure 6-4. The CH method**

[Figure 6-4](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#CH\_analysis) shows that the elbow occurs at the second cluster, indicating that stopping at two clusters is the optimum decision.

The last step for finding the optimal number of clusters is gap analysis, resulting in [Figure 6-5](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#gap\_cluster):

```
In [16]: from gap_statistic.optimalK import OptimalK 

In [17]: optimalK = OptimalK(n_jobs=8, parallel_backend='joblib') 
         n_clusters = optimalK(scaled_credit, cluster_array=np.arange(1, 10)) 

In [18]: gap_result = optimalK.gap_df 
         gap_result.head()
Out[18]:    n_clusters  gap_value         gap*  ref_dispersion_std        sk  \
         0         1.0   0.889755  5738.286952           54.033596  0.006408
         1         2.0   0.968585  4599.736451          366.047394  0.056195
         2         3.0   1.003974  3851.032471           65.026259  0.012381
         3         4.0   1.044347  3555.819296          147.396138  0.031187
         4         5.0   1.116450  3305.617917           27.894622  0.006559

                    sk*      diff        diff*
         0  6626.296782 -0.022635  6466.660374
         1  5328.109873 -0.023008  5196.127130
         2  4447.423150 -0.009186  4404.645656
         3  4109.432481 -0.065543  4067.336067
         4  3817.134689  0.141622  3729.880829

In [19]: plt.plot(gap_result.n_clusters, gap_result.gap_value)
         min_ylim, max_ylim = plt.ylim()
         plt.axhline(np.max(gap_result.gap_value), color='r',
                     linestyle='dashed', linewidth=2)
         plt.title('Gap Analysis')
         plt.xlabel('Number of Cluster')
         plt.ylabel('Gap Value')
         plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO6-1)

Importing the `OptimalK` module for calculating the gap statistic

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO6-2)

Running gap statistic using parallelization

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO6-3)

Identifying the number of clusters based on the gap statistic

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO6-4)

Storing the result of gap analysis

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0605.png" alt="gap_cluster" height="420" width="600"><figcaption></figcaption></figure>

**Figure 6-5. Gap analysis**

What we observe in [Figure 6-5](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#gap\_cluster) is a sharp increase to the point at which the gap value reaches its peak, and the analysis suggests stopping at the maximum value at which we find the optimal number for clustering. In this case, we find the value at cluster 5, so this is the cut-off point.

In light of these discussions, two clusters are chosen to be the optimal number of clusters, and the K-means clustering analysis is conducted accordingly. To illustrate, given the clustering analysis, let us visualize 2-D clusters with the following, resulting in [Figure 6-6](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#all\_clusters):

```
In [20]: kmeans = KMeans(n_clusters=2)
         clusters = kmeans.fit_predict(scaled_credit)

In [21]: plt.figure(figsize=(10, 12))
         plt.subplot(311)
         plt.scatter(scaled_credit[:, 0], scaled_credit[:, 2],
                     c=kmeans.labels_, cmap='viridis')
         plt.scatter(kmeans.cluster_centers_[:, 0],
                     kmeans.cluster_centers_[:, 2], s = 80,
                     marker= 'x', color = 'k')
         plt.title('Age vs Credit')
         plt.subplot(312)
         plt.scatter(scaled_credit[:, 0], scaled_credit[:, 2],
                     c=kmeans.labels_, cmap='viridis')
         plt.scatter(kmeans.cluster_centers_[:, 0],
                     kmeans.cluster_centers_[:, 2], s = 80,
                     marker= 'x', color = 'k')
         plt.title('Credit vs Duration')
         plt.subplot(313)
         plt.scatter(scaled_credit[:, 2], scaled_credit[:, 3],
                     c=kmeans.labels_, cmap='viridis')
         plt.scatter(kmeans.cluster_centers_[:, 2],
                     kmeans.cluster_centers_[:, 3], s = 120,
                     marker= 'x', color = 'k')
         plt.title('Age vs Duration')
         plt.show()
```

[Figure 6-6](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#all\_clusters) presents the behavior of the observations and cross sign `x` indicates the cluster center, i.e., the centroid. Age represents the more dispersed data, and the centroid of the `age` variable is located above the `credit` variable. Two continuous variables, namely `credit` and `duration`, are shown in the second subplot of [Figure 6-6](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#all\_clusters), where we observe clearly separated clusters. This figure suggests that the duration variable is more volatile compared to the credit variable. In the last subplot, the relationship between `age` and `duration` is examined via scatter analysis. It turns out that there are many overlapping observations across these two variables.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0606.png" alt="clusters" height="709" width="600"><figcaption></figcaption></figure>

**Figure 6-6. K-means clusters**

## Probability of Default Estimation with Logistic Regression

Having obtained the clusters, we are able to treat customers with similar characteristics the same way—that is, the model learns in an easier and more stable way if data with similar distributions is provided. Conversely, using all the customers for the entire sample might result in poor and unstable predictions.

This section is ultimately about calculating the probability of default with Bayesian estimation, but let’s first look at logistic regression for the sake of comparison.[1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#idm45737223570832)

Logistic regression is a classification algorithm, widely applicable in the finance industry. In other words, it proposes a regression approach to the classification problem. Logistic regression seeks to predict discrete output, taking into account some independent variables.

Let _X_ be the set of independent variables and _Y_ be a binary (or multinomial) output. Then, the conditional probability becomes:

Pr(�=1|�=�)

This can be read as: given the values of _X_, what is the probability of having _Y_ as 1? As the dependent variable of logistic regression is of the probabilistic type, we need to make sure the dependent variable cannot take on values other than between 0 and 1.

To this aim, a modification is applied known as _logistic (logit) transformation_, which is simply the log of the odds ratio (_p_ / 1 - _p_):

���(�1-�)

And the logistic regression model takes the following form:

���(�1-�)=�0+�1�

Solving _p_ results in:

�=��0+�1�1+��0+�1�

Let’s start off our application by preparing the data. First, we distinguish the clusters as 0 and 1. The credit data has a column named `risk`, suggesting the risk level of the customers. Next, the number of observations per risk in cluster 0 and cluster 1 are examined; it turns out we have 571 and 129 good customers in the cluster 0 and 1, respectively. In code:

```
In [22]: clusters, counts = np.unique(kmeans.labels_, return_counts=True) 

In [23]: cluster_dict = {}
         for i in range(len(clusters)):
             cluster_dict[i] = scaled_credit[np.where(kmeans.labels_==i)] 

In [24]: credit['clusters'] = pd.DataFrame(kmeans.labels_) 

In [25]: df_scaled = pd.DataFrame(scaled_credit)
         df_scaled['clusters'] = credit['clusters']

In [26]: df_scaled['Risk'] = credit['Risk']
         df_scaled.columns = ['Age', 'Job', 'Credit amount',
                              'Duration', 'Clusters', 'Risk']

In [27]: df_scaled[df_scaled.Clusters == 0]['Risk'].value_counts() 
Out[27]: good    571
         bad     193
         Name: Risk, dtype: int64

In [28]: df_scaled[df_scaled.Clusters == 1]['Risk'].value_counts() 
Out[28]: good    129
         bad     107
         Name: Risk, dtype: int64
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO7-1)

Obtaining cluster numbers

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO7-2)

Based on the cluster numbers, differentiating the clusters and storing them in a dictionary called `cluster_dict`

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO7-3)

Creating a `clusters` column using K-means labels

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO7-4)

Observing the number of observations of categories within a cluster

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO7-5)

Finding number of observations per category

Next, we draw a couple of bar plots to show the difference of the number of observations per risk level category (Figures [6-7](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#risk\_level1) and [6-8](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#risk\_level2)):

```
In [29]: df_scaled[df_scaled.Clusters == 0]['Risk'].value_counts()\
                                             .plot(kind='bar',
                                             figsize=(10, 6),
                                             title="Frequency of Risk Level");
In [30]: df_scaled[df_scaled.Clusters == 1]['Risk'].value_counts()\
                                             .plot(kind='bar',
                                             figsize=(10, 6),
                                             title="Frequency of Risk Level");
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0607.png" alt="cluster_2" height="381" width="600"><figcaption></figcaption></figure>

**Figure 6-7. Frequency of risk level of the first cluster**

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0608.png" alt="cluster_2" height="382" width="600"><figcaption></figcaption></figure>

**Figure 6-8. Frequency of risk level of the second cluster**

Based on the clusters we defined previously, we can analyze the frequency of risk level by histogram. [Figure 6-7](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#risk\_level1) shows that there is an imbalance distribution across risk level in the first cluster, whereas the frequency of good and bad risk levels are more balanced, if not perfectly balanced, in [Figure 6-8](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#risk\_level2).

At this point, let’s take a step back and focus on an entirely different problem: _class imbalance_. In credit risk analysis, it is not uncommon to have a class imbalance problem. Class imbalance arises when one class dominates over another. To illustrate, in our case, given the data obtained from the first cluster, we have 571 customers with a good credit record and 193 customers with a bad one. As can be readily observed, customers with good credit records dominate over customers with bad records; that is basically what we refer to as a class imbalance.

There are numerous ways to handle this issue: up-sampling, down-sampling, the synthetic minority oversampling technique (SMOTE), and the edited nearest neighbor (ENN) rule. To take advantage of a hybrid approach, we’ll incorporate a combination of SMOTE and ENN so we can clean the unwanted overlapping observations between classes, which will help us detect the optimal balancing ratio and, in turn, boost the predictive performance (Tuong et al. 2018). Converting imbalanced data into balanced data will be our first step in predicting the probability of default, but please note that we will merely apply this technique to the data obtained from the first cluster.

Now, we next apply a train-test split. To do that, we need to convert the categorical variable `Risk` into a discrete variable. The category `good` takes a value of 1, and `bad` takes a value of 0. In a train-test split, 80% of the data is devoted to training samples and 20% of is allocated to the test sample:

```
In [31]: from sklearn.model_selection import train_test_split

In [32]: df_scaled['Risk'] = df_scaled['Risk'].replace({'good': 1, 'bad': 0}) 

In [33]: X = df_scaled.drop('Risk', axis=1)
         y = df_scaled.loc[:, ['Risk', 'Clusters']]

In [34]: X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                             test_size=0.2,
                                                             random_state=42)

In [35]: first_cluster_train = X_train[X_train.Clusters == 0].iloc[:, :-1] 
         second_cluster_train = X_train[X_train.Clusters == 1].iloc[:, :-1] 
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO8-1)

Discretization of the variable

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO8-2)

Creating data based on the first cluster and dropping last column from `X_train`

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO8-3)

Creating data based on the second cluster and dropping last column from `X_train`

After these preparations, we are ready to move ahead and run the logistic regression to predict the probability of default. The library that we’ll make use of is called `statsmodels`, and it is allowed to have a summary table. The following result is based on the first cluster data. According to the result, the `age`, `credit amount`, and `job` variables are positively related with the creditworthiness of customer, while a negative association emerges between the `dependent` and `duration` variables. This finding suggests that all the estimated coefficients reveal statistically significant results at a 1% significance level. A general interpretation would be that a slide in duration and a surge in credit amount, age, and job imply a high probability of default:

```
In [36]: import statsmodels.api as sm
         from sklearn.linear_model import LogisticRegression
         from sklearn.metrics import roc_auc_score, roc_curve
         from imblearn.combine import SMOTEENN 
         import warnings
         warnings.filterwarnings('ignore')

In [37]: X_train1 = first_cluster_train
         y_train1 = y_train[y_train.Clusters == 0]['Risk'] 
         smote = SMOTEENN(random_state = 2) 
         X_train1, y_train1 = smote.fit_resample(X_train1, y_train1.ravel()) 
         logit = sm.Logit(y_train1, X_train1) 
         logit_fit1 = logit.fit() 
         print(logit_fit1.summary())
         Optimization terminated successfully.
         Current function value: 0.479511
         Iterations 6
                           Logit Regression Results
==============================================================================
Dep. Variable:                      y   No. Observations:                  370
Model:                          Logit   Df Residuals:                      366
Method:                           MLE   Df Model:                            3
Date:                Wed, 01 Dec 2021   Pseudo R-squ.:                  0.2989
Time:                        20:34:31   Log-Likelihood:                -177.42
converged:                       True   LL-Null:                       -253.08
Covariance Type:            nonrobust   LLR p-value:                 1.372e-32
================================================================================
                    coef   std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------
Age               1.3677     0.164      8.348      0.000       1.047       1.689
Job               0.4393     0.153      2.873      0.004       0.140       0.739
Credit amount     1.3290     0.305      4.358      0.000       0.731       1.927
Duration         -1.2709     0.246     -5.164      0.000      -1.753      -0.789
================================================================================
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO9-1)

Importing `SMOTEENN` to deal with the class imbalance problem

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO9-2)

Creating `y_train` based on cluster 0 and risk level

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO9-3)

Running the `SMOTEENN` method with a random state of 2

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO9-4)

Turning the imbalanced data into balanced data

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO9-5)

Configuring the logistic regression model

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO9-6)

Running the logistic regression model

In what follows, prediction analysis is conducted by creating different datasets based on clusters. For the sake of testing, the following analysis is done with test data, and results in [Figure 6-9](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#roc\_auc\_curve1\_first):

```
In [38]: first_cluster_test = X_test[X_test.Clusters == 0].iloc[:, :-1] 
         second_cluster_test = X_test[X_test.Clusters == 1].iloc[:, :-1] 

In [39]: X_test1 = first_cluster_test
         y_test1 = y_test[y_test.Clusters == 0]['Risk']
         pred_prob1 = logit_fit1.predict(X_test1) 

In [40]: false_pos, true_pos, _ = roc_curve(y_test1.values,  pred_prob1) 
         auc = roc_auc_score(y_test1, pred_prob1) 
         plt.plot(false_pos,true_pos, label="AUC for cluster 1={:.4f} "
                  .format(auc))
         plt.plot([0, 1], [0, 1], linestyle = '--', label='45 degree line')
         plt.legend(loc='best')
         plt.title('ROC-AUC Curve 1')
         plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO10-1)

Creating first test data based on cluster 0

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO10-2)

Creating second test data based on cluster 1

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO10-3)

Running prediction using `X_test1`

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO10-4)

Obtaining false and true positives using `roc_curve` function

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO10-5)

Compute the `roc-auc` score

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0609.png" alt="roc_auc1" height="421" width="600"><figcaption></figcaption></figure>

**Figure 6-9. ROC-AUC curve of the first cluster**

The ROC-AUC curve is a convenient tool in the presence of imbalanced data. The ROC-AUC curve in [Figure 6-9](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#roc\_auc\_curve1\_first) suggests that the performance of the model is not very good, because it moves just above the 45-degree line. Generally speaking, given the test results, a good ROC-AUC curve should be close to 1, implying that there is a close-to-perfect separation.

Moving on to the second set of training samples obtained from the second cluster, the signs of the estimated coefficients of `job`, `duration`, and `age` are positive, suggesting that customers with `job` type of `1` and having larger duration tend to default, and the `credit amount` variable shows a negative relation with dependent variable. However, all the estimated coefficients are statistically insignificant at 95% confidence interval; therefore, it makes no sense to further interpret the findings.

Similar to what we did with the first set of test data, we create a second set of test data to run the prediction to draw the ROC-AUC curve, resulting in [Figure 6-10](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#roc\_auc\_curve1\_second):

```
In [41]: X_train2 = second_cluster_train
         y_train2 = y_train[y_train.Clusters == 1]['Risk']
         logit = sm.Logit(y_train2, X_train2)
         logit_fit2 = logit.fit()
         print(logit_fit2.summary())
         Optimization terminated successfully.
         Current function value: 0.688152
         Iterations 4
                           Logit Regression Results
==============================================================================
Dep. Variable:                   Risk   No. Observations:                  199
Model:                          Logit   Df Residuals:                      195
Method:                           MLE   Df Model:                            3
Date:                Wed, 01 Dec 2021   Pseudo R-squ.:              -0.0008478
Time:                        20:34:33   Log-Likelihood:                -136.94
converged:                       True   LL-Null:                       -136.83
Covariance Type:            nonrobust   LLR p-value:                     1.000
================================================================================
                    coef   std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------
Age               0.0281     0.146      0.192      0.848      -0.259       0.315
Job               0.1536     0.151      1.020      0.308      -0.142       0.449
Credit amount    -0.1090     0.115     -0.945      0.345      -0.335       0.117
Duration          0.1046     0.126      0.833      0.405      -0.142       0.351
================================================================================


In [42]: X_test2 = second_cluster_test
         y_test2 = y_test[y_test.Clusters == 1]['Risk']
         pred_prob2 = logit_fit2.predict(X_test2)

In [43]: false_pos, true_pos, _ = roc_curve(y_test2.values,  pred_prob2)
         auc = roc_auc_score(y_test2, pred_prob2)
         plt.plot(false_pos,true_pos,label="AUC for cluster 2={:.4f} "
                  .format(auc))
         plt.plot([0, 1], [0, 1], linestyle = '--', label='45 degree line')
         plt.legend(loc='best')
         plt.title('ROC-AUC Curve 2')
         plt.show()
```

Given the test data, the result shown in [Figure 6-10](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#roc\_auc\_curve1\_second) is worse than the previous application, as can be confirmed by the AUC score of 0.4064. Considering this data, we are far from saying that logistic regression is doing a good job of modeling probability of default using the German credit risk dataset.

We will now use different models to see how good the logistic regression is in modeling this type of problem relative to other methods. Thus, in the following part, we will take a look at Bayesian estimation with maximum a posteriori (MAP) probability and Markov Chain Monte Carlo (MCMC) approaches. We will then explore those approaches using a few well-known ML models—SVM, random forest, and neural networks using `MLPRegressor`—and we will test the deep learning model with TensorFlow. This application will show us which model works better in modeling the probability of default.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0610.png" alt="roc_auc2" height="421" width="600"><figcaption></figcaption></figure>

**Figure 6-10. ROC-AUC curve of the second cluster**

### Probability of Default Estimation with the Bayesian Model

In this part, we’ll use the `PYMC3` package, which is a Python package for Bayesian estimation, to predict the probability of default. However, there are several approaches for running Bayesian analysis using `PYMC3`, and for the first application, we’ll use the MAP distribution discussed in [Chapter 4](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch04.html#chapter\_4). As a quick reminder, given the representative posterior distribution, MAP becomes an efficient model in this case. Moreover, we select the Bayesian model with a deterministic variable (_p_) that is entirely determined by its parents—that is, `age`, `job`, `credit amount`, and `duration`.

Let’s compare the results obtained from Bayesian analysis with that of logistic regression:

```
In [44]: import pymc3 as pm 
         import arviz as az 

In [45]: with pm.Model() as logistic_model1: 
             beta_age = pm.Normal('coeff_age', mu=0, sd=10) 
             beta_job = pm.Normal('coeff_job', mu=0, sd=10)
             beta_credit = pm.Normal('coeff_credit_amount', mu=0, sd=10)
             beta_dur = pm.Normal('coeff_duration', mu=0, sd=10)
             p = pm.Deterministic('p', pm.math.sigmoid(beta_age *
                                       X_train1['Age'] + beta_job *
                                       X_train1['Job'] + beta_credit *
                                       X_train1['Credit amount'] + beta_dur *
                                       X_train1['Duration'])) 
         with logistic_model1:
             observed = pm.Bernoulli("risk", p, observed=y_train1) 
             map_estimate = pm.find_MAP() 
Out[]: <IPython.core.display.HTML object>



In [46]: param_list = ['coeff_age', 'coeff_job',
                       'coeff_credit_amount', 'coeff_duration']
         params = {}
         for i in param_list:
             params[i] = [np.round(map_estimate[i], 6)] 

         bayesian_params = pd.DataFrame.from_dict(params)
         print('The result of Bayesian estimation:\n {}'.format(bayesian_params))
         The result of Bayesian estimation:
             coeff_age  coeff_job  coeff_credit_amount  coeff_duration
         0   1.367247   0.439128              1.32721       -1.269345
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO11-1)

Importing `PYMC3`

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO11-2)

Importing `arviz` for exploratory analysis of Bayesian models

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO11-3)

Identifying Bayesian model as `logistic_model1`

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO11-4)

Identifying the assumed distributions of the variables as normal with defined `mu` and `sigma` parameters

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO11-5)

Running a deterministic model using the first sample

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO11-6)

Running a Bernoulli distribution to model the dependent variable

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/7.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO11-7)

Fitting the MAP model to data

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/8.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO11-8)

Storing all the results of the estimated coefficients into `param`s with six decimals

The most striking observation is that the differences between estimated coefficients are so small that they can be ignored. The difference occurs in the decimals. Taking the estimated coefficient of the credit amount variable as an example, we have estimated the coefficient to be 1.3290 in logistic regression and 1.3272 in Bayesian analysis.

The story is more or less the same when it comes to comparing the analysis result based on the second cluster data:

```
In [47]: with pm.Model() as logistic_model2:
             beta_age = pm.Normal('coeff_age', mu=0, sd=10)
             beta_job = pm.Normal('coeff_job', mu=0, sd=10)
             beta_credit = pm.Normal('coeff_credit_amount', mu=0, sd=10)
             beta_dur = pm.Normal('coeff_duration', mu=0, sd=10)
             p = pm.Deterministic('p', pm.math.sigmoid(beta_age *
                                       second_cluster_train['Age'] +
                                       beta_job * second_cluster_train['Job'] +
                                       beta_credit *
                                       second_cluster_train['Credit amount'] +
                                       beta_dur *
                                       second_cluster_train['Duration']))
         with logistic_model2:
             observed = pm.Bernoulli("risk", p,
                                     observed=y_train[y_train.Clusters == 1]
                                     ['Risk'])
             map_estimate = pm.find_MAP()
Out[]: <IPython.core.display.HTML object>



In [48]: param_list = [ 'coeff_age', 'coeff_job',
                       'coeff_credit_amount', 'coeff_duration']
         params = {}
         for i in param_list:
             params[i] = [np.round(map_estimate[i], 6)]

         bayesian_params = pd.DataFrame.from_dict(params)
         print('The result of Bayesian estimation:\n {}'.format(bayesian_params))
         The result of Bayesian estimation:
             coeff_age  coeff_job  coeff_credit_amount  coeff_duration
         0   0.028069   0.153599            -0.109003        0.104581
```

The most remarkable difference occurs in the `duration` variable. The estimated coefficients of this variable are 0.1046 and 0.1045 in logistic regression and Bayesian estimation, respectively.

Instead of finding the local maximum, which is sometimes difficult to get, we look for an approximate expectation based on the sampling procedure. This is referred to as MCMC in the Bayesian setting. As we discussed in [Chapter 4](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch04.html#chapter\_4), one of the most well known methods is the Metropolis-Hastings (M-H) algorithm.

The Python code that applies Bayesian estimation based on the M-H algorithm is shown in the following and results in [Figure 6-11](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#MCMC\_risk\_cluster1). Accordingly, we draw 10,000 posterior samples to simulate the posterior distribution for two independent Markov chains. The summary table for the estimated coefficients is provided in the code as well:

```
In [49]: import logging 
         logger = logging.getLogger('pymc3') 
         logger.setLevel(logging.ERROR) 

In [50]: with logistic_model1:
             step = pm.Metropolis() 
             trace = pm.sample(10000, step=step,progressbar = False) 
         az.plot_trace(trace) 
         plt.show()
In [51]: with logistic_model1:
             display(az.summary(trace, round_to=6)[:4]) 
Out[]:                          mean        sd    hdi_3%   hdi_97%  mcse_mean  \
       coeff_age            1.392284  0.164607  1.086472  1.691713   0.003111
       coeff_job            0.448694  0.155060  0.138471  0.719332   0.002925
       coeff_credit_amount  1.345549  0.308100  0.779578  1.928159   0.008017
       coeff_duration      -1.290292  0.252505 -1.753565 -0.802707   0.006823

                             mcse_sd     ess_bulk     ess_tail     r_hat
       coeff_age            0.002200  2787.022099  3536.314548  1.000542
       coeff_job            0.002090  2818.973167  3038.790307  1.001246
       coeff_credit_amount  0.005670  1476.746667  2289.532062  1.001746
       coeff_duration       0.004826  1369.393339  2135.308468  1.001022
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO12-1)

Importing the `logging` package to suppress the warning messages

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO12-2)

Naming the package for logging

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO12-3)

Suppressing errors without raising exceptions

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO12-4)

Initiating the M-H model

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO12-5)

Running the model with 10,000 samples and ignoring the progress bar

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO12-6)

Creating a simple posterior plot using `plot_trace`

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/7.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO12-7)

Printing the first four rows of the summary result

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0611.png" alt="MCMC_risk_cluster1" height="498" width="600"><figcaption></figcaption></figure>

**Figure 6-11. Bayesian estimation with M—H with first cluster**

The result suggests that the predictive performances are supposed be very close to that of logistic regression, as the estimated coefficients of these two models are quite similar.

In [Figure 6-11](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#MCMC\_risk\_cluster1), we see the dashed and solid lines. Given the first cluster data, the plot located on the lefthand side of [Figure 6-11](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#MCMC\_risk\_cluster1) shows the sample values of the related parameters. Though it is not our present focus, we can observe the deterministic variable, _p_, located in the last plot.

In a similar vein, the result of Bayesian estimation with M-H based on the second cluster performs very closely to the logistic regression. However, the results obtained from MAP application are better, which is expected primarily because M-H works with random sampling. It is not, however, the only potential reason for this small deviation that we’ll discuss.

As for the data that we obtained from the second cluster, the result of Bayesian estimation with M-H can be seen in the following code, which also creates the plot shown in [Figure 6-12](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#MCMC\_risk\_cluster2):

```
In [52]: with logistic_model2:
             step = pm.Metropolis()
             trace = pm.sample(10000, step=step,progressbar = False)
         az.plot_trace(trace)
         plt.show()
In [53]: with logistic_model2:
             display(az.summary(trace, round_to=6)[:4])
Out[]:                          mean        sd    hdi_3%   hdi_97%  mcse_mean  \
       coeff_age            0.029953  0.151466 -0.262319  0.309050   0.002855
       coeff_job            0.158140  0.153030 -0.125043  0.435734   0.003513
       coeff_credit_amount -0.108844  0.116542 -0.328353  0.105858   0.003511
       coeff_duration       0.103149  0.128264 -0.142609  0.339575   0.003720

                             mcse_sd     ess_bulk     ess_tail     r_hat
       coeff_age            0.002019  2823.255277  3195.005913  1.000905
       coeff_job            0.002485  1886.026245  2336.516309  1.000594
       coeff_credit_amount  0.002483  1102.228318  1592.047959  1.002032
       coeff_duration       0.002631  1188.042552  1900.179695  1.000988
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0612.png" alt="MCMC_risk_cluster2" height="498" width="600"><figcaption></figcaption></figure>

**Figure 6-12. Bayesian estimation with M—H with second cluster**

Let’s now discuss the limitations of the M-H model, which may shed some light on the discrepancies across the model results. One disadvantage of the M-H algorithm is its sensitivity to step size. Small steps hinder the convergence process. Conversely, big steps may cause a high rejection rate. Besides, M-H may suffer from rare events—as the probability of these events are low, requiring a large sample to obtain a reliable estimation—and that is our focus in this case.

Now, let’s consider what happens if we use SVM to predict probability of default and compare its performance with logistic regression.

### Probability of Default Estimation with Support Vector Machines

SVM is thought to be a parametric model, and it works well with high-dimensional data. The probability of default case in a multivariate setting may provide fertile ground for running SVM. Before proceeding, it would be a good idea to briefly discuss a new approach that we will use to run hyperparameter tuning, namely `HalvingRandomSearchCV`.

`HalvingRandomSearchCV` works with iterative selection so that it uses fewer resources, thereby boosting performance and getting you some time back. `HalvingRandomSearchCV` tries to find the optimal parameters using successive halving to identify candidate parameters. The logic behind this process is as follows:

1. Evaluate all parameter combinations, exploiting a certain number of training samples at first iteration.
2. Use some of the selected parameters in the second iteration with a large number of training samples.
3. Only include the top-scoring candidates in the model until the last iteration.

Using the credit dataset, we predict the probability of default with support vector classification (SVC). Again, we use two different datasets based on the clustering we performed at the very first part of this chapter. The results are provided in the following:

```
In [54]: from sklearn.svm import SVC
         from sklearn.experimental import enable_halving_search_cv 
         from sklearn.model_selection import HalvingRandomSearchCV 
         import time

In [55]: param_svc = {'gamma': [1e-6, 1e-2],
                      'C':[0.001,.09,1,5,10],
                      'kernel':('linear','rbf')}

In [56]: svc = SVC(class_weight='balanced')
         halve_SVC = HalvingRandomSearchCV(svc, param_svc,
                                           scoring = 'roc_auc', n_jobs=-1) 
         halve_SVC.fit(X_train1, y_train1)
         print('Best hyperparameters for first cluster in SVC {} with {}'.
               format(halve_SVC.best_score_, halve_SVC.best_params_))
         Best hyperparameters for first cluster in SVC 0.8273860106443562 with
         {'kernel': 'rbf', 'gamma': 0.01, 'C': 1}

In [57]: y_pred_SVC1 = halve_SVC.predict(X_test1) 
         print('The ROC AUC score of SVC for first cluster is {:.4f}'.
               format(roc_auc_score(y_test1, y_pred_SVC1)))
         The ROC AUC score of SVC for first cluster is 0.5179
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO13-1)

Importing the library to enable successive halving search

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO13-2)

Importing the library to run the halving search

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO13-3)

Running the halving search using parallel processing

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO13-4)

Running a prediction analysis

An important step to take in SVM is hyperparameter tuning. Using a halving search approach, we try to find out the best combination of `kernel`, `gamma`, and `C`. It turns out that the only difference across the two different samples occurs in the `gamma` and `C` hyperparameters. In the first cluster, the optimal `C` score is 1, whereas it is 0.001 in the second one. The higher `C` value indicates that we should choose a smaller margin to make a better classification. As for the `gamma` hyperparameter, both clusters take the same value. Having a lower `gamma` amounts to a larger influence of the support vector on the decision. The optimal kernel is Gaussian, and the `gamma` value is 0.01 for both clusters.

The AUC performance criteria indicates that the predictive performance of SVC is slightly below that of logistic regression. More precisely, AUC of the SVC is 0.5179, and that implies that SVC performs worse than logistic regression for the first cluster.

The second cluster shows that the performance of SVC is even slightly worse than that of the first cluster, and this indicates the SVC does not perform well on this data, as it is not clearly separable data, this implies that SVC does not work well with low-dimensional spaces:

```
In [58]: halve_SVC.fit(X_train2, y_train2)
         print('Best hyperparameters for second cluster in SVC {} with {}'.
               format(halve_SVC.best_score_, halve_SVC.best_params_))
         Best hyperparameters for second cluster in SVC 0.5350758636788049 with
         {'kernel': 'rbf', 'gamma': 0.01, 'C': 0.001}

In [59]: y_pred_SVC2 = halve_SVC.predict(X_test2)
         print('The ROC AUC score of SVC for first cluster is {:.4f}'.
               format(roc_auc_score(y_test2, y_pred_SVC2)))
         The ROC AUC score of SVC for first cluster is 0.5000
```

Well, maybe we’ve had enough of parametric methods—let’s move on to nonparametric methods. Now, the word _nonparametric_ may sound confusing, but it is nothing but a model with an infinite number of parameters, and one that becomes more complex as the number of observations increases. Random forest is one of the most applicable nonparametric models in ML, and we’ll discuss that next.

### Probability of Default Estimation with Random Forest

The random forest classifier is another model we can employ to model the probability of default. Although random forest fails in high-dimensional cases, our data is not that complex, and the beauty of random forest lies in its good predictive performance in the presence of a large number of samples, so it’s plausible to think that the random forest model might outperform the SVC model.

Using halving search approach, we try to find out the best combination of `n_estimators`, `criterion`, `max_features`, `max_depth`, `min_samples_split`. The result suggests that we use `n_estimators` of 300, `min_samples_split` of 10, `max_depth` of 6 with a gini criterion, and `sqrt` `max_features` for the first cluster. As for the second cluster, we have two different optimal hyperparameters as can be seen in the following. Having larger depth in a tree-based model amounts to having a more complex model. With that said, the model proposed for the second cluster is a bit more complex. The `max_features` hyperparameter seems to be different across samples; in the first cluster, the maximum number of features is picked via numberoffeatures.

Given the first cluster data, the AUC score of 0.5387 indicates that random forest has a better performance compared to the other models:

```
In [60]: from sklearn.ensemble import RandomForestClassifier

In [61]: rfc = RandomForestClassifier(random_state=42)

In [62]: param_rfc = {'n_estimators': [100, 300],
             'criterion' :['gini', 'entropy'],
             'max_features': ['auto', 'sqrt', 'log2'],
             'max_depth' : [3, 4, 5, 6],
             'min_samples_split':[5, 10]}

In [63]: halve_RF = HalvingRandomSearchCV(rfc, param_rfc,
                                          scoring = 'roc_auc', n_jobs=-1)
         halve_RF.fit(X_train1, y_train1)
         print('Best hyperparameters for first cluster in RF {} with {}'.
               format(halve_RF.best_score_, halve_RF.best_params_))
         Best hyperparameters for first cluster in RF 0.8890871444218126 with
         {'n_estimators': 300, 'min_samples_split': 10, 'max_features': 'sqrt',
         'max_depth': 6, 'criterion': 'gini'}


In [64]: y_pred_RF1 = halve_RF.predict(X_test1)
         print('The ROC AUC score of RF for first cluster is {:.4f}'.
               format(roc_auc_score(y_test1, y_pred_RF1)))
         The ROC AUC score of RF for first cluster is 0.5387
```

The following code shows a random forest run based on the second cluster:

```
In [65]: halve_RF.fit(X_train2, y_train2)
         print('Best hyperparameters for second cluster in RF {} with {}'.
               format(halve_RF.best_score_, halve_RF.best_params_))
         Best hyperparameters for second cluster in RF 0.6565 with
         {'n_estimators': 100, 'min_samples_split': 5, 'max_features': 'auto',
         'max_depth': 5, 'criterion': 'entropy'}

In [66]: y_pred_RF2 = halve_RF.predict(X_test2)
         print('The ROC AUC score of RF for first cluster is {:.4f}'.
               format(roc_auc_score(y_test2, y_pred_RF2)))
         The ROC AUC score of RF for first cluster is 0.5906
```

Random forest has a much better predictive performance in the second cluster, with an AUC score of 0.5906. Given the predictive performance of random forest, we can conclude that random forest does a better job of fitting the data. This is partly because of the low-dimensional characteristics of the data, as random forest turns out to be a good choice when data has low dimensionality and a large number of observations.

### Probability of Default Estimation with Neural Network

Given the complexity of the probability of default estimation, unveiling the hidden structure of the data is a tough task, but the NN structure does a good job handling this, so it would be an ideal candidate model for such tasks. In setting up the NN model, `GridSearchCV` is used to optimize the number of hidden layers, optimization technique, and learning rate.

In running the model, we first employ the `MLP` library, which allows us to control for many parameters, including hidden layer size, optimization technique (solver), and learning rate. Comparing the optimized hyperparameters of the two clusters indicates that the only difference is in the number of neurons in the hidden layer. Accordingly, we have larger number of neurons in the first hidden layer in cluster one. However, the neuron number is larger in the second hidden layer in the second cluster.

The following code suggests that data based on the first cluster is only a marginal improvement. In other words, the AUC moves to 0.5263, only slightly worse than random forest:

```
In [67]: from sklearn.neural_network import MLPClassifier

In [68]: param_NN = {"hidden_layer_sizes": [(100, 50), (50, 50), (10, 100)],
                     "solver": ["lbfgs", "sgd", "adam"],
                     "learning_rate_init": [0.001, 0.05]}

In [69]: MLP = MLPClassifier(random_state=42)

In [70]: param_halve_NN = HalvingRandomSearchCV(MLP, param_NN,
                                                scoring = 'roc_auc')
         param_halve_NN.fit(X_train1, y_train1)
         print('Best hyperparameters for first cluster in NN are {}'.
               format(param_halve_NN.best_params_))
         Best hyperparameters for first cluster in NN are {'solver': 'lbfgs',
         'learning_rate_init': 0.05, 'hidden_layer_sizes': (100, 50)}

In [71]: y_pred_NN1 = param_halve_NN.predict(X_test1)
         print('The ROC AUC score of NN for first cluster is {:.4f}'.
               format(roc_auc_score(y_test1, y_pred_NN1)))
         The ROC AUC score of NN for first cluster is 0.5263
```

The ROC-AUC score obtained from the second cluster is 0.6155, with two hidden layers endowed with 10 and 100 neurons, respectively. Moreover, the best optimization technique is `adam`, and optimum initial learning rate is 0.05. This is the highest AUC score we’ve obtained, implying that the NN is able to capture the dynamics of the complex and nonlinear data, as shown here:

```
In [72]: param_halve_NN.fit(X_train2, y_train2)
         print('Best hyperparameters for first cluster in NN are {}'.
               format(param_halve_NN.best_params_))
         Best hyperparameters for first cluster in NN are {'solver': 'lbfgs',
         'learning_rate_init': 0.05, 'hidden_layer_sizes': (10, 100)}

In [73]: y_pred_NN2 = param_halve_NN.predict(X_test2)
         print('The ROC AUC score of NN for first cluster is {:.4f}'.
               format(roc_auc_score(y_test2, y_pred_NN2)))
         The ROC AUC score of NN for first cluster is 0.6155
```

### Probability of Default Estimation with Deep Learning

Let’s now take a look at the performance of a deep learning model using TensorFlow via `KerasClassifier`, which enables us to control for the hyperparameters.

The hyperparameters that we tune in this model are batch size, epoch, and dropout rate. As probability of default is a classification problem, the sigmoid activation function appears to be the optimal function to use. Deep learning is based on the structure of NNs, but provides a more complex structure, so it is expected to better capture the dynamics of data in a way that enables us to have better predictive performance.

As we can see in the following code, the predictive performance of the second sample stumbles, however, with an AUC score of 0.5628:

```
In [74]: from tensorflow import keras
         from tensorflow.keras.wrappers.scikit_learn import KerasClassifier 
         from tensorflow.keras.layers import Dense, Dropout
         from sklearn.model_selection import GridSearchCV
         import tensorflow as tf
         import logging 
         tf.get_logger().setLevel(logging.ERROR) 

In [75]: def DL_risk(dropout_rate,verbose=0):
             model = keras.Sequential()
             model.add(Dense(128,kernel_initializer='normal',
                 activation = 'relu', input_dim=4))
             model.add(Dense(64, kernel_initializer='normal',
                 activation = 'relu'))
             model.add(Dense(8,kernel_initializer='normal',
                 activation = 'relu'))
             model.add(Dropout(dropout_rate))
             model.add(Dense(1, activation="sigmoid"))
             model.compile(loss='binary_crossentropy', optimizer='rmsprop')
             return model
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO14-1)

Importing `KerasClassifier` to run grid search

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO14-2)

Importing `logging` to suppress the warning messages

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO14-3)

Naming TensorFlow for logging

Given the optimized hyperparameters of dropout, batch size, and epoch, the deep learning model produces the best performance among the models we have employed so far, with an AUC score of 0.5614. The difference between MLPClassifier and deep learning models used in this chapter is the number of neurons in the hidden layer. Technically, these two models are deep learning models with different structures.

```
In [76]: parameters = {'batch_size':  [10, 50, 100],
                   'epochs':  [50, 100, 150],
                      'dropout_rate':[0.2, 0.4]}
         model = KerasClassifier(build_fn = DL_risk) 
         gs = GridSearchCV(estimator = model,
                                param_grid = parameters,
                                   scoring = 'roc_auc') 

In [77]: gs.fit(X_train1, y_train1, verbose=0)
         print('Best hyperparameters for first cluster in DL are {}'.
               format(gs.best_params_))
         Best hyperparameters for first cluster in DL are {'batch_size': 10,
         'dropout_rate': 0.2, 'epochs': 50}
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO15-1)

Calling a predefined function named `DL_risk` to run with optimized hyperparameters

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO15-2)

Applying the grid search

```
In [78]: model = KerasClassifier(build_fn = DL_risk,                    
                                 dropout_rate = gs.best_params_['dropout_rate'],
                                 verbose = 0,
                                 batch_size = gs.best_params_['batch_size'], 
                                 epochs = gs.best_params_['epochs']) 
         model.fit(X_train1, y_train1)
         DL_predict1 = model.predict(X_test1)                           
         DL_ROC_AUC = roc_auc_score(y_test1, pd.DataFrame(DL_predict1.flatten()))
         print('DL_ROC_AUC is {:.4f}'.format(DL_ROC_AUC))
         DL_ROC_AUC is 0.5628
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO16-1)

Running deep learning algorithm with optimum hyperparameter of dropout rate

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO16-2)

Running deep learning algorithm with optimum hyperparameter of batch size

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO16-3)

Running deep learning algorithm with optimum hyperparameter of epoch number

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#co\_credit\_risk\_estimation\_CO16-4)

Computing the ROC-AUC score after flattening the prediction

```
In [79]: gs.fit(X_train2.values, y_train2.values, verbose=0)
         print('Best parameters for second cluster in DL are {}'.
               format(gs.best_params_))
         Best parameters for second cluster in DL are {'batch_size': 10,
         'dropout_rate': 0.2, 'epochs': 150}

In [80]: model = KerasClassifier(build_fn = DL_risk,
                                 dropout_rate= gs.best_params_['dropout_rate'],
                                 verbose = 0,
                                 batch_size = gs.best_params_['batch_size'],
                                 epochs = gs.best_params_['epochs'])
         model.fit(X_train2, y_train2)
         DL_predict2 =  model.predict(X_test2)
         DL_ROC_AUC = roc_auc_score(y_test2, DL_predict2.flatten())
         print('DL_ROC_AUC is {:.4f}'.format(DL_ROC_AUC))
         DL_ROC_AUC is 0.5614
```

This finding confirms that DL models have become increasingly popular in financial modeling. In the industry, however, due to the opaque nature of network structure, this method is suggested for use in conjunction with traditional models.

## Conclusion

Credit risk analysis has a long tradition but is also still a challenging task to accomplish. This chapter attempted to present a brand new ML-based approach to tackling this problem and to getting better predictive performance. In the first part of the chapter, the main concepts related to credit risk were provided. Then, we applied a well-known parametric model, logistic regression, to German credit risk data. The performance of logistic regression was then compared with Bayesian estimation based on MAP and M-H. Finally, core machine learning models—namely SVC, random forest, and NNs with deep learning—were employed, and the performance of all models was compared.

In the next chapter, a neglected dimension risk will be introduced: liquidity risk. The appreciation of liquidity risk has grown considerably since the 2007–2008 financial crisis and has turned out to be an important part of risk management.

## References

Articles cited in this chapter:

* Basel Committee on Banking Supervision, and Bank for International Settlements. 2000. “Principles for the Management of Credit Risk.” Bank for International Settlements.
* Le, Tuong, Mi Young Lee, Jun Ryeol Park, and Sung Wook Baik. 2018. “Oversampling Techniques for Bankruptcy Prediction: Novel Features from a Transaction Dataset.” _Symmetry_ 10 (4): 79.
* Tibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. “Estimating the Number of Clusters in a Data Set via the Gap Statistic.” _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_ 63 (2): 411-423.

Books and PhD theses cited in this chapter:

* Rokach, Lior, and Oded Maimon. 2005. “Clustering methods.” In _Data Mining and Knowledge Discovery Handbook_, 321-352. Boston: Springer.
* Wehrspohn, Uwe. 2002. “Credit Risk Evaluation: Modeling-Analysis-Management.” PhD dissertation. Harvard.

[1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#idm45737223570832-marker) It is useful to run logistic regression to initialize results for priors in Bayesian estimation.
