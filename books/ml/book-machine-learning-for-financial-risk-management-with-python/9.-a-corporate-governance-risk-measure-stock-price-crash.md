# 9. A Corporate Governance Risk Measure: Stock Price Crash

## Chapter 9. A Corporate Governance Risk Measure: Stock Price Crash

> Understanding corporate governance not only enlightens the discussion of perhaps marginal improvements in rich economies, but can also stimulate major institutional changes in places where they need to be made.
>
> Shleifer and Vishny (1997)

Do you think that the quality of corporate governance can be assessed using a risk measure? According to recent studies, the answer is yes. The link between corporate governance and risk measure has been established via _stock price crash risk_, which is referred to as the risk of a large negative individual stock return. This association triggered a lot of research in this field.

The importance of detecting the determinants of stock price crash lies in identifying the root causes of low (or high) quality corporate governance. Identifying these root causes help a company to concentrate on problematic managerial areas, enhancing the functioning performance of the company as well as improving its reputation. This, in turn, lowers the risk of stock price crash and increases the company’s total revenue.

Stock price crash provides a signal for investors and risk managers about the weakness and strength of a company’s _corporate governance_. Corporate governance is defined as the way corporations are directed and controlled, as well as the ways they are or are not “promoting corporate fairness, transparency, and accountability” (Wolfensohn 1999).

Following this definition, corporate governance has three pillars:

Fairness

This principle refers to equal treatment of all shareholders.

Transparency

Informing shareholders about any company events in a timely manner is called _transparency_. This implies the opposite of opaqueness, or a company’s unwillingness to disclose information to shareholders.

Accountability

This is related to setting a well-established code of conduct by which a fair, balanced, and understandable assessment of a company’s position is presented to shareholders.

Accountability is an instrument for controlling _agency cost_, which is a cost arising from competing interests of shareholders and management. Agency cost is another source of asymmetric information because managers and shareholders do not have the same amount of information. Conflict arises when managers’ and shareholders’ interests diverge. More precisely, managers are, on the one hand, willing to maximize their own power and wealth. On the other hand, shareholders are looking for a way to maximize shareholder values. These two goals may conflict, and because of the informational superiority of managers, some company policies may be intended to increase the power and wealth of the managers at the expense of shareholder interests.

Therefore, stock price crash may be a warning sign about the quality of corporate governance. For instance, in the presence of information asymmetry, agency theory suggests that outside stakeholders let managers generate more opaque financial reports to withhold bad news (Hutton, Marcus, and Tehranian 2009). The more recent explanation of this phenomenon is known as _discretionary-disclosure_ theory (Bae, Lim, and Wei 2006). According to this theory, firms prefer to announce good news immediately, but they stockpile negative information. When the accumulated negative information reaches a tipping point, it will cause a large decline. Since concealing bad news about a firm prevents taking timely corrective actions, once the accumulated bad news is released to the market, investors will revise their future expectations, and there will inevitably be a sudden decline in prices, which is called _crash risk_ (Hutton, Marcus, and Tehranian 2009 and Kim 2011).

Moreover, opaque financial reporting, which is related to the accountability principle, creates an environment in which managers are unwilling to disclose bad news. This results in an unfair presentation of the financial position of a company and, in turn, increases the likelihood of future stock price crash (Bleck and Liu (2007), Myers (2006), and Kim and Zhang (2013)).

Thus, the association between corporate governance and stock price crash is evident in various ways. In this chapter, we first visit stock price measures and then see how we can apply these measures to detect crashes.

We’ll first obtain some data from the Center for Research in Security Prices (CRSP) and Compustat database and then identify the main determinants of stock price crash.

**NOTE**

CRSP has provided data for academic research and to support classroom instructions since 1960. CRSP has high-quality data in finance, economics, and related fields. For more information see the [CRSP website](https://oreil.ly/oO3X8).

Similarly, the Compustat database has provided financial, economic, and market information about global companies since 1962. It is a product of S\&P Global Market Intelligence. For more information see [Compustat Brochure](https://oreil.ly/E4Hpj).

## Stock Price Crash Measures

The literature about stock price crash has been growing and different crash measures are employed by different researchers. Before introducing ML-based crash measures, it is worth comparing the pros and cons of these differing approaches.

The main crash measures used in the literature are:

* Down-to-up volatility (DUVOL)
* Negative coefficient of skewness (NCSKEW)
* CRASH

DUVOL is a very common crash-measure method based on the standard deviation of “down” and “up” weekly firm-specific returns. A down week is a week in which the firm-specific weekly stock return is below the mean weekly return over the fiscal year. Conversely, an up week is a week in which the firm-specific weekly stock return is above the mean weekly return over a fiscal year. Described mathematically:

DUVOL=log((��-1)∑down���2(��-1)∑up���2)

where _n_ is the number of trading weeks on stock _i_ in year _t_, �� is the number of up weeks, and �� is the number of down weeks. In a year, weeks with firm-specific returns below the annual mean are called down weeks, while the weeks with firm-specific returns above the annual mean are up weeks.

NCSKEW is calculated by taking the negative of the third moment of daily returns and dividing it by (the sample analog to) the standard deviation of daily returns raised to the third power (Chen, Hong, and Stein 2001):

NCSKEW=-(�(�-1)3/2∑���3)((�-1)(�-2)(∑���2)3/2)

The higher the values of the NCSKEW and DUVOL measures, the higher the risk of a crash.

The CRASH measure, on the other hand, is calculated based on the distance from the firm-specific weekly returns. That is, CRASH takes the value of 1 if the return is less than 3.09 (or sometimes 3.2) standard deviations below the mean, and a 0 otherwise.

## Minimum Covariance Determinant

It comes as no surprise that ML-based algorithms attract a great deal of attention, as they attack the weaknesses of the rule-based models and show good predictive performance. We’ll therefore try to estimate stock price crash risk using an ML-based method called _minimum covariance determinant_ (MCD). MCD is a method proposed to detect anomalies in a distribution with elliptically symmetric and unimodal datasets. Anomalies in stock returns are detected using the MCD estimator, and this becomes the dependent variable in the logistic panel regression by which we explore the root causes of crash risk.

The MCD estimator provides a robust and consistent method in detecting outliers. This is important because outliers may have a huge effect on the multivariate analysis. As summarized by Finch (2012), the presence of outliers in multivariate analysis can distort the correlation coefficient causing biased estimates.

The algorithm of MCD can be given as follows:

1. Detect initial robust clustering based on the data.
2. Calculate mean vector �� and positive definite[1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#idm45737208073680) covariance matrix ∑� for each cluster.
3. Compute MCD, for each observation in the cluster.
4. Assign a new observation with smaller MCD to the cluster.
5. Select a half sample, _h_, based on smallest MCD and compute �� and ∑� from _h_.
6. Repeat steps 2 through 5 until there is no room for change in _h_.
7. Detect outlier if ��=��,0.952 is less than �2.

The strength of the MCD comes in the form of its explainability, adjustability, low computational time requirement, and robustness:

Explainability

Explainability is the extent to which the algorithm behind a model can be explained. MCD assumes the data to be elliptically distributed and the outliers are computed by the Mahalanobis distance metric.

**NOTE**

Mahalanobis distance is a distance metric used in multivariate settings. Of the distance measures, Mahalanobis stands out with its ability to detect outliers, even though it is a computationally expensive method, as it considers the inter-correlation structure of the variables.

Mathematically, Mahalanobis distance is formulated as follows:

��(�,�)=(�-�)�∑-1(�-�).

where �=𝔼(�), ∑ is the covariance matrix, and _X_ is a vector.

Adjustability

Adjustability stresses the importance of having a data-dependent model that is allowed to calibrate itself on a consistent basis so that structural change can be captured.

Low computational time

This refers to fast calculation of covariance matrix and avoids using an entire sample. Instead, MCD uses a half sample in which no outliers are included so that outlying observations do not skew MCD location or shape.

Robustness

Using a half sample in MCD also ensures robustness, because it implies that the model is consistent under contamination (Hubert et al. 2018).

We’ll now apply the MCD method to detect outliers in stock returns, and the result is employed as the dependent variable. Accordingly, if there is a crash in the stock price, the dependent variable takes the value of 1 and 0 otherwise.

From the empirical standpoint, there is a built-in library to run this algorithm in Python that is _Elliptic Envelope_ and we will make use of it.

## Application of Minimum Covariance Determinant

Thus far, we have discussed the theoretical background of stock price crash detection. From this point on, we will focus on the empirical part and see how we can incorporate theory into practice. While doing this, we won’t limit our attention to stock price crash detection. After proposing an ML-based stock price crash detection, we will delve into the root causes of the crashes. To do that, given the large body of literature, we will employ a number of variables to observe how and to what extent they affect the occurrence of stock price crash. Thus, the aim of this chapter is two-fold: detecting stock price crash and identifying the root causes of the crash. Please keep in mind that there are many different and competing ideas about the detection of stock price crash and the variables that affect this crash.

In this analysis, we’ll use the stock and balance sheet information of the following companies:

| Apple                | AT\&T                        | Banco Bradesco          | Bank of America Corp. |
| -------------------- | ---------------------------- | ----------------------- | --------------------- |
| CISCO                | Coca-Cola                    | Comcast                 | DuPont de Nemours     |
| Exxon Mobil Corp.    | Facebook                     | Ford Motor              | General Electric      |
| Intel Corp.          | Johnson & Johnson            | J.P. Morgan             | Merck & Co., Inc.     |
| Microsoft            | Motus GI Holdings Inc.       | Oracle Corp.            | Pfizer Inc.           |
| Procter & Gamble Co. | Sherritt International Corp. | Sirius XM Holdings Inc. | Trisura Group Ltd.    |
| UBS                  | Verizon                      | Walmart                 | Wells Fargo & Co.     |

To move forward, we need to calculate weekly firm-specific returns, but our data is daily, so let’s jump in and do the necessary coding:

```
In [1]: import pandas as pd
        import matplotlib.pyplot as plt
        import numpy as np
        import seaborn as sns; sns.set()
        pd.set_option('use_inf_as_na', True)
        import warnings
        warnings.filterwarnings('ignore')

In [2]: crash_data = pd.read_csv('crash_data.csv')

In [3]: crash_data.head()
Out[3]: Unnamed: 0       RET      date TICKER    vwretx  BIDLO   ASKHI    PRC  \
        0    27882462  0.041833  20100104    BAC  0.017045  15.12  15.750  15.69

        1    27882463  0.032505  20100105    BAC  0.003362  15.70  16.210  16.20

        2    27882464  0.011728  20100106    BAC  0.001769  16.03  16.540  16.39

        3    27882465  0.032947  20100107    BAC  0.002821  16.51  17.185  16.93

        4    27882466 -0.008860  20100108    BAC  0.004161  16.63  17.100  16.78

                   VOL

        0  180845100.0

        1  209521200.0

        2  205257900.0

        3  320868400.0

        4  220104600.0


In [4]: crash_data.date = pd.to_datetime(crash_data.date, format='%Y%m%d') 
        crash_data = crash_data.set_index('date') 
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO1-1)

Converting date column into proper date format

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO1-2)

Setting date as index

As a reminder, the data we’re using has been collected from CRSP and Compustat. [Table 9-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#att\_exp\_2) provides a brief explanation of the data.

| Attribute | Explanation                |
| --------- | -------------------------- |
| `RET`     | The stock return           |
| `vwretx`  | The volume weighted return |
| `BIDLO`   | The lowest bid price       |
| `ASKHI`   | The highest ask price      |
| `PRC`     | The trading price          |
| `VOL`     | The trading volume         |

Given this data, let’s calculate the weekly mean return and generate [Figure 9-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#ret\_hist) with the first four stocks. To do this calculation, we’ll also calculate the weekly mean of the other variables, as we’ll use them along the way:

```
In [5]: crash_dataw = crash_data.groupby('TICKER').resample('W').\
                      agg({'RET':'mean', 'vwretx':'mean', 'VOL':'mean',
                           'BIDLO':'mean', 'ASKHI':'mean', 'PRC':'mean'}) 

In [6]: crash_dataw = crash_dataw.reset_index()
        crash_dataw.dropna(inplace=True)
        stocks = crash_dataw.TICKER.unique()

In [7]: plt.figure(figsize=(12, 8))
        k = 1

        for i in stocks[: 4]: 
            plt.subplot(2, 2, k)
            plt.hist(crash_dataw[crash_dataw.TICKER == i]['RET'])
            plt.title('Histogram of '+i)
            k+=1
        plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO2-1)

Computing weekly returns per stocks along with other variables

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO2-2)

Picking first four stocks

[Figure 9-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#ret\_hist) shows the histograms of our four first stocks, namely, Apple, Bank of America, Banco Bradesco, and Comcast. As expected, the distributions seem to be normal, but we now have returns that, generally speaking, show leptokurtic distribution.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0901.png" alt="ret_hist" height="403" width="600"><figcaption></figcaption></figure>

**Figure 9-1. Return histogram**

In what follows, we calculate return in a way to exclude market impact, which is known as finding the _firm-specific return_. To calculate the firm-specific weekly return, we run linear regression based on the following equation:

��,�=�0+�1��,�-2+�2��,�-1+�3��,�+�4��,�+1+�5��,�+2+��,�

where ��,� is the return of firm _j_ in week _t_, and ��, _t_ is the return on the CRSP value-weighted market return in week _t_. Scaling the residuals of this regression by 1 + logarithm provides us the firm-specific return.

According to the expanded market model, the firm specific weekly returns can be calculated as ��,�=��(1+��,�) (Kim, Li, and Zhang 2011):

```
In [8]: import statsmodels.api as sm
        residuals = []

        for i in stocks:
            Y = crash_dataw.loc[crash_dataw['TICKER'] == i]['RET'].values
            X = crash_dataw.loc[crash_dataw['TICKER'] == i]['vwretx'].values
            X = sm.add_constant(X)
            ols = sm.OLS(Y[2:-2], X[2:-2] + X[1:-3] + X[0:-4] + \
                         X[3:-1] + X[4:]).fit() 
            residuals.append(ols.resid)

In [9]: residuals = list(map(lambda x: np.log(1 + x), residuals)) 

In [10]: crash_data_sliced = pd.DataFrame([])
         for i in stocks:
             crash_data_sliced = crash_data_sliced.\
                                 append(crash_dataw.loc[crash_dataw.TICKER == i]
                                        [2:-2]) 
         crash_data_sliced.head()
Out[10]: TICKER       date       RET    vwretx          VOL       BIDLO
          ASKHI  \
         2   AAPL 2010-01-24 -0.009510 -0.009480  25930885.00  205.277505
          212.888450
         3   AAPL 2010-01-31 -0.005426 -0.003738  52020594.00  198.250202
          207.338002
         4   AAPL 2010-02-07  0.003722 -0.001463  26953208.40  192.304004
          197.378002
         5   AAPL 2010-02-14  0.005031  0.002970  19731018.60  194.513998
          198.674002
         6   AAPL 2010-02-21  0.001640  0.007700  16618997.25  201.102500
          203.772500

                   PRC

         2  208.146752

         3  201.650398

         4  195.466002

         5  196.895200

         6  202.636995
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO3-1)

Running linear regression by the predefined equation

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO3-2)

Computing the 1 + logarithm of the residuals

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO3-3)

Dropping the first and last two observations to align with the previous data

After all these preparations, we are ready to run the Elliptic Envelope to detect the crash.

Only two parameters are identified: `support_fraction` and `contamination`. The former parameter is used to control for the proportion of points to be included in the support of the raw MCD estimate, and the latter is used to identify the proportion of outliers in the dataset:

```
In [11]: from sklearn.covariance import EllipticEnvelope
         envelope = EllipticEnvelope(contamination=0.02, support_fraction=1) 
         ee_predictions = {}

         for i, j in zip(range(len(stocks)), stocks):
             envelope.fit(np.array(residuals[i]).reshape(-1, 1))
             ee_predictions[j] = envelope.predict(np.array(residuals[i])
                                                  .reshape(-1, 1)) 

In [12]: transform = []

         for i in stocks:
             for j in range(len(ee_predictions[i])):
                 transform.append(np.where(ee_predictions[i][j] == 1, 0, -1)) 

In [13]: crash_data_sliced = crash_data_sliced.reset_index()
         crash_data_sliced['residuals'] = np.concatenate(residuals) 
         crash_data_sliced['neg_outliers'] = np.where((np.array(transform)) \
                                                        == -1, 1, 0) 
         crash_data_sliced.loc[(crash_data_sliced.neg_outliers == 1) &
                               (crash_data_sliced.residuals > 0),
                               'neg_outliers'] = 0 
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO4-1)

Running Elliptic Envelope with `contamination` and `support_fraction` as 2 and 1, respectively

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO4-2)

Predicting the crashes

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO4-3)

Transforming crashes into desired form

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO4-4)

Obtaining a one-dimensional `numpy` array to create a new column in the dataframe

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO4-5)

Performing the final transformation for crashes, named `neg_outliers`

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO4-6)

Getting rid of crashes on the positive side (i.e., the right tail) of the distribution

The following code block is provided to visualize if the algorithm properly captures the crashes. In this analysis, General Motors, Intel, Johnson & Johnson, and J.P. Morgan are used. As suggested by the resultant [Figure 9-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#anomaly\_hist), the algorithm works fine and identifies the crashes on the negative side of the distribution (shown as black bars):

```
In [14]: plt.figure(figsize=(12, 8))
         k = 1

         for i in stocks[8:12]:
             plt.subplot(2, 2, k)
             crash_data_sliced['residuals'][crash_data_sliced.TICKER == i]\
             .hist(label='normal', bins=30, color='gray')
             outliers = crash_data_sliced['residuals']
             [(crash_data_sliced.TICKER == i) &
              (crash_data_sliced.neg_outliers > 0)]
             outliers.hist(color='black', label='anomaly')
             plt.title(i)
             plt.legend()
             k += 1
         plt.show()
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0902.png" alt="anomaly_hist" height="405" width="600"><figcaption></figcaption></figure>

**Figure 9-2. Anomaly histogram**

From this point on, we will use two different datasets, as balance sheet information is required in this analysis. So we will convert our weekly data into an annual one so that this data will be merged with the balance sheet information (which includes annual information). In addition, the annual mean and standard deviation of returns are necessary to calculate crash risk, another stock price crash risk:

```
In [15]: crash_data_sliced = crash_data_sliced.set_index('date')
         crash_data_sliced.index = pd.to_datetime(crash_data_sliced.index)

In [16]: std = crash_data.groupby('TICKER')['RET'].resample('W').std()\
               .reset_index()
         crash_dataw['std'] = pd.DataFrame(std['RET']) 

In [17]: yearly_data = crash_data_sliced.groupby('TICKER')['residuals']\
                       .resample('Y').agg({'residuals':{'mean', 'std'}})\
                       .reset_index()
         yearly_data.columns = ['TICKER', 'date', 'mean', 'std']
         yearly_data.head()
Out[17]:   TICKER       date      mean       std
         0   AAPL 2010-12-31  0.000686  0.008291
         1   AAPL 2011-12-31  0.000431  0.009088
         2   AAPL 2012-12-31 -0.000079  0.008056
         3   AAPL 2013-12-31 -0.001019  0.009096
         4   AAPL 2014-12-31  0.000468  0.006174

In [18]: merge_crash = pd.merge(crash_data_sliced.reset_index(), yearly_data,
                                how='outer', on=['TICKER', 'date']) 

In [19]: merge_crash[['annual_mean', 'annual_std']] = merge_crash\
                                                      .sort_values(by=['TICKER',
                                                                       'date'])\
                                                      .iloc[:, -2:]\
                                                      .fillna(method='bfill') 
         merge_crash['residuals'] = merge_crash.sort_values(by=['TICKER',
                                                                'date'])\
                                                               ['residuals']\
                                                      .fillna(method='ffill') 
         merge_crash = merge_crash.drop(merge_crash.iloc[: ,-4:-2], axis=1) 
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO5-1)

Resampling data to compute mean and standard deviation of returns

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO5-2)

Merging `yearly_data` and `crash_data_sliced` based on `Ticker` and `date`

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO5-3)

Backward filling for annual data

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO5-5)

Dropping columns to prevent confusion

In the literature, one of the most widely used stock price crash measures is crash risk because it has a discrete type, making it a convenient tool for comparison purposes.

So let’s now generate crash risk in Python. We’ll use the `merge_crash` data we generated in the previous snippet. Given the formula for crash risk, we check if the weekly return is less than 3.09 standard deviations below the mean. If so, it is labeled as 1, indicating a crash, otherwise it’s labeled as 0. It will turn out that we have 44 crashes out of 13,502 observations.

In the final block (`In [22]`), the crash risk measure is annualized so that we are able to include it in our final data:

```
In [20]: crash_risk_out = []

         for j in stocks:
             for k in range(len(merge_crash[merge_crash.TICKER == j])):
                 if merge_crash[merge_crash.TICKER == j]['residuals'].iloc[k] < \
                 merge_crash[merge_crash.TICKER == j]['annual_mean'].iloc[k] - \
                 3.09 * \
                 merge_crash[merge_crash.TICKER == j]['annual_std'].iloc[k]:
                     crash_risk_out.append(1)
                 else:
                     crash_risk_out.append(0)

In [21]: merge_crash['crash_risk'] = crash_risk_out
         merge_crash['crash_risk'].value_counts()
Out[21]: 0    13476
         1       44
         Name: crash_risk, dtype: int64

In [22]: merge_crash = merge_crash.set_index('date')
         merge_crash_annual = merge_crash.groupby('TICKER')\
                              .resample('1Y')['crash_risk'].sum().reset_index()
```

If you are using multiple stocks as we are here, it is not an easy task to compute DUVOL and NCSKEW. The first step is to reckon the down and up weeks. As a reminder, a down (or up) week is computed as the week in which weekly return is less than (or greater than) the annual return. In the last part of the following code block, we compute the necessary ingredients, such as square residuals, for down weeks that we’ll need to calculate the DUVOL and NCSKEW crash measures:

```
In [23]: down = []

         for j in range(len(merge_crash)):
             if merge_crash['residuals'].iloc[j] < \
                merge_crash['annual_mean'].iloc[j]:
                 down.append(1) 
             else:
                 down.append(0) 

In [24]: merge_crash = merge_crash.reset_index()
         merge_crash['down'] = pd.DataFrame(down)
         merge_crash['up'] = 1 - merge_crash['down']
         down_residuals = merge_crash[merge_crash.down == 1]\
                          [['residuals', 'TICKER', 'date']] 
         up_residuals = merge_crash[merge_crash.up == 1]\
                        [['residuals', 'TICKER', 'date']] 

In [25]: down_residuals['residuals_down_sq'] = down_residuals['residuals'] ** 2
         down_residuals['residuals_down_cubic'] = down_residuals['residuals'] **3
         up_residuals['residuals_up_sq'] = up_residuals['residuals'] ** 2
         up_residuals['residuals_up_cubic'] = up_residuals['residuals'] ** 3
         down_residuals['down_residuals'] = down_residuals['residuals']
         up_residuals['up_residuals'] = up_residuals['residuals']
         del down_residuals['residuals']
         del up_residuals['residuals']

In [26]: merge_crash['residuals_sq'] = merge_crash['residuals'] ** 2
         merge_crash['residuals_cubic'] = merge_crash['residuals'] ** 3
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO6-1)

If the conditional returns true, add 1 to the `down` list

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO6-2)

If the conditional returns true, add 0 to the `down` list

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO6-3)

Creating a new variable named `down_residuals` including down weeks

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO6-4)

Creating a new variable named `up_residuals` including up weeks

The next step is to merge `down_residuals` and `up_residuals` with `merge_crash`. Then, we specify and annualize all the variables we want to check to identify which variables matter most in explaining stock price crash:

```
In [27]: merge_crash_all = merge_crash.merge(down_residuals,
                                             on=['TICKER', 'date'],
                                             how='outer')
         merge_crash_all = merge_crash_all.merge(up_residuals,
                                                 on=['TICKER', 'date'],
                                                 how='outer')

In [28]: cols = ['BIDLO', 'ASKHI', 'residuals',
                 'annual_std', 'residuals_sq', 'residuals_cubic',
                 'down', 'up', 'residuals_up_sq', 'residuals_down_sq',
                 'neg_outliers']
         merge_crash_all = merge_crash_all.set_index('date')
         merge_grouped = merge_crash_all.groupby('TICKER')[cols]\
                         .resample('1Y').sum().reset_index() 
         merge_grouped['neg_outliers'] = np.where(merge_grouped.neg_outliers >=
                                                  1, 1, 0) 
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO7-1)

Specifying and annualizing the variables of interest

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO7-2)

Converting greater than 1 `negative outliers` observations, if any

There are two important questions remaining: how many down and up weeks do we have, and what is their sum? These questions are important because the number of up and down weeks refers to �� and �� in the DUVOL formula, respectively. So let’s do that calculation:

```
In [29]: merge_grouped = merge_grouped.set_index('date')
         merge_all = merge_grouped.groupby('TICKER')\
                     .resample('1Y').agg({'down':['sum', 'count'],
                                          'up':['sum', 'count']})\
                     .reset_index() 
         merge_all.head()

Out[29]:   TICKER       date down        up
                              sum count sum count
         0   AAPL 2010-12-31   27     1  23     1
         1   AAPL 2011-12-31   26     1  27     1
         2   AAPL 2012-12-31   28     1  26     1
         3   AAPL 2013-12-31   24     1  29     1
         4   AAPL 2014-12-31   22     1  31     1

In [30]: merge_grouped['down'] = merge_all['down']['sum'].values
         merge_grouped['up'] = merge_all['up']['sum'].values
         merge_grouped['count'] = merge_grouped['down'] + merge_grouped['up']
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO8-1)

Calculating annualized summation and count of down and up weeks

Finally, we are all set to calculate DUVOL and NCSKEW using all the inputs we’ve derived so far:

```
In [31]: merge_grouped = merge_grouped.reset_index()

In [32]: merge_grouped['duvol'] = np.log(((merge_grouped['up'] - 1) *
                                          merge_grouped['residuals_down_sq']) /
                                         ((merge_grouped['down'] - 1) *
                                          merge_grouped['residuals_up_sq'])) 

In [33]: merge_grouped['duvol'].mean()
Out[33]: -0.023371498758114867

In [34]: merge_grouped['ncskew'] = - (((merge_grouped['count'] *
                                        (merge_grouped['count'] - 1) **
                                        (3 / 2)) *
                                      merge_grouped['residuals_cubic']) /
                                      (((merge_grouped['count'] - 1) *
                                        (merge_grouped['count'] - 2)) *
                                       merge_grouped['residuals_sq'] **
                                       (3 / 2))) 

In [35]: merge_grouped['ncskew'].mean()
Out[35]: -0.031025284134663118


In [36]: merge_grouped['crash_risk'] = merge_crash_annual['crash_risk']
         merge_grouped['crash_risk'] = np.where(merge_grouped.crash_risk >=
                                                1, 1, 0)

In [37]: merge_crash_all_grouped2 = merge_crash_all.groupby('TICKER')\
                                     [['VOL', 'PRC']]\
                                    .resample('1Y').mean().reset_index()
         merge_grouped[['VOL', 'PRC']] = merge_crash_all_grouped2[['VOL', 'PRC']]

         merge_grouped[['ncskew','duvol']].corr()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO9-1)

Calculating DUVOL

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO9-2)

Calculating NCSKEW

DUVOL gives the proportion of the magnitude of the returns below the annual mean to the magnitude of the returns above the annual mean. Consequently, higher DUVOL implies left-skewed distribution or higher crash probability. Given the mean DUVOL value of -0.0233, we can come to the conclusion that stock prices are less likely to crash over the specified period.

NSCKEW, on the other hand, compares the shapes of the tails—that is, in the case of longer left tail compared to the right tail, stock prices tend to crash. As expected, the correlation between NCSKEW and DUVOL is high, confirming that both measures pick up much the same information through different ways.

## Logistic Panel Application

Since we are seeking the variables that can explain the stock price crash risk, this section provides a backbone analysis. Because we have both stocks and time series in the data, panel data analysis is a suitable technique to use.

At least three factors have contributed to the geometric growth of panel data studies (Hsiao 2014):

1. Data availability
2. Greater capacity for modeling the complexity of human behavior than a single cross-section or time series data
3. Its challenging methodology

In a nutshell, panel data analysis combines time series and cross-sectional data and, therefore, has many advantages over time series and cross-sectional analysis. Ullah (1998) summarizes these advantages this way:

> Obvious benefits are a much larger data set with more variability and less collinearity among the variables than is typical of cross-section or time-series data. With additional, more informative data, one can get more reliable estimates and test more sophisticated behavioral models with less restrictive assumptions. Another advantage of panel data sets are their ability to control for individual heterogeneity… In particular, panel data sets are better able to study complex issues of dynamic behavior.

As our data is of discrete type, logistic panel application addresses the need. However, there is a shortage of libraries for panel data analysis, and the situation is even worse when it comes to logistic panel applications. The library we’ll use is the Python Econometrics Models module (`pyeconometrics`), which has a few advanced-level models, including:

* Fixed effects logistic regression (Logit)
* Random effects logistic regression (Logit and Probit)
* Tobit I (linear regression for truncated data)

**NOTE**

A potential endogeneity problem arising from time-invariant omitted variables is one of the concerns that needs to be accounted for. To control for this, we’ll use a fixed effect logistic panel model.

To run a logistic panel application, the `pyeconometrics` module is used, but installation of this library is a bit different. Please visit its [GitHub repo](https://oreil.ly/cxATG) for more information.

**WARNING**

Installing `pyeconometrics` is a bit different than installing some of the libraries and modules we’ve used. To make sure you properly install the library, please visit its [GitHub repo](https://oreil.ly/Ap8NO).

Let’s now introduce the variables we’ll use in this analysis. Having obtained the stock price crash measures, it’s time to discuss which variables matter in estimating stock price crash risk. [Table 9-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#table-9-1) lists the independent variables.

| Variable                                | Explanation                                                                                                                                                                                                  |
| --------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Size (`log_size`)                       | Logarithm of total asset owned by company.                                                                                                                                                                   |
| Receivables (`rect`)                    | Accounts receivable/debtors.                                                                                                                                                                                 |
| Property, plant and equipment (`ppegt`) | Total property, plant, and equipment.                                                                                                                                                                        |
| Average turnover (`dturn`)              | The average monthly turnover ratio in year _t_ minus the average monthly turnover ratio in year _t_ - 1. The turnover ratio is the monthly trading volume divided by the total number of shares outstanding. |
| NCSKEW (`ncskew`)                       | The negative coefficient of skewness of firm-specific weekly returns in a year, which is the negative of the third moment of firm-specific weekly returns divided by the cubed standard deviation.           |
| Firm-specific return (`residuals`)      | The average of firm-specific weekly returns in a year.                                                                                                                                                       |
| Return on asset (`RoA`)                 | The returns on asset in a year, which is the ratio of net income to total assets.                                                                                                                            |
| Standard deviation (`annual_std`)       | The standard deviation of firm-specific weekly returns in a year.                                                                                                                                            |
| Firm-specific sentiment (`firm_sent`)   | The firm-specific investor sentiment measure obtained by PCA.                                                                                                                                                |

The return on assets and leverage variables are calculated using balance sheet data:

```
In [38]: bs = pd.read_csv('bs_v.3.csv')
         bs['Date'] = pd.to_datetime(bs.datadate, format='%Y%m%d')
         bs['annual_date'] = bs['Date'].dt.year

In [39]: bs['RoA'] = bs['ni'] / bs['at']
         bs['leverage'] = bs['lt'] / bs['at']

In [40]: merge_grouped['annual_date'] = merge_grouped['date'].dt.year
         bs['TICKER'] = bs.tic
         del bs['tic']
```

The next step is to obtain the rest of the variables merging balance sheet data (`bs`) and stock-related data (`merge_crash_all_grouped`):

```
In [41]: merge_ret_bs = pd.merge(bs, merge_grouped,
                                 on=['TICKER', 'annual_date'])

In [42]: merge_ret_bs2 = merge_ret_bs.set_index('Date')
         merge_ret_bs2 = merge_ret_bs2.groupby('TICKER').resample('Y').mean()
         merge_ret_bs2.reset_index(inplace=True)

In [43]: merge_ret_bs2['vol_csho_diff'] = (merge_ret_bs2.groupby('TICKER')
                                           ['VOL'].shift(-1) /
                                           merge_ret_bs2.groupby('TICKER')
                                           ['csho'].shift(-1))
         merge_ret_bs2['dturn1'] = merge_ret_bs2['VOL'] / merge_ret_bs2['csho']
         merge_ret_bs2['dturn'] = merge_ret_bs2['vol_csho_diff'] - \
                                  merge_ret_bs2['dturn1']

In [44]: merge_ret_bs2['p/e'] = merge_ret_bs2['PRC'] / merge_ret_bs2['ni']
         merge_ret_bs2['turnover_rate'] = merge_ret_bs2['VOL'] / \
                                          merge_ret_bs2['csho']
         merge_ret_bs2['equity_share'] = merge_ret_bs2['ceq'] / \
                                         (merge_ret_bs2['ceq'] +
                                          merge_ret_bs2['dt'])
         merge_ret_bs2['firm_size'] = np.log(merge_ret_bs2['at'])
         merge_ret_bs2['cefd'] = (((merge_ret_bs2['at'] -
                                  merge_ret_bs2['lt']) / merge_ret_bs2['csho']) -
                                  merge_ret_bs2['PRC']) / (merge_ret_bs2['at'] -
                                  merge_ret_bs2['lt']) / merge_ret_bs2['csho']

In [45]: merge_ret_bs2 = merge_ret_bs2.set_index('Date')
         merge_ret_bs2['buying_volume'] = merge_ret_bs2['VOL'] * \
                                          (merge_ret_bs2['PRC'] -
                                           merge_ret_bs2['BIDLO']) / \
                                          (merge_ret_bs2['ASKHI'] -
                                           merge_ret_bs2['BIDLO'])
         merge_ret_bs2['selling_volume'] = merge_ret_bs2['VOL'] * \
                                           (merge_ret_bs2['ASKHI'] -
                                            merge_ret_bs2['PRC']) / \
                                           (merge_ret_bs2['ASKHI'] -
                                            merge_ret_bs2['BIDLO'])
         buying_volume = merge_ret_bs2.groupby('TICKER')['buying_volume'] \
                         .resample('Y').sum().reset_index()
         selling_volume = merge_ret_bs2.groupby('TICKER')['selling_volume'] \
                         .resample('Y').sum().reset_index()
         del buying_volume['TICKER']
         del buying_volume['Date']

In [46]: buy_sel_vol = pd.concat([buying_volume,selling_volume], axis=1)
         buy_sel_vol['bsi'] = (buy_sel_vol.buying_volume -
                               buy_sel_vol.selling_volume) / \
                              (buy_sel_vol.buying_volume +
                               buy_sel_vol.selling_volume)

In [47]: merge_ret_bs2 = merge_ret_bs2.reset_index()
         merge_ret_bs2 = pd.merge(buy_sel_vol ,merge_ret_bs2,
                                  on=['TICKER', 'Date'])
```

Aside from firm-specific sentiment, the rest of the variables are widely used and quite useful in explaining stock price crash risk.

Deriving an index and using it as a proxy is something very popular among researchers when it is hard to find a suitable variable to represent a phenomenon. For instance, assume that you think that the firm-specific sentiment is a variable that includes very powerful insight about stock price crash, but how can you come up with a variable representing firm-specific sentiment? To address this issue, we can consider all the variables that are somewhat related to firm-specific sentiment, and then identify the relationships to create an index using principal component analysis. This is what we are about to do.

Despite some well-known determinants of stock price crash risk, an important aspect of crash risk that’s thought to be neglected is firm-specific investor sentiment. It is rather intuitive to say that, depending on the perceptions of investors about a company, the stock price might go up or down. That is, if an investor tends to feel optimistic about an individual stock, it is likely that they will buy the asset, which, in turn, drives the price up or down (Yin and Tian 2017).

In this respect, price-to-earnings ratio (P/E), turnover rate (TURN), equity share (EQS), closed-end fund discount (CEFD), leverage (LEV), buying and selling volume (BSI) are used in identifying the firm-specific sentiment. Explanations of these variables are provided in [Table 9-3](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#firm\_specific).

| Variable                          | Explanation                                                                                       |
| --------------------------------- | ------------------------------------------------------------------------------------------------- |
| Price-to-earnings ratio (`p/e`)   | Market value per share/earning per share                                                          |
| Turnover rate (`turnover_rate`)   | Total number of shares traded/average number of shares outstanding                                |
| Equity share (`equity_share`)     | Common stock                                                                                      |
| Closed-end fund discount (`cefd`) | Assets that raise a fixed amount of capital through an initial public offering                    |
| Leverage (`leverage`)             | Sum of long-term debt and debt in current liabilities/total assets                                |
| Buying and selling volume (`bsi`) | Buying (selling) volume is the number of shares that were associated with buying (selling) trades |

To capture the firm-specific sentiment properly, we need to extract as much information as we can, and PCA is a convenient tool to accomplish this task with:

```
In [48]: from sklearn.preprocessing import StandardScaler
         from sklearn.decomposition import PCA

In [49]: firm_sentiment = merge_ret_bs2[['p/e', 'turnover_rate',
                                         'equity_share', 'cefd',
                                         'leverage', 'bsi']]
         firm_sentiment = firm_sentiment.apply(lambda x: x.fillna(x.mean()),
                                               axis=0) 

In [50]: firm_sentiment_std = StandardScaler().fit_transform(firm_sentiment)
         pca = PCA(n_components=6)
         pca_market_sentiment = pca.fit_transform(firm_sentiment_std)
         print('Explained Variance Ratios per Component are:\n {}'\
               .format(pca.explained_variance_ratio_))
         Explained Variance Ratios per Component are:
          [0.35828322 0.2752777  0.15343653 0.12206041 0.06681776 0.02412438]

In [51]: loadings_1 = pd.DataFrame(pca.components_.T *
                                   np.sqrt(pca.explained_variance_),
                                   columns=['PC1', 'PC2', 'PC3',
                                            'PC4', 'PC5', 'PC6'],
                                   index=firm_sentiment.columns) 
         loadings_1
Out[51]: PC1       PC2       PC3       PC4       PC5       PC6
         p/e           -0.250786  0.326182  0.911665  0.056323  0.000583
          0.021730
         turnover_rate -0.101554  0.854432 -0.197381  0.201749  0.428911
          -0.008421
         equity_share  -0.913620 -0.162406 -0.133783  0.224513 -0.031672
          0.271443
         cefd           0.639570 -0.118671  0.038422  0.754467 -0.100176
          0.014146
         leverage       0.917298  0.098311  0.068633 -0.264369  0.089224
          0.265335
         bsi            0.006731  0.878526 -0.173740 -0.044127 -0.446735
          0.022520

In [52]: df_loading1 = pd.DataFrame(loadings_1.mean(axis=1)) 
         df_loading1
Out[52]:                       0
         p/e            0.177616
         turnover_rate  0.196289
         equity_share  -0.124254
         cefd           0.204626
         leverage       0.195739
         bsi            0.040529

In [53]: firm_sentiment = pd.DataFrame(np.dot(pca_market_sentiment,
                                              np.array(df_loading1)))
         merge_ret_bs2['firm_sent'] = firm_sentiment
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO10-1)

Filling missing value with mean

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO10-2)

Calculating the loadings

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#co\_a\_corporate\_governance\_risk\_measure\_\_stock\_price\_crash\_CO10-3)

Taking cross-sectional average of loadings

Having obtained the loadings of the features, the result of the cross-sectional average of components produces the following result:

SENT�,�=0.177P/E�,�+0.196TURN�,�-0.124EQS�,�+0.204CEFD�,�+0.195LEV�,�+0.040BSI�,�

The result implies that firm-specific sentiment is positively affected by all the variables except for the equity share. In addition, leverage and turnover rate have the largest impact on the firm-specific sentiment.

We have one more step to go: interpret the logistic panel data analysis. Before that, the independent and dependent variables should be defined, and the necessary libraries are used to do so:

```
In [54]: merge_ret_bs2['log_size'] = np.log(merge_ret_bs2['at'])

In [55]: merge_ret_bs2.set_index(['TICKER', 'Date'], inplace=True)

In [56]: X = (merge_ret_bs2[['log_size', 'rect', 'ppegt', 'dturn',
                         'ncskew', 'residuals', 'RoA', 'annual_std',
                         'firm_sent']]).shift(1)
         X['neg_outliers'] = merge_ret_bs2['neg_outliers']
```

Logistic panel data analysis shows us which variables have a statistically significant relationship with the `neg_outliers`, the stock price crash measure obtained from the Elliptic Envelope algorithm. The result suggests that, aside from `ppegt` and `residuals`, all other variables are statistically significant at conventional confidence intervals. Specifically, `log_size`, `dturn`, `firm_sent`, and `annual_std` do trigger a crash.

As is seen from the result, the coefficients of firm-specific investor sentiment index are positive, financially important, and statistically significant at 1% level. Literature suggests that, in times of high sentiment, under pressure of optimistic expectations, managers tend to accelerate good news but withhold bad news to maintain the positive environment (Bergman and Roychowdhury 2008). Thus, the result suggests a positive relationship between sentiment and crash risk.

With all these variables showing a strong statistically-significant relation to `neg_outliers`, we are able to run a reliable predictive analysis:

```
In [57]: from pyeconometrics.panel_discrete_models import FixedEffectPanelModel
         from sklearn.model_selection import train_test_split
         from sklearn.metrics import accuracy_score

In [58]: FE_ML = FixedEffectPanelModel()
         FE_ML.fit(X, 'neg_outliers')
         FE_ML.summary()
    ======================================================================
    ==========
    Dep. Variable:                  neg_outliers   Pseudo R-squ.:      0.09611
    Model:             Panel Fixed Effects Logit   Log-Likelihood:     -83.035
    Method:                                  MLE   LL-Null:            -91.864
    No. Observations:                        193   LLR p-value:          0.061
    Df Model:                                  9

    Converged:                                  True

    ======================================================================
                        coef  std err        t    P>|t| [95.0% Conf. Int.]
    ----------------------------------------------------------------------
    _cons                     -2.5897    1.085   -2.387    0.008    -4.716
    -0.464
    log_size                   0.1908    0.089    2.155    0.016     0.017
    0.364
    rect                      -0.0000    0.000   -4.508    0.000    -0.000
    -0.000
    ppegt                     -0.0000    0.000   -0.650    0.258    -0.000
    0.000
    dturn                      0.0003    0.000    8.848    0.000     0.000
    0.000
    ncskew                    -0.2156    0.089   -2.420    0.008    -0.390
    -0.041
    residuals                 -0.3843    1.711   -0.225    0.411    -3.737
    2.968
    RoA                        1.4897    1.061    1.404    0.080    -0.590
    3.569
    annual_std                 1.9252    0.547    3.517    0.000     0.852
    2.998
    firm_sent                  0.6847    0.151    4.541    0.000     0.389
    0.980
    ----------------------------------------------------------------------
```

For the sake of comparison, this time the dependent variable is replaced by `crash_risk`, which is of discrete type as well. Thanks to this comparison, we are able to compare the goodness of the model as well as likely predictive power. Given the goodness measure of our model, �2, the model with a dependent variable of `neg_outliers` has higher explanatory power. However, please note that �2 is not the only metric used to compare the goodness of a model. As this discussion is beyond the scope of the book, I will not go into detail.

Aside from that, what is apparent is signs of some estimated coefficients are different across these two models. For instance, according to the literature, firm sentiment (`firm_sent`) is supposed to have a positive sign, as once investor sentiment surges, bad news hoarding behavior increases, leading to a rise in the stock price crash risk. These important observations are captured in the previous model, which contains our newly introduced dependent variable `neg_outliers`. The model with `neg_outliers` yields better and more reliable predictions:

```
In [59]: del X['neg_outliers']
         X['crash_risk'] = merge_ret_bs2['crash_risk']

In [60]: FE_crash = FixedEffectPanelModel()
         FE_crash.fit(X, 'crash_risk')
         FE_crash.summary()
    ======================================================================
    Dep. Variable:                   crash_risk   Pseudo R-squ.:   0.05324
    Model:            Panel Fixed Effects Logit   Log-Likelihood:  -55.640
    Method:                                 MLE   LL-Null:         -58.769
    No. Observations:                       193   LLR p-value:       0.793
    Df Model:                                 9

    Converged:                                  True

    ======================================================================
                            coef  std err        t    P>|t| [95.0% Conf. Int.]

    ----------------------------------------------------------------------
    _cons                     -3.1859    1.154   -2.762    0.003    -5.447
    -0.925
    log_size                   0.2012    0.094    2.134    0.016     0.016
    0.386
    rect                      -0.0000    0.000   -1.861    0.031    -0.000
    0.000
    ppegt                     -0.0000    0.000   -0.638    0.262    -0.000
    0.000
    dturn                      0.0001    0.000    2.882    0.002     0.000
    0.000
    ncskew                     0.3840    0.114    3.367    0.000     0.160
    0.608
    residuals                  3.3976    2.062    1.648    0.050    -0.644
    7.439
    RoA                        2.5096    1.258    1.994    0.023     0.043
    4.976
    annual_std                 2.4094    0.657    3.668    0.000     1.122
    3.697
    firm_sent                 -0.0041    0.164   -0.025    0.490    -0.326
    0.318
    ----------------------------------------------------------------------
```

## Conclusion

In this chapter, we learned how to detect stock price crash using ML. Using the MCD method, negative anomalies in the market-adjusted firm-specific stock price returns were detected and defined as a stock price crash risk indicator. The results suggest that there is a positive relationship between sentiment and crash risk, indicating that during high sentiment times, under pressure of optimistic expectations, managers tend to withhold bad news, and this accumulated bad news leads to large declines.

In addition, other stock price crash measures, namely NCSKEW, DUVOL, and crash risk, were also obtained. Of them, we used NCSKEW and crash risk in our analysis as the independent and dependent variables, respectively.

The logistic panel analysis showed that the model with `neg_outliers` estimated coefficients with signs in conformity with the literature, making it more useful and also increasing the reliability of its predictive analysis compared to the one with `crash_risk`.

In the next chapter, a brand-new and highly popular topic in finance circles will be introduced: _synthetic data generation_ and its use in risk management.

## References

Articles and books cited in this chapter:

* Bae, Kee‐Hong, Chanwoo Lim, and KC John Wei. 2006. “Corporate Governance and Conditional Skewness In The World’s Stock Markets.” _The Journal of Business_ 79 (6): 2999-3028.
* Bergman, Nittai K., and Sugata Roychowdhury. 2008. “Investor Sentiment and Corporate Disclosure.” _Journal of Accounting Research_ 46 (5): 1057-1083.
* Bleck, Alexander, and Xuewen Liu. 2007. “Market Transparency and The Accounting Regime.” _Journal of Accounting Research_ 45 (2): 229-256.
* Chen, Joseph, Harrison Hong, and Jeremy C. Stein. 2001. “Forecasting Crashes: Trading Volume, Past Returns, and Conditional Skewness In Stock Prices.” _Journal of Financial Economics_ 61 (3): 345-381.
* Hubert, Mia, Michiel Debruyne, and Peter J. Rousseeuw. 2018. “Minimum Covariance Determinant and Extensions.” 2018. _Wiley Interdisciplinary Reviews: Computational Statistics_ 10 (3): e1421.
* Hutton, Amy P., Alan J. Marcus, and Hassan Tehranian. 2009. “Opaque Financial Reports, R2, and Crash Risk.” _Journal of Financial Economics_ 94 (1): 67-86.
* Hsiao, Cheng. 2014. _Analysis Of Panel Data_. Cambridge University Press.
* Kim J. B., Li Y., and Zhang L. 2011. “Corporate Tax Avoidance and Stock Price Crash Risk: Firm-Level Analysis.” _Journal of Financial Economics_ 100 (3): 639-662.
* Kim, Jeong‐Bon, and Liandong Zhang. 2014. “Financial Reporting Opacity and Expected Crash Risk: Evidence From Implied Volatility Smirks.” _Contemporary Accounting Research_ 31 (3): 851-875.
* Jin, Li, and Stewart C. Myers. 2006. “R2 Around The World: New Theory and New Tests.” _Journal of Financial Economics_ 79 (2): 257-292.
* Finch, Holmes. 2012. “Distribution Of Variables By Method Of Outlier Detection.” _Frontiers in Psychology_ (3): 211.
* Wolfensohn, James. 1999. “The Critical Study Of Corporate Governance Provisions In India.” _Financial Times_ 25 (4). Retrieved from [_https://oreil.ly/EnLaQ_](https://oreil.ly/EnLaQ).
* Shleifer, Andrei, and Robert W. Vishny. 1997. “A Survey Of Corporate Governance.” _The Journal of Finance_ 52 (2): 737-783.
* Ullah, Aman, ed. 1998. _Handbook Of Applied Economic Statistics_. Boca Raton: CRC Press.
* Yin, Yugang, and Rongfu Tian. 2017. “Investor Sentiment, Financial Report Quality and Stock Price Crash Risk: Role Of Short-Sales Constraints.” _Emerging Markets Finance and Trade_ 53 (3): 493-510.

[1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch09.html#idm45737208073680-marker) A symmetric matrix with all positive eigenvalues is referred to as a _positive definite matrix_.
