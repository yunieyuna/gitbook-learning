# 8. Modeling Operational Risk

## Chapter 8. Modeling Operational Risk

> ...It’s not necessarily the biggest missteps that deliver the biggest blows; share prices can plummet as a result of even the smallest events.
>
> Dunnett, Levy, and Simoes (2005)

Thus far, we have talked about three main financial risks: market, credit, and liquidity risks. Now it is time to discuss operational risk, which is more ambiguous than the other types of financial risks. This ambiguity arises from the huge variety of risk sources by which financial institutions may face huge losses.

Operational risk is the risk of direct or indirect loss resulting from inadequate or failed interval processes, people, and systems or from external events (BIS 2019). Please note that loss can be direct and/or indirect. Some direct losses would be:

* Legal liability arising from judicial process
* Write-downs due to theft or reduction in assets
* Compliance emanating from tax, license, fines, etc.
* Business interruption

Indirect cost is related to the opportunity cost in the way that a decision made by an institution may trigger a host of events resulting in a loss at an uncertain time in the future.

Normally, financial institutions allocate a certain amount of money to cover the loss emanating from operational risk, which is known as _unexpected loss_. However, allocating an appropriate amount of funds to cover unexpected loss is not as easy as it sounds. It is necessary to determine the right amount of unexpected loss; otherwise, either more funds are devoted to it, which makes it idle and creates opportunity cost, or less than the required funds are allocated, resulting in a liquidity problem.

As we briefly touched on earlier, operational risk can take on several forms. Among them, we’ll restrict our focus to the fraud risk, which is considered to be the most pervasive and disruptive type of operational risk.

Fraud may generally be characterized as an intentional act, misstatement, or omission designed to deceive others, resulting in the victim suffering a loss or the perpetrator achieving a gain (OCC 2019). A fraud can be an internal one if losses occurred from inside a financial institution or an external one if it is committed by a third party.

What makes fraud a primary concern of financial institutions? What increases the likelihood of committing fraudulent activities? To address these questions, we can refer to three important factors:

* Globalization
* Lack of proper risk management
* Economic pressure

Globalization led financial institutions to expand their operations across the world, and this came with a complexity that gave rise to a higher probability of corruption, bribery, and any kind of illegal act as financial institutions started operating in environments where they have no prior knowledge.

Lack of proper risk management has been and is the most obvious reasons for fraud. Misleading reporting and rogue, unauthorized trading plants the seeds of fraudulent acts. A very well known example is the Barings case, in which Nick Leeson, a young trader at Barings, ran a speculative trading and subsequent cover-up operation using accounting tricks that cost Barings Bank a fortune, totaling $1.3 billion. Thus, when there is a lack of well-defined risk management policies along with a well-established culture of risk, employees may tend to commit fraud.

Another motivation for fraud would be an employee’s worsening financial situation. Particularly during an economic downturn, employees might be tempted into fraudulent activities. In addition, financial institutions themselves might embrace illegal operations (such as accounting tricks) to find a way out of the downturn.

Fraud does not only cause a considerable amount of loss, but it also poses a threat to a company’s reputation, which may in turn disrupt the long-term sustainability of the company. Take the case of Enron, a good example of accounting fraud, which broke out in 2001. Enron was established in 1985 and became one of the biggest companies in the United States and the world. Let me briefly tell you the story of this big collapse.

Due to the pressure that Enron faced in the energy market, executives were motivated to rely on dubious accounting practices, resulting in inflated profits from writing huge unrealized future gains. Thanks to whistleblower Sherron Watkins, who was the former vice president of corporate development, one of the biggest fraud cases in the history of modern finance came to light. This event also stresses the importance of preventing fraudulent activities, which otherwise might lead to huge damages to an individual’s or company’s reputation or financial collapse.

In this chapter, we aim to introduce an ML-based model to detect fraud or would-be fraud operations. This is and should be a constantly growing field to stay ahead of the perpetrators. Datasets related to fraud may come in two forms: _labeled_ or _unlabeled data_. To take both into account, we first apply a supervised learning algorithm and then use an unsupervised learning algorithm pretending like we do not have labels, even though the dataset we’ll be using does include labels.

The dataset we’ll use for our fraud analysis is known as the _Credit Card Transaction Fraud Detection Dataset_ created by Brandon Harris. Credit card fraud is not a rare issue, and the goal is to detect the likelihood of fraud and inform the bank so that the bank can investigate the situation with due diligence. This is the way a bank protects itself from incurring huge losses. According to the Nilsen Report (2020), payment card fraud losses hit a record-high level of $32.04 billion, amounting to 6.8¢ for every $100 of total volume.

This dataset is a good example of a mix of attributes of variable types as we have continuous, discrete, and nominal data. You can find the data on [Kaggle](https://oreil.ly/fxxFg). An explanation of the data is provided in [Table 8-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#att\_exp).

| Attribute               | Explanation                                    |
| ----------------------- | ---------------------------------------------- |
| `trans_date_trans_time` | Date the transaction                           |
| `cc_num`                | Credit card number of the customer             |
| `merchant`              | Merchant by whom the trade occurred            |
| `amt`                   | Amount of transaction                          |
| `first`                 | First name of customer                         |
| `last`                  | Last name of customer                          |
| `gender`                | Gender of the customer                         |
| `street, city, state`   | Address of the customer                        |
| `zip`                   | Zip code of the transaction                    |
| `lat`                   | Latitude of the customer                       |
| `long`                  | Longitude of the customer                      |
| `city_pop`              | Population of the city                         |
| `job`                   | Type of the customer’s profession              |
| `dob`                   | Date of birth of the customer                  |
| `trans_num`             | Unique transaction number for each transaction |
| `unix_time`             | Time of the transaction in Unix                |
| `merch_lat`             | Merchant latitude                              |
| `merch_long`            | Merchant longitude                             |
| `is_fraud`              | Whether the transaction is fraudulent or not   |

## Getting Familiar with Fraud Data

As you probably noticed, ML algorithms work better if the number of observations among different classes are more or less equal to each other—that is, it works best with balanced data. We do not have balanced data in the fraud case, so this is called a _class imbalance_. In [Chapter 6](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#chapter\_6), we learned how to handle class imbalance problems, and we’ll use this skill again in this chapter.

Let’s start off. To begin with, it makes sense to go through the data types of the variables in the Credit Card Transaction Fraud Detection Dataset:

```
In [1]: import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        import seaborn as sns
        from scipy.stats import zscore
        import warnings
        warnings.filterwarnings('ignore')

In [2]: fraud_data = pd.read_csv('fraudTrain.csv')
        del fraud_data['Unnamed: 0']

In [3]: fraud_data.info()
        <class 'pandas.core.frame.DataFrame'>
        RangeIndex: 1296675 entries, 0 to 1296674
        Data columns (total 22 columns):
         #   Column                 Non-Null Count    Dtype
        ---  ------                 --------------    -----
         0   trans_date_trans_time  1296675 non-null  object
         1   cc_num                 1296675 non-null  int64
         2   merchant               1296675 non-null  object
         3   category               1296675 non-null  object
         4   amt                    1296675 non-null  float64
         5   first                  1296675 non-null  object
         6   last                   1296675 non-null  object
         7   gender                 1296675 non-null  object
         8   street                 1296675 non-null  object
         9   city                   1296675 non-null  object
         10  state                  1296675 non-null  object
         11  zip                    1296675 non-null  int64
         12  lat                    1296675 non-null  float64
         13  long                   1296675 non-null  float64
         14  city_pop               1296675 non-null  int64
         15  job                    1296675 non-null  object
         16  dob                    1296675 non-null  object
         17  trans_num              1296675 non-null  object
         18  unix_time              1296675 non-null  int64
         19  merch_lat              1296675 non-null  float64
         20  merch_long             1296675 non-null  float64
         21  is_fraud               1296675 non-null  int64
        dtypes: float64(5), int64(5), object(12)
        memory usage: 217.6+ MB
```

It turns out we have all types of data: object, integer, and float. However, the majority of the variables are of the object type, so additional analysis is required to turn these categorical variables into numerical ones.

The dependent variable is of considerable importance in such an analysis, as it often has imbalance characteristics that require due attention. This is shown in the following snippet (and resultant [Figure 8-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#pie\_chart\_fraud)), which indicates a highly disproportionate number of observations:

```
In [4]: plt.pie(fraud_data['is_fraud'].value_counts(), labels=[0, 1])
        plt.title('Pie Chart for Dependent Variable');
        print(fraud_data['is_fraud'].value_counts())
        plt.show()
        0    1289169
        1       7506
        Name: is_fraud, dtype: int64
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0801.png" alt="pie_chart_fraud" height="488" width="528"><figcaption></figcaption></figure>

**Figure 8-1. Pie chart for dependent variable**

As we can see, the number of observations for the nonfraud case is 1,289,169, while there are only 7,506 for the fraud case, so we know that the data is highly imbalanced, as expected.

At this point, we can use a rather different tool to detect the number of missing observations. This tool is known as `missingno`, and it also provides us with a visualization module for missing values (as can be seen in [Figure 8-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#missingno\_fraud)):

```
In [5]: import missingno as msno 

        msno.bar(fraud_data) 
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO1-1)

Importing `missingno`

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO1-2)

Creating a bar plot for missing values

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0802.png" alt="missingno_fraud" height="273" width="600"><figcaption></figcaption></figure>

**Figure 8-2. Missing observations**

[Figure 8-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#missingno\_fraud) indicates the number of nonmissing observations per variable at the top, and on the left-hand side we can see the percentage of nonmissing values. This analysis shows that the data has no missing values.

In the next step, first we convert the date variable, `trans_date_trans_time`, into a proper format, and then we break time down into days and hours, assuming that fraudulent activities surge during particular time periods. It makes sense to analyze the effect of fraud on the different categories of a variable. To do that, we’ll employ a bar plot. It becomes clearer that the number of fraud cases may change given the category of some variables. But it stays the same in gender variables, meaning that gender has no impact on fraudulent activities. Another striking and evident observation is that the fraud cases change wildly per day and hour. This can be visually confirmed in the resulting [Figure 8-3](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#bar\_fraud):

```
In [6]: fraud_data['time'] = pd.to_datetime(fraud_data['trans_date_trans_time'])
        del fraud_data['trans_date_trans_time']

In [7]: fraud_data['days'] = fraud_data['time'].dt.day_name()
        fraud_data['hour'] = fraud_data['time'].dt.hour

In [8]: def fraud_cat(cols):
            k = 1
            plt.figure(figsize=(20, 40))
            for i in cols:
                categ = fraud_data.loc[fraud_data['is_fraud'] == 1, i].\
                        value_counts().sort_values(ascending=False).\
                        reset_index().head(10) 
                plt.subplot(len(cols) / 2, len(cols) / 2, k)
                bar_plot = plt.bar(categ.iloc[:, 0], categ[i])
                plt.title(f'Cases per {i} Categories')
                plt.xticks(rotation='45')
                k+= 1
            return categ, bar_plot

In [9]: cols = ['job', 'state', 'gender', 'category', 'days', 'hour']
        _, bar_plot = fraud_cat(cols)
        bar_plot
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO2-1)

Sorting `fraud_data` based on fraudulent activities in an ascending order

Based on the analysis and our previous knowledge about the fraud analysis, we can decide on the number of variables to be used in our modeling. The categorical variables sort out so that we can create dummy variables using `pd.get_dummies`:

```
In [10]: cols=['amt','gender','state','category',
               'city_pop','job','is_fraud','days','hour']
         fraud_data_df=fraud_data[cols]

In [11]: cat_cols=fraud_data[cols].select_dtypes(include='object').columns

In [12]: def one_hot_encoded_cat(data, cat_cols):
             for i in cat_cols:
                 df1 = pd.get_dummies(data[str(i)],
                                      prefix=i, drop_first=True)
                 data.drop(str(i), axis=1, inplace=True)
                 data = pd.concat([data, df1], axis=1)
             return data

In [13]: fraud_df = one_hot_encoded_cat(fraud_data_df, cat_cols)
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0803.png" alt="bar_fraud" height="776" width="600"><figcaption></figcaption></figure>

**Figure 8-3. Bar plots per variable**

Subsequent to categorical variable analysis, it’s worth discussing the interactions between the numerical variables, namely, `amount`, `population`, and `hour`. A correlation analysis provides us with a strong tool for figuring out the interaction(s) among these variables, and the resulting heatmap ([Figure 8-4](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#heatmap\_fraud)) suggests that the correlations are very low:

```
In [14]: num_col = fraud_data_df.select_dtypes(exclude='object').columns
         fraud_data_df = fraud_data_df[num_col]
         del fraud_data_df['is_fraud']

In [15]: plt.figure(figsize=(10,6))
         corrmat = fraud_data_df.corr()
         top_corr_features = corrmat.index
         heat_map = sns.heatmap(corrmat, annot=True, cmap="viridis")
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0804.png" alt="heatmap_fraud" height="393" width="600"><figcaption></figcaption></figure>

**Figure 8-4. Heatmap**

## Supervised Learning Modeling for Fraud Examination

We have determined the peculiar characteristics of the variables using interactions, missing values, and creating dummy variables. Now we are ready to move on and run ML models for fraud analysis. The models we are about to run are:

* Logistic regression
* Decision tree
* Random forest
* XGBoost

As you can imagine, it’s key to have balanced data before doing our modeling. Even though there are numerous ways to get balanced data, we’ll choose the undersampling method because of its performance. Undersampling is a technique that matches the majority classes to minority classes, as shown in [Figure 8-5](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#imbalance).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0805.png" alt="imbalance" height="185" width="600"><figcaption></figcaption></figure>

**Figure 8-5. Undersampling**

Alternatively, the number of observations from the majority class is removed until we get the same number of observations as the minority class. We’ll apply undersampling in the following code block, where the independent and dependent variables are named `X_under` and `y_under`, respectively. In what follows, train-test split is used to obtain the train and test splits in a random fashion:

```
In [16]: from sklearn.model_selection import train_test_split
         from sklearn.linear_model import LogisticRegression
         from sklearn.model_selection import train_test_split
         from sklearn.model_selection import GridSearchCV
         from sklearn.model_selection import RandomizedSearchCV
         from sklearn.metrics import (classification_report,
                                     confusion_matrix, f1_score)

In [17]: non_fraud_class = fraud_df[fraud_df['is_fraud'] == 0]
         fraud_class = fraud_df[fraud_df['is_fraud'] == 1]

In [18]: non_fraud_count,fraud_count=fraud_df['is_fraud'].value_counts()
         print('The number of observations in non_fraud_class:', non_fraud_count)
         print('The number of observations in fraud_class:', fraud_count)
         The number of observations in non_fraud_class: 1289169
         The number of observations in fraud_class: 7506

In [19]: non_fraud_under = non_fraud_class.sample(fraud_count) 
         under_sampled = pd.concat([non_fraud_under, fraud_class], axis=0) 
         X_under = under_sampled.drop('is_fraud',axis=1) 
         y_under = under_sampled['is_fraud'] 

In [20]: X_train_under, X_test_under, y_train_under, y_test_under =\
                 train_test_split(X_under, y_under, random_state=0)
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO3-1)

Sampling `fraud_count`

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO3-2)

Concatenating the data including fraudulent cases with data including no fraudulent cases

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO3-3)

Creating independent variables by dropping `is_fraud`

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO3-4)

Creating dependent variables by `is_fraud`

After using the undersampling method, let’s now run some of the classification models we described earlier and observe the performance of these models in detecting the fraud:

```
In [21]: param_log = {'C': np.logspace(-4, 4, 4), 'penalty': ['l1', 'l2']}
         log_grid = GridSearchCV(LogisticRegression(),
                                 param_grid=param_log, n_jobs=-1)
         log_grid.fit(X_train_under, y_train_under)
         prediction_log = log_grid.predict(X_test_under)

In [22]: conf_mat_log = confusion_matrix(y_true=y_test_under,
                                         y_pred=prediction_log)
         print('Confusion matrix:\n', conf_mat_log)
         print('--' * 25)
         print('Classification report:\n',
               classification_report(y_test_under, prediction_log))
         Confusion matrix:
          [[1534  310]
          [ 486 1423]]
         --------------------------------------------------
         Classification report:
                        precision    recall  f1-score   support

                    0       0.76      0.83      0.79      1844
                    1       0.82      0.75      0.78      1909

             accuracy                           0.79      3753
            macro avg       0.79      0.79      0.79      3753
         weighted avg       0.79      0.79      0.79      3753
```

First, let’s look at the confusion matrix. The confusion matrix suggests that the number of observations in false positives and false negatives are 310 and 486, respectively. We’ll be using the confusion matrix in the cost-based method.

The _F1 score_ is the metric that is used to measure the performance of these models. It presents a weighted average of recall and precision, making it an ideal measure for a case such as this one.

The second model is decision tree, which works well in modeling fraud. After tuning hyperparameters, it turns out that F1 score is much higher, indicating that decision tree does a relatively good job. As expected, the number of false positive and false negative observations are much fewer compared to logistic regression:

```
In [23]: from sklearn.tree import DecisionTreeClassifier

In [24]: param_dt = {'max_depth': [3, 5, 10],
                     'min_samples_split': [2, 4, 6],
                     'criterion': ['gini', 'entropy']}
         dt_grid = GridSearchCV(DecisionTreeClassifier(),
                                param_grid=param_dt, n_jobs=-1)
         dt_grid.fit(X_train_under, y_train_under)
         prediction_dt = dt_grid.predict(X_test_under)

In [25]: conf_mat_dt = confusion_matrix(y_true=y_test_under,
                                        y_pred=prediction_dt)
         print('Confusion matrix:\n', conf_mat_dt)
         print('--' * 25)
         print('Classification report:\n',
               classification_report(y_test_under, prediction_dt))
         Confusion matrix:
          [[1795   49]
          [  84 1825]]
         --------------------------------------------------
         Classification report:
                        precision    recall  f1-score   support

                    0       0.96      0.97      0.96      1844
                    1       0.97      0.96      0.96      1909

             accuracy                           0.96      3753
            macro avg       0.96      0.96      0.96      3753
         weighted avg       0.96      0.96      0.96      3753
```

According to common belief, the random forest model, as an ensemble model, outperforms decision tree. However, this is true only if decision tree suffers from predictive instability in such a way that predictions of different samples vary wildly, and this is not the case here. As you can observe from the following result, random forest does not perform better than decision tree, even if it has an F1 score of 87:

```
In [26]: from sklearn.ensemble import RandomForestClassifier

In [27]: param_rf = {'n_estimators':[20,50,100] ,
                  'max_depth':[3,5,10],
                  'min_samples_split':[2,4,6],
                  'max_features':['auto', 'sqrt', 'log2']}
         rf_grid = GridSearchCV(RandomForestClassifier(),
                               param_grid=param_rf, n_jobs=-1)
         rf_grid.fit(X_train_under, y_train_under)
         prediction_rf = rf_grid.predict(X_test_under)

In [28]: conf_mat_rf = confusion_matrix(y_true=y_test_under,
                                        y_pred=prediction_rf)
         print('Confusion matrix:\n', conf_mat_rf)
         print('--' * 25)
         print('Classification report:\n',
               classification_report(y_test_under, prediction_rf))
         Confusion matrix:
          [[1763   81]
          [ 416 1493]]
         --------------------------------------------------
         Classification report:
                        precision    recall  f1-score   support

                    0       0.81      0.96      0.88      1844
                    1       0.95      0.78      0.86      1909

             accuracy                           0.87      3753
            macro avg       0.88      0.87      0.87      3753
         weighted avg       0.88      0.87      0.87      3753
```

The final model we’ll look at is XGBoost, which generates similar results to the decision tree, as it outputs an F1 score of 97:

```
In [29]: from xgboost import XGBClassifier

In [30]: param_boost = {'learning_rate': [0.01, 0.1],
                        'max_depth': [3, 5, 7],
                        'subsample': [0.5, 0.7],
                        'colsample_bytree': [0.5, 0.7],
                        'n_estimators': [10, 20, 30]}
         boost_grid = RandomizedSearchCV(XGBClassifier(),
                                         param_boost, n_jobs=-1)
         boost_grid.fit(X_train_under, y_train_under)
         prediction_boost = boost_grid.predict(X_test_under)

In [31]: conf_mat_boost = confusion_matrix(y_true=y_test_under,
                                           y_pred=prediction_boost)
         print('Confusion matrix:\n', conf_mat_boost)
         print('--' * 25)
         print('Classification report:\n',
               classification_report(y_test_under, prediction_boost))
         Confusion matrix:
          [[1791   53]
          [  75 1834]]
         --------------------------------------------------
         Classification report:
                        precision    recall  f1-score   support

                    0       0.96      0.97      0.97      1844
                    1       0.97      0.96      0.97      1909

             accuracy                           0.97      3753
            macro avg       0.97      0.97      0.97      3753
         weighted avg       0.97      0.97      0.97      3753
```

Given all the applications, here is the summary result:

| Model               | F1 score |
| ------------------- | -------- |
| Logistic regression | 0.79     |
| Decision tree       | 0.96     |
| Random forest       | 0.87     |
| XGBoost             | 0.97     |

### Cost-Based Fraud Examination

Undersampling gives us a convenient tool for dealing with imbalanced data. It comes with costs, however, and the biggest cost is its discarding of important observations. Even though different sampling procedures can be applied to sensitive analyses such as health care, fraud, and so on, it should be noted that performance metrics fail to consider the extent to which different misclassifications have varying economic impact. Hence, if a method proposes different misclassification costs, it is referred to as a _cost-sensitive classifier_. Let’s consider the fraud case, which is a classic example of cost-sensitive analysis. In this type of analysis, it is evident that a false positive is less costly than a false negative. To be more precise, a false positive means blocking an already legitimate transaction. The cost of this type of classification tends to be administrative and opportunity cost–related, such as the time and energy spent on detection and the lost potential gain a financial institution can make from the transaction.

However, failing to detect a fraud (i.e., having a false negative) means a lot for a company, as it might imply various internal weaknesses as well as poorly designed operational procedures. Having failed to detect a real fraud, a company can incur large financial costs—including the transaction amount—not to mention costs stemming from any damage to its reputation. The former type of cost puts the burden on the company’s shoulder, but the latter can be neither quantified nor ignored.

As you can see, the need to assign varying costs for different misclassifications leads us to a more pronounced, realistic solution. For the sake of simplicity, let’s assume the cost of false negative and true positive to be the transaction amount and 2, respectively. [Table 8-3](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#cost\_sen\_mat) summarizes the results. Another approach for evaluating cost sensitivity would be to assume a constant false negative, as in other cases. However, this approach is considered unrealistic.

| Model              | F1 score                            |
| ------------------ | ----------------------------------- |
| True Positive = 2  | False Negative = Transaction Amount |
| False Positive = 2 | True Negative = 0                   |

Consequently, the total cost that an institution might face with varying false negative costs takes the following form:

Cost=∑�=1���(������+(1-��)����)+(1-��)������

where �� is the predicted label, �� is the actual label, _N_ is the number of observations, and ���� and ���� correspond to administrative cost, which is 2 in our case. ���� represents transaction amount.

Now, with this information in hand, let’s revisit the ML models considering the cost-sensitive approach and calculate the changing cost of these models. However, before we start, it is worth noting that cost-sensitive models are not fast-processing ones, so as we have a large number of observations, it would be wise to sample from them to model the data in a timely manner. A class-dependent cost measure is given as follows:

```
In [32]: fraud_df_sampled = fraud_df.sample(int(len(fraud_df) * 0.2)) 

In [33]: cost_fp = 2
         cost_fn = fraud_df_sampled['amt']
         cost_tp = 2
         cost_tn = 0
         cost_mat = np.array([cost_fp * np.ones(fraud_df_sampled.shape[0]),
                              cost_fn,
                              cost_tp * np.ones(fraud_df_sampled.shape[0]),
                              cost_tn * np.ones(fraud_df_sampled.shape[0])]).T 


In [34]: cost_log = conf_mat_log[0][1] * cost_fp + conf_mat_boost[1][0] * \
                     cost_fn.mean() + conf_mat_log[1][1] * cost_tp 
         cost_dt = conf_mat_dt[0][1] * cost_fp + conf_mat_boost[1][0] * \
                   cost_fn.mean() + conf_mat_dt[1][1] * cost_tp 
         cost_rf = conf_mat_rf[0][1] * cost_fp + conf_mat_boost[1][0] * \
                   cost_fn.mean() + conf_mat_rf[1][1] * cost_tp 
         cost_boost = conf_mat_boost[0][1] * cost_fp + conf_mat_boost[1][0] * \
                      cost_fn.mean() + conf_mat_boost[1][1] * cost_tp 
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO4-1)

Sampling from `fraud_df` data

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO4-2)

Computing the cost matrix

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO4-3)

Computing the total cost per models employed

Calculating the total cost enables us to have different approaches in assessing model performance. The model with a high F1 score is expected to have low total cost, and this is what we have in [Table 8-4](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#total\_cost). Logistic regression has the highest total cost, and XGBoost has the lowest.

| Model               | Total cost |
| ------------------- | ---------- |
| Logistic Regression | 5995       |
| Decision Tree       | 5351       |
| Random Forest       | 5413       |
| XGBoost             | 5371       |

### Saving Score

There are different metrics that can be used in cost improvement, and saving score is absolutely one of them. To be able to define saving, let us give the formula of cost.

Bahnsen, Aouada, and Ottersten (2014) clearly explain the saving score formula in the following manner:

Cost(f(S))=∑�=1���(������+(1-��)����)+(1-��)(������+(1-��)����)

where _TP_, _FN_, _FP_, and _TN_ are true positive, false negative, false positive, and true negative, respectively. �� is the predicted label for each observation _i_ on training set _S_. �� is the class label and takes the value of either 1 or 0—that is, �∈0,1. Our saving formula is then:

Saving(f(S))=Cost(f(S))-�����(�)�����(�)

where �����=�������(�0(�)),����(�1(�)) where �0 predicts class 0, �0, and �1 predicts observations in class 1, �1.

In code, we have the following:

```
In [35]: import joblib
         import sys
         sys.modules['sklearn.externals.joblib'] = joblib
         from costcla.metrics import cost_loss, savings_score
         from costcla.models import BayesMinimumRiskClassifier


In [36]: X_train, X_test, y_train, y_test, cost_mat_train, cost_mat_test = \
         train_test_split(fraud_df_sampled.drop('is_fraud', axis=1),
                                    fraud_df_sampled.is_fraud, cost_mat,
                                    test_size=0.2, random_state=0)

In [37]: saving_models = []
         saving_models.append(('Log. Reg.',
                               LogisticRegression()))
         saving_models.append(('Dec. Tree',
                               DecisionTreeClassifier()))
         saving_models.append(('Random Forest',
                               RandomForestClassifier()))


In [38]: saving_score_base_all = []

         for name, save_model in saving_models:
             sv_model = save_model
             sv_model.fit(X_train, y_train)
             y_pred = sv_model.predict(X_test)
             saving_score_base = savings_score(y_test, y_pred, cost_mat_test) 
             saving_score_base_all.append(saving_score_base)
             print('The saving score for {} is {:.4f}'.
                   format(name, saving_score_base))
             print('--' * 20)
         The saving score for Log. Reg. is -0.5602
         ----------------------------------------
         The saving score for Dec. Tree is 0.6557
         ----------------------------------------
         The saving score for Random Forest is 0.4789
         ----------------------------------------

In [39]: f1_score_base_all = []

         for name, save_model in saving_models:
             sv_model = save_model
             sv_model.fit(X_train, y_train)
             y_pred = sv_model.predict(X_test)
             f1_score_base = f1_score(y_test, y_pred, cost_mat_test) 
             f1_score_base_all.append(f1_score_base)
             print('The F1 score for {} is {:.4f}'.
                   format(name, f1_score_base))
             print('--' * 20)
         The F1 score for Log. Reg. is 0.0000
         ----------------------------------------
         The F1 score for Dec. Tree is 0.7383
         ----------------------------------------
         The F1 score for Random Forest is 0.7068
         ----------------------------------------
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO5-1)

Calculating the saving score

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO5-2)

Calculating the F1 score

**WARNING**

Please note that, if you are using `sklearn` version 0.23 or higher, you need to downgrade it to 0.22 to use `costcla` library. This adjustment is required due to the `sklearn.external.six` package inside the `costcla` library.

[Table 8-5](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#saving\_score) shows that decision tree has the highest saving score among the three models, and interestingly, logistic regression produces a negative saving score, implying that the number of false negative and false positive predictions is quite large, which inflates the denominator of the saving score formula.

| Model               | Saving score | F1 score |
| ------------------- | ------------ | -------- |
| Logistic regression | -0.5602      | 0.0000   |
| Decision tree       | 0.6557       | 0.7383   |
| Random forest       | 0.4789       | 0.7068   |

### Cost-Sensitive Modeling

Thus far, we have discussed the concepts of saving score and cost sensitivity, and now we are ready to run cost-sensitive logistic regression, decision tree, and random forest. The question that we are trying to address here is what happens if fraud is modeled by considering varying costs of misclassification? How does it affect the saving score?

To undertake this investigation, we’ll use the `costcla` library. This library was specifically created to employ the cost-sensitive classifiers in which varying costs of misclassification are considered. Because, as discussed earlier, traditional fraud models assume that all correctly classified and misclassified examples carry the same cost, which is not correct due to the varying costs of misclassification in fraud (Bahnsen 2021).

Having applied the cost-sensitive models, the saving score is used to compare the models in the following code:

```
In [40]: from costcla.models import CostSensitiveLogisticRegression
         from costcla.models import CostSensitiveDecisionTreeClassifier
         from costcla.models import CostSensitiveRandomForestClassifier

In [41]: cost_sen_models = []
         cost_sen_models.append(('Log. Reg. CS',
                                 CostSensitiveLogisticRegression()))
         cost_sen_models.append(('Dec. Tree CS',
                                 CostSensitiveDecisionTreeClassifier()))
         cost_sen_models.append(('Random Forest CS',
                                 CostSensitiveRandomForestClassifier()))

In [42]: saving_cost_all = []

         for name, cost_model in cost_sen_models:
             cs_model = cost_model
             cs_model.fit(np.array(X_train), np.array(y_train),
                          cost_mat_train) 
             y_pred = cs_model.predict(np.array(X_test))
             saving_score_cost = savings_score(np.array(y_test),
                                               np.array(y_pred), cost_mat_test)
             saving_cost_all.append(saving_score_cost)
             print('The saving score for {} is {:.4f}'.
                   format(name, saving_score_cost))
             print('--'*20)
         The saving score for Log. Reg. CS is -0.5906
         ----------------------------------------
         The saving score for Dec. Tree CS is 0.8419
         ----------------------------------------
         The saving score for Random Forest CS is 0.8903
         ----------------------------------------

In [43]: f1_score_cost_all = []

         for name, cost_model in cost_sen_models:
             cs_model = cost_model
             cs_model.fit(np.array(X_train), np.array(y_train),
                          cost_mat_train)
             y_pred = cs_model.predict(np.array(X_test))
             f1_score_cost = f1_score(np.array(y_test),
                                      np.array(y_pred), cost_mat_test)
             f1_score_cost_all.append(f1_score_cost)
             print('The F1 score for {} is {:.4f}'. format(name,
                                                           f1_score_cost))
             print('--'*20)
         The F1 score for Log. Reg. CS is 0.0000
         ----------------------------------------
         The F1 score for Dec. Tree CS is 0.3281
         ----------------------------------------
         The F1 score for Random Forest CS is 0.4012
         ----------------------------------------
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO6-1)

Training the cost-sensitive models by iteration

According to [Table 8-6](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#saving\_score\_model), the best and the worst saving scores are obtained in random forest and logistic regression, respectively. This confirms two important facts: first, it implies that random forest has a low number of inaccurate observations, and second, that those inaccurate observations are less costly. To be precise, modeling fraud with random forest generates a very low number of false negatives, which is the denominator of the saving score formula.

| Model               | Saving score | F1 score |
| ------------------- | ------------ | -------- |
| Logistic regression | -0.5906      | 0.0000   |
| Decision tree       | 0.8414       | 0.3281   |
| Random forest       | 0.8913       | 0.4012   |

### Bayesian Minimum Risk

Bayesian decision can also be used to model fraud taking into account the cost sensitivity. The Bayesian minimum risk method rests on a decision process using different costs (or loss) and probabilities. Mathematically, if the transaction is predicted to be fraud, the overall risk is defined as follows:

�(��|�)=�(��|��)�(��|�)+�(��|��)�(��|�)

On the other hand, if a transaction is predicted to be legitimate, then the overall risk turns out to be:

�(��|�)=�(��|��)�(��|�)+�(��|��)�(��|�)

where �� and �� are the actual classes for fraudulent and legitimate cases, respectively. �(��|��) represents the cost when fraud is detected and the real class is fraud. Similarly, �(��|��) denotes the cost when the transaction is predicted to be legitimate and the real class is legitimate. Conversely, �(��|��) and �(��|��) calculate the cost of the off-diagonal elements in [Table 8-3](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#cost\_sen\_mat). The former calculates the cost when the transaction is predicted to be a fraud but the actual class is not, and the latter shows the cost when the transaction is legitimate but the actual class is fraud. �(��|�) indicates the predicted probability of having a legitimate transaction given _S_ and �(��|�) and the predicted probability of having a fraudulent transaction given _S_.

Alternatively, the Bayesian minimum risk formula can be interpreted as:

�(��|�)=�������(��|�)+�������(��|�)�(��|�)=0+�����(��|�)

with _admin_ is administrative cost and _amt_ is the transaction amount. With that being said, the transaction is labeled as fraud if:

�(��|�)≥�(��|�)

Alternatively:

�������(��|�)+�������(��|�)≥�����(��|�)

Well, it is time to apply the Bayesian Minimum Risk model in Python. Again, three models are employed and compared using F1 score: F1 score results can be found in [Table 8-7](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#f1\_score\_bmr), and it turns out decision tree has the highest F1 score and logistic regression has the lowest one. So, the order of saving scores is other way around, indicating the effectiveness of the cost-sensitive approach:

```
In [44]: saving_score_bmr_all = []

         for name, bmr_model in saving_models:
             f = bmr_model.fit(X_train, y_train)
             y_prob_test = f.predict_proba(np.array(X_test))
             f_bmr = BayesMinimumRiskClassifier() 
             f_bmr.fit(np.array(y_test), y_prob_test)
             y_pred_test = f_bmr.predict(np.array(y_prob_test),
                                         cost_mat_test)
             saving_score_bmr = savings_score(y_test, y_pred_test,
                                              cost_mat_test)
             saving_score_bmr_all.append(saving_score_bmr)
             print('The saving score for {} is {:.4f}'.\
                   format(name, saving_score_bmr))
             print('--' * 20)
         The saving score for Log. Reg. is 0.8064
         ----------------------------------------
         The saving score for Dec. Tree is 0.7343
         ----------------------------------------
         The saving score for Random Forest is 0.9624
         ----------------------------------------

In [45]: f1_score_bmr_all = []

         for name, bmr_model in saving_models:
             f = bmr_model.fit(X_train, y_train)
             y_prob_test = f.predict_proba(np.array(X_test))
             f_bmr = BayesMinimumRiskClassifier()
             f_bmr.fit(np.array(y_test), y_prob_test)
             y_pred_test = f_bmr.predict(np.array(y_prob_test),
                                         cost_mat_test)
             f1_score_bmr = f1_score(y_test, y_pred_test)
             f1_score_bmr_all.append(f1_score_bmr)
             print('The F1 score for {} is {:.4f}'.\
                   format(name, f1_score_bmr))
             print('--'*20)
         The F1 score for Log. Reg. is 0.1709
         ----------------------------------------
         The F1 score for Dec. Tree is 0.6381
         ----------------------------------------
         The F1 score for Random Forest is 0.4367
         ----------------------------------------
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO7-1)

Calling the Bayesian Minimum Risk Classifier library

| Model               | Saving score | F1 score |
| ------------------- | ------------ | -------- |
| Logistic regression | 0.8064       | 0.1709   |
| Decision tree       | 0.7343       | 0.6381   |
| Random forest       | 0.9624       | 0.4367   |

To create a plot of this data, we do the following (resulting in [Figure 8-6](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#score\_barplot\_fraud)):

```
In [46]: savings = [saving_score_base_all, saving_cost_all, saving_score_bmr_all]
         f1 = [f1_score_base_all, f1_score_cost_all, f1_score_bmr_all]
         saving_scores = pd.concat([pd.Series(x) for x in savings])
         f1_scores = pd.concat([pd.Series(x) for x in f1])
         scores = pd.concat([saving_scores, f1_scores], axis=1)
         scores.columns = ['saving_scores', 'F1_scores']

In [47]: model_names = ['Log. Reg_base', 'Dec. Tree_base', 'Random Forest_base',
                        'Log. Reg_cs', 'Dec. Tree_cs', 'Random Forest_cs',
                       'Log. Reg_bayes', 'Dec. Tree_bayes',
                        'Random Forest_bayes']

In [48]: plt.figure(figsize=(10, 6))
         plt.plot(range(scores.shape[0]), scores["F1_scores"],
                  "--", label='F1Score') 
         plt.bar(np.arange(scores.shape[0]), scores['saving_scores'],
                 0.6, label='Savings') 
         _ = np.arange(len(model_names))
         plt.xticks(_, model_names)
         plt.legend(loc='best')
         plt.xticks(rotation='vertical')
         plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO8-1)

Drawing the F1 score with a line plot

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO8-2)

Drawing the bar plot based on the models used

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0806.png" alt="f1_saving" height="454" width="600"><figcaption></figcaption></figure>

**Figure 8-6. F1 and saving scores**

[Figure 8-6](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#score\_barplot\_fraud) shows the F1 and saving scores across the models we have employed so far. Accordingly, the cost-sensitive and Bayesian minimum risk model outperform the base models, as expected.

## Unsupervised Learning Modeling for Fraud Examination

Unsupervised learning models are also used to detect fraudulent activities in a way that extracts the hidden characteristics of the data. The most prominent advantage of this method over the supervised model is that there is no need to apply a sampling procedure to fix the imbalanced-data problem. Unsupervised models, by their nature, do not require any prior knowledge about the data. To see how unsupervised learning models perform on this type of data, we will explore the self-organizing map (SOM) and autoencoder models.

### Self-Organizing Map

SOM is an unsupervised method to obtain a low-dimensional space from a high-dimensional space. This is a method that was introduced by Finnish scholar Teuvo Kohonen in 1980s and it became widespread. SOM is a type of artificial NN, and therefore it rests on competitive learning in the sense that output neurons compete to be activated. The activated neuron is referred to as the _winning neuron_, and each neuron has neighboring weights, so it is the spatial locations of the nodes in the output space that are indicative of the inherent statistical features in the input space (Haykin 1999).

The most distinctive features of SOM methods are as follows (Asan and Ercan 2012):

* No assumptions regarding the distribution of variables
* Dependent structure among variables
* Dealing with nonlinear structure
* Coping with noisy and missing data

Let’s walk through the important steps of the SOM technique. As you might have guessed, the first step is to identify the winning node, or the activated neuron. The winning node is identified by distance metrics—that is, Manhattan, Chebyshev, and Euclidean distances. Of these distance metrics, Euclidean distance is the most commonly used because it works well under the gradient descent process. Thus, given the following Euclidean formula, we can find the distance between sample and weight:

(��-��(�))=∑�=1�(���-����)2,�=1,2,...,�

where _x_ is sample, _w_ is weight, and the winning node, _k(t)_, is shown in [Equation 8-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#winning\_node).

**Equation 8-1. Identifying the winning node**

�(�)=argmin�(�)-�)�(�)

The other important step is to update the weight. Given the learning rate and neighborhood size, the following update is applied:

��(�+1)=��(�)+�\[�(�)-��(�)]

where ��(�) is the weight of the winning neuron _i_ at ��ℎ iteration, and � is the learning rate.

Richardson, Risien, and Shillington (2003) state that the rate of adaptation of the weights decays as it moves away from the winning node. This is defined by neighborhood function, ℎ��(�), where _i_ is index of the neighbor. Of the neighborhood functions, the most famous one is the Gaussian function with the following form:

ℎ��(�)=���(-���22�2(�))

where ���2 denotes the distance between the winning neuron and the related neuron, and �2(�) denotes the radius at iteration _t_.

Considering all this, the updating process becomes what’s shown in [Equation 8-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#weightupdate2).

**Equation 8-2. Updating the weight**

��(�+1)=��(�)+�ℎ��(�)\[�(�)-��(�)]

That’s all there is to it, but I’m aware that the process is a bit tedious. So let us summarize the steps:

1. Initialize the weights: assigning random values to weights is the most common approach.
2. Find the winning neuron using [Equation 8-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#winning\_node).
3. Update the weights as given in [Equation 8-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#weightupdate2).
4. Adjust the parameters based on the results of [Equation 8-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#weightupdate2) by setting _t_ to _t_ + 1.

We already know that there are two classes in the fraud data that we use, so the dimensions for our self organizing map should have a two-by-one structure. You can find the application in the following code:

```
In [49]: from sklearn.preprocessing import StandardScaler
         standard = StandardScaler()
         scaled_fraud = standard.fit_transform(X_under)

In [50]: from sklearn_som.som import SOM
         som = SOM(m=2, n=1, dim=scaled_fraud.shape[1]) 
         som.fit(scaled_fraud)
         predictions_som = som.predict(np.array(scaled_fraud))

In [51]: predictions_som = np.where(predictions_som == 1, 0, 1)

In [52]: print('Classification report:\n',
               classification_report(y_under, predictions_som))
         Classification report:
                        precision    recall  f1-score   support

                    0       0.56      0.40      0.47      7506
                    1       0.53      0.68      0.60      7506

             accuracy                           0.54     15012
            macro avg       0.54      0.54      0.53     15012
         weighted avg       0.54      0.54      0.53     15012
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO9-1)

Configuring the SOP

Having checked the classification report, it becomes obvious that the F1 score is somewhat similar to what we found with the other methods. This confirms that the SOM is a useful model in detecting fraud when we don’t have labeled data. In the following code, we generate [Figure 8-7](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#som\_pred), which shows the actual and predicted classes:

```
In [53]: fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 6))
         x = X_under.iloc[:,0]
         y = X_under.iloc[:,1]

         ax[0].scatter(x, y, alpha=0.1, cmap='Greys', c=y_under)
         ax[0].title.set_text('Actual Classes')
         ax[1].scatter(x, y, alpha=0.1, cmap='Greys', c=predictions_som)
         ax[1].title.set_text('SOM Predictions')
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0807.png" alt="som_prediction" height="455" width="600"><figcaption></figcaption></figure>

**Figure 8-7. SOM prediction**

### Autoencoders

An _autoencoder_ is an unsupervised deep learning model trained to transform inputs into outputs via a hidden layer. However, the network structure of autoencoder is different from other structures in the sense that autoencoder consists of two parts: an _encoder_ and a _decoder_.

The encoder serves as a feature extraction function, and the decoder works as a reconstruction function. To illustrate, let _x_ be an input and _h_ be a hidden layer. Then, the encoder function is _h_ = �(�), and the decoder function reconstructs by _r_ = �(ℎ). If an autoencoder learns by simple copying, i.e., �(�(�)) = _x_, it is not an ideal situation in that the autoencoder seeks feature extraction. This amounts to copying only the relevant aspects of the input (Goodfellow et al. 2016).

Consequently, autoencoder has a network structure such that it compresses knowledge in a way to have a lower-dimensional representation of the original input. Given the encoder and decoder functions, there are different types of autoencoders. Of them, we’ll discuss the three most commonly used autoencoders to keep ourselves on track:

* Undercomplete autoencoders
* Sparse autoencoders
* Denoising autoencoders

#### Undercomplete autoencoders

This is the most basic type of autoencoder, as the hidden layer, _h_, has a smaller dimension than training data, _x_. So the number of neurons is less than that of the training data. The aim of this autoencoder is to capture the latent attribute of the data by minimizing the loss function—that is, 𝕃(�,�(�(�))), where 𝕃 is the loss function.

Autoencoders famously face a trade-off in ML known as the bias-variance trade-off, in which autoencoders aim to reconstruct the input well while having low-dimensional representations. To remedy this issue, we’ll introduce sparse and denoising autoencoders.

#### Sparse autoencoder

Sparse autoencoders suggest a solution to this trade-off by imposing sparsity on the reconstruction error. There are two ways to enforce regularization in sparce autoencoders. The first way is to apply �1 regularization. In this case, the autoencoders optimization becomes (Banks, Koenigstein, and Giryes 2020):

argmin�,�𝕃(�,�(�(�)))+�(ℎ)

where �(�(�)) is the decoder, and _h_ is the encoder outputs. [Figure 8-8](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#sparse\_auto) illustrates the sparse autoencoder.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0808.png" alt="sparse_auto" height="222" width="600"><figcaption></figcaption></figure>

**Figure 8-8. Sparse autoencoder model stucture**

The second way to regularize the sparse autoencoders is with Kullback-Leibler (KL) divergence, which tells us the similarity of the two probability distributions simply by measuring the distance between them. KL divergence can be put mathematically as:

𝕃(�,�^)+∑���(��^)

where � and �^ are ideal and observed distributions, respectively.

#### Denoising autoencoders

The idea behind denoising autoencoders is that instead of using a penalty term, �, add noise to the input data and learn from this changed construction—that is, reconstruction. Thus, instead of minimizing 𝕃(�,�(�(�))), denoising autoencoders offer to minimize the following loss function:

𝕃(�,�(�(�^)))

where �^ is the corrupted input obtained by adding noise by, for instance, Gaussian noise. [Figure 8-9](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#corrupt\_noise) illustrates this process.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0809.png" alt="corrupt" height="455" width="548"><figcaption></figcaption></figure>

**Figure 8-9. Denoising autoencoder model structure**

In the following code, we’ll use an autoencoder model with Keras. Before moving forward, it is scaled using Standard Scaler, and then, using a batch size of 200 and an epoch number of 100, we are able to get a satisfactory prediction result. We’ll then create a reconstruction error table from the autoencoder model to compare with the true class, and it turns out that the means and standard deviations of these models are close to each other:

```
In [54]: from sklearn.preprocessing import StandardScaler
         from tensorflow import keras
         from tensorflow.keras.layers import Dense, Dropout
         from keras import regularizers

In [55]: fraud_df[['amt','city_pop','hour']] = StandardScaler().\
         fit_transform(fraud_df[['amt','city_pop','hour']])

In [56]: X_train, X_test = train_test_split(fraud_df,
                                            test_size=0.2, random_state=123)
         X_train[X_train['is_fraud'] == 0]
         X_train = X_train.drop(['is_fraud'], axis=1).values
         y_test = X_test['is_fraud']
         X_test = X_test.drop(['is_fraud'], axis=1).values


In [57]: autoencoder = keras.Sequential()
         autoencoder.add(Dense(X_train_under.shape[1], activation='tanh',
                               activity_regularizer=regularizers.l1(10e-5),
                               input_dim= X_train_under.shape[1]))
         #encoder
         autoencoder.add(Dense(64, activation='tanh')) 
         autoencoder.add(Dense(32, activation='relu')) 
         #decoder
         autoencoder.add(Dense(32, activation='elu')) 
         autoencoder.add(Dense(64,activation='tanh')) 
         autoencoder.add(Dense(X_train_under.shape[1], activation='elu'))
         autoencoder.compile(loss='mse',
                             optimizer='adam')
         autoencoder.summary();
         Model: "sequential"
         _________________________________________________________________
         Layer (type)                 Output Shape              Param #
         =================================================================
         dense (Dense)                (None, 566)               320922
         _________________________________________________________________
         dense_1 (Dense)              (None, 64)                36288
         _________________________________________________________________
         dense_2 (Dense)              (None, 32)                2080
         _________________________________________________________________
         dense_3 (Dense)              (None, 32)                1056
         _________________________________________________________________
         dense_4 (Dense)              (None, 64)                2112
         _________________________________________________________________
         dense_5 (Dense)              (None, 566)               36790
         =================================================================
         Total params: 399,248
         Trainable params: 399,248
         Non-trainable params: 0
         _________________________________________________________________
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO10-1)

Identifying 64 and 32 hidden layers in the encoder and decoder parts, respectively

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO10-2)

Identifying 32 and 64 hidden layers in the encoder and decoder parts, respectively

After configuring the autoencoder model, the next step is to fit and predict. After doing the prediction, we check the quality of the model using summary statistics, as they are a reliable way to see whether reconstruction works well:

```
In [58]: batch_size = 200
         epochs = 100

In [59]: history = autoencoder.fit(X_train, X_train,
                                   shuffle=True,
                                   epochs=epochs,
                                   batch_size=batch_size,
                                   validation_data=(X_test, X_test),
                                   verbose=0).history

In [60]: autoencoder_pred = autoencoder.predict(X_test)
         mse = np.mean(np.power(X_test - autoencoder_pred, 2), axis=1)
         error_df = pd.DataFrame({'reconstruction_error': mse,
                                 'true_class': y_test}) 
         error_df.describe()
Out[60]:        reconstruction_error     true_class
         count         259335.000000  259335.000000
         mean               0.002491       0.005668
         std                0.007758       0.075075
         min                0.000174       0.000000
         25%                0.001790       0.000000
         50%                0.001993       0.000000
         75%                0.003368       0.000000
         max                2.582811       1.000000
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#co\_modeling\_operational\_risk\_CO11-1)

Creating a table named `error_df` to compare the results obtained from the model with the real data

Finally, we create our plot ([Figure 8-10](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#autoencoder\_fraud)):

```
In [61]: plt.figure(figsize=(10, 6))
         plt.plot(history['loss'], linewidth=2, label='Train')
         plt.plot(history['val_loss'], linewidth=2, label='Test')
         plt.legend(loc='upper right')
         plt.title('Model loss')
         plt.ylabel('Loss')
         plt.xlabel('Epoch')
         plt.show()
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0810.png" alt="autoencoder_fraud" height="364" width="600"><figcaption></figcaption></figure>

**Figure 8-10. Autoencoder performance**

[Figure 8-10](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#autoencoder\_fraud) shows the results of our autoencoder modeling using a line plot, and we can see that the test loss result is more volatile than that of train but, on average, the mean loss is similar.

## Conclusion

Fraud is a hot topic in finance for several reasons. Strict regulation, reputation loss, and costs arising from fraud are the primary reasons to fight it. Until recently, fraud has been a big problem for financial institutions, as modeling fraud had not produced satisfactory results and, because of this, financial institutions had to employ more resources to handle this phenomenon. Thanks to recent advancements in ML, we now have various tools at our disposal for combatting fraud, and this chapter was dedicated to introducing these models and comparing their results. These models ranged from parametric approaches such as logistic regression to deep learning models such as autoencoders.

In the next chapter, we’ll look at a rather different financial risk model known as stock price crash risk, which will enable us to gain insight about the well-being of corporate governance. This is an important tool for financial risk management because risk management is ultimately rooted in corporate management. It would be naive to expect low risk in a company with bad corporate governance.

## References

Articles cited in this chapter:

* Asan, Umut, and Secil Ercan. 2012. “An Introduction to Self-Organizing Maps.” In _Computational Intelligence Systems in Industrial Engineering_, edited by Cengiz Kahraman. 295-315. Paris: Atlantis Press
* Bahnsen, Alejandro Correa, Djamia Aouada, and Björn Ottersten. 2014. “Example-Dependent Cost-Sensitive Logistic Regression for Credit Scoring.” In _The 13th International Conference on Machine Learning and Applications_, pp. 263-269. IEEE.
* Bank, Dor, Noam Koenigstein, and Raja Giryes. 2020. “Autoencoders.” arXiv preprint arXiv:2003.05991.
* Dunnett, Robert S., Cindy B. Levy, and Antonio P. Simoes. 2005. “The Hidden Costs of Operational Risk.” McKinsey St Company.
* Richardson, Anthony J., C. Risien, and Frank Alan Shillington. 2003. “Using Self-Organizing Maps to Identify Patterns in Satellite Imagery.” Progress in Oceanography 59 (2-3): 223-239.

Books and online resources cited in this chapter:

* Bahnsen, Alejandro Correa. 2021. “Introduction to Example-Dependent Cost-Sensitive Classification.” [_https://oreil.ly/5eCsJ_](https://oreil.ly/5eCsJ).
* Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. _Deep Learning_. Cambridge: MIT press.
* Nilsen. 2020. “Card Fraud Losses Reach $28.65 Billion.” Nilsen Report. [_https://oreil.ly/kSls7_](https://oreil.ly/kSls7).
* Office of the Comptroller of the Currency. 2019. “Operational Risk: Fraud Risk Management Principles.” CC Bulletin. [_https://oreil.ly/GaQez_](https://oreil.ly/GaQez).
* Simon, Haykin. 1999. _Neural Networks: A Comprehensive Foundation_, second edition. Englewood Cliffs, New Jersey: Prentice-Hall.
