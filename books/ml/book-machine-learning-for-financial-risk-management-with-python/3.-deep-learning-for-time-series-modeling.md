# 3. Deep Learning for Time Series Modeling

## Chapter 3. Deep Learning for Time Series Modeling

> ...Yes, it is true that a Turing machine can compute any computable function given enough memory and enough time, but nature had to solve problems in real time. To do this, it made use of the brain’s neural networks that, like the most powerful computers on the planet, have massively parallel processors. Algorithms that run efficiently on them will eventually win out.
>
> &#x20;Terrence J. Sejnowski (2018)

_Deep learning_ has recently become a buzzword for some good reasons, although recent attempts to improve deep learning practices are not the first of their kind. However, it is quite understandable why deep learning has been appreciated for nearly two decades. Deep learning is an abstract concept, which makes it hard to define in few of words. Unlike a neural network (NN), deep learning has a more complex structure, and hidden layers define the complexity. Therefore, some researchers use the number of hidden layers as a comparison benchmark to distinguish a neural network from deep learning, a useful but not particularly rigorous way to make this distinction. A better definition can clarify the difference.

At a high level, deep learning can be defined:

> Deep learning methods are representation-learning[1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#idm45737247845584) methods with multiple levels of representation, obtained by composing simple but nonlinear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level.
>
> Le Cunn et al. (2015)

Applications of deep learning date back to the 1940s, when _Cybernetics_ by Norbert Wiener was published. Connectivist thinking then dominated between the 1980s and 1990s. Recent developments in deep learning, such as backpropagation and neural networks, have created the field as we know it. Basically, there have been three waves of deep learning, so we might wonder why deep learning is in its heyday _now_? Goodfellow et al. (2016) list some plausible reasons, including:

* Increasing data sizes
* Increasing model sizes
* Increasing accuracy, complexity, and real-world impact

It seems like modern technology and data availability have paved the way for an era of deep learning in which new data-driven methods are proposed so that we are able to model time series using unconventional models. This development has given rise to a new wave of deep learning. Two methods stand out in their ability to include longer time periods: the _recurrent neural network_ (RNN) and _long short-term memory_ (LSTM). In this section, we will concentrate on the practicality of these models in Python after briefly discussing the theoretical background.

## Recurrent Neural Networks

An RNN has a neural network structure with at least one feedback connection so that the network can learn sequences. A feedback connection results in a loop, enabling us to unveil the nonlinear characteristics. This type of connection brings us a new and quite useful property: _memory_. Thus, an RNN can make use not only of the input data but also the previous outputs, which sounds compelling when it comes to time series modeling.

RNNs come in many forms, such as:

One-to-one

A one-to-one RNN consists of a single input and a single output, which makes it the most basic type of RNN.

One-to-many

In this form, an RNN produces multiple outputs for a single input.

Many-to-one

As opposed to the one-to-many structure, many-to-one has multiple inputs for a single output.

Many-to-many

This structure has multiple inputs and outputs and is known as the most complicated structure for an RNN.

A hidden unit in an RNN feeds itself back into the neural network so that the RNN has recurrent layers (unlike a feed-forward neural network) making it a suitable method for modeling time series data. Therefore, in RNNs, activation of a neuron comes from a previous time-step indication that the RNN represents as an accumulating state of the network instance (Buduma and Locascio 2017).

As summarized by Nielsen (2019):

* RNNs have time steps one at a time in an orderly fashion.
* The state of the network stays as it is from one time step to another.
* An RNN updates its state based on the time step.

These dimensions are illustrated in [Figure 3-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#RNN\_dimensions). As can be seen, the RNN structure on the right-hand side has a time step, which is the main difference between it and the feed-forward network.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0301.png" alt="RNN structure" height="280" width="600"><figcaption></figcaption></figure>

**Figure 3-1. RNN structure**[**2**](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#idm45737247426656)

RNNs have a three-dimensional input, comprised of:

* Batch size
* Time steps
* Number of features

_Batch size_ denotes the number of observations or number of rows of data. _Time steps_ are the number of times to feed the model. Finally, _number of features_ is the number of columns of each sample.

We’ll start with the following code:

```
In [1]: import numpy as np
        import pandas as pd
        import math
        import datetime
        import yfinance as yf
        import matplotlib.pyplot as plt
        import tensorflow as tf
        from tensorflow.keras.models import Sequential
        from tensorflow.keras.callbacks import EarlyStopping
        from tensorflow.keras.layers import (Dense, Dropout,
                                             Activation, Flatten,
                                             MaxPooling2D, SimpleRNN)
        from sklearn.model_selection import train_test_split

In [2]: n_steps = 13 
        n_features = 1 

In [3]: model = Sequential() 
        model.add(SimpleRNN(512, activation='relu',
                            input_shape=(n_steps, n_features),
                            return_sequences=True)) 
        model.add(Dropout(0.2)) 
        model.add(Dense(256, activation = 'relu')) 
        model.add(Flatten()) 
        model.add(Dense(1, activation='linear')) 

In [4]: model.compile(optimizer='rmsprop',
                      loss='mean_squared_error',
                      metrics=['mse']) 

In [5]: def split_sequence(sequence, n_steps):
            X, y = [], []
            for i in range(len(sequence)):
                end_ix = i + n_steps
                if end_ix > len(sequence) - 1:
                    break
                seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
                X.append(seq_x)
                y.append(seq_y)
            return np.array(X), np.array(y) 
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO1-1)

Defining the number of steps for prediction

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO1-2)

Defining the number of features as 1

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO1-3)

Calling a sequential model to run the RNN

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO1-4)

Identifying the number of hidden neurons, activation function, and input shape

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO1-5)

Adding a dropout layer to prevent overfitting

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO1-6)

Adding one more hidden layer with 256 neurons with the `relu` activation function

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/7.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO1-7)

Flattening the model to transform the three-dimensional matrix into a vector

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/8.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO1-8)

Adding an output layer with `linear` activation function

[![9](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/9.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO1-9)

Compiling the RNN model

[![10](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/10.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO1-10)

Creating a dependent variable `y`

**ACTIVATION FUNCTIONS**

Activation functions are mathematical equations that are used to determine the output in a neural network structure. These tools introduce nonlinearity in the hidden layers so that we are able to model the nonlinear issues.

The following are the most famous activation functions:

Sigmoid

This activation function allows us to incorporate a small amount of output as we introduce small changes in the model. It takes values between 0 and 1. The mathematical representation of sigmoid is:

sigmoid(�)=11+���(-∑�����-�)

where _w_ is weight, _x_ denotes data, _b_ represents bias, and subscript _i_ shows features.

Tanh

If you are handling negative numbers, tanh is your activation function. As opposed to the sigmoid function, it ranges between -1 and 1. The tanh formula is:

tanh(�)=���ℎ(�)���ℎ(�)Linear

Using the linear activation function enables us to build linear relationships between independent and dependent variables. The linear activation function takes the inputs and multiplies by the weights to form the outputs proportional to the inputs. It is a convenient activation function for time-series models. Linear activation functions take the form of:

�(�)=��Rectified linear

The rectified linear activation function, known as ReLu, can take 0 if the input is zero or below zero. If the input is greater than 0, it goes up in line with _x_. Mathematically:

ReLu(x)=max(0,�)Softmax

Like sigmoid, this activation function is widely applicable to classification problems because softmax converts input into probabilistic distribution proportional to the exponential of the input numbers:

softmax(��)=���(��)∑����(��)

After configuring the model and generating a dependent variable, let’s extract the data and run the prediction for the stock prices for both Apple and Microsoft:

```
In [6]: ticker = ['AAPL', 'MSFT']
        start = datetime.datetime(2019, 1, 1)
        end = datetime.datetime(2020, 1 ,1)
        stock_prices = yf.download(ticker,start=start, end = end, interval='1d')\
                       .Close
        [*********************100%***********************]  2 of 2 completed

In [7]: diff_stock_prices = stock_prices.diff().dropna()

In [8]: split = int(len(diff_stock_prices['AAPL'].values) * 0.95)
        diff_train_aapl = diff_stock_prices['AAPL'].iloc[:split]
        diff_test_aapl = diff_stock_prices['AAPL'].iloc[split:]
        diff_train_msft = diff_stock_prices['MSFT'].iloc[:split]
        diff_test_msft = diff_stock_prices['MSFT'].iloc[split:]

In [9]: X_aapl, y_aapl = split_sequence(diff_train_aapl, n_steps) 
        X_aapl = X_aapl.reshape((X_aapl.shape[0],  X_aapl.shape[1],
                                 n_features)) 

In [10]: history = model.fit(X_aapl, y_aapl,
                             epochs=400, batch_size=150, verbose=0,
                             validation_split = 0.10) 

In [11]: start = X_aapl[X_aapl.shape[0] - n_steps] 
         x_input = start 
         x_input = x_input.reshape((1, n_steps, n_features))

In [12]: tempList_aapl = [] 
         for i in range(len(diff_test_aapl)):
             x_input = x_input.reshape((1, n_steps, n_features)) 
             yhat = model.predict(x_input, verbose=0) 
             x_input = np.append(x_input, yhat)
             x_input = x_input[1:]
             tempList_aapl.append(yhat) 

In [13]: X_msft, y_msft = split_sequence(diff_train_msft, n_steps)
         X_msft = X_msft.reshape((X_msft.shape[0],  X_msft.shape[1],
                                  n_features))

In [14]: history = model.fit(X_msft, y_msft,
                             epochs=400, batch_size=150, verbose=0,
                             validation_split = 0.10)

In [15]: start = X_msft[X_msft.shape[0] - n_steps]
         x_input = start
         x_input = x_input.reshape((1, n_steps, n_features))

In [16]: tempList_msft = []
         for i in range(len(diff_test_msft)):
             x_input = x_input.reshape((1, n_steps, n_features))
             yhat = model.predict(x_input, verbose=0)
             x_input = np.append(x_input, yhat)
             x_input = x_input[1:]
             tempList_msft.append(yhat)
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO2-1)

Calling the `split_sequence` function to define the lookback period

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO2-2)

Reshaping training data into a three-dimensional case

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO2-3)

Fitting the RNN model to Apple’s stock price

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO2-4)

Defining the starting point of the prediction for Apple

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO2-5)

Renaming the variable

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO2-6)

Creating an empty list to store predictions

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/7.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO2-7)

Reshaping the `x_input`, which is used for prediction

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/8.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO2-8)

Running prediction for Apple stock

[![9](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/9.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO2-9)

Storing `yhat` into `tempList_aapl`

For the sake of visualization, the following code block is used, resulting in [Figure 3-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#rnn):

```
In [17]: fig, ax = plt.subplots(2,1, figsize=(18,15))
         ax[0].plot(diff_test_aapl, label='Actual Stock Price', linestyle='--')
         ax[0].plot(diff_test_aapl.index, np.array(tempList_aapl).flatten(),
                    linestyle='solid', label="Prediction")
         ax[0].set_title('Predicted Stock Price-Apple')
         ax[0].legend(loc='best')
         ax[1].plot(diff_test_msft, label='Actual Stock Price', linestyle='--')
         ax[1].plot(diff_test_msft.index,np.array(tempList_msft).flatten(),
                    linestyle='solid', label="Prediction")
         ax[1].set_title('Predicted Stock Price-Microsoft')
         ax[1].legend(loc='best')


         for ax in ax.flat:
             ax.set(xlabel='Date', ylabel='$')
         plt.show()
```

[Figure 3-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#rnn) shows the stock price prediction results for Apple and Microsoft. Simply eyeballing this, we can readily observe that there is room for improvement in terms of predictive performance of the model in both cases.

Even if we can have satisfactory predictive performance, the drawbacks of the RNN model should not be overlooked. The main drawbacks of the model are:

* The vanishing or exploding gradient problem (please see the following note for a detailed explanation).
* Training an RNN is a very difficult task as it requires a considerable amount of data.
* An RNN is unable to process very long sequences when the _tanh_ activation function is used.

**NOTE**

A vanishing gradient is a commonplace problem in deep learning scenarios that are not properly designed. The vanishing gradient problem arises if the gradient tends to get smaller as we conduct the backpropagation. It implies that neurons are learning so slowly that optimization grinds to a halt.

Unlike the vanishing gradient problem, the exploding gradient problem occurs when small changes in the backpropagation results in huge updates to the weights during the optimization process.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0302.png" alt="RNN predictions" height="491" width="600"><figcaption></figcaption></figure>

**Figure 3-2. RNN prediction results**

The drawbacks of RNNs are well stated by Haviv et al. (2019):

> This is due to the dependency of the network on its past states, and through them on the entire input history. This ability comes with a cost—RNNs are known to be hard to train (Pascanu et al. 2013a). This difficulty is commonly associated with the vanishing gradient that appears when trying to propagate errors over long times (Hochreiter 1998). When training is successful, the network’s hidden state represents these memories. Understanding how such representation forms throughout training can open new avenues for improving learning of memory-related tasks.

## Long-Short Term Memory

The LSTM deep learning approach was developed by Hochreiter and Schmidhuber (1997) and is mainly based on the _gated recurrent unit_ (GRU).

GRU was proposed to deal with the vanishing gradient problem, which is common in neural network structures and occurs when the weight update becomes too small to create a significant change in the network. GRU consists of two gates: _update_ and _reset_. When an early observation is detected as highly important, then we do not update the hidden state. Similarly, when early observations are not significant, that leads to resetting the state.

As previously discussed, one of the most appealing features of an RNN is its ability to connect past and present information. However, this ability turns out to be a failure when _long-term dependencies_ comes into the picture. Long-term dependencies mean that the model learns from early observations.

For instance, let’s examine the following sentence:

_Countries have their own currencies as in the USA, where people transact with dollars…_

In the case of short-term dependencies, it is known that the next predicted word is about a currency, but what if it is asked _which_ currency it’s about? Things get complicated because we might have mentioned various currencies earlier on in the text, implying long-term dependencies. It is necessary to go way back to find something relevant about the countries using dollars.

LSTM tries to attack the weakness of RNN regarding long-term dependencies. LSTM has a quite useful tool to get rid of the unnecessary information so that it works more efficiently. LSTM works with gates, enabling it to forget irrelevant data. These gates are:

* Forget gates
* Input gates
* Output gates

Forget gates are created to sort out the necessary and unnecessary information so that LSTM performs more efficiently than RNN. In doing so, the value of the activation function, _sigmoid_, becomes zero if the information is irrelevant. Forget gates can be formulated as:

��=�(����+ℎ�-1��+��)

where � is the activation function, ℎ�-1 is the previous hidden state, �� and �� are weights, and finally, �� is the bias parameter in the forget cell.

Input gates are fed by the current timestep, ��, and the hidden state of the previous timestep, �-1. The goal of input gates is to determine the extent that information should be added to the long-term state. The input gate can be formulated like this:

��=�(����+ℎ�-1��+��)

Output gates basically determine the extent of the output that should be read, and work as follows:

��=�(����+ℎ�-1��+��)

These gates are not the sole components of LSTM. The other components are:

* Candidate memory cell
* Memory cell
* Hidden state

Candidate memory cell determines the extent to which information passes to the cell state. Differently, the activation function in the candidate cell is tanh and takes the following form:

��^=�(����+ℎ�-1��+��)

Memory cell allows LSTM to remember or to forget the information:

��=��⊙�+�-1+��⊙��^

where ⊙ is Hadamard product.

In this recurrent network, hidden state is a tool to circulate information. Memory cell relates output gate to hidden state:

ℎ�=�(��)⊙��

[Figure 3-3](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#LSTM\_structure) exhibits the LSTM structure.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0303.png" alt="LSTM_structure" height="326" width="600"><figcaption></figcaption></figure>

**Figure 3-3. LSTM structure**

Now, let’s predict the stock prices using LSTM:

```
In [18]: from tensorflow.keras.layers import LSTM


In [19]: n_steps = 13 
         n_features = 1 

In [20]: model = Sequential()
         model.add(LSTM(512, activation='relu',
                   input_shape=(n_steps, n_features),
                   return_sequences=True)) 
         model.add(Dropout(0.2)) 
         model.add(LSTM(256,activation='relu')) 
         model.add(Flatten())
         model.add(Dense(1, activation='linear')) 

In [21]: model.compile(optimizer='rmsprop', loss='mean_squared_error',
                       metrics=['mse']) 

In [22]: history = model.fit(X_aapl, y_aapl,
                             epochs=400, batch_size=150, verbose=0,
                             validation_split = 0.10) 

In [23]: start = X_aapl[X_aapl.shape[0] - 13]
         x_input = start
         x_input = x_input.reshape((1, n_steps, n_features))
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO3-1)

Defining the number of steps for prediction

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO3-2)

Defining the number of feature as 1

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO3-3)

Identifying the number of hidden neurons, the activation function, which is `relu`, and input shape

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO3-4)

Adding a dropout layer to prevent overfitting

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO3-5)

Adding one more hidden layer with 256 neurons, with a `relu` activation function

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO3-6)

Flattening the model to vectorize the three-dimensional matrix

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/7.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO3-7)

Adding an output layer with a `linear` activation function

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/8.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO3-8)

Compiling LSTM with Root Mean Square Propagation, `rmsprop`, and mean squared error (MSE), `mean_squared_error`

[![9](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/9.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#co\_deep\_learning\_for\_time\_series\_modeling\_CO3-9)

Fitting the LSTM model to Apple’s stock price

**NOTE**

Root Mean Square Propagation (`RMSProp`) is an optimization method in which we calculate the moving average of the squared gradients for each weight. We then find the difference of weight, which is to be used to compute the new weight:

��=���-1+1-���2���=-��+�����+1=��+���

Pursuing the same procedure and given the Microsoft stock price, a prediction analysis is carried out:

```
In [24]: tempList_aapl = []
         for i in range(len(diff_test_aapl)):
             x_input = x_input.reshape((1, n_steps, n_features))
             yhat = model.predict(x_input, verbose=0)
             x_input = np.append(x_input, yhat)
             x_input = x_input[1:]
             tempList_aapl.append(yhat)

In [25]: history = model.fit(X_msft, y_msft,
                             epochs=400, batch_size=150, verbose=0,
                             validation_split = 0.10)

In [26]: start = X_msft[X_msft.shape[0] - 13]
         x_input = start
         x_input = x_input.reshape((1, n_steps, n_features))

In [27]: tempList_msft = []
         for i in range(len(diff_test_msft)):
             x_input = x_input.reshape((1, n_steps, n_features))
             yhat = model.predict(x_input, verbose=0)
             x_input = np.append(x_input, yhat)
             x_input = x_input[1:]
             tempList_msft.append(yhat)
```

The following code creates the plot ([Figure 3-4](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#LSTM)) that shows the prediction results:

```
In [28]: fig, ax = plt.subplots(2, 1, figsize=(18, 15))
         ax[0].plot(diff_test_aapl, label='Actual Stock Price', linestyle='--')
         ax[0].plot(diff_test_aapl.index, np.array(tempList_aapl).flatten(),
                    linestyle='solid', label="Prediction")
         ax[0].set_title('Predicted Stock Price-Apple')
         ax[0].legend(loc='best')
         ax[1].plot(diff_test_msft, label='Actual Stock Price', linestyle='--')
         ax[1].plot(diff_test_msft.index, np.array(tempList_msft).flatten(),
                    linestyle='solid', label="Prediction")
         ax[1].set_title('Predicted Stock Price-Microsoft')
         ax[1].legend(loc='best')

         for ax in ax.flat:
             ax.set(xlabel='Date', ylabel='$')
         plt.show()
```

LSTM seems to outperform the RNN, particularly in the way it captures the extreme values better.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0304.png" alt="LSTM predictions" height="488" width="600"><figcaption></figcaption></figure>

**Figure 3-4. LSTM prediction results**

## Conclusion

This chapter was about predicting stock prices based on deep learning. The models used are RNN and LSTM, which have the ability to process longer time periods. These models do not suggest remarkable improvement but still can be employed to model time series data. LSTM considers, in our case, a 13-step lookback period for prediction. For an extension, it would be a wise approach to include multiple features in the models based on deep learning, which is not allowed in parametric time series models.

In the next chapter, we will discuss volatility predictions based on parametric and ML models so that we can compare their performance.

## References

Articles cited in this chapter:

* Ding, Daizong, et al. 2019. “Modeling Extreme Events in Time Series Prediction.” _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_. 1114-1122.
* Haviv, Doron, Alexander Rivkind, and Omri Barak. 2019. “Understanding and Controlling Memory in Recurrent Neural Networks.” arXiv preprint. arXiv:1902.07275.
* Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-term Memory.” _Neural Computation_ 9 (8): 1735-1780.
* LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. “Deep Learning.” _Nature_ 521, (7553): 436-444.

Books cited in this chapter:

* Buduma, N., and N. Locascio. 2017. _Fundamentals of Deep Learning: Designing Next-generation Machine Intelligence Algorithms_. Sebastopol: O’Reilly.
* Goodfellow, I., Y. Bengio, and A. Courville. 2016. _Deep Learning_. Cambridge, MA: MIT Press.
* Nielsen, A. 2019. _Practical Time Series Analysis: Prediction with Statistics and Machine Learning_. Sebastopol: O’Reilly.
* Patterson, Josh, and Adam Gibson. 2017. _Deep Learning: A Practitioner’S Approach_. Sebastopol: O’Reilly.
* Sejnowski, Terrence J. 2018. _The Deep Learning Revolution_. Cambridge, MA: MIT Press.

[1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#idm45737247845584-marker) Representation learning helps us define a concept in a unique way. For instance, if the task is to detect whether something is a circle, then edges play a key role, as a circle has no edge. So using color, shape, and size, we can create a representation for an object. In essence, this is how the human brain works, and we know that deep learning structures are inspired by the brain’s functioning.

[2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#idm45737247426656-marker) Patterson et. al, 2017. “Deep learning: A practitioner’s approach.”
