# 2. Introduction to Time Series Modeling

## Chapter 2. Introduction to Time Series Modeling

> Market behavior is examined using large amounts of past data, such as high-frequency bid-ask quotes of currencies or stock prices. It is the abundance of data that makes possible the empirical study of the market. Although it is not possible to run controlled experiments, it is possible to extensively test on historical data.
>
> &#x20;Sergio Focardi (1997)

Some models account better for some phenomena; certain approaches capture the characteristics of an event in a solid way. Time series modeling is a good example of this because the vast majority of financial data has a time dimension, which makes time series applications a necessary tool for finance. In simple terms, the ordering of the data and its correlation is important.

This chapter of the book will discuss classical time series models and compare the performance of these models. Deep learning‚Äìbased time series analysis will be introduced in [Chapter 3](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch03.html#chapter\_3); this is an entirely different approach in terms of data preparation and model structure. The classical models include the moving average (MA), autoregressive (AR), and autoregressive integrated moving average (ARIMA) models. What is common across these models is the information carried by the historical observations. If these historical observations are obtained from error terms, we refer to this as a _moving average_; if these observations come out of time series itself, it is called _autoregressive_. The other model, ARIMA, is an extension of these models.

Here is a formal definition of _time series_ from Brockwell and Davis (2016):

> A time series is a set of observations ÔøΩÔøΩ, each one being recorded at a specific time _t_. A discrete-time time series‚Ä¶ is one in which the set ÔøΩ0 of times at which observations are made is a discrete set, as is the case, for example, when observations are made at fixed time intervals. Continuous time series are obtained when observations are recorded continuously over some time interval.

Let‚Äôs observe what data with time dimension looks like. [Figure 2-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#raw\_oil\_price) exhibits the oil prices for the period of 1980‚Äì2020, and the following Python code shows us a way of producing this plot:

```
In [1]: import quandl
        import matplotlib.pyplot as plt
        import warnings
        warnings.filterwarnings('ignore')
        plt.style.use('seaborn')

In [2]: oil = quandl.get("NSE/OIL", authtoken="insert you api token",
                         start_date="1980-01-01",
                         end_date="2020-01-01") 

In [3]: plt.figure(figsize=(10, 6))
        plt.plot(oil.Close)
        plt.ylabel('$')
        plt.xlabel('Date')
        plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO1-1)

Extracting data from Quandl database

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0201.png" alt="raw_oil" height="353" width="600"><figcaption></figcaption></figure>

**Figure 2-1. Oil prices between 1980 and 2020**

**NOTE**

An API is a tool designed for retrieving data using code. We will make use of different APIs throughout the book. In the preceding practice, Quandl API is used.

Quandl API allows us to access financial, economic, and alternative data from the Quandl website. To get your Quandl API, please visit the [Quandl website](https://oreil.ly/1IFDc) first and follow the necessary steps to get your own API key.

As can be understood from the definition provided previously, time series models can be applicable to diverse areas such as:

* Health care
* Finance
* Economics
* Network analysis
* Astronomy
* Weather

The superiority of the time series approach comes from the idea that correlations of observations in time better explain the current value. Having data with a correlated structure in time implies a violation of the famous identically and independently distributed (IID) assumption, which is at the heart of many models.

**THE DEFINITION OF IID**

IID assumption enables us to model joint probability of data as the product of probability of observations. The process ÔøΩÔøΩ is said to be an IID with mean 0 and variance ÔøΩ2:

ÔøΩÔøΩ‚àºÔøΩÔøΩÔøΩ(0,ÔøΩ2)

So, due to the correlation in time, the dynamics of a contemporaneous stock price can be better understood by its own historical values. How can we comprehend the dynamics of the data? This is a question that we can address by elaborating the components of time series.

## Time Series Components

Time series has four components: trend, seasonality, cyclicality, and residual. In Python, we can easily visualize the components of a time series with the `seasonal_decompose` function:

```
In [4]: import yfinance as yf
        import numpy as np
        import pandas as pd
        import datetime
        import statsmodels.api as sm
        from statsmodels.tsa.stattools import adfuller
        from statsmodels.tsa.seasonal import seasonal_decompose

In [5]: ticker = '^GSPC' 
        start = datetime.datetime(2015, 1, 1) 
        end = datetime.datetime(2021, 1, 1) 
        SP_prices = yf.download(ticker, start=start, end=end, interval='1mo')\
                    .Close 
        [*********************100%***********************]  1 of 1 completed

In [6]: seasonal_decompose(SP_prices, period=12).plot()
        plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO2-1)

Denoting ticker of S\&P 500

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO2-2)

Identifying the start and end dates

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO2-4)

Accessing the closing price of S\&P 500

In the top panel of [Figure 2-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#time\_series\_decompose), we see the plot of raw data, and in the second panel, trend can be observed showing upward movement. In the third panel, seasonality is exhibited, and finally residual is presented showing erratic fluctuations. You might wonder where the cyclicality component is; noise and the cyclical component are put together under the residual component.

Becoming familiar with time series components is important for further analysis so that we are able to understand characteristics of the data and propose a suitable model. Let‚Äôs start with the trend component.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0202.png" alt="decomp" height="403" width="600"><figcaption></figcaption></figure>

**Figure 2-2. Time series decomposition of S\&P 500**

### Trend

_Trend_ indicates a general tendency of an increase or decrease during a given time period. Generally speaking, trend is present when the starting and ending points are different or have upward/downward slope in a time series. The following code shows what a trend looks like:

```
In [7]: plt.figure(figsize=(10, 6))
        plt.plot(SP_prices) 
        plt.title('S&P-500 Prices')
        plt.ylabel('$')
        plt.xlabel('Date')
        plt.show()
```

Aside from the period in which the S\&P 500 index price plunges, we see a clear upward trend in [Figure 2-3](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#raw\_sp\_price) between 2010 and 2020.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0203.png" alt="raw_sp" height="368" width="600"><figcaption></figcaption></figure>

**Figure 2-3. S\&P 500 price**

A line plot is not the only option for understanding trend. Rather, we have some other strong tools for this task. So, at this point, it is worthwhile to talk about two important statistical concepts:

* Autocorrelation function
* Partial autocorrelation function

The autocorrelation function (ACF) is a statistical tool to analyze the relationship between the current value of a time series and its lagged values. Graphing ACF enables us to readily observe the serial dependence in a time series:

ÔøΩ^(‚Ñé)=Cov(ÔøΩÔøΩ,ÔøΩÔøΩ-‚Ñé)Var(ÔøΩÔøΩ)

[Figure 2-4](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#acf) denotes the ACF plot. The vertical lines represent the correlation coefficients; the first line denotes the correlation of the series with its 0 lag‚Äîthat is, it is the correlation with itself. The second line indicates the correlation between series value at time _t_ - 1 and _t_. In light of these, we can conclude that the S\&P 500 shows a serial dependence. There appears to be a strong dependence between the current value and lagged values of S\&P 500 data because the correlation coefficients, represented by lines in the ACF plot, decay in a slow fashion.

Here is how we can plot the ACF in Python:

```
In [8]: sm.graphics.tsa.plot_acf(SP_prices, lags=30) 
        plt.xlabel('Number of Lags')
        plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO3-1)

Plotting ACF

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0204.png" alt="acf_sp" height="431" width="600"><figcaption></figcaption></figure>

**Figure 2-4. ACF plot of the S\&P 500**

Now the question is, what are the likely sources of autocorrelations? Here are some causes:

* The primary source of autocorrelation is ‚Äúcarryover,‚Äù meaning that the preceding observation has an impact on the current one.
* Model misspecification.
* Measurement error, which is basically the difference between observed and actual values.
* Dropping a variable, which has an explanatory power.

Partial autocorrelation function (PACF) is another method of examining the relationship between ÔøΩÔøΩ and ÔøΩÔøΩ-ÔøΩ,ÔøΩ‚àà‚Ñ§. ACF is commonly considered as a useful tool in the MA(q) model simply because PACF does not decay fast but approaches toward 0. However, the pattern of ACF is more applicable to MA. PACF, on the other hand, works well with the AR(p) process.

PACF provides information on the correlation between the current value of a time series and its lagged values, controlling for the other correlations.

It is not easy to figure out what is going on at first glance. Let me give you an example. Suppose that we want to compute the partial correlation ÔøΩÔøΩ and ÔøΩÔøΩ-‚Ñé.

Put mathematically:

ÔøΩ^(‚Ñé)=Cov(ÔøΩÔøΩ,ÔøΩÔøΩ-‚Ñé|ÔøΩÔøΩ-1,ÔøΩÔøΩ-2...ÔøΩÔøΩ-‚Ñé-1)Var(ÔøΩÔøΩ|ÔøΩÔøΩ-1,ÔøΩÔøΩ-2,...,ÔøΩÔøΩ-‚Ñé-1)Var(ÔøΩÔøΩ-‚Ñé|ÔøΩÔøΩ-1,ÔøΩÔøΩ-2,...,ÔøΩÔøΩ-‚Ñé-1)

where _h_ is the lag. Take a look at the Python code for a PACF plot of the S\&P 500 in the following snippet:

```
In [9]: sm.graphics.tsa.plot_pacf(SP_prices, lags=30) 
        plt.xlabel('Number of Lags')
        plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO4-1)

Plotting PACF

[Figure 2-5](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#pacf) exhibits the PACF of raw S\&P 500 data. In interpreting the PACF, we focus on the spikes outside the dark region representing confidence interval. [Figure 2-5](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#pacf) exhibits some spikes at different lags, but lag 10 is outside the confidence interval. So it may be wise to select a model with 10 lags to include all the lags up to lag 10.

As discussed, PACF measures the correlation between current values of series and lagged values in a way to isolate in-between effects.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0205.png" alt="partial autocorrelation SP" height="436" width="600"><figcaption></figcaption></figure>

**Figure 2-5. PACF plot of the S\&P 500**

### Seasonality

Seasonality exists if there are regular fluctuations over a given period of time. For instance, energy usages can show a seasonality characteristic. To be more specific, energy usage goes up and down during certain periods over a year.

To show how we can detect the seasonality component, let‚Äôs use the Federal Reserve Economic Database (FRED), which includes more than 500,000 economic data series from over 80 sources covering many areas, such as banking, employment, exchange rates, gross domestic product, interest rates, trade and international transactions, and so on:

```
In [10]: from fredapi import Fred
         import statsmodels.api as sm

In [11]: fred = Fred(api_key='insert you api key')

In [12]: energy = fred.get_series("CAPUTLG2211A2S",
                                  observation_start="2010-01-01",
                                  observation_end="2020-12-31") 
         energy.head(12)
Out[12]: 2010-01-01    83.7028
         2010-02-01    84.9324
         2010-03-01    82.0379
         2010-04-01    79.5073
         2010-05-01    82.8055
         2010-06-01    84.4108
         2010-07-01    83.6338
         2010-08-01    83.7961
         2010-09-01    83.7459
         2010-10-01    80.8892
         2010-11-01    81.7758
         2010-12-01    85.9894
         dtype: float64

In [13]: plt.plot(energy)
         plt.title('Energy Capacity Utilization')
         plt.ylabel('$')
         plt.xlabel('Date')
         plt.show()
In [14]: sm.graphics.tsa.plot_acf(energy, lags=30)
         plt.xlabel('Number of Lags')
         plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO5-1)

Accessing the energy capacity utilization from the FRED for the period of 2010‚Äì2020

[Figure 2-6](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#energy\_seasonality) indicates periodic ups and downs over a nearly 10-year period with high-capacity utilization during the first months of every year and then going down toward the end of year, confirming that there is seasonality in energy-capacity utilization.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0206.png" alt="energy_seas" height="428" width="600"><figcaption></figcaption></figure>

**Figure 2-6. Seasonality in energy capacity utilization**

An ACF plot can also provide information about the seasonality as the periodic ups and downs can be observable using ACF, too. [Figure 2-7](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#energy\_acf) shows the correlation structure in the presence of seasonality.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0207.png" alt="energy_acf" height="436" width="600"><figcaption></figcaption></figure>

**Figure 2-7. ACF of energy capacity utilization**

### Cyclicality

What if data does not show fixed period movements? At this point, cyclicality comes into the picture. It exists when higher periodic variation than the trend emerges. Some confuse cyclicality and seasonality in a sense that they both exhibit expansion and contraction. We can, however, think of cyclicality as business cycles, which take a long time to complete their cycles and the ups and downs are over a long horizon. So cyclicality is different from seasonality in the sense that there is no fluctuation in a fixed period. An example of cyclicality may be house purchases (or sales) depending on mortgage rate. That is, when a mortgage rate is cut (or raised), it leads to a boost for house purchases (or sales).

### Residual

Residual is known as an irregular component of time series. Technically speaking, residual is equal to the difference between observations and related fitted values. We can think of it as a leftover from the model.

As we have discussed before, time series models lack some core assumptions, but this does not necessarily mean that time series models are free from assumptions. I would like to stress the most prominent one, which is called _stationarity_.

Stationarity means that statistical properties such as mean, variance, and covariance of the time series do not change over time.

There are two forms of stationarity:

Weak stationarity

Time series ÔøΩÔøΩ is said to be stationarity if:

* ÔøΩÔøΩ has finite variance, ùîº(ÔøΩÔøΩ2)<‚àû,‚àÄÔøΩ‚àà‚Ñ§
* The mean value of ÔøΩÔøΩ is constant and does solely depend on time, ùîº(ÔøΩÔøΩ)=ÔøΩ,ÔøΩ‚àÄ‚àà‚Ñ§
* Covariance structure, ÔøΩ(ÔøΩ,ÔøΩ+‚Ñé), depends on the time difference only:

ÔøΩ(‚Ñé)=ÔøΩ‚Ñé+ÔøΩ(ÔøΩ+‚Ñé,ÔøΩ)

In other words, time series should have finite variance with constant mean and a covariance structure that is a function of the time difference.

Strong stationarity

If the joint distribution of ÔøΩÔøΩ1,ÔøΩÔøΩ2,...ÔøΩÔøΩÔøΩ is the same with the shifted version of set ÔøΩÔøΩ1+‚Ñé,ÔøΩÔøΩ2+‚Ñé,...ÔøΩÔøΩÔøΩ+‚Ñé, it is referred to as strong stationarity. Thus, strong stationarity implies that distribution of random variables of a random process is the same with a shifting time index.

The question is now why do we need stationarity? The reason is twofold.

First, in the estimation process, it is essential to have some distribution as time goes on. In other words, if distribution of a time series changes over time, it becomes unpredictable and cannot be modeled.

The ultimate aim of time series models is forecasting. To do that, we should estimate the coefficients first, which corresponds to learning in ML. Once we learn and conduct forecasting analysis, we assume that the distribution of the data in the estimation stays the same in a way that we have the same estimated coefficients. If this is not the case, we should reestimate the coefficients because we are unable to forecast with the previous estimated coefficients.

Having structural breaks, such as a financial crisis, generates a shift in distribution. We need to take care of this period cautiously and separately.

The other reason for having stationarity is, by assumption, some statistical models require stationary data, but that does not mean that some models requires stationary only. Instead, all models require stationarity but even if you feed the model with nonstationary data, some models, by design, turn it into stationary data and process it.

[Figure 2-4](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#acf) showed the slow-decaying lags amounting to nonstationary because persistence of the high correlation between lags of the time series continues.

There are, by and large, two ways to detect nonstationarity: visualization and statistical methods. The latter, of course, is a better and more robust way of detecting the nonstationarity. However, to improve our understanding, let‚Äôs start with the ACF. Slow-decaying ACF implies that the data is nonstationary because it presents a strong correlation in time. That is what I observe in S\&P 500 data.

We first need to check and see if the data is stationary or not. Visualization is a good but ultimately inadequate tool for this task. Instead, a more powerful statistical method is needed, and the augmented Dickey-Fuller (ADF) test provides this. Assuming that the confidence interval is set to 95%, the following result indicates that the data is not stationary:

```
In [15]: stat_test = adfuller(SP_prices)[0:2] 
         print("The test statistic and p-value of ADF test are {}"
               .format(stat_test)) 
         The test statistic and p-value of ADF test are (0.030295120072926063,
          0.9609669053518538)
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO6-1)

ADF test for stationarity

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO6-2)

Test statistic and p-value of ADF test

Taking the difference is an efficient technique for removing the stationarity. This just means subtracting the current value of the series from its first lagged value, i.e., ÔøΩÔøΩ-ÔøΩÔøΩ-1, and the following Python code presents how to apply this technique (and creates Figures [2-8](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#detrendedSP500) and [2-9](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#diff\_acfSP500)):

```
In [16]: diff_SP_price = SP_prices.diff() 

In [17]: plt.figure(figsize=(10, 6))
         plt.plot(diff_SP_price)
         plt.title('Differenced S&P-500 Price')
         plt.ylabel('$')
         plt.xlabel('Date')
         plt.show()
In [18]: sm.graphics.tsa.plot_acf(diff_SP_price.dropna(),lags=30)
         plt.xlabel('Number of Lags')
         plt.show()
In [19]: stat_test2 = adfuller(diff_SP_price.dropna())[0:2] 
         print("The test statistic and p-value of ADF test after differencing are {}"\
               .format(stat_test2))
         The test statistic and p-value of ADF test after differencing are
          (-7.0951058730170855, 4.3095548146405375e-10)
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO7-1)

Taking the difference of S\&P 500 prices

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO7-2)

ADF test result based on differenced S\&P 500 data

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0208.png" alt="differenced SP" height="368" width="600"><figcaption></figcaption></figure>

**Figure 2-8. Detrended S\&P 500 price**

After taking the first difference, we rerun the ADF test to see if it worked, and yes, it does. The very low p-value of ADF tells me that S\&P 500 data is stationary now.

This can be observed from the line plot provided in [Figure 2-8](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#detrendedSP500). Unlike the raw S\&P 500 plot, this plot exhibits fluctuations around the mean with similar volatility, meaning that we have a stationary series.

[Figure 2-9](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#diff\_acfSP500) shows that there is only one statistical significant correlation structure at lag 7.

Needless to say, trend is not the only indicator of nonstationarity. Seasonality is another source of it, and now we are about to learn a method to deal with it.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0209.png" alt="differenced_acf" height="436" width="600"><figcaption></figcaption></figure>

**Figure 2-9. Detrended S\&P 500 price**

First, take a look at the ACF of energy capacity utilization in [Figure 2-7](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#energy\_acf), which shows periodic ups and downs, a sign of nonstationarity.

To get rid of seasonality, we first apply the _resample_ method to calculate annual mean, which is used as the denominator in the following formula:

SeasonalIndex=ValueofaSeasonalTimeSeriesSeasonalAverage

Thus, the result of the application, _seasonal index_, gives us the deseasonalized time series. The following code shows us how we code this formula in Python:

```
In [20]: seasonal_index = energy.resample('Q').mean() 

In [21]: dates = energy.index.year.unique() 
         deseasonalized = []
         for i in dates:
             for j in range(1, 13):
                 deseasonalized.append((energy[str(i)][energy[str(i)]\
                                                       .index.month==j])) 
         concat_deseasonalized = np.concatenate(deseasonalized) 

In [22]: deseason_energy = []
         for i,s in zip(range(0, len(energy), 3), range(len(seasonal_index))):
             deseason_energy.append(concat_deseasonalized[i:i+3] /
                                    seasonal_index.iloc[s]) 
         concat_deseason_energy = np.concatenate(deseason_energy)
         deseason_energy = pd.DataFrame(concat_deseason_energy,
                                        index=energy.index)
         deseason_energy.columns = ['Deaseasonalized Energy']
         deseason_energy.head()
Out[22]:             Deaseasonalized Energy
         2010-01-01                1.001737
         2010-02-01                1.016452
         2010-03-01                0.981811
         2010-04-01                0.966758
         2010-05-01                1.006862

In [23]: sm.graphics.tsa.plot_acf(deseason_energy, lags=10)
         plt.xlabel('Number of Lags')
         plt.show()
In [24]: sm.graphics.tsa.plot_pacf(deseason_energy, lags=10)
         plt.xlabel('Number of Lags')
         plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO8-1)

Calculating quarterly mean of energy utilization

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO8-2)

Defining the years in which seasonality analysis is run

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO8-3)

Computing the numerator of _Seasonal Index_ formula

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO8-4)

Concatenating the deseasonalized energy utilization

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO8-5)

Computing _Seasonal Index_ using the predefined formula

[Figure 2-10](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#deseasonalized\_acf\_energy) suggests that there is a statistically significant correlation at lag 1 and 2, but ACF does not show any periodic characteristics, which is another way of saying deseasonalization.

Similarly, in [Figure 2-11](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#deseasonalized\_pacf\_energy), although there is a spike at some lags, PACF does not show any periodic ups and downs. So we can say that the data is deseasonalized using the Seasonal Index Formula.

What we have now are the less periodic ups and down in energy-capacity utilization, meaning that the data turns out to be deseasonalized.

Finally, we are ready to move forward and discuss the time series models.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0210.png" alt="deseasonalized energy acf" height="436" width="600"><figcaption></figcaption></figure>

**Figure 2-10. Deseasonalized ACF of energy utilization**

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0211.png" alt="deseasonalized energy pacf" height="436" width="600"><figcaption></figcaption></figure>

**Figure 2-11. Deseasonalized PACF of energy utilization**

## Time Series Models

Traditional time series models are univariate models, and they follow these phases:

Identification

In this process, we explore the data using ACF and PACF, identifying patterns and conducting statistical tests.

Estimation

We estimate coefficients via the proper optimization technique.

Diagnostics

After estimation, we need to check if information criteria or ACF/PACF suggest that the model is valid. If so, we move on to the forecasting stage.

Forecast

This part is more about the performance of the model. In forecasting, we predict future values based on our estimation.

[Figure 2-12](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#modeling1) shows the modeling process. Accordingly, subsequent to identifying the variables and the estimation process, the model is run. Only after running proper diagnostics are we able to perform the forecast analysis.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0212.png" alt="modeling1" height="297" width="600"><figcaption></figcaption></figure>

**Figure 2-12. Modeling process**

In modeling data with a time dimension, we should consider correlation in adjacent points in time. This consideration takes us to time series modeling. My aim in modeling time series is to fit a model and comprehend statistical character of a time series, which fluctuates randomly in time.

Recall the discussion about the IID process, which is the most basic time series model and is sometimes referred to as _white noise_. Let‚Äôs touch on the concept of white noise.

## White Noise

The time series ÔøΩÔøΩ is said to be white noise if it satisfies the following:

ÔøΩÔøΩ‚àºÔøΩÔøΩ(0,ÔøΩÔøΩ2)Corr(ÔøΩÔøΩ,ÔøΩÔøΩ)=0,‚àÄÔøΩ‚â†ÔøΩ

In other words, ÔøΩÔøΩ has mean of 0 and a constant variance. Moreover, there is no correlation between successive terms of ÔøΩÔøΩ. Well, it is easy to say that the white noise process is stationary and that the plot of white noise exhibits fluctuations around mean in a random fashion in time. However, as the white noise is formed by an uncorrelated sequence, it is not an appealing model from a forecasting standpoint. Uncorrelated sequences prevent us from forecasting future values.

As we can observe from the following code snippet and [Figure 2-13](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#WN), white noise oscillates around mean and is completely erratic:

```
In [25]: mu = 0
         std = 1
         WN = np.random.normal(mu, std, 1000)

         plt.plot(WN)
         plt.xlabel('Number of Simulations')
         plt.show()
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0213.png" alt="WN process" height="423" width="600"><figcaption></figcaption></figure>

**Figure 2-13. White noise process**

From this point on, we need to identify the optimum number of lags before running the time series model. As you can imagine, deciding the optimal number of lags is a challenging task. The most widely used methods are ACF, PACF, and _information criteria_. ACF and PACF have already been discussed; see the following sidebar for more about information criteria, and specifically the Aikake information criterion (AIC).

**INFORMATION CRITERIA**

Determining the optimal number of lags is a cumbersome task. We need to have a criterion to decide which model fits best to the data as there may be numerous potentially good models. Cavanaugh and Neath (2019) describe the AIC as follows:

> AIC is introduced as an extension to the Maximum Likelihood Principle. Maximum likelihood is conventionally applied to estimate the parameters of a model once structure and dimension of the model have been formulated.

AIC can be mathematically defined as:

ÔøΩÔøΩÔøΩ=-2ÔøΩÔøΩ(ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ‚ÑéÔøΩÔøΩÔøΩ)+2ÔøΩ

where _d_ is the total number of parameters. The last term, 2_d_, aims at reducing the risk of overfitting. It is also called a _penalty term_, by which the unnecessary redundancy in the model can be filtered out.

The Bayesian information criterion (BIC) is the other information criterion used to select the best model. The penalty term in BIC is larger than that of AIC:

ÔøΩÔøΩÔøΩ=-2ÔøΩÔøΩ(ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ‚ÑéÔøΩÔøΩÔøΩ)+ÔøΩÔøΩ(ÔøΩ)ÔøΩ

where _n_ is the number of observations.

Please note that you need to treat the AIC with caution if the proposed model is finite dimensional. This fact is well put by Hurvich and Tsai (1989):

> If the true model is infinite dimensional, a case which seems most realistic in practice, AIC provides an asymptotically efficient selection of a finite dimensional approximating model. If the true model is finite dimensional, however, the asymptotically efficient methods, e.g., Akaike‚Äôs FPE (Akaike 1970), AIC, and Parzen‚Äôs CAT (Parzen 1977), do not provide consistent model order selections.

Let‚Äôs get started visiting classical time series models with the moving average model.

### Moving Average Model

MA and residuals are closely related models. MA can be considered a smoothing model, as it tends to take into account the lag values of residual. For the sake of simplicity, let us start with MA(1):

ÔøΩÔøΩ=ÔøΩÔøΩ+ÔøΩÔøΩÔøΩ-1

As long as ÔøΩ‚â†0, it has nontrivial correlation structure. Intuitively, MA(1) tells us that the time series has been affected by ÔøΩÔøΩ and ÔøΩÔøΩ-1 only.

In general form, MA(q) becomes:

ÔøΩÔøΩ=ÔøΩÔøΩ+ÔøΩ1ÔøΩÔøΩ-1+ÔøΩ2ÔøΩÔøΩ-2...+ÔøΩÔøΩÔøΩÔøΩ-ÔøΩ

From this point on, to be consistent, we will model the data of two major tech companies, namely Apple and Microsoft. Yahoo Finance provides a convenient tool to access closing prices of the related stocks for the period between 01-01-2019 and 01-01-2021.

As a first step, we dropped the missing values and checked if the data is stationary, and it turns out neither Apple‚Äôs nor Microsoft‚Äôs stock prices have a stationary structure as expected. Thus, taking the first difference to make these data stationary and splitting the data as _train_ and _test_ are the steps to take at this point. The following code (which produces [Figure 2-14](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#acf\_ma)) shows how we can do this in Python:

```
In [26]: ticker = ['AAPL', 'MSFT']
         start = datetime.datetime(2019, 1, 1)
         end = datetime.datetime(2021, 1, 1)
         stock_prices = yf.download(ticker, start, end, interval='1d')\
                        .Close 
         [*********************100%***********************]  2 of 2 completed

In [27]: stock_prices = stock_prices.dropna()

In [28]: for i in ticker:
             stat_test = adfuller(stock_prices[i])[0:2]
             print("The ADF test statistic and p-value of {} are {}"\
                   .format(i,stat_test))
         The ADF test statistic and p-value of AAPL are  (0.29788764759932335,
          0.9772473651259085)
         The ADF test statistic and p-value of MSFT are  (-0.8345360070598484,
          0.8087663305296826)

In [29]: diff_stock_prices = stock_prices.diff().dropna()

In [30]: split = int(len(diff_stock_prices['AAPL'].values) * 0.95) 
         diff_train_aapl = diff_stock_prices['AAPL'].iloc[:split] 
         diff_test_aapl = diff_stock_prices['AAPL'].iloc[split:] 
         diff_train_msft = diff_stock_prices['MSFT'].iloc[:split] 
         diff_test_msft = diff_stock_prices['MSFT'].iloc[split:] 

In [31]: diff_train_aapl.to_csv('diff_train_aapl.csv') 
         diff_test_aapl.to_csv('diff_test_aapl.csv')
         diff_train_msft.to_csv('diff_train_msft.csv')
         diff_test_msft.to_csv('diff_test_msft.csv')

In [32]: fig, ax = plt.subplots(2, 1, figsize=(10, 6))
         plt.tight_layout()
         sm.graphics.tsa.plot_acf(diff_train_aapl,lags=30,
                                  ax=ax[0], title='ACF - Apple')
         sm.graphics.tsa.plot_acf(diff_train_msft,lags=30,
                                  ax=ax[1], title='ACF - Microsoft')
         plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO9-1)

Retrieving monthly closing stock prices

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO9-2)

Splitting data as 95% and 5%

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO9-3)

Assigning 95% of the Apple stock price data to the train set

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO9-4)

Assigning 5% of the Apple stock price data to the test set

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO9-5)

Assigning 95% of the Microsoft stock price data to the train set

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO9-6)

Assigning 5% of the Microsoft stock price data to the test set

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/7.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO9-7)

Saving the data for future use

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0214.png" alt="acf all" height="361" width="600"><figcaption></figcaption></figure>

**Figure 2-14. ACF after first difference**

Looking at the top panel of [Figure 2-14](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#acf\_ma), we can see that there are significant spikes at some lags and, therefore, we‚Äôll choose lag 9 for the short MA model and 22 for the long MA for Apple. These imply that an order of 9 will be our short-term order and 22 will be our long-term order in modeling MA:

```
In [33]: short_moving_average_appl = diff_train_aapl.rolling(window=9).mean() 
         long_moving_average_appl = diff_train_aapl.rolling(window=22).mean() 

In [34]: fig, ax = plt.subplots(figsize=(10, 6))
         ax.plot(diff_train_aapl.loc[start:end].index,
                 diff_train_aapl.loc[start:end],
                 label='Stock Price', linestyle='--') 
         ax.plot(short_moving_average_appl.loc[start:end].index,
                 short_moving_average_appl.loc[start:end],
                 label = 'Short MA', linestyle='solid') 
         ax.plot(long_moving_average_appl.loc[start:end].index,
                 long_moving_average_appl.loc[start:end],
                 label = 'Long MA', linestyle='solid') 
         ax.legend(loc='best')
         ax.set_ylabel('Price in $')
         ax.set_title('Stock Prediction-Apple')
         plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO10-1)

Moving average with short window for Apple stock

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO10-2)

Moving average with long window for Apple stock

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO10-3)

Line plot of first differenced Apple stock prices

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO10-4)

Visualization of short-window MA result for Apple

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO10-5)

Visualization of long-window MA result for Apple

[Figure 2-15](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#ma\_apple) exhibits the short-term MA model result with a solid line and the long-term MA model result with a dash-dot marker. As expected, it turns out that the short-term MA tends to be more responsive to daily changes in Apple‚Äôs stock price compared to the long-term MA. This makes sense because taking into account a long MA generates smoother predictions.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0215.png" alt="ma apple" height="357" width="600"><figcaption></figcaption></figure>

**Figure 2-15. MA model prediction result for Apple**

In the next step, we try to predict Microsoft‚Äôs stock price using an MA model with different window. But before proceeding, let me say that choosing the proper window for short and long MA analysis is key to good modeling. In the bottom panel of [Figure 2-14](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#acf\_ma), there seem to be significant spikes at 2 and 22, so we‚Äôll use these lags in our short and long MA analysis, respectively. After identifying the window length, we‚Äôll fit data to the MA model with the following application:

```
In [35]: short_moving_average_msft = diff_train_msft.rolling(window=2).mean()
         long_moving_average_msft = diff_train_msft.rolling(window=22).mean()

In [36]: fig, ax = plt.subplots(figsize=(10, 6))
         ax.plot(diff_train_msft.loc[start:end].index,
                 diff_train_msft.loc[start:end],
                 label='Stock Price', linestyle='--')
         ax.plot(short_moving_average_msft.loc[start:end].index,
                 short_moving_average_msft.loc[start:end],
                 label = 'Short MA', linestyle='solid')
         ax.plot(long_moving_average_msft.loc[start:end].index,
                 long_moving_average_msft.loc[start:end],
                 label = 'Long MA', linestyle='-.')
         ax.legend(loc='best')
         ax.set_ylabel('$')
         ax.set_xlabel('Date')
         ax.set_title('Stock Prediction-Microsoft')
         plt.show()
```

Similarly, predictions based on short MA analysis tend to be more responsive than those of the long MA model, as shown in [Figure 2-16](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#ma\_msft). But in Microsoft‚Äôs case, the short-term MA prediction appears to be very close to the real data. This is something we expect in time series models in that a window with a short-term horizon is able to better capture the dynamics of the data, and this, in turn, helps us obtain better predictive performance.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0216.png" alt="ma msft" height="371" width="600"><figcaption></figcaption></figure>

**Figure 2-16. MA model prediction result for Microsoft**

### Autoregressive Model

The dependence structure of successive terms is the most distinctive feature of the AR model, in the sense that current value is regressed over its own lag values in this model. So we basically forecast the current value of the time series ÔøΩÔøΩ by using a linear combination of its past values. Mathematically, the general form of AR(p) can be written as:

ÔøΩÔøΩ=ÔøΩ+ÔøΩ1ÔøΩÔøΩ-1+ÔøΩ2ÔøΩÔøΩ-2...+ÔøΩÔøΩÔøΩÔøΩ-ÔøΩ+ÔøΩÔøΩ

where ÔøΩÔøΩ denotes the residuals and _c_ is the intercept term. The AR(p) model implies that past values up to order _p_ have somewhat explanatory power on ÔøΩÔøΩ. If the relationship has shorter memory, then it is likely to model ÔøΩÔøΩ with a fewer number of lags.

We have discussed one of the main properties of time series, stationarity; the other important property is _invertibility_. After introducing the AR model, it is time to show the invertibility of the MA process. It is said to be invertible if it can be converted to an infinite AR model.

Under some circumstances, MA can be written as an infinite AR process. These circumstances are having stationary covariance structure, deterministic part, and invertible MA process. In doing so, we have another model called _infinite AR_ thanks to the assumption of |ÔøΩ|<1.

ÔøΩÔøΩ=ÔøΩÔøΩ+ÔøΩÔøΩÔøΩ-1=ÔøΩÔøΩ+ÔøΩ(ÔøΩÔøΩ-1-ÔøΩÔøΩÔøΩ-2)=ÔøΩÔøΩ+ÔøΩÔøΩÔøΩ-1-ÔøΩ2ÔøΩÔøΩ-2=ÔøΩÔøΩ+ÔøΩÔøΩÔøΩ-1-ÔøΩ2(ÔøΩÔøΩ-2+ÔøΩÔøΩÔøΩ-3)=ÔøΩÔøΩ+ÔøΩÔøΩÔøΩ-1-ÔøΩ2ÔøΩÔøΩ-2+ÔøΩ3ÔøΩÔøΩ-3)=...=ÔøΩÔøΩÔøΩ-1-ÔøΩ2ÔøΩÔøΩ-2+ÔøΩ3ÔøΩÔøΩ-3-ÔøΩ4ÔøΩÔøΩ-4+...-(-ÔøΩ)ÔøΩÔøΩÔøΩ-ÔøΩ

After doing the necessary math, the equation gets the following form:

ÔøΩÔøΩÔøΩÔøΩ-ÔøΩ=ÔøΩÔøΩ-‚àëÔøΩ=0ÔøΩ-1ÔøΩÔøΩÔøΩÔøΩ-ÔøΩ

In this case, if |ÔøΩ|<1, then ÔøΩ‚Üí‚àû:

ùîº(ÔøΩÔøΩ-‚àëÔøΩ=0ÔøΩ-1ÔøΩÔøΩÔøΩÔøΩ-ÔøΩ)2=ùîº(ÔøΩ2ÔøΩÔøΩÔøΩ-ÔøΩ2‚Üí‚àû)

Finally, the MA(1) process turns out to be:

ÔøΩÔøΩ=‚àëÔøΩ=0‚àûÔøΩÔøΩÔøΩÔøΩ-ÔøΩ

Due to the duality between the AR and MA processes, it is possible to represent AR(1) as infinite MA, MA(‚àû). In other words, the AR(1) process can be expressed as a function of past values of innovations:

ÔøΩÔøΩ=ÔøΩÔøΩ+ÔøΩÔøΩÔøΩ-1=ÔøΩ(ÔøΩÔøΩÔøΩ-2+ÔøΩÔøΩ-1)+ÔøΩÔøΩ=ÔøΩ2ÔøΩÔøΩ-2+ÔøΩÔøΩÔøΩ-1+ÔøΩÔøΩ=ÔøΩ2(ÔøΩÔøΩÔøΩ-3+ÔøΩÔøΩÔøΩ-2)ÔøΩÔøΩÔøΩ-1+ÔøΩÔøΩÔøΩÔøΩ=ÔøΩÔøΩ+ÔøΩÔøΩ-1+ÔøΩ2ÔøΩÔøΩ-2+...+ÔøΩÔøΩÔøΩÔøΩ

As ÔøΩ‚Üí‚àû, ÔøΩÔøΩ‚Üí0, so I can represent AR(1) as an infinite MA process.

In the following analysis, we run the AR model to predict Apple and Microsoft stock prices. Unlike MA, partial ACF is a useful tool to find out the optimum order in the AR model. This is because, in AR, we aim to find out the relationship of a time series between two different times, say ÔøΩÔøΩ and ÔøΩÔøΩ-ÔøΩ, and to do that we need to filter out the effect of other lags in between, resulting in Figures [2-17](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#pacf\_appl) and [2-18](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#pacf\_msft):

```
In [37]: sm.graphics.tsa.plot_pacf(diff_train_aapl, lags=30)
         plt.title('PACF of Apple')
         plt.xlabel('Number of Lags')
         plt.show()
In [38]: sm.graphics.tsa.plot_pacf(diff_train_msft, lags=30)
         plt.title('PACF of Microsoft')
         plt.xlabel('Number of Lags')
         plt.show()
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0217.png" alt="pacf_appl" height="437" width="600"><figcaption></figcaption></figure>

**Figure 2-17. PACF for Apple**

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0218.png" alt="pacf_msft" height="437" width="600"><figcaption></figcaption></figure>

**Figure 2-18. PACF for Microsoft**

In [Figure 2-17](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#pacf\_appl), obtained from the first differenced Apple stock price, we observe a significant spike at lag 29, and in [Figure 2-18](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#pacf\_msft), we have a similar spike at lag 26 for Microsoft. Thus, 29 and 26 are the lags that we are going to use in modeling AR for Apple and Microsoft, respectively:

```
In [39]: from statsmodels.tsa.ar_model import AutoReg
         import warnings
         warnings.filterwarnings('ignore')

In [40]: ar_aapl = AutoReg(diff_train_aapl.values, lags=29)
         ar_fitted_aapl = ar_aapl.fit() 

In [41]: ar_predictions_aapl = ar_fitted_aapl.predict(start=len(diff_train_aapl),
                                                      end=len(diff_train_aapl)\
                                                      + len(diff_test_aapl) - 1,
                                                      dynamic=False) 

In [42]: for i in range(len(ar_predictions_aapl)):
             print('==' * 25)
             print('predicted values:{:.4f} & actual values:{:.4f}'\
                   .format(ar_predictions_aapl[i], diff_test_aapl[i])) 
         ==================================================
         predicted values:1.6511 & actual values:1.3200
         ==================================================
         predicted values:-0.8398 & actual values:0.8600
         ==================================================
         predicted values:-0.9998 & actual values:0.5600
         ==================================================
         predicted values:1.1379 & actual values:2.4600
         ==================================================
         predicted values:-0.1123 & actual values:3.6700
         ==================================================
         predicted values:1.7843 & actual values:0.3600
         ==================================================
         predicted values:-0.9178 & actual values:-0.1400
         ==================================================
         predicted values:1.7343 & actual values:-0.6900
         ==================================================
         predicted values:-1.5103 & actual values:1.5000
         ==================================================
         predicted values:1.8224 & actual values:0.6300
         ==================================================
         predicted values:-1.2442 & actual values:-2.6000
         ==================================================
         predicted values:-0.5438 & actual values:1.4600
         ==================================================
         predicted values:-0.1075 & actual values:-0.8300
         ==================================================
         predicted values:-0.6167 & actual values:-0.6300
         ==================================================
         predicted values:1.3206 & actual values:6.1000
         ==================================================
         predicted values:0.2464 & actual values:-0.0700
         ==================================================
         predicted values:0.4489 & actual values:0.8900
         ==================================================
         predicted values:-1.3101 & actual values:-2.0400
         ==================================================
         predicted values:0.5863 & actual values:1.5700
         ==================================================
         predicted values:0.2480 & actual values:3.6500
         ==================================================
         predicted values:0.0181 & actual values:-0.9200
         ==================================================
         predicted values:0.9913 & actual values:1.0100
         ==================================================
         predicted values:0.2672 & actual values:4.7200
         ==================================================
         predicted values:0.8258 & actual values:-1.8200
         ==================================================
         predicted values:0.1502 & actual values:-1.1500
         ==================================================
         predicted values:0.5560 & actual values:-1.0300

In [43]: ar_predictions_aapl = pd.DataFrame(ar_predictions_aapl) 
         ar_predictions_aapl.index = diff_test_aapl.index 

In [44]: ar_msft = AutoReg(diff_train_msft.values, lags=26)
         ar_fitted_msft = ar_msft.fit() 

In [45]: ar_predictions_msft = ar_fitted_msft.predict(start=len(diff_train_msft),
                                                      end=len(diff_train_msft)\
                                                      +len(diff_test_msft) - 1,
                                                      dynamic=False) 

In [46]: ar_predictions_msft = pd.DataFrame(ar_predictions_msft) 
         ar_predictions_msft.index = diff_test_msft.index 
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO11-1)

Fitting Apple stock data with AR model

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO11-2)

Predicting the stock prices for Apple

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO11-3)

Comparing the predicted and real observations

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO11-4)

Turning array into dataframe to assign index

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO11-5)

Assigning test data indices to predicted values

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO11-6)

Fitting Microsoft stock data with AR model

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/7.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO11-7)

Predicting the stock prices for Microsoft

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/8.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO11-8)

Turning the array into a dataframe to assign index

[![9](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/9.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO11-9)

Assigning test data indices to predicted values

The following code, resulting in [Figure 2-19](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#ar\_all), shows the predictions based on the AR model. The solid lines represent the Apple and Microsoft stock price predictions, and the dashed lines denote the real data. The result reveals that the MA model outperforms the AR model in capturing the stock price:

```
In [47]: fig, ax = plt.subplots(2,1, figsize=(18, 15))

         ax[0].plot(diff_test_aapl, label='Actual Stock Price', linestyle='--')
         ax[0].plot(ar_predictions_aapl, linestyle='solid', label="Prediction")
         ax[0].set_title('Predicted Stock Price-Apple')
         ax[0].legend(loc='best')
         ax[1].plot(diff_test_msft, label='Actual Stock Price', linestyle='--')
         ax[1].plot(ar_predictions_msft, linestyle='solid', label="Prediction")
         ax[1].set_title('Predicted Stock Price-Microsoft')
         ax[1].legend(loc='best')
         for ax in ax.flat:
             ax.set(xlabel='Date', ylabel='$')
         plt.show()
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0219.png" alt="ar all" height="495" width="600"><figcaption></figcaption></figure>

**Figure 2-19. AR model prediction results**

### Autoregressive Integrated Moving Average Model

The ARIMA is a function of past values of a time series and white noise. ARIMA has been proposed as a generalization of AR and MA, but they do not have an integration parameter, which helps us to feed the model with the raw data. In this respect, even if we include nonstationary data, ARIMA makes it stationary by properly defining the integration parameter.

ARIMA has three parameters, namely _p_, _d_, and _q_. As should be familiar from previous time series models, _p_ and _q_ refer to the order of AR and MA, respectively. The _d_ parameter controls for level difference. If _d_ = 1, it amounts to first difference, and if it has a value of 0, that means that the model is ARIMA.

It is possible to have a _d_ greater than 1, but it‚Äôs not as common as having a _d_ of 1. The ARIMA (p, 1, q) equation has the following structure:

ÔøΩÔøΩ=ÔøΩ1ÔøΩÔøΩÔøΩ-1+ÔøΩ2ÔøΩÔøΩÔøΩ-2...+ÔøΩÔøΩÔøΩÔøΩÔøΩ-ÔøΩ+ÔøΩÔøΩ+ÔøΩ1ÔøΩÔøΩ-1+ÔøΩ2ÔøΩÔøΩ-2...+ÔøΩÔøΩÔøΩÔøΩ-ÔøΩ

where _d_ refers to difference.

As it is a widely embraced and applicable model, let‚Äôs discuss the pros and cons of the ARIMA model to get more familiar with it.

Pros

* ARIMA allows us to work with raw data without considering if it is stationary.
* It performs well with high-frequency data.
* It is less sensitive to the fluctuation in the data compared to other models.

Cons

* ARIMA might fail in capturing seasonality.
* It works better with long series and short-term (daily, hourly) data.
* As no automatic updating occurs in ARIMA, no structural break during the analysis period should be observed.
* Having no adjustment in the ARIMA process leads to instability.

Now, let‚Äôs see how ARIMA works using the same stocks, namely Apple and Microsoft. But this time, a different short-term lag structure is used to compare the result with the AR and MA models:

```
In [48]: from statsmodels.tsa.arima_model import ARIMA

In [49]: split = int(len(stock_prices['AAPL'].values) * 0.95)
         train_aapl = stock_prices['AAPL'].iloc[:split]
         test_aapl = stock_prices['AAPL'].iloc[split:]
         train_msft = stock_prices['MSFT'].iloc[:split]
         test_msft = stock_prices['MSFT'].iloc[split:]

In [50]: arima_aapl = ARIMA(train_aapl,order=(9, 1, 9)) 
         arima_fit_aapl = arima_aapl.fit() 

In [51]: arima_msft = ARIMA(train_msft, order=(6, 1, 6)) 
         arima_fit_msft = arima_msft.fit() 

In [52]: arima_predict_aapl = arima_fit_aapl.predict(start=len(train_aapl),
                                                           end=len(train_aapl)\
                                                           + len(test_aapl) - 1,
                                                           dynamic=False) 
         arima_predict_msft = arima_fit_msft.predict(start=len(train_msft),
                                                           end=len(train_msft)\
                                                           + len(test_msft) - 1,
                                                           dynamic=False) 

In [53]: arima_predict_aapl = pd.DataFrame(arima_predict_aapl)
         arima_predict_aapl.index = diff_test_aapl.index
         arima_predict_msft = pd.DataFrame(arima_predict_msft)
         arima_predict_msft.index = diff_test_msft.index 
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO12-1)

Configuring the ARIMA model for Apple stock

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO12-2)

Fitting the ARIMA model to Apple‚Äôs stock price

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO12-3)

Configuring the ARIMA model for Microsoft stock

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO12-4)

Fitting the ARIMA model to Microsoft‚Äôs stock price

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO12-5)

Predicting the Apple stock prices based on ARIMA

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO12-6)

Predicting the Microsoft stock prices based on ARIMA

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/7.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO12-7)

Forming index for predictions

The next snippet, resulting in [Figure 2-20](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#arima\_all), shows the result of the prediction based on Apple‚Äôs and Microsoft‚Äôs stock price, and as we employ the short-term orders from the AR and MA model, the result is not completely different:

```
In [54]: fig, ax = plt.subplots(2, 1, figsize=(18, 15))

         ax[0].plot(diff_test_aapl, label='Actual Stock Price', linestyle='--')
         ax[0].plot(arima_predict_aapl, linestyle='solid', label="Prediction")
         ax[0].set_title('Predicted Stock Price-Apple')
         ax[0].legend(loc='best')
         ax[1].plot(diff_test_msft, label='Actual Stock Price', linestyle='--')
         ax[1].plot(arima_predict_msft, linestyle='solid', label="Prediction")
         ax[1].set_title('Predicted Stock Price-Microsoft')
         ax[1].legend(loc='best')
         for ax in ax.flat:
             ax.set(xlabel='Date', ylabel='$')
         plt.show()
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0220.png" alt="arima all" height="495" width="600"><figcaption></figcaption></figure>

**Figure 2-20. ARIMA prediction results**

At this point, it is worthwhile to discuss an alternative method for optimum lag selection for time series models. AIC is the method that I apply here to select the proper number of lags. Please note that, even though the result of AIC suggests (4, 0, 4), the model does not converge with these orders. So, (4, 1, 4) is applied instead:

```
In [55]: import itertools

In [56]: p = q = range(0, 9) 
         d = range(0, 3) 
         pdq = list(itertools.product(p, d, q)) 
         arima_results_aapl = [] 
         for param_set in pdq:
             try:
                 arima_aapl = ARIMA(train_aapl, order=param_set) 
                 arima_fitted_aapl = arima_aapl.fit() 
                 arima_results_aapl.append(arima_fitted_aapl.aic) 
             except:
                 continue
         print('**'*25)
         print('The Lowest AIC score is' + \
               '{:.4f} and the corresponding parameters are {}'.format( \
                   pd.DataFrame(arima_results_aapl).where( \
                   pd.DataFrame(arima_results_aapl).T.notnull().all()).min()[0],
                   pdq[arima_results_aapl.index(min(arima_results_aapl))])) 
         **************************************************
         The Lowest AIC score is 1951.9810 and the corresponding parameters are
         (4, 0, 4)

In [57]: arima_aapl = ARIMA(train_aapl, order=(4, 1, 4)) 
         arima_fit_aapl = arima_aapl.fit() 

In [58]: p = q = range(0, 6)
         d = range(0, 3)
         pdq = list(itertools.product(p, d, q))
         arima_results_msft = []
         for param_set in pdq:
             try:
                 arima_msft = ARIMA(stock_prices['MSFT'], order=param_set)
                 arima_fitted_msft = arima_msft.fit()
                 arima_results_msft.append(arima_fitted_msft.aic)
             except:
                 continue
         print('**' * 25)
         print('The lowest AIC score is {:.4f} and parameters are {}'
               .format(pd.DataFrame(arima_results_msft)
                       .where(pd.DataFrame(arima_results_msft).T.notnull()\
                              .all()).min()[0],
                       pdq[arima_results_msft.index(min(arima_results_msft))])) 
         **************************************************
         The Lowest AIC score is 2640.6367 and the corresponding parameters are
         (4, 2, 4)

In [59]: arima_msft = ARIMA(stock_prices['MSFT'], order=(4, 2 ,4)) 
         arima_fit_msft= arima_msft.fit() 

In [60]: arima_predict_aapl = arima_fit_aapl.predict(start=len(train_aapl),
                                                           end=len(train_aapl)\
                                                           +len(test_aapl) - 1,
                                                           dynamic=False) 
         arima_predict_msft = arima_fit_msft.predict(start=len(train_msft),
                                                           end=len(train_msft)\
                                                           + len(test_msft) - 1,
                                                           dynamic=False) 

In [61]: arima_predict_aapl = pd.DataFrame(arima_predict_aapl)
         arima_predict_aapl.index = diff_test_aapl.index
         arima_predict_msft = pd.DataFrame(arima_predict_msft)
         arima_predict_msft.index = diff_test_msft.index
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO13-1)

Defining a range for AR and MA orders

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO13-2)

Defining a range difference term

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO13-3)

Applying iteration over _p_, _d_, and _q_

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO13-4)

Creating an empty list to store AIC values

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO13-5)

Configuring the ARIMA model to fit Apple data

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO13-6)

Running the ARIMA model with all possible lags

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/7.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO13-7)

Storing AIC values into a list

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/8.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO13-8)

Printing the lowest AIC value for Apple data

[![9](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/9.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO13-9)

Configuring and fitting the ARIMA model with optimum orders

[![10](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/10.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO13-11)

Running the ARIMA model with all possible lags for Microsoft data

[![11](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/11.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO13-12)

Fitting the ARIMA model to Microsoft data with optimum orders

[![12](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/12.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#co\_introduction\_to\_time\_series\_modeling\_CO13-14)

Predicting Apple and Microsoft stock prices

Orders identified for Apple and Microsoft are (4, 1, 4) and (4, 2, 4), respectively. ARIMA does a good job in predicting the stock prices as shown below. However, please note that improper identification of the orders results in a poor fit, and this, in turn, produces predictions that are far from being satisfactory. The following code, resulting in [Figure 2-21](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch02.html#arima\_all\_2), shows these results:

```
In [62]: fig, ax = plt.subplots(2, 1, figsize=(18, 15))

         ax[0].plot(diff_test_aapl, label='Actual Stock Price', linestyle='--')
         ax[0].plot(arima_predict_aapl, linestyle='solid', label="Prediction")
         ax[0].set_title('Predicted Stock Price-Apple')
         ax[0].legend(loc='best')
         ax[1].plot(diff_test_msft, label='Actual Stock Price', linestyle='--')
         ax[1].plot(arima_predict_msft, linestyle='solid', label="Prediction")
         ax[1].set_title('Predicted Stock Price-Microsoft')
         ax[1].legend(loc='best')
         for ax in ax.flat:
             ax.set(xlabel='Date', ylabel='$')
         plt.show()
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0221.png" alt="arima all" height="495" width="600"><figcaption></figcaption></figure>

**Figure 2-21. ARIMA prediction results**

## Conclusion

Time series analysis has a central role in financial analysis. This is simply because most financial data has a time dimension, and this type of data should be modeled cautiously. This chapter worked out a first attempt at modeling data with a time dimension, and to do so, we employed classical time series models, namely MA, AR, and finally, ARIMA. But do you think that‚Äôs the whole story? Absolutely not! In the next chapter, we will see how a time series can be modeled using deep learning models.

## References

Articles cited in this chapter:

* Cavanaugh, J. E., and A. A. Neath. 2019. ‚ÄúThe Akaike Information Criterion: Background, Derivation, Properties, Application, Interpretation, and Refinements.‚Äù _Wiley Interdisciplinary Reviews: Computational Statistics_ 11 (3): e1460.
* Hurvich, Clifford M., and Chih-Ling Tsai. 1989. ‚ÄúRegression and Time Series Model Selection in Small Samples.‚Äù _Biometrika_ 76 (2): 297-30.

Books cited in this chapter:

* Brockwell, Peter J., and Richard A. Davis. 2016. _Introduction to Time Series and Forecasting_. Springer.
* Focardi, Sergio M. 1997. _Modeling the Market: New Theories and Techniques_. The Frank J. Fabozzi Series, Vol. 14. New York: John Wiley and Sons.
