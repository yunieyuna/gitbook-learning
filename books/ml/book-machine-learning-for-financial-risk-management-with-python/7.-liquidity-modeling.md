# 7. Liquidity Modeling

## Chapter 7. Liquidity Modeling

> When the music stops, in terms of liquidity, things will be complicated. But as long as the music is playing, you’ve got to get up and dance. We’re still dancing.
>
> Chuck Prince (2007)

Liquidity is another important source of financial risk. However, it has been long neglected, and the finance industry has paid a huge price for modeling risk without considering liquidity.

The causes of liquidity risk are departures from the complete markets and symmetric information paradigm, which can lead to moral hazard and adverse selection. To the extent that such conditions persist, liquidity risk is endemic in the financial system and can cause a vicious link between funding and market liquidity, prompting systemic liquidity risk (Nikolaou 2009).

Tapping into the time lag between a changing value of the variable and its impact on the real market turns out to be a success criterion in modeling. For instance, interest rates, to some extent, diverge from real market dynamics from time to time, and it takes some time to settle. Together with this, uncertainty is the solely source of risk in traditional asset pricing models; however, it is far from reality. To fill the gap between financial models and real-market dynamics, the liquidity dimension stands out. A model with liquidity can better adjust itself to developments in the financial markets in that liquidity affects both the required returns of assets and also the level of uncertainty. Thus, liquidity is quite an important dimension in estimating probability of default (Gaygisiz, Karasan, and Hekimoglu 2021).

The importance of liquidity has been highlighted and has gained much attention since the global mortgage crisis broke out in 2007–2008. During this crisis, most financial institutions were hit hard by liquidity pressures, resulting in several strict measures taken by regulatory authorities and central banks. Since then, debates over the need to include liquidity, originating from the lack of tradable securities, have intensified.

The concept of liquidity is multifaceted. By and large, a _liquid asset_ is defined by the extent to which a large amount of it is sold without a considerable price impact. This is also known as _transaction cost_. This is, however, not the only important facet of liquidity. Rather, during a period of stress, resilience stands out as investors seek prompt price discovery (Sarr and Lybek 2002). This is pointed out by Kyle (1985): “Liquidity is a slippery and elusive concept, in part because it encompasses a number of transactional properties of markets.”

With that said, liquidity is an ambiguous concept, and to define it we need to focus on its different dimensions. In the literature, different researchers come up with different dimensions of liquidity, but for the purposes of this book, we will identify four defining characteristics of liquidity:

Tightness

The ability to trade an asset at the same price at the same time. This refers to the transaction cost occurring during a trade. If the transaction cost is high, the difference between buy and sell prices will be high or vice versa. So, a narrow transaction cost defines how tight the market is.

Immediacy

The speed at which a large amount of buy or sell orders can be traded. This dimension of liquidity provides us with valuable information about the financial market, as low immediacy refers to malfunctioning of parts of the market such as clearing, settlement, and so forth.

Depth

The presence of large numbers of buyers and sellers who are able to cover abundant orders at various prices.

Resiliency

A market’s ability to bounce back from nonequilibrium. It can be thought of as a price-recovery process in which order imbalance dies out quickly.

Given the definition and interconnectedness of liquidity, it is not hard to see that modeling liquidity is a tough task. In the literature, many different types of liquidity models are proposed, however, considering the multidimensionality of liquidity, it may be wise to cluster the data depending on which dimension it captures. To this end, we will come up with different liquidity measures to represent all four dimensions. These liquidity measures are volume-based measures, transaction cost–based measures, price impact-based measures, and market-impact measures. For all these dimensions, several different liquidity proxies will be used.

Using clustering analysis, these liquidity measures will be clustered, which helps us to understand which part of liquidity an investor should focus on, because it is known that different dimensions of liquidity prevail in an economy during different time periods. Thus, once we are done with clustering analysis, we end up with a smaller number of liquidity measures. For the sake of clustering analysis, we will use the Gaussian mixture model (GMM) and the Gaussian mixture copula model (GMCM) to tackle this problem. GMM is a widely recognized clustering model that works well under elliptical distribution. GMCM is an extension of the GMM in that we include a copula analysis to take correlation into account. We will discuss these models in detail, so let us start by identifying the liquidity measures based on different liquidity dimensions.

## Liquidity Measures

The role of liquidity has finally been recognized by finance practitioners and economists, which makes it even more important to understand and develop liquidity measurement. Existing literature concentrates on a single measure by which it is hard to conceptualize an elusive concept like liquidity. Instead, we will cover four dimensions to develop a more comprehensive application:

* Volume
* Transaction cost
* Price impact
* Market impact

Let’s get started with the volume-based liquidity measures.

### Volume-Based Liquidity Measures

Large orders are covered when the market has depth, that is, a deep financial market has the ability to meet abundant orders. This, in turn, provides information about the market, and if the market lacks depth, order imbalance and discontinuity emerge in the market. Given the market’s depth, volume-based liquidity measures can be used to distinguish liquid and illiquid assets. Moreover, volume-based liquidity measures have a strong association with bid-ask spread: a large bid-ask spread implies low volume, while a narrow bid-ask spread implies high volume (Huong and Gregoriou 2020).

As you can imagine, a large portion of the variation in liquidity arises from trading activities. The importance of the volume-based approach is stressed by Blume, Easley, and O’Hara (1994) saying that volume traded generates information that cannot be extracted from alternative statistics.

To properly represent the depth dimension of liquidity, the following volume-based measures will be introduced:

* Liquidity ratio
* Hui-Heubel ratio
* Turnover ratio

#### Liquidity ratio

This ratio measures the extent to which volume is required to induce a price change of 1%:

����=∑�=1�������∑�=1�|����|

where ��� is the total price of stock _i_ on day _t_, ��� represents the volume traded of stock _i_ on day _t_, and finally, |����| is the absolute value of difference between price at time _t_ and _t_ - 1.

The higher the ratio ���� is, the higher the liquidity of asset _i_ will be. This implies that higher traded volume, ������, and low price difference, |����|, amount to high liquidity level. Conversely, if a low volume is necessary to initiate a price change, then this asset is referred to as illiquid. Obviously, this conceptual framework focuses more on the price aspect than on the issue of time or on the execution costs typically present in a market (Gabrielsen, Marzo, and Zagaglia 2011).

Let’s first import the data and observe it via the following codes. As it is readily observable, the main variables in the dataset are ask (`ASKHI`), bid (`BIDLO`), open (`OPENPRC`), and trading price (`PRC`) along with the volume (`VOL`), return (`RET`), volume-weighted return (`vwretx`) of the stock, and number of shares outstanding (`SHROUT`):

```
In [1]: import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        import warnings
        warnings.filterwarnings("ignore")
        plt.rcParams['figure.figsize'] = (10, 6)
        pd.set_option('use_inf_as_na', True)

In [2]: liq_data = pd.read_csv('bid_ask.csv')

In [3]: liq_data.head()

Out[3]: Unnamed: 0        Date  EXCHCD TICKER      COMNAM  BIDLO   ASKHI    PRC
         \
        0     1031570  2019-01-02     3.0   INTC  INTEL CORP  45.77  47.470
         47.08
        1     1031571  2019-01-03     3.0   INTC  INTEL CORP  44.39  46.280
         44.49
        2     1031572  2019-01-04     3.0   INTC  INTEL CORP  45.54  47.570
         47.22
        3     1031573  2019-01-07     3.0   INTC  INTEL CORP  46.75  47.995
         47.44
        4     1031574  2019-01-08     3.0   INTC  INTEL CORP  46.78  48.030
         47.74


                  VOL       RET     SHROUT  OPENPRC    vwretx

        0  18761673.0  0.003196  4564000.0   45.960  0.001783

        1  32254097.0 -0.055013  4564000.0   46.150 -0.021219

        2  35419836.0  0.061362  4564000.0   45.835  0.033399

        3  22724997.0  0.004659  4564000.0   47.100  0.009191

        4  22721240.0  0.006324  4564000.0   47.800  0.010240
```

Calculating some liquidity measures requires a rolling-window estimation, such as the calculation of the bid price for five days. To accomplish this task, the list named `rolling_five` is generated using the following code:

```
In [4]: rolling_five = []

        for j in liq_data.TICKER.unique():
            for i in range(len(liq_data[liq_data.TICKER == j])):
                rolling_five.append(liq_data[i:i+5].agg({'BIDLO': 'min',
                                                        'ASKHI': 'max',
                                                         'VOL': 'sum',
                                                         'SHROUT': 'mean',
                                                         'PRC': 'mean'})) 

In [5]: rolling_five_df = pd.DataFrame(rolling_five)
        rolling_five_df.columns = ['bidlo_min', 'askhi_max', 'vol_sum',
                                   'shrout_mean', 'prc_mean']
        liq_vol_all = pd.concat([liq_data,rolling_five_df], axis=1)


In [6]: liq_ratio = []

        for j in liq_vol_all.TICKER.unique():
            for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
                liq_ratio.append((liq_vol_all['PRC'][i+1:i+6] *
                                  liq_vol_all['VOL'][i+1:i+6]).sum()/
                                 (np.abs(liq_vol_all['PRC'][i+1:i+6].mean() -
                                         liq_vol_all['PRC'][i:i+5].mean())))
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO1-1)

Calculating the required statistical measures for five-day window

Now, we have minimum bid price, max ask price, summation of volume traded, mean of the number of shares outstanding, and mean of the trading price per five days.

#### Hui-Heubel ratio

Another measure that captures the depth is the Hui-Heubel liquidity ratio, known as ���:

���=����-��������/�/�¯×shrout

where ���� and ���� show maximum and minimum price over the determined period, respectively. �¯ is average closing price over determined period. What we have in the numerator is the percentage change in the stock price, and the volume traded is divided by market capitalization, i.e., �¯×shrout in the denominator. One of the most distinguish features of Hui-Heubel liquidity measure is that it is applicable to a single stock, not only portfolios.

As discussed by Gabrielsen, Marzo, and Zagaglia (2011), ���� and ���� can be replaced by bid-ask spread but due to low volatility in bid-ask spread, it tends to bias downward.

To compute the Hui-Heubel liquidity ratio, we first have the liquidity measures in a list, then we add all these measures into the dataframe to have all-encompassing data:

```
In [7]: Lhh = []

        for j in liq_vol_all.TICKER.unique():
            for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
                Lhh.append((liq_vol_all['PRC'][i:i+5].max() -
                            liq_vol_all['PRC'][i:i+5].min()) /
                           liq_vol_all['PRC'][i:i+5].min() /
                           (liq_vol_all['VOL'][i:i+5].sum() /
                            liq_vol_all['SHROUT'][i:i+5].mean() *
                            liq_vol_all['PRC'][i:i+5].mean()))
```

#### Turnover ratio

Turnover ratio has long been treated as a proxy for liquidity. It is basically the ratio of volatility to number of shares outstanding:

����=1���∑�=1������∑�=1�shrout��

where ��� denotes the number of trading days, ����� is the number of shares traded at time _t_, and shrout�� shows the number of shares outstanding at time _t_. A large turnover rate indicates a high level of liquidity, in that turnover implies trading frequency. As turnover rate incorporates the number of shares outstanding, it makes it a more subtle measure of liquidity.

Turnover ratio is calculated based on daily data, and then all the volume-based liquidity measures are converted into a dataframe and are included in `liq_vol_all`:

```
In [8]: turnover_ratio = []

        for j in liq_vol_all.TICKER.unique():
            for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
                turnover_ratio.append((1/liq_vol_all['VOL'].count()) *
                                      (np.sum(liq_vol_all['VOL'][i:i+1]) /
                                       np.sum(liq_vol_all['SHROUT'][i:i+1])))

In [9]: liq_vol_all['liq_ratio'] = pd.DataFrame(liq_ratio)
        liq_vol_all['Lhh'] = pd.DataFrame(Lhh)
        liq_vol_all['turnover_ratio'] = pd.DataFrame(turnover_ratio)
```

### Transaction Cost–Based Liquidity Measures

In the real world, buyers and sellers do not magically meet in a frictionless environment. Rather, intermediaries (brokers and dealers), equipment (computers and the like), patience (trades cannot be realized instantaneously), and a rule book that stipulates how orders are to be handled and turned into trades are required. Also, the orders of large, institutional investors are big enough to affect market prices. All of these imply the existence of trading costs, and just how to structure a market (and a broader marketplace) to contain these costs is a subtle and intricate challenge (Baker and Kıymaz (2013). This led to the emergence of transaction cost.

_Transaction cost_ is a cost an investor must bear during trade. It is referred to as any expenses related to the execution of trade. A distinction of transaction cost as explicit and implicit costs is possible. The former relates to order processing, taxes, and brokerage fees, while the latter includes more latent costs, such as bid-ask spread, timing of execution, and so on.

Transaction cost is related to the tightness and immediacy dimensions of liquidity. High transaction costs discourage investors to trade and this, in turn, decreases the number of buyers and sellers in the market so that the trading place diverges away from the more centralized market into a fragmented one, which result in a shallow market (Sarr and Lybek 2002). To the extent that transaction cost is low, investors are willing to trade and this results in a flourished trading environment in which markets will be more centralized.

Similarly, an abundance of buyers and sellers in a low transaction cost environment refers to the fact that a large number of orders are traded in a short period of time. So, immediacy is the other dimension of liquidity, which is closely related to the transaction cost.

Even though there is an ongoing debate about the goodness of bid-ask spread as well as the assurance that these models provide, bid-ask spread is a widely recognized proxy for transaction cost. To the extent that bid-ask spread is a good analysis of transaction cost, it is also a good indicator of liquidity by which the ease of converting an asset into cash (or a cash equivalent) might be determined. Without going into further detail, bid-ask spread can be measured by quoted spread, effective spread, and realized spread methods. So at first glance, it may seem strange to calculate bid-ask spread, which can be easily calculated by these methods. But this is not the case in reality. When the trade cannot be realized inside the quotes, then the spread is no longer the observed spread on which these methods are based.

#### Percentage quoted and effective bid-ask spreads

The other two well-known bid-ask spreads are _percentage quoted_ and _percentage effective_ bid-ask spreads. Quoted spread measures the cost of completing a trade, that is, the difference in the bid-ask spread. There are different forms of quoted spread but for the sake of scaling, we’ll choose the percentage quoted spread:

Percentagequotedspread=����-��������

where ���� is the ask price of the stock and ���� is the bid price of the stock.

The effective spread measures the deviation between trading price and the mid-price, which is sometimes called the true underlying value of the stock. When trades occur either within or outside the quotes, a better measure of trading costs is the percentage effective half spread, which is based on the actual trade price, and is computed on a percentage basis (Bessembinder and Venkataraman 2010):

Effectivespread=2|��-����|����

where �� is the trading price of the stock and ���� is the midpoint of the bid-ask offer prevailing at the time of the trade.

It is relatively easy to calculate the percentage quoted and effective bid-ask spreads, as shown in the following:

```
In [10]: liq_vol_all['mid_price'] = (liq_vol_all.ASKHI + liq_vol_all.BIDLO) / 2
         liq_vol_all['percent_quoted_ba'] = (liq_vol_all.ASKHI -
                                             liq_vol_all.BIDLO) / \
                                             liq_vol_all.mid_price
         liq_vol_all['percent_effective_ba'] = 2 * abs((liq_vol_all.PRC -
                                                      liq_vol_all.mid_price)) / \
                                                      liq_vol_all.mid_price
```

#### Roll’s spread estimate

One of the first and foremost spread measures was proposed by Roll (1984). The _Roll spread_ can be defined as:

Roll=-cov(���,���-1)

where ��� and ���-1 are the price differences at time _t_ and at time _t_ – 1, and cov denotes the covariance between these price differences.

Assuming that the market is efficient[1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#idm45737216974144) and the probability of distribution of observed price changes is stationary, Roll’s spread is motivated by the fact that serial correlation of price changes is a good proxy for liquidity.

One of the most important things to note in calculating Roll’s spread is that positive covariance is not well-defined, and it consists of almost half of the cases. The literature puts forth several methods to remedy this shortcoming, and we’ll embrace Harris’s (1990) approach in the following:

```
In [11]: liq_vol_all['price_diff'] = liq_vol_all.groupby('TICKER')['PRC']\
                                     .apply(lambda x:x.diff())
         liq_vol_all.dropna(inplace=True)
         roll = []

         for j in liq_vol_all.TICKER.unique():
              for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
                 roll_cov = np.cov(liq_vol_all['price_diff'][i:i+5],
                                   liq_vol_all['price_diff'][i+1:i+6]) 
                 if roll_cov[0,1] < 0: 
                     roll.append(2 * np.sqrt(-roll_cov[0, 1]))
                 else:
                      roll.append(2 * np.sqrt(np.abs(roll_cov[0, 1]))) 
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO2-1)

Calculating the covariance between price differences for the five-day window

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO2-2)

Checking the case where the covariance is negative

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO2-3)

In the case of positive covariance, Harris’s approach is applied

#### The Corwin-Schultz spread

The _Corwin-Schultz spread_ is rather intuitive and easy to apply. It rests mainly on the following assumption: given that the daily high and low prices are typically buyer and seller initiated, respectively, the observed price change can be split into effective price volatility and bid-ask spread. So the ratio of high-to-low prices for a day reflects both the stock’s variance and its bid-ask spread (Corwin and Schultz 2012; Abdi and Ranaldo 2017).

This spread proposes an entirely new approach based on the daily high and low prices only, and the logic behind it is summarized by Corwin and Schultz (2012) as “the sum of the price ranges over 2 consecutive single days reflect 2 days’ volatility and twice the spread, while the price range over one 2-day period reflects 2 days’ volatility and one spread”:

�=2(��-1)1+���=2�-�3-22-�3-22�=𝔼(∑�=01\[��(��+�0��+�0)]2)�=𝔼(∑�=01\[��(��+10��+10)]2)

where ��� (���) denotes actual high (low) prices on day _t_ and ��� or ��� the observed high (low) stock price on day _t_.

The Corwin-Schultz spread requires multiple steps to calculate, as it includes many variables. The following code presents our way of doing this calculation:

```
In [12]: gamma = []

         for j in liq_vol_all.TICKER.unique():
             for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
                 gamma.append((max(liq_vol_all['ASKHI'].iloc[i+1],
                                   liq_vol_all['ASKHI'].iloc[i]) -
                               min(liq_vol_all['BIDLO'].iloc[i+1],
                                   liq_vol_all['BIDLO'].iloc[i])) ** 2)
                 gamma_array = np.array(gamma)

In [13]: beta = []

         for j in liq_vol_all.TICKER.unique():
             for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
                 beta.append((liq_vol_all['ASKHI'].iloc[i+1] -
                              liq_vol_all['BIDLO'].iloc[i+1]) ** 2 +
                             (liq_vol_all['ASKHI'].iloc[i] -
                              liq_vol_all['BIDLO'].iloc[i]) ** 2)
                 beta_array = np.array(beta)

In [14]: alpha = ((np.sqrt(2 * beta_array) - np.sqrt(beta_array)) /
                (3 - (2 * np.sqrt(2)))) - np.sqrt(gamma_array /
                                                  (3 - (2 * np.sqrt(2))))
         CS_spread = (2 * np.exp(alpha - 1)) / (1 + np.exp(alpha))

In [15]: liq_vol_all = liq_vol_all.reset_index()
         liq_vol_all['roll'] = pd.DataFrame(roll)
         liq_vol_all['CS_spread'] = pd.DataFrame(CS_spread)
```

### Price Impact–Based Liquidity Measures

In this section, we will introduce price impact–based liquidity measures by which we are able to gauge the extent to which price is sensitive to volume and turnover ratio. Recall that resiliency refers to the market responsiveness about new orders. If the market is responsive to the new order—that is, a new order correct the imbalances in the market—then it is said to be resilient. Thus, given a change in volume and/or turnover ratio, high price adjustment amounts to resiliency or vice versa.

We have three price impact–based liquidity measures to discuss:

* The Amihud illiquidity measure
* The Florackis, Andros, and Alexandros (2011) price impact ratio
* Coefficient of elasticity of trading (CET)

#### Amihud illiquidity

This liquidity proxy is a celebrated and widely recognized measure. Amihud illiquidity (2002) basically measures the sensitivity of the return to trading volume. More concretely, it gives us a sense about a change in absolute return as trading volume changes by $1. The Amihud illiquidity measure, or ILLIQ for short, is well known among academics and practitioners:

ILLIQ=1���∑�=1���|����|����

where ���� is the stock return on day _t_ at month _t_, ���� represents the dollar volume on day _d_ at month _t_, and _D_ is the number of observation days in month _t_.

The Amihud measure has two advantages over many other liquidity measures. First, the Amihud measure has a simple construction that uses the absolute value of the daily return-to-volume ratio to capture price impact. Second, the measure has a strong positive relation with expected stock return (Lou and Tao 2017).

The Amihud illiquidity measure is not hard to calculate. However, before directly calculating the Amihud’s measure, the dollar volume of stocks needs to be computed:

```
In [16]: dvol = []

         for j in liq_vol_all.TICKER.unique():
             for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
                 dvol.append((liq_vol_all['PRC'][i:i+5] *
                              liq_vol_all['VOL'][i:i+5]).sum())
         liq_vol_all['dvol'] = pd.DataFrame(dvol)

In [17]: amihud = []

         for j in liq_vol_all.TICKER.unique():
             for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
                 amihud.append((1 / liq_vol_all['RET'].count()) *
                               (np.sum(np.abs(liq_vol_all['RET'][i:i+1])) /
                                       np.sum(liq_vol_all['dvol'][i:i+1])))
```

#### The price impact ratio

Florackis, Andros, and Alexandros (2011) aimed to improve the Amihud illiquidity ratio, and came up with a new liquidity measure, Return-to-Turnover (RtoTR). The disadvantages of Amihud’s illiquidity measure are listed by authors as:

* It is not comparable across stocks with different market capitalizations.
* It neglects the investor’s holding horizon.

To deal with these drawbacks, Florackis, Andros, and Alexandros presented a new measure, RtoTR, that replaces the volume ratio of Amihud’s model with turnover ratio so that the new measure is able to capture the trading frequency:

RtoTR=1���∑�=1���|����|�����

where ����� is the monetary volume of stock _i_ on day _d_ at month _t_, and the rest of the components are the same as in Amihud’s illiquidity measure.

The measure is as easy to calculate as Amihud’s measure, and it has no size bias because it includes the turnover ratio for capturing the trading frequency. This also helps us to examine the price and size effect.

The calculation of the price impact ratio is provided below:

```
In [18]: florackis = []

         for j in liq_vol_all.TICKER.unique():
             for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
                 florackis.append((1 / liq_vol_all['RET'].count()) *
                                  (np.sum(np.abs(liq_vol_all['RET'][i:i+1]) /
                                          liq_vol_all['turnover_ratio'][i:i+1])))
```

#### Coefficient of elasticity of trading

CET is a liquidity measure proposed to remedy the shortcomings of time-related liquidity measures such as number of trades and orders per unit of time. These measures are adopted to assess the extent to which market immediacy affects the liquidity level.

Market immediacy and CET go hand in hand as it measures the price elasticity of trading volume and if price is responsive (i.e., elastic) to the trading volume, that amounts to a greater level of market immediacy:

CET=%��%��

where %�� refers to change in trading volume and %�� denotes a change in price.

The Python code of the CET formula is provided below. As a first part of this application, percentage difference in volume and price are calculated. Then all price impact-based liquidity measures are stored in the `liq_vol_all` dataframe:

```
In [19]: liq_vol_all['vol_diff_pct'] = liq_vol_all.groupby('TICKER')['VOL']\
                                       .apply(lambda x: x.diff()).pct_change() 
         liq_vol_all['price_diff_pct'] = liq_vol_all.groupby('TICKER')['PRC']\
                                       .apply(lambda x: x.diff()).pct_change() 

In [20]: cet = []

         for j in liq_vol_all.TICKER.unique():
             for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
                 cet.append(np.sum(liq_vol_all['vol_diff_pct'][i:i+1])/
                            np.sum(liq_vol_all['price_diff_pct'][i:i+1]))

In [21]: liq_vol_all['amihud'] = pd.DataFrame(amihud)
         liq_vol_all['florackis'] = pd.DataFrame(florackis)
         liq_vol_all['cet'] = pd.DataFrame(cet)
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO3-1)

Calculating the percentage volume difference

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO3-2)

Calculating the percentage price difference

### Market Impact-Based Liquidity Measures

Identifying the source of information is a big deal in finance because an unknown source of information might mislead investors and lead to unintended consequences. A price surge, for instance, arising from the market does not provide the same information as one arising from an individual stock. With that being said, a new source of information should be identified in a way to capture price movement properly.

To accomplish this task, we use the capital asset pricing model (CAPM), by which we can distinguish systematic and unsystematic risk. The famous slope coefficient in CAPM indicates systematic risks, and the unsystematic risk is attributable to individual stocks as long as market risk is removed.

As it is referenced in Sarr and Lybek (2002), Hui-Heubel embraces the following approach:

��=�+���+��

where �� is the daily return on ��ℎ stock, and �� is the idiosyncratic or unsystematic risk.

Once we estimate residuals, ��, from the equation, it is regressed over the volatility, ��, and the estimated coefficient of �� gives the liquidity level of the related stock, also known as the idiosyncratic risk:

��2=�1+�2��+��

where ��2 denotes the squared residuals, �� is the daily percentage change in trading volume, and �� is the residual term.

Larger �2 implies larger price movements, and this gives us a sense about the liquidity of the stock. Conversely, the smaller �2 leads to smaller price movements, indicating higher liquidity levels. In code, we have:

```
In [22]: import statsmodels.api as sm

In [23]: liq_vol_all['VOL_pct_change'] = liq_vol_all.groupby('TICKER')['VOL']\
                                         .apply(lambda x: x.pct_change())
         liq_vol_all.dropna(subset=['VOL_pct_change'], inplace=True)
         liq_vol_all = liq_vol_all.reset_index()

In [24]: unsys_resid = []

         for i in liq_vol_all.TICKER.unique():
             X1 = liq_vol_all[liq_vol_all['TICKER'] == i]['vwretx'] 
             y = liq_vol_all[liq_vol_all['TICKER'] == i]['RET'] 
             ols = sm.OLS(y, X1).fit() 
             unsys_resid.append(ols.resid) 
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO4-1)

Assigning volume-weighted returns of all tickers as the independent variable

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO4-2)

Assigning returns of all tickers as the dependent variable

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO4-3)

Running the linear regression model with the defined variables

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO4-4)

Storing the residuals coming from the linear regression as an unsystematic factor

And then we calculate the market impact-based liquidity ratio:

```
In [25]: market_impact = {}

         for i, j in zip(liq_vol_all.TICKER.unique(),
                         range(len(liq_vol_all['TICKER'].unique()))):
             X2 = liq_vol_all[liq_vol_all['TICKER'] == i]['VOL_pct_change'] 
             ols = sm.OLS(unsys_resid[j] ** 2, X2).fit()
             print('***' * 30)
             print(f'OLS Result for {i}')
             print(ols.summary())
             market_impact[j] = ols.resid 
********************************************************************************
OLS Result for INTC
                        OLS Regression Results
================================================================================
Dep. Variable:                  y   R-squared (uncentered):                0.157
Model:                        OLS   Adj. R-squared (uncentered)            0.154
Method:             Least Squares   F-statistic:                           46.31
Date:            Thu, 02 Dec 2021   Prob (F-statistic):                 7.53e-11
Time:                    15:33:38   Log-Likelihood:                       1444.9
No. Observations:             249   AIC:                                  -2888.
Df Residuals:                 248   BIC:                                  -2884.
Df Model:                       1
Covariance Type:        nonrobust
================================================================================
                   coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------
VOL_pct_change    0.0008     0.000      6.805      0.000       0.001       0.001
==============================================================================
Omnibus:                      373.849   Durbin-Watson:                   1.908
Prob(Omnibus):                  0.000   Jarque-Bera (JB):            53506.774
Skew:                           7.228   Prob(JB):                         0.00
Kurtosis:                      73.344   Cond. No.                         1.00
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not
contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is
correctly specified.
********************************************************************************
OLS Result for MSFT
                        OLS Regression Results
================================================================================
Dep. Variable:                  y   R-squared (uncentered):                0.044
Model:                        OLS   Adj. R-squared (uncentered):           0.040
Method:             Least Squares   F-statistic:                           11.45
Date:            Thu, 02 Dec 2021   Prob (F-statistic):                 0.000833
Time:                    15:33:38   Log-Likelihood:                       1851.0
No. Observations:             249   AIC:                                 -3700.
Df Residuals:                 248   BIC:                                 -3696.
Df Model:                       1
Covariance Type:        nonrobust
================================================================================
                   coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------
VOL_pct_change  9.641e-05  2.85e-05     3.383      0.001      4.03e-05     0.000
==============================================================================
Omnibus:                      285.769   Durbin-Watson:                   1.533
Prob(Omnibus):                  0.000   Jarque-Bera (JB):            11207.666
Skew:                           4.937   Prob(JB):                         0.00
Kurtosis:                      34.349   Cond. No.                         1.00
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not
contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is
correctly specified.

********************************************************************************
OLS Result for IBM
                         OLS Regression Results
================================================================================
Dep. Variable:                  y   R-squared (uncentered):                0.134
Model:                        OLS   Adj. R-squared (uncentered):           0.130
Method:             Least Squares   F-statistic:                           38.36
Date:            Thu, 02 Dec 2021   Prob (F-statistic):                 2.43e-09
Time:                    15:33:38   Log-Likelihood:                       1547.1
No. Observations:             249   AIC:                                  -3092.
Df Residuals:                 248   BIC:                                  -3089.
Df Model:                       1
Covariance Type:        nonrobust
================================================================================
                  coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------
VOL_pct_change  0.0005   7.43e-05      6.193      0.000       0.000       0.001
==============================================================================
Omnibus:                      446.818   Durbin-Watson:                   2.034
Prob(Omnibus):                  0.000   Jarque-Bera (JB):           156387.719
Skew:                           9.835   Prob(JB):                         0.00
Kurtosis:                     124.188   Cond. No.                         1.00
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not
contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is
correctly specified.
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO5-1)

Assigning percentage change in volume of all tickers as the independent variable

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO5-2)

Market impact the residual of this linear regression

Then, we include the market impact in our dataframe and observe the summary statistics of all the liquidity measures that we’ve introduced so far:

```
In [26]: append1 = market_impact[0].append(market_impact[1])
         liq_vol_all['market_impact'] = append1.append(market_impact[2]) 

In [27]: cols = ['vol_diff_pct', 'price_diff_pct', 'price_diff',
                 'VOL_pct_change', 'dvol', 'mid_price']
         liq_measures_all = liq_vol_all.drop(liq_vol_all[cols], axis=1)\
                            .iloc[:, -11:]
         liq_measures_all.dropna(inplace=True)
         liq_measures_all.describe().T
Out[27]:                       count          mean           std           min \
         liq_ratio             738.0  7.368514e+10  2.569030e+11  8.065402e+08
         Lhh                   738.0  3.340167e-05  5.371681e-05  3.966368e-06
         turnover_ratio        738.0  6.491127e-03  2.842668e-03  1.916371e-03
         percent_quoted_ba     738.0  1.565276e-02  7.562850e-03  3.779877e-03
         percent_effective_ba  738.0  8.334177e-03  7.100304e-03  0.000000e+00
         roll                  738.0  8.190794e-01  6.066821e-01  7.615773e-02
         CS_spread             738.0  3.305464e-01  1.267434e-01  1.773438e-40
         amihud                738.0  2.777021e-15  2.319450e-15  0.000000e+00
         florackis             738.0  2.284291e-03  1.546181e-03  0.000000e+00
         cet                   738.0 -1.113583e+00  3.333932e+01 -4.575246e+02
         market_impact         738.0  8.614680e-05  5.087547e-04 -1.596135e-03

                                   25%           50%           75%           max
    liq_ratio             1.378496e+10  2.261858e+10  4.505784e+10  3.095986e+12
    Lhh                   1.694354e-05  2.368095e-05  3.558960e-05  5.824148e-04
    turnover_ratio        4.897990e-03  5.764112e-03  7.423111e-03  2.542853e-02
    percent_quoted_ba     1.041887e-02  1.379992e-02  1.878123e-02  5.545110e-02
    percent_effective_ba  3.032785e-03  6.851479e-03  1.152485e-02  4.656669e-02
    roll                  4.574986e-01  6.975982e-01  1.011879e+00  4.178873e+00
    CS_spread             2.444225e-01  3.609800e-01  4.188028e-01  5.877726e-01
    amihud                1.117729e-15  2.220439e-15  3.766086e-15  1.320828e-14
    florackis             1.059446e-03  2.013517e-03  3.324181e-03  7.869841e-03
    cet                  -1.687807e-01  5.654237e-01  1.660166e+00  1.845917e+02
    market_impact        -3.010645e-05  3.383862e-05  1.309451e-04  8.165527e-03
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO6-1)

Appending market impact into the `liq_vol_all` dataframe.

These are the liquidity measures that we take advantage of in the process of modeling the liquidity via GMM. Now let’s discuss this via a probabilistic unsupervised learning algorithm.

## Gaussian Mixture Model

What happens if we have data with several modes that represent different aspects of the data? Or let’s put it in the context of liquidity measures, how can you model liquidity measures with different mean variance? As you can imagine, data consisting of liquidity measures is multimodal, meaning that there exists several different high-probability masses and our task is to find out which model fits best to this type of data.

It is evident that the proposed model is supposed to include a mixture of several components, and without knowing the specific liquidity measure, it should be clustered based on the values obtained from the measures. To recap, we will have one big dataset that includes all liquidity measures, and assuming for the moment that we forgot to assign labels to these measures, we need a model that presents different distributions of these measures without knowing the labels. This model is GMM, which enables us to model multimodal data without knowing the names of the variables.

Considering the different focus of the liquidity measures introduced before, if we somehow manage to model this data, that means we can capture different liquidity level at different times. For instance, liquidity in a high-volatility period cannot be modeled in the same way as a low-volatility period. In a similar vein, given the depth of the market, we need to focus on these different aspects of liquidity. GMM provides us with a tool to address this problem.

Long story short, if a market is experiencing a boom period, which coincides with high volatility, volume and transaction cost–based measures would be good choices, and if a market ends up with price discontinuity, price-based measures would be the optimal choice. Of course, we are not talking about one-size-fits-all measures—there may be some instances in which a mixture of measures would work better.

As put by VanderPlas (2016), for K-means to succeed, cluster models must have circular characteristics. Nevertheless, many financial variables exhibit non-circular shapes that make it hard to model via K-means. As is readily observable, liquidity measures overlap and do not have circular shapes, so GMM with its probabilistic nature would be a good choice for modeling this type of data, as described by Fraley and Raftery (1998):

> One advantage of the mixture-model approach to clustering is that it allows the use of approximate Bayes factors to compare models. This gives a systematic means of selecting not only the parameterization of the model (and hence the clustering method), but also the number of clusters.

In this part, we would like to import necessary libraries to be used in the GMM. Also, scaling is applied, which is an essential step in clustering as we have mixed numerical values in the dataframe. In the last part of the code that follows, a histogram is drawn to observe the multimodality in the data ([Figure 7-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#modality\_liq)). This is a phenomenon that we have discussed in the very first part of this section.

```
In [28]: from sklearn.mixture import GaussianMixture
         from sklearn.preprocessing import StandardScaler

In [29]: liq_measures_all2 = liq_measures_all.dropna()
         scaled_liq = StandardScaler().fit_transform(liq_measures_all2)

In [30]: kwargs = dict(alpha=0.5, bins=50,  stacked=True)
         plt.hist(liq_measures_all.loc[:, 'percent_quoted_ba'],
                  **kwargs, label='TC-based')
         plt.hist(liq_measures_all.loc[:, 'turnover_ratio'],
                  **kwargs, label='Volume-based')
         plt.hist(liq_measures_all.loc[:, 'market_impact'],
                  **kwargs, label='Market-based')
         plt.title('Multi Modality of the Liquidity Measures')
         plt.legend()
         plt.show()
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0701.png" alt="modality" height="365" width="600"><figcaption></figcaption></figure>

**Figure 7-1. Multimodality of the liquidity measures**

And now, given the transaction cost, volume, and market-based liquidity measures, multimodality (i.e., three peaks) can be easily observed in [Figure 7-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#modality\_liq). Due to the scaling issue, the price impact–based liquidity dimension is not included in the histogram.

Now, let’s run GMM and see how we can cluster the liquidity measures. But first, a common question arises: how many clusters should we have? To address this question, we’ll use BIC again, and generate the plot shown in [Figure 7-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#scree\_gmm):

```
In [31]: n_components = np.arange(1, 10)
         clusters = [GaussianMixture(n, covariance_type='spherical',
                                     random_state=0).fit(scaled_liq)
                   for n in n_components] 
         plt.plot(n_components, [m.bic(scaled_liq) for m in clusters]) 
         plt.title('Optimum Number of Components')
         plt.xlabel('n_components')
         plt.ylabel('BIC values')
         plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO7-1)

Generating different BIC values based on different numbers of clusters

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO7-2)

Drawing a line plot for BIC values given number of components

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0702.png" alt="opt_cluster" height="365" width="600"><figcaption></figcaption></figure>

**Figure 7-2. Optimum number of components**

[Figure 7-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#scree\_gmm) shows us that the line seems to flatten out after the third cluster, making that an ideal point at which to stop.

Using the following code, we are able to detect the state by which data is best represented. The term _state_ represents nothing but the cluster with the highest posterior probability. It means that this specific state accounts for the dynamics of the data most. In this case, State-3 with a probability of 0.55 is the most likely state to explain the dynamics of the data:

```
In [32]: def cluster_state(data, nstates):
             gmm = GaussianMixture(n_components=nstates,
                                   covariance_type='spherical',
                                   init_params='kmeans') 
             gmm_fit = gmm.fit(scaled_liq) 
             labels = gmm_fit.predict(scaled_liq) 
             state_probs = gmm.predict_proba(scaled_liq) 
             state_probs_df = pd.DataFrame(state_probs,
                                           columns=['state-1','state-2',
                                                    'state-3'])
             state_prob_means = [state_probs_df.iloc[:, i].mean()
                                 for i in range(len(state_probs_df.columns))] 
             if np.max(state_prob_means) == state_prob_means[0]:
                 print('State-1 is likely to occur with a probability of {:4f}'
                       .format(state_prob_means[0]))
             elif np.max(state_prob_means) == state_prob_means[1]:
                 print('State-2 is likely to occur with a probability of {:4f}'
                       .format(state_prob_means[1]))
             else:
                 print('State-3 is likely to occur with a probability of {:4f}'
                       .format(state_prob_means[2]))
             return state_probs

In [33]: state_probs = cluster_state(scaled_liq, 3)
         print(f'State probabilities are {state_probs.mean(axis=0)}')

         State-3 is likely to occur with a probability of 0.550297
         State probabilities are [0.06285593 0.38684657 0.5502975 ]
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO8-1)

Configuring the GMM

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO8-2)

Fitting GMM with scaled data

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO8-3)

Running prediction

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO8-4)

Obtaining the state probabilities

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO8-5)

Computing the average of all three state probabilities

All right, does it not make sense to apply GMM to cluster liquidity measures and extract the likely state to represent it as one-dimensional data? It literally makes our lives easier because at the end of the data, we come up with only one cluster with highest probability. But what would you think if we applied PCA to fully understand which variables are correlated with the prevailing state? In PCA, we are able to build a bridge between components and features using loadings so that we can analyze which liquidity measures have the defining characteristics of a specific period.

As a first step, let’s apply PCA and create a scree plot ([Figure 7-3](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#scree\_plot\_gmm)) to determine the number of components we are working with:

```
In [34]: from sklearn.decomposition import PCA

In [35]: pca = PCA(n_components=11)
         components = pca.fit_transform(scaled_liq)
         plt.plot(pca.explained_variance_ratio_)
         plt.title('Scree Plot')
         plt.xlabel('Number of Components')
         plt.ylabel('% of Explained Variance')
         plt.show()
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_0703.png" alt="scree_plot_gmm" height="370" width="600"><figcaption></figcaption></figure>

**Figure 7-3. Scree plot**

Based on [Figure 7-3](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#scree\_plot\_gmm), we’ll decide to stop at component 3.

As we now have determined the number of components, let’s rerun PCA with three components and GMM. Similar to our previous GMM application, posterior probability is calculated and assigned to a variable named `state_probs`:

```
In [36]: def gmm_pca(data, nstate):
             pca = PCA(n_components=3)
             components = pca.fit_transform(data)
             mxtd = GaussianMixture(n_components=nstate,
                                    covariance_type='spherical')
             gmm = mxtd.fit(components)
             labels = gmm.predict(components)
             state_probs = gmm.predict_proba(components)
             return state_probs,pca

In [37]: state_probs, pca = gmm_pca(scaled_liq, 3)
         print(f'State probabilities are {state_probs.mean(axis=0)}')
         State probabilities are [0.7329713  0.25076855 0.01626015]
```

In what follows, we find out the state with the highest probability, and it turns out to be State-1 with a probability of 73%:

```
In [38]: def wpc():
             state_probs_df = pd.DataFrame(state_probs,
                                           columns=['state-1', 'state-2',
                                                    'state-3'])
             state_prob_means = [state_probs_df.iloc[:, i].mean()
                                 for i in range(len(state_probs_df.columns))]
             if np.max(state_prob_means) == state_prob_means[0]:
                 print('State-1 is likely to occur with a probability of {:4f}'
                       .format(state_prob_means[0]))
             elif np.max(state_prob_means) == state_prob_means[1]:
                 print('State-2 is likely to occur with a probability of {:4f}'
                       .format(state_prob_means[1]))
             else:
                 print('State-3 is likely to occur with a probability of {:4f}'
                       .format(state_prob_means[2]))
         wpc()
         State-1 is likely to occur with a probability of 0.732971
```

Let’s now turn our attention to finding which liquidity measures matter most using loading analysis. This analysis suggests that `turnover_ratio`, `percent_quoted_ba`, `percent_effective_ba`, `amihud`, and `florackis` ratios are the liquidity ratios composing the State-1. The following code shows the result:

```
In [39]: loadings = pca.components_.T * np.sqrt(pca.explained_variance_) 
         loading_matrix = pd.DataFrame(loadings,
                                       columns=['PC1', 'PC2', 'PC3'],
                                       index=liq_measures_all.columns)
         loading_matrix
Out[39]:                            PC1       PC2       PC3
         liq_ratio             0.116701 -0.115791 -0.196355
         Lhh                  -0.211827  0.882007 -0.125890
         turnover_ratio        0.601041 -0.006381  0.016222
         percent_quoted_ba     0.713239  0.140103  0.551385
         percent_effective_ba  0.641527  0.154973  0.526933
         roll                 -0.070192  0.886080 -0.093126
         CS_spread             0.013373 -0.299229 -0.092705
         amihud                0.849614 -0.020623 -0.488324
         florackis             0.710818  0.081948 -0.589693
         cet                  -0.035736  0.101575  0.001595
         market_impact         0.357031  0.095045  0.235266
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO9-1)

Calculating loading from PCA

## Gaussian Mixture Copula Model

Given the complexity and sophistication of financial markets, it is not possible to suggest one-size-fits-all risk models. Thus, financial institutions develop their own models for credit, liquidity, market, and operational risks so that they can manage the risks they face more efficiently and realistically. However, one of the biggest challenges that these financial institutions come across is the correlation, also known as joint distribution, of the risk, as put by Rachev and Stein (2009):

> With the emergence of the sub-prime crisis and the following credit crunch, academics, practitioners, philosophers and journalists started searching for causes and failures that led to the turmoil and (almost) unprecedented market deteriorations... the arguments against several methods and models used at Wall Street and throughout the world are, in many cases, putting those in the wrong light. Beyond the fact that risks and issues were clouded by the securitization, tranching and packaging of underlyings in the credit markets as well as by the unfortunate and somehow misleading role of rating agencies, mathematical models were used in the markets which are now under fire due to their incapability of capturing risks in extreme market phases.

The task of modeling “extreme market phases” leads us to the concept of joint distribution by which we are allowed to model multiple risks with a single distribution.

A model disregarding the interaction of risks is destined for failure. In this respect, an intuitive yet simple approach is proposed: _copulas_.

Copula is a function that maps marginal distribution of individual risks to multivariate distribution, resulting in a joint distribution of many standard uniform random variables. If we are working with a known distribution, such as normal distribution, it is easy to model joint distribution of variables, known as bivariate normal. However, the challenge here is to define the correlation structure between these two variables, and this is the point at which copulas come in (Hull 2012).

With Sklar’s theorem, let _F_ be a marginal continuous cumulative distribution function (CDF) of ��. A CDF transformation maps a random variable to a scalar that is uniformly distributed in \[0,1]. However, the joint distribution of all these marginal CDFs does not follow uniform distribution and a copula function (Kasa and Rajan 2020):

�:\[0,1]�→\[0,1]

where _i_ shows the number of marginal CDFs. In other words, in the bivariate case, _i_ takes the value of 2 and the function becomes:

�:\[0,1]2→\[0,1]

Hence:

�(�1,�2)≡�(�1(�1),...,��(��))

where _C_ is copula and unique given the marginal distribution of ��s are continuous and _F_ is joint cumulative distribution.

Alternatively, the copula function can be described by individual marginal densities:

�(�)=�(�1(�1),...,��(��))∏�=1���(��)

where �(�) denotes multivariate density, and �� is marginal density of the ��ℎ asset.

We cannot complete our discussion without stating the assumptions that we need to satisfy for copulas. Here are the assumptions from Bouye (2000):

1. �=�1×�2, where �1 and �2 are non-empty subsets of \[0,1].
2.  _C_ is an increasing function such that 0≤�1≤�2≤1 and 0≤�1≤�2≤1.

    �(\[�1,�1]×\[�2,�2])≡�(�2,�2)-�(�2,�2)-�(�1,�2)+�(�1,�1)≥0
3. For every _u_ in �1 and for every _v_ in �2: _C_(_u_, 1) = _u_ and _C_(1, _v_) = _v_.

After a long theoretical discussion about copulas, you may be tempted to think about the complexity of its coding in Python. No worries, we have a library for that and it is really easy to apply. The name of the Python library for copulas is called Copulae, and we will make use of it in the following:

```
In [40]: from copulae.mixtures.gmc.gmc import GaussianMixtureCopula 

In [41]: _, dim = scaled_liq.shape
         gmcm = GaussianMixtureCopula(n_clusters=3, ndim=dim) 

In [42]: gmcm_fit = gmcm.fit(scaled_liq,method='kmeans',
                             criteria='GMCM', eps=0.0001) 
         state_prob = gmcm_fit.params.prob
         print(f'The state {np.argmax(state_prob) + 1} is likely to occur')
         print(f'State probabilities based on GMCM are {state_prob}')
         The state 2 is likely to occur
         State probabilities based on GMCM are [0.3197832  0.34146341 0.
         33875339]
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO10-1)

Importing `GaussianMixtureCopula` from `copulae`

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO10-2)

Configuring GMCM with the number of clusters and dimensions

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#co\_liquidity\_modeling\_CO10-3)

Fitting the GMCM

The result suggests that when the correlation is taken into account, State-2 prevails, but the posterior probabilities are very close to each other, implying that when correlation between liquidity measures comes into the picture, commonality in liquidity stands out.

## Conclusion

Liquidity risk has been under a microscope for over a decade as it is an important source of risk by itself and also has high correlation with other financial risks.

This chapter introduces a new method for liquidity modeling based on GMM, which allows us to model multivariate data and generate clusters. Given the posterior probability of these clusters, we were able to determine which cluster represented the defining characteristics of the data. However, without considering the correlation structure of the liquidity measures, our model may not have been a good representation of reality. Thus, to address this concern, we introduced GMCM, and the defining cluster was redefined by taking into account the correlation structure among the variables.

After completing the liquidity modeling, we are now ready to discuss another important source of financial risk: _operational risk_. Operational risk may arise for a variety of reasons, but we will discuss operational risk via fraudulent activities.

## References

Articles cited in this chapter:

* Abdi, Farshid, and Angelo Ranaldo. 2017. “A Simple Estimation of Bid-Ask Spreads from Daily Close, High, and Low Prices.” _The Review of Financial Studies_ 30 (12): 4437-4480.
* Baker, H. Kent, and Halil Kiymaz, eds. 2013. _Market Microstructure in Emerging and Developed Markets: Price Discovery, Information Flows, and Transaction Costs_. Hoboken, New Jersey: John Wiley and Sons.
* Bessembinder, Hendrik, and Kumar Venkataraman. 2010. “Bid–Ask Spreads.” in _Encyclopedia of Quantitative Finance_, edited b. Rama Cont. Hoboken, NJ: John Wiley and Sons.
* Blume, Lawrence, David Easley, and Maureen O’Hara. 1994 “Market Statistics and Technical Analysis: The Role of Volume.” The Journal of Finance 49 (1): 153-181.
* Bouyé, Eric, Valdo Durrleman, Ashkan Nikeghbali, Gaël Riboulet, and Thierry Roncalli. 2000. “Copulas for Finance: A Reading Guide and Some Applications.” Available at SSRN 1032533.
* Chuck, Prince. 2007. “Citigroup Chief Stays Bullish on Buy-Outs.” _Financial Times_. [_https://oreil.ly/nKOZk_](https://oreil.ly/nKOZk).
* Corwin, Shane A., and Paul Schultz. 2012. “A Simple Way to Estimate Bid‐Ask Spreads from Daily High and Low Prices.” _The Journal of Finance_ 67 (2): 719-760.
* Florackis, Chris, Andros Gregoriou, and Alexandros Kostakis. 2011. “Trading Frequency and Asset Pricing on the London Stock Exchange: Evidence from a New Price Impact Ratio.” _Journal of Banking and Finance_ 35 (12): 3335-3350.
* Fraley, Chris, and Adrian E. Raftery. 1998. “How Many Clusters? Which Clustering Method? Answers via Model-Based Cluster Analysis.” The Computer Journal 41 (8): 578-588.
* Gabrielsen, Alexandros, Massimiliano Marzo, and Paolo Zagaglia. 2011. “Measuring Market Liquidity: An Introductory Survey.” _SRN Electronic Journal_.
* Harris, Lawrence. 1990. “Statistical Properties of the Roll Serial Covariance Bid/Ask Spread Estimator.” _The Journal of Finance_ 45 (2): 579-590.
* Gaygisiz, Esma, Abdullah Karasan, and Alper Hekimoglu. 2021. “Analyses of factors of Market Microstructure: Price impact, liquidity, and Volatility.” _Optimization_ (Forthcoming).
* Kasa, Siva Rajesh, and Vaibhav Rajan. 2020. “Improved Inference of Gaussian Mixture Copula Model for Clustering and Reproducibility Analysis using Automatic Differentiation.” arXiv preprint arXiv:2010.14359.
* Kyle, Albert S. 1985. “Continuous Auctions and Insider Trading.” _Econometrica_ 53 (6): 1315-1335.
* Le, Huong, and Andros Gregoriou. 2020. “How Do You Capture Liquidity? A Review of the Literature on Low‐Frequency Stock Liquidity.” _Journal of Economic Surveys_ 34 (5): 1170-1186.
* Lou, Xiaoxia, and Tao Shu. 2017. “Price Impact or Trading Volume: Why Is the Amihud (2002) measure Priced?.” _The Review of Financial Studies_ 30 (12): 4481-4520.
* Nikolaou, Kleopatra. 2009. “Liquidity (Risk) concepts: Definitions and Interactions.” European Central Bank Working Paper Series 1008.
* Rachev, S. T., W. Sun, and M. stein. 2009. “Copula Concepts in Financial Markets.” _Portfolio Institutionell_ (4): 12-15.
* Roll, Richard. 1984. “A Simple Implicit Measure of the Effective Bid‐Ask Spread in an Efficient Market.” _The Journal of Finance_ 29 (4): 1127-1139.
* Sarr, Abdourahmane, and Tonny Lybek. 2002. “Measuring liquidity in financial markets.” IMF Working Papers (02/232): 1-64.

Books and online sources cited in this chapter:

* Hull, John. 2012. _Risk Management and Financial Institutions_. Hoboken, New Jersey: John Wiley and Sons.
* VanderPlas, Jake. 2016. _Python Data Science Handbook: Essential Tools for Working with Data_. Sebastopol: O’Reilly.

[1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch07.html#idm45737216974144-marker) _Efficient market_ refers to how well and fast current prices reflect all available information about the value of the underlying asset(s).
