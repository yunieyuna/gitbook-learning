# 10. Synthetic Data Generation and The Hidden Markov Model in Finance

## Chapter 10. Synthetic Data Generation and The Hidden Markov Model in Finance

> The data does not have to be rooted in the real world to have value: it can be fabricated and slotted in where some is missing or hard to get hold of.
>
> Ahuja (2020)

Synthetic data generation has been gaining attention in finance due to rising concerns about confidentiality and increasing data requirements. So, instead of working with real data, why not feed your model with synthetic data as long as it mimics the requisite statistical properties? It sounds appealing, doesn’t it? Synthetic data generation is one part of this chapter; the other part is devoted to another underappreciated but quite important and interesting topic: the hidden Markov model (HMM). You may be tempted to ask: what is the common ground between synthetic data and HMM? Well, we can generate synthetic data from HMM—and this is one of the aims of this chapter. The other aim is to introduce these two important topics, as they are often used in machine learning.

## Synthetic Data Generation

The confidentiality, sensitivity, and cost of financial data greatly restricts its usage. This, in turn, hinders the progress and dissemination of useful knowledge in finance. Synthetic data addresses these drawbacks and helps researchers and practitioners conduct their analyses and disseminate the results.

Synthetic data is data generated from a process by which it mimics the statistical properties of the real data. Even though there is a belief that data must be modeled in its original form, generating synthetic data from real data is not the only way we can create it (Patki, Wedge, and Veeramachaneni 2016). Rather, there are three ways we can generate synthetic data:

* Synthetic data can be generated from the _real data_. The workflow of this process starts with getting real data, and continues with modeling to unveil the distribution of the data, and as a last step synthetic data is sampled from this existing model.
* Synthetic data can be obtained from a _model or knowledge_. Generally speaking, this type of synthetic data generation can be applied either by using an existing model or knowledge of the researcher.
* A _hybrid_ process includes the previous two steps, because sometimes only a part of the data becomes available and this part of the real data is used to generate synthetic data and the other part of the synthetic data can be obtained from a model.

We will soon see how we can apply these techniques to generate synthetic data. By its nature, the synthetic data generation process has an uncompromising trade-off between privacy and utility. To be exact, synthetic data generation from real undisclosed data results in high utility. However, the utility of the synthetic data generation depends greatly on the deidentification and aggregation of real public data. The utility of synthetic data generation is dependent upon successful modeling or the expertise of the analyst.

The privacy-utility trade-off in the context of data-generating processes is illustrated in [Figure 10-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#synt\_sch).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_1001.png" alt="synt_sch" height="458" width="600"><figcaption></figcaption></figure>

**Figure 10-1. Privacy-utility trade-off**

## Evaluation of the Synthetic Data

As you can imagine, various tools can be applied to measure the effectiveness of the synthetic data; however, we will restrict our attention to four commonly embraced methods: KL-divergence, distinguishable, ROC curve, and comparing the main statistics such as mean, median, etc. As KL-divergence and ROC were discussed in Chapters [8](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch08.html#chapter\_8) and [6](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch06.html#chapter\_6), respectively, we will skip over those and start with the distinguishable method.

_Distinguishable_, as its name implies, tries to distinguish between real and synthetic records by assigning 1 if they are real and 0 if not using a classification model that distinguishes between real and synthetic data. If the output is closer to 1, it predicts that the record is real, otherwise it predicts it is synthetic data using a _propensity score_ (El Emam 2020).

The other method is easy yet powerful, and is based on comparing the main statistics of the real and synthetic data. Given the model employed, the mean (or other statistic) of the real data and the synthetic data can be compared to get a sense of the extent to which the synthetic data mimics the real data.

Let’s discuss the advantages and disadvantages of synthetic data generation:

AdvantagesIncreased availability of data

Synthetic data generation equips us with a strong tool by which we can overcome the difficulty of accessing real data, which can be costly and proprietary.

Improved analytical skill

Synthetic data as a good proxy of the real data can be used in various analytical processes, and this, in turn, improves our understanding of a specific topic. Besides, synthetic data can be used for labeling, paving the way for highly accurate analyses.

Handling common statistical problems

Synthetic data generation can mitigate the problems arising from real data. Real data may come with issues—such as missing values, outliers, and so on—that badly affect the performance of the model. Synthetics data provides a tool to cope with these statistical problems so that we might end up with improved modeling performance.

DisadvantagesInability to preserve confidentiality

Due to cyberattacks, synthetic data might turn out to be a source of private-information leakage. For instance, the credentials of customers can be obtained by reverse engineering.

Quality concerns

There are two important things that need to be taken into account during the synthetic data generation process: the researcher’s ability and the characteristics of data. These two points determine the quality process of synthetic data generation. If these points are lacking, it is likely to expect low-quality synthetic data.

## Generating Synthetic Data

Let’s start off with generating synthetic data first from real data, and then from a model. We will use real data from `fetch_california_housing` to generate synthetic data, and we will also use the CTGAN library (`CTGANSynthesizer`) in this process. The CTGAN library enables us to generate synthetic data with high fidelity to the original data based on generative adversarial networks (GANs). In generating synthetic data, the number of training steps is controlled by `epoch` parameter, which enables us to obtain synthetic data in a short period of time:

```
In [1]: from sklearn.datasets import fetch_california_housing 
        import pandas as pd
        import numpy as np
        import matplotlib. pyplot as plt
        import yfinance as yf
        import datetime
        import warnings
        warnings.filterwarnings('ignore')

In [2]: X, y = fetch_california_housing(return_X_y=True) 

In [3]: import numpy as np
        california_housing=np.column_stack([X, y]) 
        california_housing_df=pd.DataFrame(california_housing)

In [4]: from ctgan import CTGANSynthesizer 

        ctgan = CTGANSynthesizer(epochs=10) 
        ctgan.fit(california_housing_df)
        synt_sample = ctgan.sample(len(california_housing_df)) 
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO1-1)

Importing the `fetch_california_housing` data from `sklearn`

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO1-2)

Generating independent and dependent variables from `fetch_california_​housing`

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO1-3)

Stacking two arrays using the stack function

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO1-4)

Importing `CTGANSynthesizer` for synthetic data generation

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO1-5)

Initializing the synthetic data generation process from `CTGANSynthesizer` with an `epoch` of 10

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/6.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO1-6)

Generating the sample

After generating the synthetic data, the similarity of the synthetic data can be checked by descriptive statistics. As always, descriptive statistics is handy, but we have another tool, the `evaluate` package from the Synthetic Data Vault (SDV). The output of this function will be a number between 0 and 1, which will indicate how similar the two tables are, with 0 being the worst and 1 being the best possible score. In addition, the result of the generation process can be visualized (in the resulting Figures [10-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#eval\_synt1) and [10-3](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#eval\_synt2)) and compared with the real data so we can fully understand whether the synthetic data is a good representation of the real data or not:

```
In [5]: california_housing_df.describe()

Out[5]:              0             1             2             3             4 \
   count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000
   mean       3.870671     28.639486      5.429000      1.096675   1425.476744
   std        1.899822     12.585558      2.474173      0.473911   1132.462122
   min        0.499900      1.000000      0.846154      0.333333      3.000000
   25%        2.563400     18.000000      4.440716      1.006079    787.000000
   50%        3.534800     29.000000      5.229129      1.048780   1166.000000
   75%        4.743250     37.000000      6.052381      1.099526   1725.000000
   max       15.000100     52.000000    141.909091     34.066667  35682.000000

                     5             6             7             8
   count  20640.000000  20640.000000  20640.000000  20640.000000
   mean       3.070655     35.631861   -119.569704      2.068558
   std       10.386050      2.135952      2.003532      1.153956
   min        0.692308     32.540000   -124.350000      0.149990
   25%        2.429741     33.930000   -121.800000      1.196000
   50%        2.818116     34.260000   -118.490000      1.797000
   75%        3.282261     37.710000   -118.010000      2.647250
   max     1243.333333     41.950000   -114.310000      5.000010


In [6]: synt_sample.describe()
Out[6]:              0             1             2             3             4 \
   count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000
   mean       4.819246     28.954316      6.191938      1.172562   2679.408170
   std        3.023684     13.650675      2.237810      0.402990   2127.606868
   min       -0.068225     -2.927976      0.877387     -0.144332   -468.985777
   25%        2.627803     19.113346      4.779587      0.957408   1148.179104
   50%        4.217247     29.798105      5.779768      1.062072   2021.181784
   75%        6.254332     38.144114      7.058157      1.285233   3666.957652
   max       19.815551     54.219486     15.639807      3.262196  12548.941245

                     5             6             7             8
   count  20640.000000  20640.000000  20640.000000  20640.000000
   mean       3.388233     36.371957   -119.957959      2.584699
   std        1.035668      2.411460      2.306550      1.305122
   min        0.650323     32.234033   -125.836387      0.212203
   25%        2.651633     34.081107   -122.010873      1.579294
   50%        3.280092     36.677974   -119.606385      2.334144
   75%        3.994524     38.023437   -118.080271      3.456931
   max        7.026720     43.131795   -113.530352      5.395162


In [7]: from sdv.evaluation import evaluate 

        evaluate(synt_sample, california_housing_df) 
Out[7]: 0.4773175572768998

In [8]: from table_evaluator import TableEvaluator 

        table_evaluator =  TableEvaluator(california_housing_df, synt_sample) 

        table_evaluator.visual_evaluation() 
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO2-1)

Importing the `evaluate` package for assessing the similarity of synthetic and real data

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO2-2)

Running the `evaluate` package on our real and synthetic data

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/3.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO2-3)

Importing `TableEvaluator` for visual inspection of the similarities between synthetic and real data

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/4.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO2-4)

Running `TableEvaluator` with real and synthetic data

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/5.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO2-5)

Conducting visual analysis with `visual_evaluation` package

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_1002.png" alt="eval_synt1" height="333" width="600"><figcaption></figcaption></figure>

**Figure 10-2. Evaluation of synthetic data generation-1**

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_1003.png" alt="eval_synt2" height="171" width="600"><figcaption></figcaption></figure>

**Figure 10-3. Evaluation of synthetic data generation-2**

Figures [10-2](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#eval\_synt1) and [10-3](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#eval\_synt2) allow us to visually compare the performance of the real and synthetic data using the mean, standard deviation, and heatmaps. Even though `evaluation` has many different tools, it is worth restricting our attention to these for now.

As can you see, it is not hard to generate synthetic data from real data. Let’s now walk through a process by which we can generate synthetic data based on a model. I will use `sklearn`, the Swiss knife library for ML applications, both for classification and regression models. `make_regression` is helpful for generating synthetic data for running a regression model. Likewise, `make_classification` generates synthetic data for the purpose of running a classification model. The following code also generates [Figure 10-4](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#regression\_noise):

```
In [9]: from sklearn.datasets import make_regression 
        import matplotlib.pyplot as plt
        from matplotlib import cm

In [10]: X, y = make_regression(n_samples=1000, n_features=3, noise=0.2,
                                random_state=123) 

         plt.scatter(X[:, 0], X[:, 1], alpha= 0.3, cmap='Greys', c=y)

In [11]: plt.figure(figsize=(18, 18))
         k = 0

         for i in range(0, 10):
             X, y = make_regression(n_samples=100, n_features=3, noise=i,
                                    random_state=123)
             k+=1
             plt.subplot(5, 2, k)
             profit_margin_orange = np.asarray([20, 35, 40])
             plt.scatter(X[:, 0], X[:, 1], alpha=0.3, cmap=cm.Greys, c=y)
             plt.title('Synthetic Data with Different Noises: ' + str(i))
         plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO3-1)

Importing the `make_regression` package

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO3-2)

Generating synthetic data for regression with 1000 samples, 3 features, and the standard deviation of the noise

[Figure 10-4](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#regression\_noise) shows the effects of varying noise on synthetic data generation. As expected, as the standard deviation goes up, the `noise` parameter gets bigger and bigger.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_1004.png" alt="regression_noise" height="591" width="600"><figcaption></figcaption></figure>

**Figure 10-4. Synthetic data generation with different noises**

How about generating synthetic data for classification? Well, it does sound easy. We will follow a very similar process as regression. This time, we’ll use the `make_classification` package. After generating synthetic data, the effect of different numbers of classes will be observed via scatter plot ([Figure 10-5](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#classification\_class)):

```
In [12]: from sklearn.datasets import make_classification 

In [13]: plt.figure(figsize=(18, 18))
         k = 0

         for i in range(2, 6):
             X, y = make_classification(n_samples=100,
                                        n_features=4,
                                        n_classes=i,
                                        n_redundant=0,
                                        n_informative=4,
                                        random_state=123) 
             k+=1
             plt.subplot(2, 2, k)
             plt.scatter(X[: ,0], X[:, 1], alpha=0.8, cmap='gray', c=y)
             plt.title('Synthetic Data with Different Classes: ' + str(i))
         plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO4-1)

Importing `make_classification` package

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO4-2)

Generating synthetic data for classification with 100 samples, 4 features, and 4 informative features

[Figure 10-5](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#classification\_class) shows us the effect of having different classes on the synthetic data generation; in this case, synthetic data was generated with classes 2 to 5.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_1005.png" alt="classification_class" height="591" width="600"><figcaption></figcaption></figure>

**Figure 10-5. Synthetic data generation with different classes**

It is also possible to generate synthetic data from unsupervised learning. `make_blobs` is a package that can be used for this purpose. So we will generate synthetic data and eyeball the effect of different numbers of clusters on the synthetic data and produce [Figure 10-6](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#synthetic\_cluster):

```
In [14]: from sklearn.datasets import make_blobs 

In [15]: X, y = make_blobs(n_samples=100, centers=2,
                               n_features=2, random_state=0) 

In [16]: plt.figure(figsize=(18, 18))
         k = 0
         for i in range(2, 6):
             X, y = make_blobs(n_samples=100, centers=i,
                               n_features=2, random_state=0)
             k += 1
             plt.subplot(2, 2, k)
             my_scatter_plot = plt.scatter(X[:, 0], X[:, 1],
                                           alpha=0.3, cmap='gray', c=y)
             plt.title('Synthetic Data with Different Clusters: ' + str(i))
         plt.show()
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO5-1)

Importing the `make_blobs` package from `sklearn`

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO5-2)

Generating synthetic data with 100 samples, 2 centers, and 2 features

[Figure 10-6](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#synthetic\_cluster) shows how the synthetic data looks with varying clusters.

Up until now, we have learned how to generate synthetic data using real data and models, using both supervised learning (regression and classification) and unsupervised learning. From this point on, we will explore the HMM and how to use it. From the financial standpoint, we will accomplish this task by factor investing. Factor investing is not a new topic but it has become more and more appealing after the celebrated Fama-French three-factor model (Fama and French 1993). We will see the impact of HMM on identifying different states in the economy and take it into account in investment strategies. In the end, we will be able to compare the effectiveness of factor-investing based on the three-factor Fama-French model with HMM using the Sharpe ratio.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_1006.png" alt="synthetic_cluster" height="590" width="600"><figcaption></figcaption></figure>

**Figure 10-6. Synthetic data generation with different noises**

## A Brief Introduction to the Hidden Markov Model

HMM gives us a probability distribution over sequential data, which is modeled by a Markov process with hidden states. HMM enables us to estimate probability transition from one state to another.

To illustrate, let us consider the stock market, in which stocks go up, go down, or stay constant. Pick a random state—say, going up. The next state would be either going up, going down, or staying constant. In this context, the state is thought to be a _hidden_ state because we do not know with certainty which state will prevail next in the market.

In general, there are two fundamental assumptions that HMM makes: first, that all observations are solely dependent on the current state and are conditionally independent of other variables, and second, that the transition probabilities are homogenous and depend only on the current hidden state (Wang, Lin, and Mikhelson 2020).

## Fama-French Three-Factor Model Versus HMM

The model proposed by Fama and French (1993) paved the way for further studies expanding on CAPM. The model suggests brand-new explanatory variables to account for the changes in stock returns. The three factors in this model are market risk (��-��), small minus big (SMB), and high minus low (HML). Let’s briefly discuss these factors, as we will use them in the model below.

(��-��) is basically the return of a market portfolio minus the risk-free rate, which is a hypothetical rate proxied by government-issued T-bills or similar assets.

SMB is a proxy for size effect. Size effect is an important variable used to explain several phenomenon in corporate finance. It is represented by different variables like logarithm of total assets. Fama-French takes size effect into account by calculating between returns of small-cap companies and big-cap companies.

The third factor is HML, which represents the spread in returns between companies with high book-to-market and companies with low book-to-market, comparing a company’s book value to its market value.

Empirical studies suggest that smaller SMB, higher HML, and smaller (��-��) boosts stock returns. Theoretically speaking, identifying states before running the Fama-French three-factor model would boost the performance of the model. To see if this is the case with real data, let us run the factor investment model with or without the HMM.

The data is compiled from the [Kenneth R. French data library](https://oreil.ly/m5ShJ). As can be seen in the following, the variables included in the data are: `Date`, `Mkt-RF`, `SMB`, `HML`, and `RF`. It turns out all the variables are numerical aside from the date as expected. To save some time processing the model, the data has been trimmed to start from 2000-01-03:

```
In [17]: ff = pd.read_csv('FF3.csv', skiprows=4)
         ff = ff.rename(columns={'Unnamed: 0': 'Date'})
         ff = ff.iloc[:-1]
         ff.head()
Out[17]:        Date  Mkt-RF   SMB   HML     RF
         0  19260701    0.10 -0.24 -0.28  0.009
         1  19260702    0.45 -0.32 -0.08  0.009
         2  19260706    0.17  0.27 -0.35  0.009
         3  19260707    0.09 -0.59  0.03  0.009
         4  19260708    0.21 -0.36  0.15  0.009

In [18]: ff.info()
         <class 'pandas.core.frame.DataFrame'>
         RangeIndex: 24978 entries, 0 to 24977
         Data columns (total 5 columns):
          #   Column  Non-Null Count  Dtype
         ---  ------  --------------  -----
          0   Date    24978 non-null  object
          1   Mkt-RF  24978 non-null  float64
          2   SMB     24978 non-null  float64
          3   HML     24978 non-null  float64
          4   RF      24978 non-null  float64
         dtypes: float64(4), object(1)
         memory usage: 975.8+ KB

In [19]: ff['Date'] = pd.to_datetime(ff['Date'])
         ff.set_index('Date', inplace=True)
         ff_trim = ff.loc['2000-01-01':]

In [20]: ff_trim.head()
Out[20]:             Mkt-RF   SMB   HML     RF
         Date
         2000-01-03   -0.71  0.61 -1.40  0.021
         2000-01-04   -4.06  0.01  2.06  0.021
         2000-01-05   -0.09  0.18  0.19  0.021
         2000-01-06   -0.73 -0.42  1.27  0.021
         2000-01-07    3.21 -0.49 -1.42  0.021
```

Well, we have obtained the variables explaining the dynamic behind the stock return, but which stock return is it? It is supposed to be a return representing general well-being of the economy. A potential candidate for this kind of variable is the S\&P 500 exchange-traded fund (ETF).

**NOTE**

ETF is a special type of investment fund and exchange-traded product that tracks industry, commodities, and so on. SPDR S\&P 500 ETF (SPY) is a very well-known example tracking the S\&P 500 Index. Some other ETFs are:

* Vanguard Total International Stock ETF (VXUS)
* Energy Select Sector SPDR Fund (XLE)
* iShares Edge MSCI Min Vol USA ETF (USMV)
* iShares Morningstar Large-Cap ETF (JKD)

We’ll collect the daily closing SPY price from 2000-01-03 to 2021-04-30 to match the period we’re examining. After accessing the data, `ff_trim` and `SP_ETF` are merged so that we end up with the data including return and volatility on which the hidden states are determined:

```
In [21]: ticker = 'SPY'
         start = datetime.datetime(2000, 1, 3)
         end = datetime.datetime(2021, 4, 30)
         SP_ETF = yf.download(ticker, start, end, interval='1d').Close
         [*********************100%***********************]  1 of 1 completed

In [22]: ff_merge = pd.merge(ff_trim, SP_ETF, how='inner', on='Date')

In [23]: SP = pd.DataFrame()
         SP['Close']= ff_merge['Close']

In [24]: SP['return'] = (SP['Close'] / SP['Close'].shift(1))-1
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO6-1)

Calculating return of the SPY

It is assumed that there are three states in the economy: up, down, and constant. With this in mind, we run the HMM with full covariance, indicating independent components and a number of iterations (`n_iter`) of 100. The following code block shows how we can apply Gaussian HMM and predict the hidden state:

```
In [25]: from hmmlearn import hmm

In [26]: hmm_model = hmm.GaussianHMM(n_components=3,
                                     covariance_type="full",
                                     n_iter=100)

In [27]: hmm_model.fit(np.array(SP['return'].dropna()).reshape(-1, 1))
         hmm_predict = hmm_model.predict(np.array(SP['return'].dropna())
                                         .reshape(-1, 1))
         df_hmm = pd.DataFrame(hmm_predict)

In [28]: ret_merged = pd.concat([df_hmm,SP['return'].dropna().reset_index()],
                                axis=1)
         ret_merged.drop('Date',axis=1, inplace=True)
         ret_merged.rename(columns={0:'states'}, inplace=True)
         ret_merged.dropna().head()
Out[28]:    states    return
         0       1 -0.039106
         1       1  0.001789
         2       1 -0.016071
         3       1  0.058076
         4       2  0.003431
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO7-1)

Fitting the Gaussian HMM with return data

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO7-2)

Given the return data, predicting the hidden states

After predicting the hidden states, return data is concatenated with the hidden state so that we are able to see which return belongs to which state.

Let’s now examine the result we obtained after running our Gaussian HMM analysis. In the following code block, we compute the mean and standard deviations of different states. Also, covariance, initial probability, and transition matrix are estimated:

```
In [29]: ret_merged['states'].value_counts()
Out[29]: 0    3014
         2    2092
         1     258
         Name: states, dtype: int64

In [30]: state_means = []
         state_std = []

         for i in range(3):
             state_means.append(ret_merged[ret_merged.states == i]['return']
                                .mean())
             state_std.append(ret_merged[ret_merged.states == i]['return']
                              .std())
         print('State Means are: {:.4f}'.format(state_means))
         print('State Standard Deviations are: {:.4f}'.format(state_std))
         State Means are: [0.0009956956923795376, -0.0018371952883552139, -0.
         0005000714110860054]
         State Standard Deviations are: [0.006006540155737148, 0.
         03598912028897813, 0.01372712345328388]

In [31]: print(f'HMM means\n {hmm_model.means_}')
         print(f'HMM covariances\n {hmm_model.covars_}')
         print(f'HMM transition matrix\n {hmm_model.transmat_}')
         print(f'HMM initial probability\n {hmm_model.startprob_}')
         HMM means
          [[ 0.00100365]
          [-0.002317  ]
          [-0.00036613]]
         HMM covariances
          [[[3.85162047e-05]]

          [[1.26647594e-03]]

          [[1.82565269e-04]]]
         HMM transition matrix
          [[9.80443302e-01 1.20922866e-06 1.95554886e-02]
          [1.73050704e-08 9.51104459e-01 4.88955238e-02]
          [2.67975578e-02 5.91734590e-03 9.67285096e-01]]
         HMM initial probability
          [0.00000000e+000 1.00000000e+000 2.98271922e-120]
```

The number of observations per state is given in [Table 10-1](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#hmm\_states).

| State | Number of observations | Return means | Covariances |
| ----- | ---------------------- | ------------ | ----------- |
| 0     | 3014                   | 0.0010       | 3.8482e-05  |
| 1     | 2092                   | -0.0023      | 1.2643e-05  |
| 2     | 258                    | -0.0003      | 1.8256e-05  |

We assume that the economy has three states, but this assumption rests on theory. However, if we want to make sure, there is a strong and convenient tool that can be applied: _Elbow Analysis_. After running Gaussian HMM, we obtain the likelihood result, and if there is no room for improvement—that is, the likelihood value becomes relatively stagnant—this is the point at which we can stop the analysis. Given the following result (along with the resultant [Figure 10-7](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#optimum\_cluster\_hmm)), it turns out that three components is a good choice:

```
In [32]: sp_ret = SP['return'].dropna().values.reshape(-1,1)
         n_components = np.arange(1, 10)
         clusters = [hmm.GaussianHMM(n_components=n,
                                     covariance_type="full").fit(sp_ret)
                    for n in n_components] 
         plt.plot(n_components, [m.score(np.array(SP['return'].dropna())\
                                         .reshape(-1,1)) for m in clusters]) 
         plt.title('Optimum Number of States')
         plt.xlabel('n_components')
         plt.ylabel('Log Likelihood')
In [33]: hmm_model = hmm.GaussianHMM(n_components=3,
                                 covariance_type="full",
                                 random_state=123).fit(sp_ret)
         hidden_states = hmm_model.predict(sp_ret)
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO8-1)

Creating ten clusters based on Gaussian HMM via list comprehension

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/2.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO8-2)

Calculating log-likelihood given the number of components

[Figure 10-7](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#optimum\_cluster\_hmm) shows the likelihood values per state. It is readily observable that after the third component, the curve becomes flatter.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_1007.png" alt="optimum_cluster_hmm" height="411" width="600"><figcaption></figcaption></figure>

**Figure 10-7. Gaussian HMM scree plot**

Let us now visualize the states that we have obtained via Gaussian HMM and produce [Figure 10-8](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#HMM\_Regime\_plot):

```
In [34]: from matplotlib.dates import YearLocator, MonthLocator
         from matplotlib import cm

In [35]: df_sp_ret = SP['return'].dropna()

         hmm_model = hmm.GaussianHMM(n_components=3,
                                     covariance_type="full",
                                     random_state=123).fit(sp_ret)

         hidden_states = hmm_model.predict(sp_ret)

         fig, axs = plt.subplots(hmm_model.n_components, sharex=True,
                                 sharey=True, figsize=(12, 9))
         colors = cm.gray(np.linspace(0, 0.7, hmm_model.n_components))

         for i, (ax, color) in enumerate(zip(axs, colors)):
             mask = hidden_states == i
             ax.plot_date(df_sp_ret.index.values[mask],
                          df_sp_ret.values[mask],
                          ".-", c=color)
             ax.set_title("Hidden state {}".format(i + 1), fontsize=16)
             ax.xaxis.set_minor_locator(MonthLocator())

         plt.tight_layout()
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_1008.png" alt="HMM_Regime_plot" height="445" width="600"><figcaption></figcaption></figure>

**Figure 10-8. Gaussian HMM states**

[Figure 10-8](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#HMM\_Regime\_plot) shows the behavior of the hidden states, and as expected, the distributions of these states are entirely different from each other, highlighting the importance of identifying the states.

Given the states, the SPY returns differ, which is something that we expect. After all these preparations, we can move forward and run the Fama-Frech three-factor model with and without Gaussian HMM. The Sharpe ratio, which we’ll calculate after modeling, will tell us which is the better risk-adjusted return. Analysis with Gaussian HMM reveals a Sharpe ratio of nearly 0.0981:

```
In [36]: ret_merged.groupby('states')['return'].mean()
Out[36]: states
         0    0.000996
         1   -0.001837
         2   -0.000500
         Name: return, dtype: float64

In [37]: ff_merge['return'] = ff_merge['Close'].pct_change()
         ff_merge.dropna(inplace=True)

In [38]: split = int(len(ff_merge) * 0.9)
         train_ff= ff_merge.iloc[:split].dropna()
         test_ff = ff_merge.iloc[split:].dropna()

In [39]: hmm_model = hmm.GaussianHMM(n_components=3,
                                     covariance_type="full",
                                     n_iter=100, init_params="")

In [40]: predictions = []

         for i in range(len(test_ff)):
             hmm_model.fit(train_ff)
             adjustment = np.dot(hmm_model.transmat_, hmm_model.means_) 
             predictions.append(test_ff.iloc[i] + adjustment[0])
         predictions = pd.DataFrame(predictions)

In [41]: std_dev = predictions['return'].std()
         sharpe = predictions['return'].mean() / std_dev
         print('Sharpe ratio with HMM is {:.4f}'.format(sharpe))
Out[41]: Sharpe ratio with HMM is 0.0981
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/1.png)](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#co\_synthetic\_data\_generation\_and\_\_\_span\_class\_\_keep\_together\_\_the\_hidden\_markov\_model\_in\_finance\_\_span\_\_CO9-1)

Adjustment based on transition matrix

The traditional way to run Fama-Frech three-factor model is to apply linear regression and the following code block does that. After running linear regression, we can make predictions and then calculate the Sharpe ratio. We’ll see that linear regression produces a lower Sharpe ratio (0.0589) compared to one with Gaussian HMM:

```
In [42]: import statsmodels.api as sm

In [43]: Y = train_ff['return']
         X = train_ff[['Mkt-RF', 'SMB', 'HML']]

In [44]: model = sm.OLS(Y, X)
         ff_ols = model.fit()
         print(ff_ols.summary())

                        OLS Regression Results
================================================================================
Dep. Variable:             return   R-squared (uncentered):                0.962
Model:                        OLS   Adj. R-squared (uncentered):           0.962
Method:             Least Squares   F-statistic:                       4.072e+04
Date:            Tue, 30 Nov 2021   Prob (F-statistic):                     0.00
Time:                    00:05:02   Log-Likelihood:                       22347.
No. Observations:            4827   AIC:                              -4.469e+04
Df Residuals:                4824   BIC:                              -4.467e+04
Df Model:                       3
Covariance Type:        nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Mkt-RF         0.0098   2.82e-05    348.173      0.000       0.010       0.010
SMB           -0.0017   5.71e-05    -29.005      0.000      -0.002      -0.002
HML        -6.584e-05   5.21e-05     -1.264      0.206      -0.000    3.63e-05
==============================================================================
Omnibus:                     1326.960   Durbin-Watson:                   2.717
Prob(Omnibus):                  0.000   Jarque-Bera (JB):            80241.345
Skew:                           0.433   Prob(JB):                         0.00
Kurtosis:                      22.955   Cond. No.                         2.16
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not
contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is
correctly specified.

In [45]: ff_pred = ff_ols.predict(test_ff[["Mkt-RF", "SMB", "HML"]])
         ff_pred.head()
Out[45]: Date
         2019-03-14   -0.000340
         2019-03-15    0.005178
         2019-03-18    0.004273
         2019-03-19   -0.000194
         2019-03-20   -0.003795
         dtype: float64

In [46]: std_dev = ff_pred.std()
         sharpe = ff_pred.mean() / std_dev
         print('Sharpe ratio with FF 3 factor model is {:.4f}'.format(sharpe))
Out[46]: Sharpe ratio with FF 3 factor model is 0.0589
```

This result suggests that Gaussian HMM delivers better risk-adjusted returns, making it useful in portfolio allocation, among other analyses.

The following analysis tries to show what happens if the states of the index return need to be predicted based on the unseen data that can be used for backtesting for future analysis:

```
In [47]: split = int(len(SP['return']) * 0.9)
         train_ret_SP = SP['return'].iloc[split:].dropna()
         test_ret_SP = SP['return'].iloc[:split].dropna()

In [48]: hmm_model = hmm.GaussianHMM(n_components=3,
                                     covariance_type="full",
                                     n_iter=100)
         hmm_model.fit(np.array(train_ret_SP).reshape(-1, 1))
         hmm_predict_vol = hmm_model.predict(np.array(test_ret_SP)
                                             .reshape(-1, 1))
         pd.DataFrame(hmm_predict_vol).value_counts()
Out[48]: 0    4447
         1     282
         2      98
         dtype: int64
```

As we have discussed, HMM provides a helpful and strong way for further expanding our analysis to get more reliable and accurate results. Before concluding this chapter, it is worthwhile to show the synthetic data generation process using Gaussian HMM. To do that, we should first define our initial parameters. These parameters are: initial probability (`startprob`), transition matrix (`transmat`), mean (`means`), and covariance (`covars`). Having defined the parameters, we can run Gaussian HMM and apply a random sampling procedure to end up with a desired number of observations, which is 1,000 in our case. The following code results in Figures [10-9](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#HMM\_syn\_hist) and [10-10](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#HMM\_syn\_line):

```
In [49]: startprob = hmm_model.startprob_
         transmat = hmm_model.transmat_
         means = hmm_model.means_
         covars = hmm_model.covars_

In [50]: syn_hmm = hmm.GaussianHMM(n_components=3, covariance_type="full")

In [51]: syn_hmm.startprob_ = startprob
         syn_hmm.transmat_ = transmat
         syn_hmm.means_ = means
         syn_hmm.covars_ = covars

In [52]: syn_data, _ = syn_hmm.sample(n_samples=1000)


In [53]: plt.hist(syn_data)
         plt.show()
In [54]: plt.plot(syn_data, "--")
         plt.show()
```

The distribution and line plot based on the synthetic data can be seen in Figures [10-9](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#HMM\_syn\_hist) and [10-10](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/ch10.html#HMM\_syn\_line). As we have a large enough sample size coming out of our Gaussian HMM, we observe normally distributed data.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_1009.png" alt="HMM_syn_hist" height="409" width="600"><figcaption></figcaption></figure>

**Figure 10-9. Gaussian HMM synthetic data histogram**

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492085249/files/assets/mlfr_1010.png" alt="HMM_syn_line" height="403" width="600"><figcaption></figcaption></figure>

**Figure 10-10. Gaussian HMM synthetic data line plot**

## Conclusion

In this final chapter, we discussed two relatively new yet promising topics. Synthetic data generation enables us to conduct analysis in the absence of real data or in the case of breaching confidentiality, so it can be life-saver for a practitioner in these situations. In the second part of this chapter, we looked at Gaussian HMM and its usefulness in financial analysis and then used Gaussian HMM to generate synthetic data.

We saw how Gaussian HMM helps us obtain better results in portfolio allocations, but it is worth noting that this is not the only area in which we can apply HMM. Rather, there are many different areas where researchers take advantage of this method, and it’s a safe bet that more are to come.

## References

Articles and books cited in this chapter:

* Ahuja, Ankana. 2020. “The promise of synthetic data”. _Financial Times_. [_https://oreil.ly/qphEN_](https://oreil.ly/qphEN).
* El Emam, Khaled, Lucy Mosquera, and Richard Hoptroff. 2020. _Practical Synthetic Data Generation: Balancing Privacy and the Broad Availability of Data_. Sebastopol: O’Reilly.
* Fama, Eugene F., and Kenneth R. French. 1993. “Common Risk Factors in the Returns on Stocks and Bonds.” _Journal of Financial Economics_ 33 (3): 56.
* Patki, Neha, Roy Wedge, and Kalyan Veeramachaneni. 2016. “The Synthetic Data Vault.” In the 2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA), 399-410.
* Wang, Matthew, Yi-Hong Lin, and Ilya Mikhelson. 2020. “Regime-Switching Factor Investing with Hidden Markov Models.” _Journal of Risk and Financial Management_ 13 (12): 311.
