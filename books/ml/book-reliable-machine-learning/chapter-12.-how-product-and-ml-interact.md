# Chapter 12. How Product and ML Interact

As companies rush to use ML capabilities to meet customer needs, they are eager to leverage cutting-edge research to tackle a wide variety of business applications. Many of the product teams and business managers, still anchored in traditional software product development methodologies, find themselves in a new and unfamiliar territory: building ML products.

Building your first ML product can be overwhelming. It’s not just a question of getting ML right, difficult enough in itself; rather, the integration of the ML into the rest of the product (and the rest of the business) requires many things that need to work together. Among these, data collection practices and governance, the quality of data, definition of product behavior, UI/UX, and business goals all contribute to the success of an ML-based product or feature.

## Different Types of Products

One of the important and useful features of ML is that it can be applied to many types of products. It can be used in analytics applications to derive insights about business trends and metrics. It can be rolled into an appliance or device to be shipped to consumers. Sophisticated ML systems are built into self-driving cars to detect other objects and to make decisions about driving. The breadth of ML applications is huge and growing. As a result of this huge diversity of use cases, organizations focused on integrating ML into their existing or new products face an extremely steep learning curve and numerous choices about their implementation.

We don’t cover most of these different kinds of ML in this chapter. Specifically, it is not feasible for this chapter to seriously consider each of the many common types of ML-product integrations that exist. Instead, we will focus on the use case we have been discussing throughout the book: _yarnit.ai_ (our ecommerce web store) wanting to integrate ML.

Considering this use case specifically has some nice aspects, and not just because it extends an example that is touched on in every chapter of this book. The _yarnit.ai_ set of use cases includes some from the backend (e.g., consuming browsing and purchase logs to predict interest in a product) and many frontend integrations as well (e.g., looking up what products this customer might be interested in right now). It has some complexity without being overwhelmingly difficult to understand for most readers. Of course, it clearly can’t be an appropriate analogue for every other possible system. Nonetheless, we hope there is something of use to you here.

## Agile ML?

Outside certain well-defined environments, many contemporary software engineering teams have moved to developing in iterative, focused, short loops called _sprints_—as per the [Agile Manifesto](https://agilemanifesto.org/). However, applying Agile to ML systems is far from straightforward. Agile, as a methodology, has several foundational characteristics: short feedback loops, customer-oriented stories or story points, and estimation geared toward a small team working on those story points.

Integration of ML into the product violates most of these assumptions. The feedback loops are long (sometimes months or years) and come indirectly from the data rather than directly from the customer. Small team execution is less useful because the integration often involves people across the company (see [Chapter 13](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch13.html#integrating\_ml\_into\_your\_organization) for more perspective on this). Model development can have arbitrarily long delays while we integrate the model into the product and wait for the results to show up in the data. Additionally, we cannot build first and validate later since even the building phase needs to be based on the data. There’s one final, and perhaps most important point: ML models are neither perfectly reproducible nor stable over time. As has been pointed out, we can train the same model multiple times with different results, or we can leave the same model in place but have the world change sufficiently that our value is no longer the same. ML models are never “done,” so ML product integration has limited determinism.

Still, even though Agile approaches may not be super well suited for ML, adopting a standard ML development lifecycle, as discussed in this chapter, can facilitate your organization taking advantage of ML systems to power great customer experiences and increase your revenues.

## ML Product Development Phases

To manage uncertainty during the product development lifecycle, ML projects need to be highly iterative from the beginning. The most important phases of an ML product development project are discovery and definition, business goal setting, MVP construction and validation, model and product development, deployment, and support and maintenance ([Figure 12-1](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch12.html#ml\_product\_development\_phases-id0000033)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098106218/files/assets/reml_1201.png" alt="ML product development phases" height="437" width="600"><figcaption></figcaption></figure>

**Figure 12-1. ML product development phases**

### Discovery and Definition

The development of an ML product should always start with _discovery and definition_. Skipping this step is tempting, particularly if you come from a build-first, validate-later background, but in an ML context, this step is critical and helps product teams handle the uncertainty early. This begins by defining the problem space, and understanding and framing the business problem and the desired outcome. It finishes by mapping the problems to a solution space, which is not always an easy task. Technical teams are looking for a quantitative baseline to measure feasibility. On the other hand, business is struggling to achieve clarity and looking for clear cost-benefit analysis at a time when the outcomes are unclear. Balancing stakeholder needs, business-problem framing, defining goals and outcomes, and building relationships with the business will be key.

Given that our product is designed to display results to humans in the end, there are no shortcuts to good old-fashioned user research. Conduct thorough user research to identify the pain points of the user and prioritize them according to their needs. This helps build a user journey map, identifying critical workflows and potential roadblocks. Further, the roadmap is useful for defining the processes that need to be modified for the ML solution to work in the first place. You can do a market-sizing exercise to estimate the business potential. Then the next question is, how do we know if ML can help address our user problems? Numerous ML applications exist but, at its core, ML is best suited for making decisions or predictions. In general, our style of human-facing ML system will have one or more of the following characteristics as deployed in production:

Complex logic that’s impractical to solve with human-defined rulesFor example, search engines often have multiple phases of ranking that happen in series, such as initial retrieval from the text index, primary ranking using text similarity, contextual ranking, and personalized ranking.Large-scale personalizationIf the problem is expected to scale to thousands of users or more, it could be a good use case for ML. In the _yarnit.ai_ web store example, we expect to have thousands of customers take advantage of a new offer/discount within three to six months, which implies being able to support personalization across a large user population.Rules that change quickly over timeIf the rules generally remain static year after year, a heuristic solution is preferred. However, if the business’s success depends on quick adaptation and rule changes, ML is a good route. For example, if users of a web store are writing product reviews constantly, the algorithm for recommending relevant products needs to adapt in real time and is amenable to an ML solution.A clear evaluation metricFor example, we want the model to provide recommendations that result in a sale. In search, typing “kids hat” should return lists of patterns and yarn normally associated with kids colors or knitting gauge needles that are associated with hats.No requirement for 100% accuracyIf business success can be achieved with a high probability of accuracy rather than with perfection, ML is a good option. For example, recommendation systems will not be considered faulty if users don’t always want what is served. Users can still have a great experience, and the ML model can learn from the lack of sales to deliver improved recommendations in the future.

### Business Goal Setting

The selection of ML requirements takes a careful combination of design and business goals. Product managers (PMs) need to understand the end user and the long-term business goals to drive value. For example, we want to create improvements to the web store that encourage customers to find and buy more products they want, but really we also want that to happen repeatedly, every time they return. Ultimately, we do this by focusing on the end user and developing a deep understanding of a specific problem the product can solve. Otherwise, there will be a risk of developing an extremely powerful system only to address a small (or possibly nonexistent) problem.

ML is rarely error free, and developing without guardrails can have serious consequences. We need to acknowledge that getting it wrong could carry huge costs and that understanding the cost of getting it wrong is a significant part of building an ML product. For example, say we are trying to predict and cancel orders on behalf of customers when they reach out to the support team about order errors and/or billing issues. The cost here is that errors in the model might wrongly interpret the user’s intent as a cancellation, thereby having financial consequences for the user and the company. On the other hand, in the case of a recommender system to suggest similar products on the product details page, for a user with no purchase history, the impact of bad recommendations might be just low conversion rates, and perhaps a vague sense that our web store is not very trustworthy or useful. We must then think about how to improve the model, but the consequences of getting it wrong are not catastrophic in our use case (although they clearly are in many other ML products).

ML relies to a large extent on probabilities. Therefore, there will always be a chance that the model gives the wrong output. PMs are responsible for anticipating and recognizing the consequences of a wrong prediction. One great way to anticipate consequences like the ones noted previously is to test, test, and test some more. Understand what makes up the probabilities computed by the model. We need to use the business purpose of ML in order to think about the desired precision (and recall) of the model.

Once all of the consequences of getting a prediction wrong are identified, the PM needs to make sure relevant _safety nets_ are defined and built into the product to mitigate the risks. Additionally, PMs may think of ways to change the underlying product (change interaction flow, modify the input and output data paths, etc.) in order to make incorrect predictions much less likely. As such, safety nets can be intrinsic and extrinsic, and they are ingrained in all products. ML is not an exception. As described in the following list, defining the safety nets and business performance metrics that are relevant to the product we are building is a critical step before introducing ML into the product:

Intrinsic safety netsThese recognize the impossibilities that are fundamental to the nature of the product. For example, at our web store, a user cannot cancel an order if no order has been made in the first place. Emails received from such users, with no order numbers and with a subject of “canceling orders” can be ignored by the model that is trying to learn the reasons for order cancellations. However, it’s a good idea to have a customer support agent look into that case. A useful activity is to map out the user journey for the product and identify the states that the user can go through. This helps weed out impossible predictions. Intrinsic safety nets are invisible to the user.Extrinsic safety netsExtrinsic safety nets are visible to the user. They can take the form of confirming user intent or double-checking the potential outcome. Some message systems have a model that tries to detect the intent of a message and suggest replies to its users. In most cases, however, these systems do not automatically assume the reply is correct and send it without a human confirming the choice. More commonly, these systems ask the user to pick from a list of potential replies.Business performance goalsAlong with the general ML model performance metrics that we’ve discussed in [Chapter 8](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch08.html#serving-id0000021), it is extremely important for PMs to clearly define the business performance metrics to measure the success of the ML systems in production. To evaluate business performance, it is necessary to start with a product or feature goal. For example, increasing revenue could be a great business goal for our _yarnit.ai_ web store. Once this is defined, a product metric should be assigned to evaluate success. The best metrics are specific and measurable. Specific metrics reduce ambiguity and increase focus. Also, the easier it is to measure success, the more certain we can be that we’re achieving the results we set out for in the first place.

These metrics can vary from product to product. For example, the following are a few important business metrics to track for ML-based recommendations on an ecommerce store like _yarnit.ai_:

Click-through rate (CTR)The number of product page views from a recommendations list divided by the total number of products displayed on the recommendations list. For example, on the shopping cart page, this metric would help determine the success of the ML model powering the “Frequently bought together” list.Conversion rateThe number of add-to-cart events from a recommendation list divided by the total number of products displayed on the recommendations list. For instance, on the product details page, this metric would help determine the success of the ML model powering the “Compare with similar items” list.Average order value (AOV)The average value of orders from all purchase events. AOV is equal to the total revenue divided by the number of orders.Recommender-engaged AOVThe average value of orders that include at least one item selected from a recommendations list. This is calculated from the recommender-engaged revenue divided by the number of orders with at least one item that was selected from a recommendations list.Total revenueThe total revenue from all recorded purchase events. This value includes shipping and taxes.Recommender-engaged revenueThe revenue for purchase events that include at least one catalog item selected from a recommendations list. This value includes shipping and taxes and any discount applied.

Also, ecommerce businesses usually track a lot more business metrics,[1](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch12.html#ch01fn135) including customer acquisition cost (CAC), customer lifetime value (CLV), customer retention rate (CRR), refund and return rates (RrR), and cart abandonment rate, as well as vanity metrics such as social media engagement, website traffic, and page views.

### MVP Construction and Validation

Investing in ML models in order to integrate with our product is likely to be expensive. To figure out whether the ML integration into our product will work, we need to answer two questions: (1) can we make a model that works (or works well enough), and (2) can we integrate that model into our product in a compelling and useful way? Building a dataset with the right features and labels, training the model, and putting it in production can range from a few weeks to a few months. We will want to get a signal of usefulness early on to validate whether the model will work. This can be accomplished through careful offline evaluation of the model we have built, analyzing the responses to various common use cases we expect from the application.

To evaluate the utility in the user interaction, it is a good idea to fake that interaction first, for a small set of users. Sometimes this is referred to as launching a _minimal viable product_ (_MVP_) with a fixed set of rules or heuristics (without real ML models in place) to prove the point that the feature will really solve the customer needs.

For example, considering the use case of personalization, have a list of items ready for a user to select based on what they have selected last. Simple rule-based engines are often the first steps to evolution into a more complex ML model. Here are some examples of rule-based recommenders:

* If the user bought a knitting pattern, they probably need the yarn for that pattern as well as needles and other knitting-related supplies.
* If a user buys new yarn every fall as the weather gets colder, we should recommend it to them every fall in their part of the world.
* If a user always pays by credit card, we surface that as the default payment option next time.

It will obviously be impossible to write rules to cover every case, and that is when ML can be used best, but a few simple rule-based proxies go a long way toward validating the outcome of the ML approach. The idea is to test whether users respond positively to ML. While these techniques might not give the best results, they are important for getting a signal. Getting a signal early on can save time and effort, and help correct the vision and direction of the product. This is your best shot at guaranteeing returns on the investment put into building an ML system.

### Model and Product Development

With a clear set of goals and targets developed through previous stages, the next step is to build the models and integrate them with customer-facing features. For a general introduction to this, see Chapters [3](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch03.html#basic\_introduction\_to\_models) and [7](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch07.html#training\_systems). But in general, this is a collaborative endeavor with design, engineering, ML researchers, business owners, and PMs working together, and PMs can drive a lot of value by continuing deliberate stakeholder management. The build will go from simple to complex—PMs should prepare to drive the product through train, test, and validate cycles; manage standups with scientists who may or may not be making progress; and navigate the iterative dependence between design, science, and engineering teams. We discuss the roles and responsibilities of various teams in [Chapter 13](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch13.html#integrating\_ml\_into\_your\_organization).

### Deployment

In the production deployment stage, the ML system is introduced to the infrastructure, where it will serve the live customer traffic, and gather feedback data that is fed into the ML training pipelines to improve the models. Feedback loops help measure the impact of a model and can add to the general understanding of usability. In the context of an ML system, feedback is also important for a model to learn and become better. We discuss various model serving architectures and best practices in [Chapter 8](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch08.html#serving-id0000021).

Feedback loops are an important data collection mechanism, which yield labeled datasets that can be directly plugged into the learning mechanism. In our _yarnit.ai_ web store example, the feedback loop could be quite simple—does the guest click on the recommendations, and is this recommendation bought?

Rolling out to production should be heavily tied to observing the business metrics (discussed earlier in this chapter) in a controlled manner, to understand the impact and to inform decisions about increasing the rollout or turning it off and investigating unexpected results. In particular, one challenge is that the specific initiatives or projects within a large company that ultimately contain the day-to-day work of building an ML model may have many layers of separation from the higher-level business goals. Personalization is a good example: at a high level, personalization may be about increasing a metric like average revenue per user or increasing the growth of new user acquisitions. But when a model is developed, it is likely to be optimized around improving various low-level user experience metrics like scores of engagement.

PMs have a crucial job to be a translator between these two domains and to constantly work to improve the methods of user observability that can help connect improvements in the lower level with hypotheses around impact in the higher levels. So it’s extremely important for product teams to have a plan for how an ML system will be deployed incrementally. We discuss various methods of model evaluations in [Chapter 5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch05.html#evaluating\_model\_validity\_and\_quality).

### Support and Maintenance

ML systems designed to integrate into products are not complete on their first shipping version. Arguably, they are never complete because they are attempting to model the state of the world and deliver useful value about the state of the world in the product. While developing and deploying ML systems can be relatively affordable, maintaining them over time can be more difficult and expensive than commonly assumed.[2](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch12.html#ch01fn136)

Organizations that are serious about integrating ML into their products need to be serious about continuing the maintenance of those ML models and the infrastructure that produces them. This is at least part of why the Agile approach doesn’t fit ML product integrations well: they never really ship, or rather they never finish. As our product changes, as the needs of our customers change, as our understanding of the business changes, and as the world changes, we will need to keep developing and shipping models.

It is a feature of ML systems that it’s not usually clear when the work a team is doing is “maintenance” or “development,” so conceptions of development work that rely on a clean separation between them are tricky to apply.

## Build Versus Buy

It is important in this section that we carefully scope the question of whether to build or buy. Models are local to the organization that owns the data and the infrastructure. In general, models must come from your learning on your own data.[3](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch12.html#ch01fn137) But the tools and infrastructure we use to create our models are a different story. The world of ML has no shortage of tools available, both open source and commercial. The decision of build versus buy is usually a trade-off balancing cost, risk, customization, long-term resource availability, intellectual property (IP), and vendor lock-in. However, in the age of ML, we have more dimensions to this traditional question to consider, including make-or-buy models, data processing infrastructure/tools, and the end-to-end platform that holds it all together. Here are some considerations to account for when deciding what’s best for the business.

### Models

As we’ve stated, models are generally local and not as easily acquired outside our own organization. That said, many industry-specific ML models and applications are available. Vendor-provided solutions may present a significant time- and effort-saving potential, but the key aspects to assess build versus buy for models are discussed next.

#### Generic use cases

Buying a prebuilt solution requires the data and process currently used in our organization to be highly compatible with the expected input/output and behavior of that solution. If a use case’s details, data, or processes are fairly specific to the organization, the effort spent adjusting internal systems and processes to match the vendor-provided solution’s specifications may wipe away the benefit from buying something ready-made. If a packaged solution is truly generic, it can provide great benefit. Simple reading of this paragraph might sound as though this would be a comparatively rare happening: how likely is the existence of a generic model that does what we need without any need of retraining or extension on our data?

As ML service providers multiply, the odds here are getting better all the time. Consider some incredibly useful but really generic use cases:

Object recognition in imagesTell me what is in a picture.Text-to-speech and speech-to-text in multiple languagesCarry out spoken interaction with users.Sentiment analysisTell me how positive or negative a set of text is about its subject.

In all of these examples, pretrained models can work well across a large number of use cases, including ours at YarnIt. In these cases, we might be able to skip training a model entirely.

#### Company’s data initiatives

The benefit of work done on deploying a specific ML use case goes way beyond that use case itself. Often, first use cases are the start of a full data initiative. As a result, ML models and/or application build-or-buy decisions should also take into account the company’s data strategy: sometimes the acquisition of expertise and technology are actual ends in and of themselves.

### Data Processing Infrastructure

Data processing and infrastructure tools and technologies have evolved and are still constantly evolving at a great pace. Traditional vendors, open source, and cloud-based solutions compete with each other on every segment of the data infrastructure, processing, and storage stack. Build or buy has taken a slightly different meaning and is usually more centered around delegating responsibility as follows:

Support and/or maintenanceTypically full open source versus an independent third-party software vendorOperationsCloud versus on-premisesAccessibilityPackaged cloud tools versus native cloud tools

When it comes to infrastructure for data processing, the main decision drivers are no longer about use cases but instead about internally available skills and vendor lock-in risk factors. Commercial and open source platforms both have their pros and cons.

In the ML space, many commercial platforms are new, and are from new providers and lack any long track record or history. As such, they may present significant opportunities but also significant risks.

On the other hand, open source solutions, whether for ML or general-purpose software, may be widely adopted, and in many cases have a longer track record than some of the commercial solutions. But they also have downsides. In particular, they may have slower release cycles, but more importantly, they rarely offer a full solution to a problem. Open source ML solutions are generally great point solutions within a broader context but still require that we do the work necessary to integrate them into our environment and assemble a complete solution.

### End-to-End Platforms

At the present state of maturity, essentially no ML platforms are available that provide relatively well-integrated solutions starting with data and ending with a model that is available in a serving system. This isn’t necessarily a huge problem right now. Models, data processing infrastructure, human resources, and business value all evolve at different paces in various ways. Orchestrating everything together to ensure sustainable success in ML requires significant effort. But again, here, the decision drivers are different, now based on the fundamentals of the organization’s data approach:

Long-term or one-off data initiativesML tools and technologies are evolving, with no sign of reducing the pace of innovation. This presents a serious problem, similar to the one that we have seen in other contexts: technology investments must have time to pay off. And if the utility of a platform is gone before we have finished getting the implementation cost out of it, we may realize that we’ve made a mistake. While this has sometimes been a significant concern with distributed computing or data storage platforms, it is particularly difficult in the ML space, where tools and technologies may become obsolete in the time it takes to properly implement them.[4](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch12.html#ch01fn138) If the initiative is meant to be long-term, acquiring a full platform able to accommodate the fast-evolving data technologies may be relevant, while for short-term initiatives, it may be possible to assemble based on today’s components.Expected scale of the data initiativeOn small-scale ML projects, little overarching cohesion or coherence is required, which leads to a more natural “build” scenario. If the broader vision is to expand data to augment a significant part of the organization’s activity, it will require the inclusion of many skills and company processes to successfully execute on that vision. In this case, a buy approach is likely more relevant.

For example, commercial ML platforms not only allow teams to complete one data project from start to finish one time, but also introduce efficiencies everywhere to scale. That includes features for the following:

* Spending less time cleaning data and doing other parts of the data processing flow that don’t provide direct business value
* Smoothing production issues and avoiding reinventing the wheel when deploying models on a daily basis
* Improving documentation and reproducibility to ease compliance with some regulatory requirements

### Scoring Approach for Making the Decision

Once we are clear on the problem we’re trying to solve and that we need ML specifically to solve the problem, at a minimum we need to consider each of the following factors to evaluate whether building or buying ML is the right choice for the business:

AlignmentHow well does the product or technology meet our needs? Are our objectives met? Are customizations required? Are data privacy and security requirements met?InvestmentIs there a specific net present value (NPV) or ROI that must be achieved? What is the total cost of ownership including human capital, software, and hardware?TimeHow fast can we develop and deploy the solution in production?Competitive advantageDoes the technology provide an asset or IP that is proprietary? Does this IP offer increased value to customers, employees, and investors?Maintenance and supportWhat is the effort and cost of support for human resources, hardware, and software?

We can make a decision about which parts of our platform to build or to buy based on our weight of each of these factors.

### Making the Decision

Whether building or buying, incorporating ML technologies as a key business tool is a strategic decision that should not be made quickly or without all the components in place to support either endeavor. It’s important to carefully evaluate how a decision will affect the company’s long-term goals and roadmaps.

Additionally, don’t forget to convene with key stakeholders to review decision criteria and determine the pros and cons. We discuss more details on the roles and responsibilities of various teams and key stakeholders in Chapters [13](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch13.html#integrating\_ml\_into\_your\_organization) and [14](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch14.html#practical\_ml\_org\_implementation\_example). It’s essential to include your leadership, product, and sales teams to gather consensus and feedback for effective change management. Finally, talk to various vendors to weigh in on the options. Be transparent with them and get their honest opinion about your choices and situation. We could achieve the results through either path but need to leverage the results of the previous discovery steps to help make the right solution for the organization’s unique needs.

## Sample YarnIt Store Features Powered by ML

ML can be used in web stores like _yarnit.ai_ to meet various business goals. ML can increase revenue by improving conversion rates and average order value, can increase profit margins, and can even improve customer loyalty. Today’s consumers don’t want to be treated as one of many customers. They prefer a highly personalized experience. Product recommendations is an area in which a lot of features can be powered by ML models. Just as humans get to know someone and are then able to choose what the best birthday gift for them would be, ML models can leverage data including the product catalog, search queries, viewing history, past purchases, items placed in the shopping cart, products recommended on social media, location, customer segments/buyer personas, and so on. The following are a few example recommendation use cases.

### Showcasing Popular Yarns by Total Sales

This nonpersonalized technique is not based on a user’s individual choices but rather on collective preference. Recommendations could be displayed on the home page based on criteria such as the following:

* The number of yarns or patterns purchased (e.g., when a new pattern is released during holiday season and everyone rushes to get it).
* How much time shoppers spent viewing a specific type of yarn or pattern.
* Number of views and purchases in the user’s country or area. Often we need to consider cultural or seasonal aspects when operating the web store in multiple countries or regions.

In this example, although we may not be really personalizing the customer’s experience, showcasing popular items by each category (yarn types, patterns, brands, etc.) allows us to target first-time users who have no account history yet (a.k.a. the _cold-start problem_). This has the obvious benefit of relying only on time-tested database technologies, which are much more common and reliable than ML.

### Recommendations Based on Browsing History

Just as its name states, users will be getting new product suggestions based on the products they’ve already viewed. For example, if the user is searching and browsing for “wool yarns,” we can display products by popular brands that specialize in making wool-based yarns and patterns.

### Cross-selling and Upselling

Most modern web stores have some infrastructure designed to _cross-sell_ and _upsell_. Both of these are aimed at helping users choose the best items possible while also increasing revenue even if we’re selling to a new customer. For example, when the customer is looking for “baby yarns” on the product page, showing cross-selling recommendations like “Patterns featuring this yarn” and/or “Popular baby clothes featuring this yarn” can not only help increase the average order value but also save a lot of time for customers. Similarly, showing special discounts on bigger sizes or bundles might help customers save money as well as increase revenues for our business.

### Content-Based Filtering

We could leverage the metadata of the products to power recommendations. Accuracy of the metadata will have a greater impact on the quality of such recommendations.

For example, as shown in [Figure 12-2](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch12.html#content\_based\_recommendations\_with\_the), many web stores have a “Similar products” feature either on the product detail page or before/after checkout. In our _yarnit.ai_ web store, if our user is always buying “red” and “blue” yarns that are of type “cotton” from the brand “xyz,” we could consider recommending the same color and type of yarns from a different brand, “abc,” when the recommended items also have similar quality ratings and “abc” is running a special sale event.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098106218/files/assets/reml_1202.png" alt="Content-based recommendations with the “Similar products” feature" height="257" width="600"><figcaption></figcaption></figure>

**Figure 12-2. Content-based recommendations with the “Similar products” feature**

### Collaborative Filtering

_Collaborative filtering_ might be considered the most popular of all product recommender methods. This technique relies solely on how other users have positively interacted with a product (either a view or a purchase or a positive rating on a purchase). The method stems from the idea that people with similar past preferences will probably like the same things in the future too. On top of this, it trusts real choices people have made as opposed to simple ratings, which can be just an estimated guess.

For example, as shown in [Figure 12-3](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch12.html#product\_recommendations\_using\_collabora), user A and user B have similar personas in terms of their browsing and purchase histories as well as product feedback on the web store. Using the persona similarities, when user B is on a “blanket yarn” product details page, we could show the tools and accessories like “markers” and “needles” that were bought by user A along with the same “blanket yarn” product. Because both users have similar tastes/personas, showing them contextual and relevant recommendations would improve not only the user experience but also the chances to upsell, which directly impacts the overall revenue for the business.

Most major web stores use both the collaborative and content-based filtering techniques to improve the accuracy of the personalized recommendations. To evaluate whether a recommender model will work, we can do a simple [A/B test](https://oreil.ly/13N37) by starting with a couple of predictions and a hypothesis like “This algorithm will improve engagement and/or conversion by _x_%”(multivariate testing is also an option). Each alternative will be based on a separate recommender technique.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098106218/files/assets/reml_1203.png" alt="Product recommendations using collaborative filtering" height="579" width="600"><figcaption></figcaption></figure>

**Figure 12-3. Product recommendations using collaborative filtering**

## Conclusion

For ML projects to have a massive business impact, PMs and business owners need to be relentlessly focused on asking the right questions at every stage. Rather than dive directly into technical details and ML implementations, teams must ensure they understand the business problems and goals as specifically as possible. This requires speaking with stakeholders and translating the needs into technical requirements. Once the business goals are understood, we must first assess whether ML is really needed to achieve those goals. If ML is the solution, the very next question that needs to be answered is whether we should integrate with an existing solution provided by an external service provider (buy) or invest in building the ML systems ourselves (build). In either case, a lot of planning and coordination is needed to integrate the ML solutions into customer-facing products.

Clearly defining business goals and measurable product metrics, choosing appropriate measures of success, and deploying solutions iteratively are each important steps toward building great ML products. But ML has a host of other requirements, many of which are different from the requirements of traditional software application development. Success here looks like having a high degree of awareness of the new situation that ML presents you with, while keeping a clear eye on what is actually of business benefit, and driving toward that—potentially over bumpy roads.

[1](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch12.html#ch01fn135-marker) Learn more about the general ecommerce business metrics in [“7 E-Commerce Metrics to Help You Measure Business Success”](https://learn.g2.com/e-commerce-metrics) by Anastasia Stefanuk.

[2](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch12.html#ch01fn136-marker) This paragraph may be a slight overstatement for emphasis. While the work of a functional ML product requires ongoing maintenance, and significantly more maintenance than a comparable non-ML project, it is still true that most of the work occurs toward the first half of the project. As the model matures (and perhaps as the behavior or situation it seeks to model also stabilizes), some organizations will reach a steady state, as further investments in model improvements have a diminishing return.

[3](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch12.html#ch01fn137-marker) One exception here is for transfer learning, whereby we get a general model (say, one that does image recognition) from an external provider and then train it on our own data. But even then, the final model is ours.

[4](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch12.html#ch01fn138-marker) A bit of nuance exists here. It is true that algorithms change quickly (certainly yearly), as new approaches are tried and found to be more successful at some types of problems, Some ML platforms, such as TensorFlow and PyTorch most notably, are maintained over longer periods of time (those two since 2015 and 2016, respectively). However, even in that time, both have changed significantly, and other ML platforms have arisen that may be more effective ways to solve the same problems. So even the most stable ML platforms and tools are stable only over modest periods of time.
