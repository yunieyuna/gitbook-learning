# Chapter 6. Fairness, Privacy, and Ethical ML Systems

This chapter is devoted to topics related to ethical considerations and legal obligations when creating or deploying ML systems. We cannot offer an exhaustive source of guidance on these topics, but this resource can point you in the right direction. At the end of this chapter, you should have a good sense of fundamental ethical considerations for ML deployment as well as concrete language and conceptual categories that will get you started in educating yourself more thoroughly in the domains most immediately applicable to your own work.

**NOTE**

Editor’s note: When we put together the list of topics for what MLOps folks truly need to know, issues of fairness, privacy, and ethical concerns in AI and ML systems were right at the top of the list. However, we also knew that it was difficult for a group of authors with strong industry affiliations to provide truly unbiased views on these complex issues. Therefore, we invited Aileen Nielsen, author of [_Practical Fairness_](https://oreil.ly/tsjGP) (O’Reilly, 2020), to contribute this chapter independently. While we gave feedback on drafts for clarity, the views here are entirely hers, and she had full editorial control on this chapter. You’re getting it straight from a world-class expert!

We should also note from the outset that fairness and ethics in AI remain highly contested topics. Indeed, one reasonable position right now is that a quite viable approach to promoting fairness in computing systems is _not_ to use AI/ML. However, for those who find themselves compelled to do so, or who believe that this skepticism of algorithmic solutions can be overcome in their specific use cases, this chapter provides a starting point for understanding the challenges of how to do it right.

What’s more, it should be recognized that in some cases existing or otherwise traditional solutions to problems in the pre-algorithmic age (e.g., having designated human decision makers, or no clear decision makers) haven’t always been great.[1](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn42) For example, a great deal of empirical research suggests that race influences judges’ sentencing decisions and even informal decisions by store clerks as to whether to allow customers to return items. Therefore, when we think about fairness and ethics in AI and ML, we also have to recognize that they may very well provide improvements in some cases relative to human decision makers. So, despite the gloom and doom you will find in this chapter, we also recognize from the outset that some uses of algorithms have been tremendously successful in terms of increasing overall fairness, even if there isn’t yet a clear solution to making AI and ML fair and just in a guaranteed or global sense.

In this chapter, you will find sections that focus on specific hot topics—notably, fairness, privacy, and Responsible AI. We recognize the primacy of these topics both in terms of public awareness and concern as well as in terms of attention devoted from industry and scholarly groups alike. A lot of development is occurring in these areas, and we want to empower you to dive into these topics with a good initial background that you can pick up from this chapter. Within this chapter, we also provide notes on how you might consider refactoring work at your organization, in practical ways, so as to enhance fairness and ethics in your own AI/ML work.

## Fairness (a.k.a. Fighting Bias)

_Algorithmic fairness_, and other variations of this term, are a hot topic in ML and have been for many years. When you read about this subject, you’ll most often see fairness used as a concept directly related to bias—that is, as fairness being the absence of bias, and bias being the condition of unfairness.[2](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn43)

For quite some time, ML researchers, legal scholars, and activists alike have manifested growing concern about the possibility that ML could perpetuate existing social biases, or even create new forms of bias. In framing such discussions, many have emphasized that this could happen because data used to train ML systems could be taken from biased systems or collected in a biased way. In short, people have made arguments akin to the notion of garbage in, garbage out.

But you should be careful to realize that the _garbage in_ does not necessarily refer to bad data. The garbage is the _outcome_ that results from unrepresentative training data or algorithmic bias. This is a key point: it’s important to realize that the _fundamental_ source of bias can come from many steps in the ML pipeline, including bad data, but also including bad modeling choices. Here’s a nonexhaustive list of some commonly cited sources of bias:

Sampling bias

In sampling bias, the process of collecting data itself has bias built into a system. A commonly given example is that marijuana use among white and Black Americans is thought to be roughly equal, but the rate of arrests for marijuana possession is far higher for Black Americans than for white Americans. This is almost certainly due to (among other reasons) sampling bias. Because of racism manifesting at both the level of individual decisions and at a systemic level, Black Americans are far more likely to find themselves searched by police for marijuana possession than are white Americans (some may be familiar with a related manifestation of racism, [driving while Black](https://oreil.ly/4w8oQ)).

_ML-relevant example:_

Crime-prediction algorithms, such as those built to influence police patrolling allocations, likely suffer from sampling bias.[3](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn44) Across many nations and continents, it remains a consistent pattern that policing is directed at communities of low socioeconomic status. Data coming from such a process oversamples crime in certain areas or among certain demographics and so likely misrepresents the underlying base rates of crime in different communities due to this sampling bias. Yet, this data, sampled in a biased fashion, then creates new biased inputs for more ML modeling to allocate future police patrols. The algorithm likely keeps sending police back to the same areas too often because other areas are not oversampled in the same way.

Disparate treatment

Bias can result from individuals explicitly being treated differently. Disparate treatment is banned both by the government and by some regulated areas of the private sector when there is no justified reason for it. In almost all examples of disparate treatment on the basis of race (such as segregated schools or policies of hiring only whites), courts have not found a justified reason for such discrimination (after the Civil Rights movement). For disparate treatment on the basis of gender, courts have sometimes found that reasons offered to treat genders differently (such as different performance thresholds for physical fitness tests) could be justified by compelling interests.

_ML-relevant example:_

ML algorithms are often celebrated for their ability to spot patterns that elude humans. However, sometimes these patterns are plain old sexism. Amazon [famously developed but did not deploy](https://oreil.ly/zyKHl) an in-house hiring algorithm that applied a strong negative parameter for attending a woman’s college.[4](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn45) Thus we see a case of an algorithm using irrelevant factors to make a decision in a way that directly discriminates against a group (in this case, women going to women’s colleges).[5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn46) If the algorithm had been used, this would have looked like a case of disparate treatment.

Systemic bias

This source of bias can be challenging to both identify and mitigate as compared to the preceding examples in this list. Broadly, we can think of systemic bias as a host of factors that likely cannot be identified in individual cases as explanatory features but that, on an aggregate level, clearly influence differences in outcomes for individuals due to the structural limitations (limitations that are baked into our social, educational, and employment systems, among others). AI systems cannot be separated from the societal contexts in which they are built, so AI built without an understanding of context is likely to worsen systemic bias. Indeed, even the particular problems to which AI is applied are often highlighted as themselves manifestations of systemic bias. For example, some have asked why predictive policing has been actively used for so long to predict crimes among socioeconomically vulnerable populations, while algorithmic systems to identify or predict _police_ misbehavior are relatively uncommon.

_ML-relevant example:_

In training an ML system, sometimes features are selected because they seem to provide “common sense” examples of “merit” as some employers or educational institutions might conceptualize that notion. For example, middle class high school students are often told to participate in extracurricular activities to show that they are highly motivated leaders. However, using data about participation in extracurricular activities (in an algorithm or in human decision making) without additional context would contribute to a system that rewards middle class students who have access to such possibilities. This system would likewise disadvantage low-income students whose schools may not offer such activities or who are not able to participate in such activities because they need to work or contribute to family obligations. Thus what “common sense” dictates to people unfamiliar with different social contexts and unversed in the consequences of systemic bias could look quite biased once this additional information is incorporated into the analysis.

Tyranny of the majority

This source of bias relates to the form of training process that is used. In many commonly used modeling systems, the numerical majority category typically affects training loss the most through total numbers. Therefore, if the modeling process does not account for this, many kinds of models will de facto favor the majority and minimize the error for majority groups more directly than for minority groups. This same concern sometimes motivates the design of political systems so as to protect minority interests specifically rather than trusting to majority rule.

_ML-relevant example:_

It is well known that training ML systems with data that does not involve relatively equal distributions of characteristics (imbalanced datasets) can be quite challenging if we seek to achieve good performance for all classes. Such a situation is likely to arise when working with datasets that include different kinds of people, as most contain some kind of imbalance in data, be it gender imbalance, racial imbalance, or other forms of human diversity, such as geography or language. Therefore, modeling human behavior should require paying particular attention to make sure that the model is giving everyone a fair shake rather than modeling everyone according to a prototype that in fact is reasonably accurate only for someone in the majority class.

Tragically, it remains indisputable that these forms of bias (and many others) continue to pop up in a wide range of ML applications, even with increasing media attention about algorithmic fairness. There are many reasons for this. Social biases are widespread and are not always apparent at the individual level. Someone might think they are making an unbiased decision both because they do not have all the details about the system in which they are making the decision, and because of their own unconscious biases. This is not a problem that will be solved by a book chapter, but one that a book chapter can make readers aware of and motivated to do something about.

The ML/AI research community has been developing methods and techniques to systematically identify and remediate some of these biases. Practitioners who are trying to develop responsible and fair AI systems should be aware of these emerging tools that might help. What’s more, ML/AI may very well provide a way forward to make systems fairer than they have been with the use of unaccountable humans, so long as ML development is done carefully and with appropriate safeguards, when aimed at appropriate problems.

**A NOTE ON SOLUTIONISM**

This chapter strongly emphasizes how complicated and difficult it can be to implement AI in a fair and responsible fashion. One neat trick can help you avoid the entire problem: don’t use ML/AI (if you can get away with it). Although many systems undoubtedly work better with some degree of automation, that’s not always the case. Sometimes the human touch is needed. Sometimes the human touch is even more efficient, as well as fairer, than a machine touch.

And, if you do conclude that some degree of automation and algorithm use is justified, make sure you ask yourself exactly _how_ using that automation makes sense. Most situations allow many degrees of use between full algorithm and full human, so think carefully about _what_ an algorithm should be doing even after you decide _whether_ an algorithm should be doing anything at all.

One of the bigger problems with ML is that it often adds complexity, risk, and cost, with little value and no net advantage. We call these uses _solutionism_: merely because we have a readily available (and trendy) solution (ML), we are trying to figure out what problem it solves.

ML systems are necessarily complicated. They can accomplish amazing things when applied well to problems that fit, but they can add enormous risk (and significant bias or other forms of unfairness) to other kinds of applications. The smartest thing an ML team can do is to consistently and skeptically consider whether ML is necessary or useful at all. By doing so, we can confine our efforts to ML applications that truly add significant value, and therefore are worth the effort required to ensure privacy and fairness as we implement the system.

### Definitions of Fairness

In the ML community, we have not cohered around a single definition of fairness, but common categories exist that have intuitive and appealing descriptions. For example, some definitions of _fairness_ emphasize individual fairness. These notions of fairness argue that, for two individuals who are “the same” other than irrelevant factors (race, gender, etc.), these individuals should be treated the same.[6](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn47) Other definitions of fairness emphasize that ML should look to fairness at the level of groups. That is, error rates should be the same and of the same quality across groups, and overall the level of performance should likewise be equally high among groups. Still other definitions of fairness might get more complicated and look to establish causal mechanisms to understand what might drive individuals to success or failure, before looking to categorize them algorithmically.

Two common categories of fairness that are intuitive and appealing are group parity and calibration-based assessments. These are far from being the only notions of fairness, but we select these two to discuss for two reasons. First, each of these definitions is straightforward and has a strong intuitive appeal. Second, each highlights two distinct notions of fairness.

As its name suggests, _group parity_ emphasizes fairness as matter to be compared on the basis of comparing groups. It puts emphasis on recognizing that outcomes for individuals should be looked at, at least in part, on the basis of sensitive attributes of their identity, and most particularly on legally protected attributes, such as gender and race, but also on attributes that may not be legally protected but that some consider morally important, such as economic status. There are good reasons for this, including that when we look at the world, such features often turn out to be horribly influential in actual outcomes.

On the other hand, _calibration_ emphasizes individual fairness, which is another value most of us find quite intuitive and appealing. We likewise feel strongly that individuals should be treated on the basis of who they are and what they do as individuals, and not based on where they come from.

It would seem that these two definitions of fairness need not be at odds philosophically. In a perfect world, where groups and individuals are treated fairly, we would get the same outcomes. Unfortunately, this turns out not to be the case in our imperfect world, for a variety of reasons, including fundamental mathematical limitations.[7](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn48) For this reason—at least for now—practitioners must choose their definition of fairness. As we will describe, different use cases will have different reasonable definitions of fairness.

While applying different notions of fairness in different situations may seem strange, mature readers will realize that we likewise apply the same principles in real life. Consider the US, a strongly market-driven economy. For the most part—and even in our current era of populism—there tends to be a pervasive individualized notion of fairness when it comes to the labor market. Americans seem to think people should get what they can get on the open market, albeit with some concerns that wages not go too low (thus, there is a legal minimum wage) and likewise with some concerns that the highest rates of pay have gone too high (demonstrated by increasing concerns about rising inequality and the rise of the billionaire class). On the other hand, when it comes to healthcare, most Americans think everyone should get good medical care regardless of differences in individual health or inborn genetics.

It seems (at least to us armchair anthropologists) that at some level Americans are comfortable with an allocation of economic benefits based, likely in part, on inborn ability but are not comfortable with this notion when it comes to an allocation of medical benefits. Thus, ordinary people in fact seem to commonly apply different notions of fairness to different domains of life, and we propose that the same could plausibly be true in different contexts or use cases of ML.

**WARNING**

It might seem that the ML ethics community should be attempting to converge on a single definition of _fairness_. This might seem especially true when we can see that different definitions of fairness are divergent or even contradictory in practice. At present, given the state of maturity of the industry and this field, it is almost certainly not a goal to converge on a single definition of fairness. This is for two reasons.

First, we don’t need to. The state of the art of most models deployed in the real world is still fundamentally unfair by any metric (or at least of unexamined fairness). We have many useful definitions of fairness to select from, and picking one or several to work toward provides us with the opportunity for rapid progress right away without needing to await further theoretical or legal developments.

Second, and more structurally, the field of AI and ML theory is currently largely controlled by people from the dominant groups of society. These are the very people most likely to miss the ways that AI and ML systems can negatively impact disempowered or underrepresented groups. Surely, these same people shouldn’t be the arbiters of what will be set down as the proper definition of fairness to the exclusion of all others in the future.

_Group parity_ fairness definitions require that relevant rates of algorithmic performance be the same across various groups. A group parity requirement could, for example, require that the hiring rate across all ethnic groups be the same, but might also require that the accuracy for [diagnosing low oxygen](https://oreil.ly/wU0mJ) be the same regardless of skin color. This idea is compelling and intuitive because it paints a picture of the society many of us would like to see—one in which bad luck or opportunity are distributed equally among communities.

In contrast to group parity, _calibration_ fairness definitions require that a ML model work equally well for all individuals. While this cannot be directly measured, what can be measured is that a model score means the same thing for an individual regardless of their group membership. So a calibration-oriented definition of fairness would require that for any group, the ML score means the same thing.

This sounds intimidating and highly technical, but it can be easier to digest with a specific example. A commonly given example of calibration is that of the Correctional Offender Management Profiling for Alternative Sanctions, or [COMPAS](https://oreil.ly/KEVqN), recidivism scoring algorithm, which is used by criminal courts in many US states. Because this algorithm has been shown to be calibrated correctly across racial categories, the same COMPAS score (say, 0.5) means the same thing for both Black and white data subjects: that is, a white offender and a Black offender with the same risk score have the same probability of reoffending.[8](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn49) When understood in plain English, this seems intuitive as well—that a model score should mean the same thing for everyone, no matter what group they belong to.

You may wonder whether having many definitions of fairness is a problem. After all, there are many correct and intuitive ways of describing the world in other contexts, so why not in the case of fairness? The difficulty comes from the fact that—at least in the world we currently live in—these definitions of fairness come into conflict. The COMPAS algorithm is a compelling example. That algorithm was not necessarily implemented in a careless fashion, but was, in fact, sometimes adopted as part of a movement to reduce bias in the criminal justice system. The algorithm passed calibration checks that were considered to be the gold standard in ensuring fairness and nondiscrimination.

However, reporters at ProPublica, a nonprofit newsroom, later demonstrated that—despite this calibration—the algorithm had higher rates of false positives in labeling Black defendants high risk, and higher rates of false negatives in labeling white defendants low risk. In other words, Black defendants were more likely than whites to be labeled as having a high probability of a violent re-offense and then not go on to commit such a re-offense, while whites were more likely to be labeled as having a low probability of re-offending but then going on to in fact re-offend.

This led to a public outcry, prompting many mathematicians and computer scientists to work on the problem. However, scholars quickly realized that their goal—of making a risk score that demonstrated both calibration and statistical parity–was impossible with most real-world data. Statistical parity and calibration cannot be mathematically satisfied at the same time, unless base rates of events are the same in each group. However, equality of base rates is a condition rarely met in the real world.[9](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn50)

Mathematically, not all fairness definitions can be satisfied at the same time, given real world conditions. We have to decide which fairness goals to pursue in an ML context, and likewise decide whether focusing on a specific fairness metric might even tend to reduce the tendency toward a holistic viewpoint that might otherwise be more helpful in enhancing fairness and other ethical values.

For now, we consider that for a specific ML tool, it may make sense to choose and emphasize a particular fairness metric, while recognizing that it will not be possible to satisfy all intuitive and normatively desirable notions of fairness at the same time.

We do not want to allow perfection to become the enemy of the good. Rather, practitioners have found that having many definitions of fairness is more actionable than having none, and that working toward a metric of fairness is broadly likely to enhance many other forms of fairness as well. What’s more, some researchers have recognized that different situations may call for different fairness metrics, given the relative harms or policy goals of a particular use case of an algorithm, and so interested readers can find useful guidance as to how to choose a fairness metric for a specific task given specific concerns about the likely consequences of various kinds of mistakes.

**WHAT ARE NORMATIVE VALUES?**

_Normative values_ (or, in short, _norms_) are deliberate choices that individuals or societies make as to what is morally or socially desirable, and also as to what is not acceptable. Normative values are distinct from empirical realities. A normative statement is about what _should_ happen in the world, while an empirical statement is about what _does_ happen in the world.

An empirical statement relating to algorithmic bias could be that algorithmic bias causes economic inefficiency because, for example, deserving job candidates are weeded out and thus lost as hiring opportunities. Such an empirical statement does not make a value judgment but simply observes the world as it is. In contrast, an example normative statement could be that algorithmic bias is wrong because all equally qualified people should have an equal chance at a job as a matter of fundamental fairness. This example normative statement doesn’t look to describing the results of a social phenomenon but rather purports to judge that social phenomenon or offer proscriptions about what should be changed about that social phenomenon.

Of course, normative values can be difficult to articulate and defend when it comes to implementing them in practice. For example, the statement that all equally qualified people should have an equal chance at a job obscures the tricky question of what “equally qualified” means and whether jobs should always go to the most equally qualified applicant. There might be some other notion of a more deserving applicant in some scenarios. Consider that in a family business, many would consider it fairer to hire a reasonably well qualified family member rather than a slightly more qualified stranger. On the other hand, others would argue against this policy as entrenching nepotism and inequality. And so, we can see how different sets of similar but not identical normative values can lead to conflicts that will continuously surface in real-world debates and decisions about the best way to enhance fairness in society.

### Reaching Fairness

Concretely, a classic ML setup has three modes of working toward fairer (less biased) outcomes. Here we provide a flavor of how these work, so you can approach the literature with an overview and understand the benefits and costs of different approaches:

Preprocessing

These methods intervene in data rather than in models. Preprocessing methods take multiple approaches to reducing unfairness in the inputs into ML training. Most bluntly, data points can be relabeled. For example, some approaches, such as that of [Kamiran and Calders (2011)](https://oreil.ly/I2fDD), offer ways of identifying data that should be relabeled because the data suggests a biased outcome.[10](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn51)

Another, less drastic approach, is to seek a representation of the data that reduces information about the group to which an individual belongs; an approach pioneered by [Zemel et al.](https://oreil.ly/vW9fa) (2013), for example, proposes describing the data such that an individual’s sensitive attributes can no longer be guessed accurately. Because these methods look to the data as the point of intervention, they are model agnostic. A general rule of thumb is that _intervening at the earliest possible stage in the ML pipeline is most likely to yield best results_ (and leave options open for subsequent additional interventions).

But, there are, of course, valid concerns about preprocessing approaches. Changing the data labels aggressively challenges a data-driven discipline, such as ML, in fundamentally removing information. Likewise, methods that look to “transform” the data rather than directly manipulate the labels also challenge the fundamental tenets of a data-driven approach when data is deliberately removed or changed. Also it can be difficult to identify exactly what about the model changes as a result of changing the data, since in most cases it will not be possible to know all that clearly affects the relabeling a few data points.

In-processing

These methods intervene during model training. This can manifest in any way in which the actual step-by-step training of a model is affected by fairness considerations. In many cases, this has been addressed through adjusting the loss function used during model training. Various “penalties” can be added to reflect the fairness costs imposed by biased outcomes, as is reflected in the work of [Kamishima et al.](https://oreil.ly/rEc4P) (2011). Such methods are similar to regularization techniques.

Another method is similar to that described in preprocessing (learned fair representations) in terms of motivation to remove identifying information from the model’s knowledge about the sensitive attribute. An adversarial model is trained simultaneously with the model of interest, such that the adversarial model’s target is to guess which sensitive category an output is associated with. The model of interest is trained to a task but also optimized to reduce the information that its outputs transmit to the adversarial model. Some of these methods, such as the work of [Zhang et al.](https://oreil.ly/SVmNt) (2018), can be model agnostic.

Post-processing

These methods intervene on the model’s labels rather than on the model directly. In this way, they correct outcomes from the model based on meeting certain targets. An examples of post-processing is introducing randomization, as in [Hardt et al. (2016)](https://oreil.ly/b9EhM), in the case where—without such randomization—false negatives might be different among different groups (an example of applying group parity).

Another example of post-processing is to set different thresholds—say, for a [credit score](https://oreil.ly/DzYYL) or college admissions scores—for different groups such that the predictions or decisions made with the score can be equally accurate for different groups.[11](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn52) Because these methods intervene after a model has run, they are model agnostic.

Which method of intervention to choose depends on a variety of factors. In some cases, an organization may need to apply a specific stage of intervention because that is the intervention over which they have control. For example, an organization may have received a pretrained neural network that it will fine-tune. Since the organization doesn’t have access to the original training data or method, post-processing might prove a more viable option.

On the other hand, another organization might find that the options offered by post-processing are normatively problematic because they violate certain fundamental values held by some people. For example, people may be uncomfortable with the idea of having explicitly different score cutoffs for different groups, and they also might be uncomfortable with the notion of eliminating, and replacing with randomized numbers, those outputs from an algorithm that are, in fact, likely to be correct.

To date, there are no well-established guidelines from regulators or prominent ethical leaders regarding the best way to intervene,[12](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn53) and we suspect that this will have to be a highly context-sensitive analysis. But the need for highly contextualized case-by-case decision making is no different from other elements of running an organization or making decisions that affect other people’s lives. Algorithms will help increase efficiency and uniformity, but there will never be a single go-to algorithmic fairness solution for all scenarios.

**ALGORITHMIC FALLBACKS FOR FAIRNESS?**

As you will read about in other chapters of this book, sometimes an algorithm’s performance will be obviously or catastrophically bad. Other times an algorithm may be unavailable. In such cases, you’ll need a fallback. A technical fallback can be a helpful idea for fairness-related concerns too.

Here are a few scenarios where you might want to suddenly cut access to an algorithm and have an algorithmic fallback in place:

* A human user reports an algorithmic output so offensive to basic notions of fairness that the system should be completely unavailable until an audit determines the source of the problem and corrects that source.
* Continuous monitoring systems you have wisely put in place indicate a drift of results beyond acceptable fairness thresholds (for whichever fairness metric you have chosen).
* The algorithm recommends such a drastic (or possibly harsh) output that you send the output for human review prior to making the output available to the decision subject.

In these and other such cases, you could have a variety of algorithmic fallbacks in place, including the following:

* Temporarily using a nonalgorithmic solution. For example, consider giving everyone the same output—perhaps the average output from the algorithm—while you sort out the algorithmic issue.
* Shunting the algorithmic outputs to a human reviewer and having a prompt to “please wait until we can contact you.”
* Radical transparency in which an appropriately tailored error message is provided: “Sorry our algorithm is having some trouble, and we don’t want to give you irresponsible outputs. Check back later.”

### Fairness as a Process Rather than an Endpoint

This is a difficult set of topics, and it can be strongly discouraging to know that AI can and does cause so many harms. Don’t let this bad news dissuade you from all of the advantages that can be achieved with a thoughtfully engineered ML/AI system. The news about fairness doesn’t have to be bad.

Let’s consider two problems:

* Fairness wise, you can’t please everyone. There is no “perfectly” fair solution to any algorithmic fairness challenge (at least none has been identified so far).
* Likewise, there is no perfect way to enforce fairness even if you do settle on a particular definition of fairness (that is, defining a metric doesn’t guarantee you can make a model perfectly conform to that metric or indicate how you should attempt to do so).

Let us rebut both of these points in terms of their relevance to your choices in your ML modeling:

* You have many good fairness definitions to choose from.
* Any process to work toward fairness is far better than a world in which we ignore these issues—because ignoring hasn’t led to good outcomes.

It can be helpful to think of fairness as a process rather than a specific endpoint. While this can be disappointing to those who like to think they can build an ML algorithm to solve a business problem and move on, the reality is more complicated. Products take upkeep for many reasons. The world changes, and so deployment conditions change, and so models must evolve. Fairness is no different.

### A Quick Legal Note

It is far beyond the scope of this work to provide legal guidance; that is what your company’s legal department is for! Nonetheless, it is worth mentioning that despite the seeming newness of algorithmic fairness as a topic, the concept of discrimination has had an important and explicit place in law for centuries in one form or another. The topics most in the forefront today, such as racial and gender equality, have likewise occupied a prominent role in law for decades. This has especially been the case in areas of civic life that are understood as core to human life, such as employment, education, healthcare, housing, and access to financial credit services. If you are doing ML work that touches on these core areas, you likely need to be deeply concerned about fairness—and, specifically, about fairness as it is implemented in your nation’s antidiscrimination laws.

## Privacy

_Privacy_ is a notion that has proven notoriously difficult to define in scholarship. Privacy measures have therefore proven difficult to create, particularly with respect to future-proofing. The rise of big data has proven a dramatic example of this.

Experts used to think that de-identified data was appropriate and safe to release. Such data would have removed what was thought to be identifying information, such as name or address, that could easily be matched to a person. However, information about other factors associated with, but seemingly not directly related to, a specific person’s identity was still released, such as birthday, race, or zip code.

With the advent of big data, many datasets were compiled, often about overlapping groups of people, and it then became possible to use different datasets together to identify people from de-identified information. For example, for most Americans, it turned out to be possible to identify them in a de-identified dataset merely from knowing their birthday, zip code, and gender.[13](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn54) And, the world is increasingly full of new datasets, sometimes resulting from data breaches but other times resulting from people voluntarily sharing information about themselves—information that becomes very easy to access [by any casual creepy stalker](https://oreil.ly/AOVNs). We might like to think the world is an anonymous place, but it simply isn’t the case today.

Two key ideas about privacy are described here, both in terms of how they relate to a notion of privacy, but also in how they relate to taking an existing dataset with personal information and turning it into a dataset that can be used or released without compromising individual identity:

k-anonymity

The idea behind k-anonymity is that in a given dataset, for any given combination of categories of interest, there should be at least _k_ individuals (externally specified) who fall into any given bucket. So, for example, we could apply k-anonymity to a dataset listing individuals in a town by requiring that data be bucketed such that for any zip code / birth information / gender category, there were at least 10 individuals. There might be many ways to accomplish this. Two potential opportunities would be to report birthdays at the month or year level and to report only the first four digits of a zip code rather than all five.

This notion of privacy and preventive measure essentially conceptualizes privacy as not being in too small a group within a dataset. [Recommended size](https://oreil.ly/zl6MO) is at least 5 in the medical domain, although _k_ sizes far larger (even greater than 50) have been reported. The appropriate size for _k_ will depend on the particular domain, which could have different implications regarding sensitivity of the data, possibility to build useful datasets with large _k_, and possibility of reidentification given other known potentially linkable datasets.

Differential privacyThis mathematical method adds noise to data such that probabilistic guarantees can be made regarding the possibility (or more importantly, lack of possibility) to make inferences about a specific individual when given access to that noisified data. The idea behind differential privacy is that it would be a privacy problem for one individual’s inclusion, or non-inclusion, in a dataset to influence operations, such as the calculation of averages or other statistics, so that the dataset’s information could be inferred based on aggregate reporting. So for example, if the mean age of a class is reported and the size of the group, with and without an individual, it is possible to know that individual’s age. However, if differential privacy is applied, it would, in probability, not be possible to infer that individual’s age at a pre-specified level of precision and given a pre-specified query budget.[14](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn55) Differential privacy can apply quite broadly, not just to the computation of aggregate statistics but also to the training of ML models, with methods to ensure that a model’s outputs or training is not contingent on inclusion of a particular data point.

These notions of privacy may seem quite technical, but are relevant to real privacy problems, such as managing the responsible and privacy-preserving release of open datasets or ensuring that legal rights have a meaningful technical translation. For example, the EU’s GDPR gives an individual a right to data deletion. However, if an ML model has already been trained, that right to deletion may not be entirely meaningful. For example, does an individual’s right to delete their data mean that they can also force a company to prove that its ML models have also “forgotten” their data?[15](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn56)

Some have explored how training models with differential privacy might address this concern. However, technical challenges remain because producing ML models with differential privacy guarantees is quite a technical challenge. It is challenging to ensure both differential privacy and a high level of performance at the same time.[16](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn57)

### Methods to Preserve Privacy

The definitions and scenarios described previously with respect to k-anonymity and differential privacy refer to very specific notions of privacy and computational measures. And, indeed, privacy is itself a highly technical and specialized field, perhaps best left to experts in terms of implementation.[17](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn58) However, a variety of accessible privacy-enhancing measures are transparent, straightforward to implement, and meaningful. You should include them in your own workflow and will likely need to customize them.

These are likely to already be familiar concepts to systems administrators and those who deal with compliance issues. However, these sometimes are painfully unfamiliar to data scientists and ML engineers. It is my hope that they will become part of basic project specification and daily practice and consideration in the future.

#### Technical measures

The following are some technical measures to take:

Access controlsA key way to preserve privacy and reduce threats is to implement robust access controls. Any data about people in a database should be treated as a “need to know” resource, with ML engineers requesting access for specific purposes rather than being able to freely access or browse data. Likewise, access should be revisited periodically to make sure that staff members do not retain access to data for which they no longer have a valid active reason for access.Access loggingKeep track of who is accessing specific forms of data and when. This makes it possible to understand data use patterns, see when someone might be inappropriately accessing data, and preserve evidence in case allegations of inappropriate use are made later. An analysis of such logs may also indicate ways in which data storage schema could be refactored to reduce the extent of data that different use patterns can access. For example, if an ML model calls for access to a sensitive table of data merely to access one column, consider splitting off that column of data rather than granting access to a full table of additional but unnecessary pieces of information.Data minimizationThe collection and use of data should be minimized. Data should not be collected merely because it “could be useful” in the future. Data should be logged only when there is an immediate use case for this data (preferably, with some benefit to those about whom the data is collected).Data separationThe data needed for legitimate business uses should be separated from sensitive data (such as names and addresses) that is unlikely to be relevant to creating an ML model. For example, to predict users’ clicks, there doesn’t seem to be a justification for knowing a user’s name or address.[18](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn59) Therefore, there is no reason for that information to be stored with information that might be useful for that particular prediction task, such as past browsing history or demographic information.[19](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn60) As noted previously, studying your data access logs can help you identify ways in which data storage can be refactored to minimize exposure of data to ML applications.

#### Institutional measures

Here are some institutional measures available as well:

Ethics trainingEveryone needs ethics training when they enter a new domain, and the same goes for designing ML products. ML engineers should be given—but usually are not—a thorough review not only of general ethics training (such as the possibility of bias in data) but also domain-specific training when building algorithms for a specific use case. Too often organizations do not have any formal discussions or training about privacy or ethics more generally, and even basic training, no matter how “corny,” serves to bring the issues to the fore.Data access guidelinesIn addition to technical measures already described, it makes sense to have explicit rules that are readily available regarding what constitutes appropriate access to data and use of that data and what use cases are expressly prohibited. A lack of clear and explicit ethics rules can lead to an institutional culture without accountability. Organizations should have clear data access and appropriate data use guidelines in place in a location that is readily apparent and accessible.Privacy by designPrivacy by design is a set of design principles that can apply to any digital product, including ML pipelines and ML-driven products. The notion of privacy by design is that privacy is something that shouldn’t be tacked on to the end of a process that already exists. Rather, privacy should be intrinsic to design considerations from the start and should be a question and concern addressed at all working stages. Privacy by design can provide a flexible but holistic way to ensure privacy in all elements of ML development.

While these may all seem like basic and obvious notions, few organizations, large or small, do these basic things, or even pursue anything akin to a privacy or fairness agenda. Whether you are at a startup, an academic institution, or a large corporation, there is almost surely something you can contribute to enhance privacy in your organizer’s ML pipeline.

**DATA ACCESS GUIDELINES**

If you’re in an organization that lacks formal data access guidelines, here’s a short list to get you started:

* No personal use of data (i.e., no looking up whether your cousin has ever ended up in the database)
* No examining data of someone you know (same point about your cousin)
* No analyses you wouldn’t want to see the light of day or that could cause a PR problem (think of [Uber’s misjudged blog post](https://oreil.ly/578qJ) on “Rides of Glory”)

In short, there are a variety of ways to protect privacy by taking concrete steps in your organization. Unfortunately, these steps are rarely taken, but they can be simple and effective. You have nothing to lose (and lots to gain) by initiating such simple efforts in your own workflow.

### A Quick Legal Note

It is far beyond the scope of this work to give an extensive review of privacy laws that usually affect digital products. Here we highlight a few key categories of laws that are related to privacy and ML:

Data breach notification lawsData breach notification laws require those who hold data to notify people whose data they hold if that data was compromised in a data breach. To ensure compliance, such laws usually apply strict penalties if a holder of data becomes aware of a data breach and does not make the appropriate notifications. Such laws can sometimes also apply to a data holder who should have been aware of a data breach, to ensure that firms cannot simply choose to remain ignorant. Empirical research to date suggests that such laws have not prevented data breaches from becoming increasingly, even exponentially, more common over time. Nonetheless, such laws are useful in providing notice to consumers as to when they may be most at risk because of exposure of their personal data.Data protection and personal privacy lawsThe most prominent example of data protection law is the EU’s GDPR. Many countries around the world have data protection laws that give people basic rights, such as the right to know what data is collected about them by online venues, what the venues do with that data, and whether the data is even correct. Some laws even go further, giving consumers the power to opt out of data collection, or even giving consumers the right to have data deleted or blocked from data sales. Unfortunately, empirical research has shown that such laws appear to be widely disregarded in consumer-facing applications. Nevertheless, such laws give consumers active ways that they can take steps to protect their privacy.Laws against unfair and deceptive practicesThis category of more general consumer protection law is important in the US context, since the US otherwise lacks a comprehensive national personal data privacy regime. The US Federal Trade Commission, an important source of consumer protection enforcement, has sometimes brought actions finding that companies infringed basic expectations of fairness or honesty in their business practices as a means of protecting consumer privacy. A key example of this is companies that do not even respect the terms of the privacy policies they themselves author and leave on their websites. Thus it is—at the least—essential to ensure that, as your organization builds out data collection and ML modeling capabilities, you ensure that such practices are consistent with public-facing privacy policies and terms of service.

## Responsible AI

_Responsible AI_ has come to be used as a catchall for ethical concerns we should contemplate when training or deploying an ML system. This is a growing area, and it’s safe to assume that neither industry nor the academic community yet has a firm grasp on the scope of harms that should be, or can be, addressed by Responsible AI. Here, under this rubric, we address some additional questions that have received a good deal of attention in recent years. However, we emphasize that the issues here are highlights. We do not purport to offer an exhaustive list of Responsible AI values.

### Explanation

ML model _explanation_ is the process of analyzing and presenting information about an ML system to describe how that system works. This process and the goal of making the model amenable to human understanding is often discussed in shorthand as _explainability_, and it’s a key area of interest with respect to ML. Many people have a desire to understand why an ML system is working the way that it does as part of understanding why it has reached a particular outcome that affects them.[20](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn61)

For both technical and ethical reasons, it is desirable to have methods that can “explain” how an ML model works or why it reached a particular outcome in a particular case. The technical motivation for explainable AI is related to controlling model quality and possibly learning about the data through the model. Technologists who develop explanations of their models can gain insights into why their model is working well or poorly, and may even learn something about the underlying domain of the data. Ethicists and advocates who seek explainable ML have similar concerns but for different reasons. They find it empowering to know how a model came to a conclusion, so that the conclusion can make sense to those affected by it, or even be challenged where the ML conclusion doesn’t make sense.

However, explanation is not a simple thing. Explanations can serve any of a number of purposes and therefore quite a wide variety of information can constitute an explanation, depending on the purpose for which it is sought and the audience for whom it is prepared. In this way, model explanation can feel a lot like thinking through the problems that highly trained professions, such as doctors or lawyers, face in trying to give advice or a diagnosis of a real-world situation to a particular audience. For example, when doctors explain a recommended treatment and its associated risks, they will tune the explanation to their audience. They’d probably give one explanation to a fellow physician, another less technical but still rigorous explanation to a patient known to have a bioengineering degree, and still another to a person who was suffering from dementia but who might still be competent to make their own medical decisions. Thus, we can see that an explanation depends on the _audience_ or _consumer_ of that explanation.

The explanation that is provided also depends on its _purpose_. If the purpose of an ML explanation is to inspect model quality, such as to make sure the model is making the right decisions for the right reasons, then a global explanation might be preferred. A _global_ explanation indicates how the model works generally and why certain general decision rules will be followed. On the other hand, if the purpose of the explanation is to enable a specific person who was refused credit (or had another undesired outcome) to know why in a way that could help her improve her chances in the future, that person would want a local explanation. A _local_ explanation explains why that person in particular was refused and the most actionable adjustments to possibly make in the future.

We won’t get into more detail on explanations for the moment. For our purposes here, we want you to walk away understanding these key points:

* There is no one single correct explanation of a model.
* Explanations need to be tuned to audience and purpose.
* Explanations need to be useful and sometimes actionable.

In terms of what you should do concretely, the following could be the minimum steps to take at the start of your journey to get better acquainted with ML explanation techniques:

* At the least, it is helpful for an end user to know the inputs used for an algorithm (even this minimal information is often not available). It is even better if you can provide a list of relative feature importances. This would be most meaningful if placed in a prominent location and in accessible language so that ordinary people can see and understand the model.
* Another easy way to get some intuitive and concrete information that can be explanatory is to generate test inputs, perhaps counterfactual pairings, and see what these look like. For example, some counterfactual pairings shouldn’t matter for some use cases (e.g., gender counterfactuals should not change credit decisions), while others most certainly should (e.g., body weight counterfactuals probably will often change medical intervention decisions). These can constitute a basic smell test as well as a way of providing example explanations to those who are the decision subjects.
* Contemplate whether you can offer global (explaining the model overall) or local (explaining a particular ML model decision/classification) explanations and explore a few techniques for doing so. Is a certain technique particularly appropriate for your audience and the kind of decisions your ML product is making?

As various explanation techniques have emerged, some researchers have offered useful guidance as to what systems might serve different purposes and be appropriate to use.[21](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn62) The most important consideration is to determine the level of sophistication of the end user of the explanation and the purpose of providing an explanation. From there, you will often be able to identify at least one, and usually more, technical options, some open source and already implemented by experts.[22](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn63)

### Effectiveness

ML _effectiveness_—that is, an ML product actually achieving its desired target, and for the right reasons—is key to responsibly deploying ML. Yet, as highlighted quite famously by Cathy O’Neil’s _Weapons of Math Destruction_ (Crown, 2016), a particularly worrying element of ML is that in many deployment scenarios, ML can be a self-fulfilling prophecy: ML might appear to work, but this could be for entirely wrong reasons.

Consider, as O’Neil did, ML products for hiring purposes, in which candidates can be flagged to be rejected automatically. Perhaps the ML algorithm is correct that someone would be a bad hire, but, also quite likely, we may never know whether someone was a good hire because we didn’t hire them. But if a variety of ML algorithms are all using the same logic to not hire a particular candidate, that candidate never gets the chance at a job, and we never actually know whether that candidate could have done a good job (systematically missing data).

If someone is labeled by a ML algorithm in a particular way, sometimes that label is trusted to settle the issue, even if humans are supposed to be exercising some level of supervision.[23](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn64) In the employment scenario, someone could be labeled a poor candidate and not be hired by the human who is using the algorithmic assistance. That job seeker is perhaps not hired only because the algorithm labeled them as a poor option rather than because they actually were a poor option. And perhaps the use of the same or similar algorithms across many potential employers makes the situation even worse. Perhaps that job seeker will face the same algorithm deployed by many potential employers, thus facing an extended or even indefinite period of joblessness. At some point, their extended period of unemployment will itself become another factor that an ML algorithm is likely to use as a flag against hiring someone.

This concern isn’t just limited to scenarios that O’Neil identifies in potential anecdotes. Lawmakers too are concerned. For example, in the recently proposed [Algorithmic Justice and Online Platform Transparency Act of 2021](https://oreil.ly/iVwyj), US Senator Ed Markey and US Representative Doris Matsui have proposed that only safe and effective algorithms be legal. The effectiveness of an algorithm would be established by showing that the ML algorithm “has the ability to produce its desired or intended result.”[24](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn65)

Given this law, or similar requirements of efficacy, we can think through how O’Neil’s example of self-fulfilling hiring algorithms could play out. The designer of a hiring ML algorithm should show that the algorithm actually predicted who would be a good employee, rather than simply who was unlikely to be hired. One way of thinking about this is _external validity_—that is, the notion that the findings or conclusions that power an algorithm translate to a general and real-world concept that matters. A requirement of showing efficacy is akin to a requirement that the algorithm’s logic is externally valid, that it generalizes to some reasonable degree.

Self-fulfilling algorithms are, of course, not the only concern related to algorithmic effectiveness. Some other terms related to this concept are briefly described here:

RobustnessAlgorithms can be effective only where they are resistant to foreseeable attacks or misuse and are designed to limit or prevent such foreseeable abuse.Validated performanceModels must work in their deployed use cases. Models should be checked for good performance anytime they are deployed in a new situation, such as on a new population, or even simply being applied over a lengthy period of time.LogicWhile some celebrate ML on the basis that such models can “find patterns that humans can’t see,” sometimes the patterns identified don’t make any sense, or make sense for the wrong reason. If the logic of a particular input’s relevance doesn’t pass a basic smell test, it’s time to ask questions. Requiring some degree of logic is also consistent with Responsible AI goals related to efficacy.

### Social and Cultural Appropriateness

Another element of consideration for the responsible use of ML relates to the acceptable use of technology more generally in social situations or with social ramifications. Are all roles, as observer or decider, appropriate for a machine? For example, would you want to be told that you were going to die of a terrible illness…by an algorithm? Likewise, do you want your child “watched” by an ML product or by a human babysitter? Perhaps the answer is that you don’t care, and perhaps the answer is that you do. Sometimes, when people refuse algorithmic products it is because of ideas related to human dignity or social respect, rather than safety concerns about an algorithmic product. That is, just because something can be automated well doesn’t guarantee that people will feel respected when they are interacting with an algorithm.

These cultural concerns about human dignity can apply to the users of an ML product, rather than merely the subjects. Consider, for example, a recent Twitter Responsible AI study in which the team ultimately concluded that the best way to make a feature fair (in this case, a photo-cropping feature) was to [remove](https://oreil.ly/YsWXs) the feature rather than refine it, in part to encourage autonomy and agency by those who were posting photos. Sometimes the best solution is the remove a technical “solution.”

## Responsible AI Along the ML Pipeline

The various specific concerns related to Responsible AI that we’ve discussed previously will necessarily overlap in the course of a real-world ML pipeline. In this section, we include specific points to consider relating to pragmatic questions you should pose to yourself and your team, depending on where along the ML pipeline you find yourself.

### Use Case Brainstorming

If you are brainstorming potential use cases, perhaps because you see a new business opportunity or see the possibility of obtaining new data, you should be thinking about the following fundamental questions related to your potential project:

* Is this a use case that will undermine privacy, and if so, how will you take precautions from the start to build in privacy protections?
* Is this a use case that touches on fundamental concerns about human dignity or social expectations, which could add additional limitations to the scope of appropriate use of algorithmic approaches?
* Is the decision or classification to be made by ML an important decision, where fairness should be particularly guarded, and if so, are there indicators that this can be accomplished? What constitutes _important_ will vary, but sometimes these are understood to be decisions with legal effect or legal-like effects (such as hiring or education). Another way of defining _important_ would be to first identify those models in your own organization as areas where a mistaken decision would have the most significant impact for a decision subject.

### Data Collection and Cleaning

At this point in the pipeline, you have decided on a use case and are looking for data and preparing data for modeling. Now, the following concerns should be addressed:

* Will data be acquired in a way that respects informed consent?[25](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn66) Have you disclosed the purposes for which you will use the data to the subjects in a reasonably informative and transparent way?
* Have you stored data in a manner that promotes privacy and minimizes the likelihood of unintentional disclosures?
* Have you done exploratory analyses of the data to look for potential bias in that data?

### Model Creation and Training

Now you are in receipt of data, and it’s time to do some modeling:

* Have you come up with an affirmative plan to monitor and address bias? How will you choose from various forms of fairness interventions based on the potential harms of bias and based on the particular normative values or legal restrictions of your use case?
* Are you training in a manner that will reduce data leaks from the trained model and enhance robustness against [malicious attacks](https://arxiv.org/abs/1412.6572)?
* Have you considered in your loss function the relative degree of harm from different mistakes, rather than lazily setting all mistakes to the same loss value?
* Have you chosen your model architecture (say, opaque neural network versus interpretable linear function) by understanding the relative importance of accuracy and explainability for your particular application?
* If your domain has something like scientific laws with strong predictive value, have you included this domain knowledge in your model architecture and training choices?

### Model Validation and Quality Assessment

At this stage of the pipeline, you may receive a model that you are told is as good as it is going to be in terms of accuracy. Your job is to kick the tires in a reasoned way and decide whether to give this model approval to go forward, or send it back to training:

* Have you asked whether the model was trained on a proxy, and if so, what data is available to justify the use of that proxy?
* Have you tested the model robustly, with a fair selection of held-out data in realistically challenging situations? Basic accuracy remains an ethical obligation as well as a business target and technical measure.
* Are you able to identify and understand the logic driving the model globally and to generate individual explanations in case an individual might ask for them? (See [Chapter 9](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#monitoring\_and\_observability\_for\_models) for more information.)

### Model Deployment

Now it’s time to make the model available for its planned use:

* Have you put in place monitoring programs to continually assess the performance of the system in actual use? Such performance could be assessed with respect to traditional performance metrics (such as accuracy) as well as with respect to fairness metrics (as discussed earlier).
* Have you established ex ante criteria to assess whether the model is working as expected?[26](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn67)
* Have you tried running the model in an online mode so that you can watch how it performs, counterfactually, in advance of an actual product launch? Or in the alternative, you can maintain a shadow model, which makes predictions on productive data even when users cannot see the results. This can be an intermediate step to understanding likely deployment performance without running the risks associated with actual deployment.

### Products for the Market

Whether models are intended for internal or external use, they need to meet the same standards for fairness and privacy protections. But models that will be directly accessed by external users generally have an additional set of requirements that must be met.

If the end goal for your model is to be directly accessed by human users, consider the following:

* Will you make available a recourse or method to challenge a decision, and if so how?
* Will you make available explanations about how the ML system works, possibly even specific guidance for individuals about how their outcomes were formed?[27](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn68)
* How will you detect events or problems you haven’t anticipated? How will you learn about what you currently do not know that you do not know?

## Conclusion

We have reviewed many fairness, privacy, and other ethical considerations that affect the design, training, and deployment of real-world ML systems. These topics are all highly complex. This chapter should give you and your organization a sampling of key issues you should factor into designing and deploying ML systems. To progress further, many excellent learning materials and academic research papers are out there on all these topics for you to look for more information.[28](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn69)

In the meantime, this complexity shouldn’t discourage you! We recommend the following steps:

* Create basic institutional rules and safeguards in your workplace.
* Human-readable changes can be easier to understand and more transparent than computational techniques.
* Institutional changes encourage people to bring up ethical concerns at meetings. It’s better for fairness not to be off in a dark corner somewhere. Try raising your hand and mentioning a fairness concern you have. Start with a small one (data access, a recourse interface, statistical parity), and you can work your way up to more sophisticated targets over time. Over time, organizations can move even further—for example, by creating a _red team_ that intentionally and proactively tries to manipulate products or identify harmful outcomes.
* Ethical guidelines can also serve as a high-level but practical checklist for initiating new projects or approving the launch of completed products.
* Address fairness, privacy, and ethics concerns routinely at product inception.
* Most fairness, privacy, and other ethical problems in ML originate at the level of product conception and creation. Start asking relevant questions when contemplating whether a project is even a good idea at the outset.
* Are you using fair data that you obtained in a legal and ethical way? Guidance from [Gebru et al. (2021)](https://oreil.ly/6Vo2R) can provide a great framework for documenting your data and its appropriate use.
* Are appropriate working conditions in place to maintain the security and privacy of data?
* If you succeed at the goal you propose, will that be a good (or at least neutral) thing for the world at large?
* Create a virtuous cycle of Responsible AI practices.
* The more you learn and the longer you keep Responsible AI concerns in mind, the more your ML pipeline and ML offerings will reflect good fairness practices.
* If you can make a commitment to learning a little about Responsible AI each week—and implementing it slowly in your own work—at the end of a year you’ll note a remarkable degree of progress.

We wish you good luck as you begin practical steps to make the world a better place, starting with your ML products (including, sometimes, not using them).

[1](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn42-marker) Examples of designated human decision makers include judges as the formal decision makers in legal adjudication, or university professors as the formal decision makers in grading decisions. Examples of no clear decision makers include “flat” organizations, in which it can sometimes be unclear who holds final decision-making authority.

[2](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn43-marker) As a general matter, we take issue with this undue narrowing of the concept of fairness, and for the sake of clear communication, we address a host of other issues that we view as related to _fairness_ in [“Responsible AI”](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#responsible\_ai). In this section, we focus on the mainstream use of _fairness_ as exclusively related to bias.

[3](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn44-marker) [“Machine Bias”](https://oreil.ly/38he8) by Julia Angwin et al. is the seminal study, published by ProPublica, that identified this bias. This work, discussing the use of algorithmic risk scores in the US criminal justice system, is foundational in the understanding of the kind of harms that AI can create and in launching the Responsible AI movement. The article discusses the use of algorithmic risk scores in the criminal justice system in the US. The most common use of these biased risk scores is to determine whether defendants should be allowed free while awaiting trial in an overly burdened criminal justice system. The factor being predicted in this case should be whether the accused will show up for trial and whether they will commit crimes in the meantime, not generally at any point in the future. It is not clear that a single prediction about “will be accused of a crime in the future” can be useful in decisions about bail, sentencing, and parole. This article discusses the COMPAS algorithm, which is discussed in some detail later in this chapter.

[4](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn45-marker) This was one of several factors that led some governments, notably that of New York City, to propose regulation of AI when used in hiring. See [“New York City Proposes Regulating Algorithms Used in Hiring”](https://oreil.ly/xjN4P) by Tom Simonite.

[5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn46-marker) A larger debate arises about whether discrimination is “taste based” or “statistical.” Some argue that the decision to attend a women’s college might in some way be indicative of a personality type that is relevant to a hiring decision. However, such a hypothesis seems unlikely to apply. Those interested in learning more can look to research literature in both law and economics about the mechanisms of and motivations for discrimination, including disparate treatment.

[6](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn47-marker) We use scare quotes in this sentence because, of course, defining what or who is “the same” or “of equal merit” is itself an exercise of judgment and not an objective truth.

[7](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn48-marker) For an accessible demonstration of the conflict between calibration and group parity, we strongly recommend [“Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments”](https://arxiv.org/abs/1610.07524) by Alexandra Chouldechova.

[8](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn49-marker) See the [Chouldechova paper](https://arxiv.org/abs/1610.07524) referenced in the preceding footnote.

[9](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn50-marker) If we look for and interpret questions about algorithmic fairness broadly, other impossibility theorems are related to fairness concerns. For example, Arrow’s impossibility theorem demonstrates that three intuitive fairness criteria for a specific form of voting cannot all be met at the same time. These three criteria, stated quite roughly, are that (1) universally shared individual preferences between two options will necessarily translate into an election outcome reflecting that preference, (2) stable individual preferences will translate into stable election outcomes, and (3) no single voter will possess the power to determine the election outcome. Thus it is a common problem across disciplines and technological or organizational mechanisms that we cannot always have our fairness cake and eat it too. Many thanks to Niall Murphy for this example.

[10](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn51-marker) For example, individuals from a favored group who receive an improbably favorable outcome given their merit.

[11](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn52-marker) This can sometimes be understood as a way of correcting where calibration is lacking. For example, it is well documented that credit scores used in the US do not seem to mean the same thing across racial groups with respect to propensity to repay. So banks that rely exclusively on the score and do not adjust the thresholds could inadvertently label credit seekers with false-negative rates that are different by racial group. However, different thresholds for different groups necessarily raises other legal and ethical concerns. And so, the reader can see that fairness in the real world can be quite challenging indeed.

[12](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn53-marker) At the time this book is going to press (August 2022), Europe has put forth a wide range of groundbreaking legislation on a host of issues that relate to fairness in digital environments, including AI. Most notably, the [EU published a proposed AI regulatory framework in April 2021](https://oreil.ly/b5DZn) that would entail a risk-based set of regulations, with different legal requirements for different levels of risk from an AI use case. The proposed law would also ban certain high-risk use cases, including social credit scoring. Likewise, [China brought sweeping new AI regulations](https://oreil.ly/U5xVq) into force as of March 2022 to regulate a variety of common practices that disadvantage consumers, including price discrimination on the basis of personal information and content-aggregation algorithms.

[13](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn54-marker) For an early example of reidentification, see [“‘Anonymized’ Data Really Isn’t—And Here’s Why Not”](https://oreil.ly/7nFhr) by Nate Anderson, describing Latanya Sweeney’s work on the topic.

[14](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn55-marker) Querying a differentially private dataset many times can lead to violations of the differential privacy guarantees. This is because such guarantees are valid with only a given privacy budget, which limits the number of queries that can be made while ensuring differential privacy guarantees.

[15](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn56-marker) In theory, we could retrain a model entirely every time a bit of training data was marked for deletion. In practice, that would be wildly wasteful and impractical—and also possibly unnecessary. More work needs to be done in this area to understand how technical needs, sustainability concerns, and individual rights to privacy can successfully coexist.

[16](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn57-marker) One reason that differentially private modes do not achieve the highest levels of accuracy relates to the methods used to create differentially private models. One key technique is noise injection, which by definition reduces accuracy.

[17](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn58-marker) One recommended starting point is [TensorFlow Privacy](https://github.com/tensorflow/privacy), which includes training algorithms for differentially private models.

[18](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn59-marker) A separate question is when click prediction is a useful activity and when it raises troubling AI concerns. No doubt, predicting someone’s preferences or interests has beneficial uses, as is the case for many examples of click prediction. On the other hand, increasing research both on the mutability of human preferences generally and on the particular scenario of manipulation of behavior and preferences in online environments also points to the danger of confusing click prediction that represents preference _accommodation_ (giving people what they want) with preference _manipulation_ (making people want what you have). The latter obviously has lots of fairness concerns and is related to a growing literature on _dark patterns_, which are digital design patterns that tend to lead users of digital products to do things that are against their interests (but in the interests of the designers of those products).

[19](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn60-marker) Of course, the inclusion of demographic information may prove problematic, as some datasets may have a strong prediction relationship between seemingly anonymous information and PII, and this should be factored into the way data separation is conducted.

[20](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn61-marker) A good starting point for understanding the centrality of explainability to Responsible AI, but also the complexity of getting it right, is [“Explaining Explanations in AI”](https://oreil.ly/OOeoT) by Brent Mittelstadt et al.

[21](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn62-marker) [“Explainable Machine Learning for Public Policy: Use Cases, Gaps, and Research Directions”](https://arxiv.org/pdf/2010.14374.pdf) by Kasun Amarasinghe et al. provides a great and accessible example of considerations that go into what kind of explanation is likely to be useful.

[22](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn63-marker) IBM’s [AI Explainability 360 library](https://aix360.mybluemix.net/) is an easy-to-use open source library that includes a wide variety of cutting-edge research methods for model explanation. The toolkit provides an API as well as tutorials that provide a wide range of example use cases for applying methods from the library via the explainability API.

[23](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn64-marker) Consider as an extreme example Poland’s rollout of a classification algorithm to categorize unemployed job seekers into three categories. An initial algorithmic assessment was made, from which staff could, in theory, deviate. In practice, they deviated from the algorithmic label in only 0.58% of cases. See [“Profiling the Unemployed in Poland”](https://oreil.ly/eBvcv) by Jędrzej Niklas et al.

[24](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn65-marker) The full quotation from the bill is, “An algorithmic process is effective if the online platform employing or otherwise utilizing the algorithmic process has taken reasonable steps to ensure that the algorithmic process has the ability to produce its desired or intended result.”

[25](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn66-marker) It’s worth noting that defining _consent_ can be complicated. After all, no one appreciates all the pop-up consent notices that have proliferated after GDPR. _Consent_ here should be interpreted in a broad rather than narrow sense of the word.

[26](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn67-marker) It’s good to specify these in advance so you can’t “cheat” or otherwise rationalize poor performance after the fact. This doesn’t have to involve rigid adherence to those criteria, but it will keep you honest about what constitutes a good job even if you do reassess those criteria over time.

[27](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn68-marker) Model Cards, as previously mentioned, are one system to accomplish just this.

[28](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#ch01fn69-marker) As a shameless self-promotion, consider Aileen Nielsen’s book-length introduction to hands-on ML ethics: [_Practical Fairness_](https://oreil.ly/tsjGP), another O’Reilly title.
