# Chapter 2. Data Management Principles

In this book, we are rarely concerned with the algorithmic details of how models are constructed or how they’re structured. The most exciting algorithmic development of last year is the mundane executable of next year. Instead, we are overwhelmingly interested in two things: the data used to construct the models, and the processing pipeline that takes the data and transforms it into models.

Ultimately, ML systems are data processing pipelines, and their purpose is to extract usable and repeatable insights from data. There are some key differences between ML pipelines and conventional log processing or analysis pipelines, however. ML pipelines have some very different and specific constraints and fail in different ways. Their success is hard to measure, and many failures are difficult to detect. (We cover these topics at length in [Chapter 9](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#monitoring\_and\_observability\_for\_models).) Fundamentally, they consume data, and output a processed representation of that data (though vastly different forms of both). As such, ML systems depend thoroughly and completely on the structure, performance, accuracy, and reliability of their underlying data systems. This is the most useful way to think about ML systems from the reliability point of view.

In this chapter, we will start with a deep dive on data itself:

* Where data comes from
* How to interpret data
* Data quality
* Updating data sources (which we use and how we use them)
* Assembling data into an appropriate form for use

We’ll cover the production requirements of data and show that, just like models, _data in production has a lifecycle_:

* Ingestion
* Cleaning and data consistency
* Enrichment and extension
* Storage and replication
* Use in training
* Deletion

The stability of data and metadata definitions as well as version control of those definitions are crucial, and we’ll explain how to achieve them.[1](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn12) We’ll also cover data access constraints, privacy, and auditability concerns and show some approaches to ensuring _data provenance_ (where the data comes from) and _data lineage_ (who has been responsible for it since we got it). At the end of this chapter, we expect you to have a complete but superficial understanding of the primary issues involved in making the data processing chain reliable and manageable.

## Data as Liability

Writing about ML almost universally suggests that data is an important _asset_ in ML systems. This perspective is sound: it’s certainly impossible to have an ML system without data. As shown in [Figure 2-1](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#trade\_offs\_of\_data\_sizecomma\_model\_err), it is often true that a simple (or even simplistic) ML system with more (and higher-quality) training data can outperform a more sophisticated system with less, or less representative, data.[2](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn13)

Organizations continue to scramble to collect as much data as possible, hoping to find ways to turn that data into value. Indeed, many organizations have made this into a profoundly successful business model. Think of Netflix, whose ability to recommend high-quality shows and movies to customers was an early differentiator. Netflix also reportedly used this data, once it got into the content production side of the business, to figure out what shows to make for which audiences, based on a detailed understanding of what people want to watch.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098106218/files/assets/reml_0201.png" alt="Illustrative trade-offs of data size, model error rates, and risk of problems or issues associated with the data" height="355" width="600"><figcaption></figcaption></figure>

**Figure 2-1. Illustrative trade-offs of data size, model error rates, and risk of problems or issues associated with the data**

Of course, just like anything could be an asset, under the right (wrong) circumstances, it can also be a liability. In the case of data, the most important thing to say is that the acquisition, collection, and curation of data can expose areas of unexpected nuance and complexity in the data. Not accounting for these can lead to potential harm for us and for our users. All of these methods must be scoped appropriately for the type of data—medical records probably require different treatment from job history, for example. Of course, the best ways to curate data are high cost, so there’s no free lunch here.

The intent of this short section is not to be the authoritative work on data collection, storage, reporting, and deletion practices. That is well beyond the scope of this section and even of this book. The intent here is to enumerate enough of the complexity to dissuade any readers from simply thinking “more data == better” or thinking “this stuff is easy.” Let’s go through the lifecycle of data and see where some of the challenges come from.

First, the data must be collected in compliance with applicable laws, which might be based on where our organization is located, where the data originates, and on organizational policies. We’ll definitely need to think this through (and talk to lawyers for all of the jurisdictions we might be operating in). There are significant restrictions on what counts as data about people, how to get permission to store the data, how to store and retrieve the permission that was granted, whether we need to provide access to the data to the people who provided it, and under what circumstances. These restrictions may come from laws, industry practices, insurance regulations, corporate governance policies, or any range of other sources. Examples of common restrictions in some jurisdictions include prohibition against collecting personally identifiable information (PII) about an individual without their explicit written consent, along with the requirement to delete that data upon request by the data subject. Whether and how to collect data is not a technical question. It is a question of policy and governance. (Parts of this topic are covered much more thoroughly in [Chapter 6](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#fairnesscomma\_privacycomma\_and\_ethical).)

If we are allowed to collect and store the data, it must be secured from external access. Very few good things happen to organizations as a result of revealing their users’ private data. Moreover, access must be restricted, even to employees. Employees should not be able to view or change private user data without restriction and without detailed logging of that.

Another approach to reducing data access and reducing the auditing surface is to anonymize the data. One relatively simple and valuable option is to use _pseudonymization_. Here, private identifiers are replaced with others in a reversible fashion, and reversing the pseudonymization requires access to an additional data or system. This protects the data from casual inspection by engineers working on the pipeline but permits discovery if we find that we need to reverse the anonymization. Pseudonymization also hopefully preserves the properties of data that are relevant to our model. In other words, if it is important that a data field be similar in a certain way under particular circumstances (think of postal codes, for example, which are prefix-identical when they are in the same town or the same part of the same city), then our pseudonymization might need to preserve that. While this level of protection has value against casual inspection, it is important to treat pseudonymized data as potentially just as risky as completely nonanonymized data. History is full of cases of this data being used to expose the very real and private information of users (see the following sidebar).

**EXAMPLE APPLICATIONS FOR DIFFERENT LEVELS OF ANONYMIZATION**

Different controls are required for data, depending on the sensitivity of the data that is being accessed:

Raw dataThe data contains no PII _or_ we have extremely strict access controls in place and the PII itself is critical (e.g., medical data).Pseudonymized dataSome strict access controls are in place. Occasional, controlled manual inspection is required, but the PII data itself is not relevant to the inspection (e.g., credit card data with the prefix blanked or transformed, leaving only the final four digits: XXXX-XX-1234—still a lot of data, but some reasonable protection from casual inspection).Anonymized dataAll other cases for which we collect PII but do not require it for troubleshooting or model performance.

A better approach is permanently removing any direct connection between private data about a person and the data that we use to train on. If we are able to permanently remove any connection between the private data and the person, we substantially reduce the risk of the data and increase the flexibility we have to handle it. This is, of course, harder than it seems.[3](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn14) Doing this in a way that’s not trivially reversible but still valuable can be difficult. Although many good techniques exist, one common basic idea is to combine collections of data to ensure that no reported piece of data is tied to a unique identifier for any fewer than a certain number of real people. This is the approach taken by many serious population research organizations, including, notably, the United States Census Bureau. There are, however, many subtleties in getting this right. Correct anonymization is a topic that is mostly beyond the scope of this book, although we’ll refer to it.

**DON’T ANONYMIZE YOURSELF**

Several very high-profile failures over the past several years should have made the difficulties of anonymization clear. The work, in particular, of [Dr. Latanya Sweeney](http://latanyasweeney.org/) in Harvard’s government department is worth reading. Sweeney demonstrated with ease that she could identify the health records of William Weld, then governor of Massachusetts, although they were “anonymized,” simply by knowing his gender, age, and zip code (which were freely available from voter records). Sweeney further demonstrated that 87% of all people in the US could be uniquely identified via only three pieces of information: gender, age, and zip code; see [“Anonymized Data Really Isn’t—and Here’s Why Not”](https://oreil.ly/V39HT) by Nate Anderson for more details.

Similar work has shown that a browser user agent string combined with zip code or several other kinds of information uniquely identifies humans. The moral of the story is that anonymization is hard, and you will almost certainly have to develop expertise in it yourself, or outsource it to people or platforms that do; it is a significant specialization in and of itself. The topic of correct anonymization is closely aligned with other mathematically complex topics like cryptography that are properly a separate specialization.

Finally, we will ultimately need to be able to delete data. We might do this at the request of individual users, local laws, regulations like the European Union’s General Data Protection Regulation, or [GDPR](https://gdpr-info.eu/), or in other cases where we no longer have permission to store the data. It turns out that deleting data and having it _actually_ be deleted is surprisingly hard.

This has been true since at least the early MS-DOS days, when deleting a file just removed the reference to it and not the actual data itself, meaning you could reconstruct the file with sufficient determination and luck. Everything about today’s computing environment makes deletion even harder than it was in those days, from having to track down multiple copies of the data to metadata management. In most distributed storage systems, data is divided into many pieces and stored across a collection of physical machines. Depending on the implementation, it may be practically impossible to determine every durable storage device (hard disk drive or solid state drive) that might have the data written on it.

It’s important to be certain that people want their data deleted without putting up arbitrary barriers. One way to balance this is to impose a short delay before really deleting the data. A user might be granted a few hours or even days after requesting data be deleted to cancel that request. But at some point, once we’ve confirmed the request is intended and legitimate, we will need to track down every copy of the data and eliminate it. Depending on how it is stored, data structures, indices, and backups may be reconstructed to make accessing nearby data as efficient and reliable as it had been.

As with most things, the task of deleting data is made better by putting explicit thought into it. If your system hasn’t had that much thought put in, you can use a couple of workarounds. Here are two common optimizations:

Periodically rewrite the dataWhen there is a process that regenerates the data, we can take advantage of the “don’t delete data immediately” recommendation noted previously to simply schedule the “deleted” data to not be included the next time the data is rewritten. This assumes that the period of data regeneration matches the expected and acceptable delay in deletion. This also assumes that the rewrite of the data is effective at actually deleting the data, which may well not be the case at all.Encrypt all of the data and throw away some keysA system designed in this way has some significant advantages. In particular, it protects private data “at rest” (written to persistent storage). Deleting data is also trivial: if we lose the key to a user’s data, we can no longer read that data. The downsides are mostly avoidable but worth considering seriously: anyone employing this strategy will need very, very reliable key-handling systems because if the keys are lost, _all_ of the data is lost. This can also make it difficult to reliably delete a single key from every backup of the key system.

## The Data Sensitivity of ML Pipelines

The primary difference between ML pipelines and most data processing pipelines is that ML pipelines are unusually sensitive to their input data compared to most other data processing pipelines. All data processing pipelines are, in some sense, subject to the correctness and volume of their input data, but ML pipelines are furthermore sensitive to subtle changes in _distribution_ of the data. A pipeline can easily go from mostly right to significantly wrong simply by omitting a small fraction of the data, provided that small fraction is not random, or is somehow not evenly sampled in the range of characteristics our model is sensitive to.

An easy thought experiment here is to consider a real-world system like _yarnit.ai_ that somehow loses all of the data from a particular country, region, or language. For example, if we drop all of the data from December 31 of a given year, we lose the ability to detect New Year’s Eve shopping trends, which may be substantially different from the surrounding days in December and January. In many of these cases, losing a small amount of data that turns out to be systematically biased results in significant confusion in the understanding and predictions of our models.

As a result of this sensitivity, the ability to aggregate, process, and monitor _data_, rather than only the live systems, is critical to successfully managing ML data pipelines. We discuss monitoring data in some depth in [Chapter 9](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#monitoring\_and\_observability\_for\_models), but here is a preview. A key insight to monitoring data is the slicing, or division, of the data along various axes (determining _which_ axes are the best to slice the data is an important activity in exploratory data analysis, or EDA, and is beyond the scope of this book). In a system that is trying to track real-time activities, we might divide data in buckets of data age: recent, one to two hours old, three to six hours old, etc. We might track which buckets we are currently processing data from so that we can understand how far behind we have gotten. But we can, and should, track various other histograms that are relevant to our application. The ability to detect when _all_ or _almost all_ of a subset of the data is gone will matter enormously.

For example, in our shopping site _yarnit.ai_, we may train on searches to try to predict the best results for any given search (where “best” is “most likely to be purchased”—we are in the selling business, after all!). We operate our site in multiple languages in multiple markets. Let’s say that a widespread payments outage affects only our Spanish language site, resulting in a drastically lower number of completed orders from people searching in Spanish. A model whose job is to recommend products for customers to buy will learn that Spanish-language results are significantly less likely to result in purchases than results in other languages. The model will show fewer results in Spanish and may even start showing results in other languages to Spanish-speaking users. Of course, the model will not be able to “know” the reasons for this change in behavior.

This might result in a small decline in total purchases if our site is predominantly a North American or European site, but a massive decline in the total number of searches and purchases in Spanish. If we train on this data, our model will probably have terrible results for Spanish-language searches. It might learn that Spanish-language queries never buy anything (since this will essentially be true). The model might start exhibiting exploratory behavior—if all of the Spanish language results are equally terrible, then any result _might_ be good, so the model will try to find anything that Spanish-language searchers might actually buy. This will result in terrible search results and lower sales once our Spanish site’s payment outage is over. This is a pretty bad outcome. (For a more complete treatment of a similar problem, see [Chapter 11](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch11.html#incident\_response).)

The change in query volume might not be easily detectable at the gross query volume level either, as the total queries in Spanish might be small compared to those in other languages in total. We’ll talk about ways to monitor training pipelines and detect these kinds of shifts in distribution in [Chapter 9](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#monitoring\_and\_observability\_for\_models). This example is given just to motivate the thought that ML pipelines really are somewhat more difficult to operate reliably because of these kinds of subtle failure modes.

With these constraints in mind, let’s review the lifecycle of data in our system. We’ll have to think about the data as under our care from the moment of its creation until we delete it.

## Phases of Data

Most teams rely on the platforms they use, including their data storage and processing platforms, to provide most of what they need. YarnIt is not a large organization, but we’ll still involve staff responsible for business management, data engineering, and operations to help us understand and meet the requirements here. We are lucky enough to have SREs who will address reliability concerns related to the storage and processing of data.

The data management stage is fundamentally about transforming the data we have into a format and storage organization suitable for using it for later stages of the process. During this process, we will also probably apply a range of data transformations that are model specific (or at least model-domain specific) in order to prepare the data for training. Our next operations on the data will be to train ML models, anonymize certain sensitive items of data, and delete data when we no longer need it or are asked to do so. To prepare for these operations on the data, we will continue to take input from the business leaders to answer questions about our primary use cases for the data, along with possible areas for future exploration.

As with most of the sections of this book, deep familiarity with ML is not required or sometimes even desirable in order to design and run reliable ML systems. However, a basic understanding of model training does directly inform what we do with data as we get it ready. Data management in modern ML environments consists of multiple phases before feeding the data into model-training pipelines, as illustrated in [Figure 2-2](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ml\_data\_management\_phases):

* Creation
* Ingestion
* Processing (which includes validation, cleaning, and enrichment)
* Post-processing (which includes data management, storage, and analysis)

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098106218/files/assets/reml_0202.png" alt="ML data management phases" height="300" width="600"><figcaption></figcaption></figure>

**Figure 2-2. ML data management phases**

### Creation

It may seem either odd or obvious to state, but ML training data comes from _somewhere_. Perhaps your dataset comes from elsewhere, such as a colleague in another department or from an academic project, and was created there. But datasets are all created at some point via some process. Here we are referring to data creation as the process of generating or capturing the data in _some_ data storage system but not _our_ data storage system. Examples here include logs from serving systems, large collections of images captured from an event, diagnostic data from medical procedures, and so on. Implicit in this process is that we will want to design new systems and adapt existing systems to generate more data than we might otherwise, so that our ML systems have something to work with.

Some datasets work well when they are static (or at least don’t change quickly), and others are useful only when frequently updated. A photo-recognition dataset, for example, may be usable for many months, as long as it is suitably representative of the types of photos we would like to recognize with our model. On the other hand, if the outside photos represent only winter environments from a temperate climate, the distribution of the photo set will be significantly different from the expected set of images we need to recognize. It will therefore not be useful as the environment warms during spring in those places. Likewise, if we’re trying to recognize fraud in transactions at _yarnit.ai_ automatically, we will want to be training our model continuously on recent transactions along with information about whether those transactions were fraudulent. Otherwise, someone might come up with a neat idea of how to steal all of our knitting supplies that is hard to detect, and we might never teach our model how to detect it.

The kinds of data we collect and the data artifacts we create can be unstructured, semi-structured, or very well structured ([Figure 2-3](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ml\_training\_data\_categories)).

_Structured data_ is quantitative with a predefined data model/schema, highly organized, and stored in tabular formats like spreadsheets or relational databases. Names, addresses, geolocation, dates, and payment information are all common examples of structured data. Since it is well formatted, structured data can be easily processed by relatively simple code using obvious heuristics.

On the other hand, _unstructured data_ is qualitative without a standard data model/schema, so it cannot be processed and analyzed using conventional data methods and tools. Examples of unstructured data include email body text, product descriptions, web text, and video and audio files. Semi-structured data doesn’t have a specific data model/schema but includes tags and semantic markers and so is a type of structured data that lies between structured and unstructured data. Examples of semi-structured data include email, which can be searched by Sender, Receiver, Inbox, Sent, Drafts, etc., and social media content that may be categorized as Public, Private, and Friends, as well as by user-maintained labels such as hashtags. The character of the internal structure of the data will have significant implications for the way we process, store, and use it.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098106218/files/assets/reml_0203.png" alt="ML training data categories" height="193" width="600"><figcaption></figcaption></figure>

**Figure 2-3. ML training data categories**

Although bias in models comes from the structure of the model as well as the data, the circumstances of data creation have profound implications for correctness, fairness, and ethics. Though we treat this in considerably more detail in Chapters [5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch05.html#evaluating\_model\_validity\_and\_quality) and [6](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#fairnesscomma\_privacycomma\_and\_ethical), the primary recommendation we can make here is that you have some kind of process to establish whether your model is biased. We can do this in numerous ways and the simplest is probably the [Model Cards](https://oreil.ly/h7E8h) approach, but having any process that does this and having it be organizationally accepted is much better than not having such a process.[4](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn15) It’s definitely the first thing to do as we begin to address ethics and fairness considerations in our ML system. You could relatively easily combine the effort to detect bias into a data provenance or data lifecycle meeting or tracking process, for example. But every organization doing ML should establish some kind of process, and review it as part of continuous improvement.

Recall, though, that bias comes from many sources and shows up at many stages through the process. There is no guaranteed way to ensure data fairness. One useful precursor to success here is to have an inclusive company culture full of people from all kinds of backgrounds with differing and creative points of view. There’s good evidence that people with very different backgrounds and perspectives, while working in an environment of trust and respect, produce much better and more useful ideas than teams that are all similar. This can be part of a robust defense against the kinds of bias that slips through the checks described previously. It will not, however, prevent all bad outcomes without process and tooling, just as no human effort prevents all bad outcomes without systems help.

One final note on dataset creation, or rather, dataset augmentation: if we have a small amount of training data but not enough to train a high-quality model on, we may need to augment that data. Tools are available that can do so. For example, [Snorkel](https://www.snorkel.org/features) provides a programmatic interface for taking a small number of data points and permuting them into a larger number of more varied data points, essentially making up imaginary but statistically valid training data. This is one good place to start, as it allows us to easily expand a small dataset into a large one. Although it might appear that these programmatically created datasets are somehow less valuable or less useful, there is good evidence that this approach can yield extremely good results at low cost, although it does need to be used with caution.[5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn16)

### Ingestion

The data needs to be received into the system and written to storage for further processing. At this stage, a filtering and selection step necessarily occurs. Not all data created may be ingested or collected, because we don’t want or need to.

We may filter data by type at this stage (data fields or elements that we do not believe will be useful for our model). We may also simply sample at this stage if we have so much data we do not believe we can afford to process all of it, as ML training and other data processing is often extremely computationally expensive. Sampling data can be an effective way to save money on intermediate processing and training costs, but it is important to measure the quality cost of sampling and compare that to the savings. Data should also be sampled proportional to the volume/rate per time period or per other slice in the data we care about. This will avoid missing detail during bursty periods. However, sampling is likely to occasionally lose some detail for some events. This is unavoidable.

In general, ML training systems perform better with more data. Pedants will immediately think of many exceptions, but this is a useful starting point. Therefore, any reduction in data may well impact quality at the same time as reducing costs.

Depending on the volume of data and the complexity of our service, the ingestion phase may be as simple as “dump some files in that directory over there” or as sophisticated as a remote procedure call (RPC) endpoint that receives specifically formatted files and confirms a reference to the data bundle that was received so that its progress through the system can be tracked. In most cases, we will want to provide data ingestion via at least a simple API because this provides an obvious place to acknowledge receipt/storage of the data, log the ingestion, and apply any governance policies about the data.

Reliability concerns during the ingestion phase typically focus on correctness and throughput. _Correctness_ is the general property that the data is properly read and written in the correct place without being unnecessarily skipped or misplaced. While the idea of misplacing data sounds amusing, it absolutely happens, and it’s easy to see how it could. A date- or time-oriented bucketing system in storage combined with an off-by-one error in the ingestion process could end up with every day’s data stored in the previous day’s directory. Monitoring the existence and condition of data before and during ingestion is the most difficult part of the data pipeline.

### Processing

Once we have successfully loaded (or ingested) the data into a reasonable feature storage system, most data scientists or modelers will go through a set of common operations to make the data ready for training. These operations—validation, cleaning and ensuring data consistency, and enriching and extending—are detailed next.

#### Validation

No matter how efficient and powerful our ML models are, they can never do what we want them to do with bad data. In production, a common reason for errors in the data is bugs in the code that is collecting the data in the first place. Data ingested from external sources might have a lot of errors even though a well-defined schema exists for each source (for example, having a float value for an integer field). So, it is extremely important to validate the incoming data, especially when there is a well-defined schema and/or an ability to compare against the last-known valid feed.

Validation is performed against a common definition of the field—i.e., is it what we expect it to be? To perform this validation, we need to both store and be able to reference those standard definitions. Using a comprehensive metadata system to manage the consistency and track the definition of fields is critical to maintaining an accurate representation of the data. This is covered in much more detail in [Chapter 4](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch04.html#feature\_and\_training\_data).

#### Cleaning and ensuring data consistency

Even with a decent validation framework in place, most data is still messy. It may have missing fields, duplicates, misclassifications, or even encoding errors. The more data we have, the more likely that cleaning and data consistency will be its own stage of processing.

This might seem frustrating and unnecessary: building an entire system simply to check the data sounds a bit overblown to many people doing so for the first time. The reality is that our ML pipeline will assuredly have code whose job is to clean the data. We can either put this code in a single place, where it can be reviewed and improved, or put aspects of it throughout the training pipeline. That second strategy makes for extremely fragile pipelines as the assumptions about data correctness grow but our ability to ensure that they are met does not. Moreover, as we improve some of the code for validating and correcting data, we might neglect to implement those improvements in all of the many places where we are performing that work. Or worse, we can be counterproductive. For example, we can “correct” the same data multiple times in ways that eliminate the original information in the data. We can also have potential race conditions whereby different parts of the process are cleaning or making consistent the same data differently.

Another set of data consistency tasks during this portion is the normalization of data. _Normalization_ generally refers to a set of techniques used to transform the input data into a similar scale, which is useful for methods like deep learning that rely on gradient descent or similar numerical optimization methods for training. Some standard techniques here, depicted in [Figure 2-4](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#normalization\_techniques\_as\_presented), include the following:

Scaling to a rangeMapping all of the X values of the data to a fixed range, often 0 to 1, but sometimes (for something like height or age) other values that represent the common min and max values.ClippingCutting off the maximum values of the data. This is useful when the dataset has a small number of extreme outliers.Log scaling_x_’ = log(_x_). This is useful when the data has a power law distribution, with a small number of very large values and a large number of very small values.Z-score normalizationMaps the variable to the number of standard deviations from the mean.

It is important to note that each of these techniques can be dangerous if any of the range or distribution or means are calculated on a set of test data with different properties than the dataset where it is later applied.

A final related and common technique is that of putting the data into _buckets_: we map a range of data into a much smaller set of groups that represent the same range. For example, we could measure age in years, but when training we could bucket on decades so that everyone who is 30 to 39 gets put into the “30s” bucket. Bucketing can be the source of many difficult-to-detect errors. Imagine, for example, that one system buckets age on decade boundaries and another on five-year boundaries. When we bucket the data, we have to give serious consideration to preserving the existing data and writing out a new, correctly formatted field for each record. If (when?) we change the bucketing strategy, we’ll be glad we did; otherwise, it will be impossible to switch.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098106218/files/assets/reml_0204.png" alt="Normalization techniques as presented in Google Developers Data Prep course" height="759" width="600"><figcaption></figcaption></figure>

**Figure 2-4. Normalization techniques as presented in Google Developers** [**Data Prep course**](https://oreil.ly/0cgBm)

#### Enriching and extending

In this stage, we combine our data with data from other sources. The most common and fundamental way to extend the data is through _labeling_. This process identifies a given event or record by bringing in confirmation from an outside source of data (sometimes a human). Labeled data is the key driver of all supervised ML and is often one of the most challenging and expensive parts of the whole ML process. Without a sufficient volume of high-quality labeled data, supervised learning won’t work. (Labeling and labeling systems are covered extensively in [Chapter 4](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch04.html#feature\_and\_training\_data).)

But labeling is only one way to extend data. We might use many external data sources to extend our training data. Let’s say we believe for some reason that the temperature at a person’s location will predict what they will buy.[6](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn17) We can take our logs of searches on the _yarnit.ai_ site and add the temperature at the approximate geolocation of the user at the time they were visiting the web page. We could do this by finding or creating a temperature history service or dataset. This will allow us to train a model based on `source temperature` as a feature and see what kind of predictions we can make with it. This is probably a terrible idea, but it is not completely impractical.

### Storage

Finally, we need to store the data somewhere. How and where we store the data is mostly driven by how we tend to use it, which is really a set of questions about training and serving systems. We go into this considerably more in [Chapter 4](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch04.html#feature\_and\_training\_data), but there are two predominant concerns here: efficiency of storage and metadata.

The efficiency of a storage system is driven by access patterns that are influenced by the model structure, team structure, and training process. To make sensible choices about our storage system, here are some basic questions we need to answer:

* Are we training models once over this data or many times?
* Will each model read _all_ of the data or only parts of it? If only some of the data will be read, is the subset being read selected by the type of data (some fields and not others) or by randomly sampling the data (30% of all records)?
* In particular, do related teams read slightly different subsets of the fields in the data?
* Do we need to read the data in any particular order?

On the subject of reuse of data, it turns out that almost all data is read multiple times and the storage system should be built for that, even if model owners assert that they will train only one model on the data once. Why? Model development is inherently an iterative process. An ML engineer makes a model (reading the data necessary to do so), measures how well the model performs at its designed task, and then deploys it. Then they get another idea: an idea of how to improve the model in some way. Before you know it, they’re back rereading the same data to try out their new idea. Every system we build, from data to training all the way to serving, should be built with the assumption that model developers will semi-continuously retrain the same models in order to improve them. Indeed, as they do so, they might read different subsets of the data each time.

Given this, a column-oriented storage scheme with one column per feature is a common design architecture, especially for models training on structured data.[7](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn18) Most readers will be familiar with row-oriented storage, in which every fetch of data from the database retrieves all of the fields of a matching row. This is an appropriate architecture for collections of applications that all use most or all of the data—in other words, a collection of very similar applications. Column-oriented data facilitates the retrieval of only a subset of fields. This is much more useful for a collection of applications (ML training pipelines in this case) that each use a given subset of the data. In other words, column-oriented data storage allows different models that efficiently read different subsets of features and do so without reading in the whole row of data every time. The more data we collect in one place, the more likely it is that we will have different models using very different subsets of that data.

This approach is way too complicated for some training systems, however. As an example, if we’re training on a lot of images that aren’t preprocessed, we really don’t need to have a column-oriented storage system—we can just read image files from a directory or a bucket. However, say we are reading something more structured, such as transaction data logs with fields like timestamp, source, referrer, amount, item, shipping method, and payment mechanism. Then, assuming that some models will use some of those features and others will use others will motivate us to use a column-oriented structure.

Metadata helps humans interact with the storage. When multiple people work on building models on the same data (or when the same person works on this over time), metadata about the stored features provides huge value. It is the roadmap to understanding how the last model was put together and how we might want to put together the model. Metadata for storage systems is one of the more commonly undervalued parts of an ML system.

This section should clarify that our data management system is primarily motivated by two factors:

The business purpose to which we intend to put the dataWhat problem are we trying to solve? What is the value of that problem to our organization or our customers?The model structure and strategyWhat models do we plan to build? How are they put together? How often are they refreshed? How many of them are there? How similar to one another are they?

Every choice we make about our data management system is constrained by, and constrains, these two factors. And if data management is about how and why we write data, ML training pipelines are about how and why we read it.

### Management

Typically, data storage systems implement credential-based access controls to restrict unauthorized users from accessing the data. Such simple techniques can serve only a basic ML implementation. In more advanced scenarios, especially when the data contains confidential information, we need to have more granular data access management methodologies in place. For example, we might want to allow only model developers to access the features that they directly work on or restrict their access to a subset of the data in some other way (perhaps only recent data). Alternatively, we might anonymize or pseudononymize the data either at rest or when it is being accessed. Finally, we might allow production engineers to access all of the data, but only after demonstrating that they need to during an incident and having their access carefully logged and monitored by a separate team. (Some interesting aspects of this are discussed in [Chapter 11](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch11.html#incident\_response).)

SREs can configure data access restrictions on the storage system in production to allow data scientists to read data securely via authorized networks like virtual private networks (VPNs), implement audit logging to track which users and training jobs are accessing what data, generate reports, and monitor usage patterns. Business owners and/or product managers can define user permissions based on the use cases. We may need to generate and use different kinds of metadata for these dimensions of heterogeneity to maximize our ability to access and modify the data for subsequent phases.

### Analysis and Visualization

_Data analysis and visualization_ is the process of transforming large amounts of data into an easy-to-navigate representation using statistical and/or graphical techniques and tools.[8](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn19) It is an essential task of ML architectures and knowledge-discovery techniques to make data less confusing and more accessible. It is not enough just to show a pie chart or bar graph. We need to provide the human reader an explanation of what each record in the dataset means, how it is linked with the records in other datasets, and whether it is clean and safe to use for training models. It is practically impossible for data scientists to look at large datasets without well-defined and high-performant data visualization tools and processes.

## Data Reliability

Because our data processing system needs to work, the data must have several characteristics as it traverses the system. Articulating these characteristics can be controversial. To some people, they seem obvious, and to others, they seem impossible to really guarantee. Both of these perspectives miss the point.

The intent of articulating a set of invariants that should always be true about our data is that it permits us all to notice when they are not true, or when a system cannot properly guarantee that they will be true. This permits us to take action to do better in the future. Note that the topic of reliability, even of data management systems, is extremely broad and cannot be covered completely here; for much more detail, see [_Site Reliability Engineering: How Google Runs Production Systems_](https://oreil.ly/ZzIkN), edited by Betsy Beyer et al. (O’Reilly, 2016).

This section covers just the basics of making sure that the data is not lost (durability), is the same for all copies (consistency), and is tracked carefully as it changes over time (version control). We also cover how to think about how fast the data can be read (performance) and how often it’s not ready to be read (availability). A quick overview of these concepts should prepare us to focus on the right areas.

### Durability

When spelling out the requirements for storage systems, durability is the most often overlooked because it is assumed. _Durability_ is the property of the storage system having your data, and having not lost, deleted, overwritten, or corrupted it. We definitely want that property with a very high probability.

Durability is normally expressed as an annual percentage of bytes or blocks that are not lost irretrievably. Common values for good storage systems here are 11 or 12 nines, which can also be expressed as “99.999999999% or more of bytes stored are not lost.” While this may be the offering of the raw underlying storage system, our guarantees might be quite a bit more modest because we’re writing software that interacts with the storage system.

One important note is that some systems have extremely durable data, in the sense that nothing is lost, but do have failure modes leaving some data inaccessible for extremely long periods of time. This might include cases where the data needs to be recovered from another slower storage system (say, a tape drive) or copied from off-site over a slow network connection. If this is raw data and is important to the model, you may have to recover it. But for data that is somehow derived from existing raw data, reliability engineers will want to consider whether it might be easier to simply re-create the data rather than restore it.

For an ML storage system with many data transformations, we need to be careful about how those transformations are written and monitored. We should log data transformations and, if we can afford to, store copies of the data pre- and post-transformation. The hardest place to keep track of the data is on ingestion, when the data goes from an unmanaged to a managed state. Since we recommend using an API for ingestion, this provides a clear place to ensure that the data is stored, to log the transaction, and to acknowledge the receipt of the data. If the data is not cleanly and durably received, the sending system can, of course, retry the send operation while the data is still available.

At each stage of data transformation, if we can afford it, we should store pre- and post-transformation copies of the data. We should monitor throughput of the transformations as well and expected data size changes. For example, if we sample the data by 30%, the post-transformation data should obviously be 30% the size of the pre-transformation data, unless an error occurs. On the other hand, if we’re transforming a float into an integer by bucketing, depending on the data representation, we could expect the resulting data size to remain unchanged. If it’s much bigger or much smaller, we almost certainly have a problem.

### Consistency

We may want to guarantee that, as we access data from multiple computers, the data is the same with every read; this is the property of _consistency_. ML systems of any scale are usually distributed. Most of the processing that we are doing is fundamentally parallelizable, and assuming that we’ll be using clusters of machines from the beginning is generally worthwhile. This means that the storage system will be available via a networking protocol from other computers, and introduces challenges for reliability. Significantly, it introduces the fact that different versions of the same data might be available at the same time. It is difficult to guarantee that data is replicated, available, and consistent everywhere.

Whether the model-training system cares about consistency is actually a property of the model and the data. Not all training systems are sensitive to inconsistency in the data. For that matter, not all _data_ is sensitive to inconsistencies. One way to think about this is to consider how dense or sparse the data is. Data is _sparse_ when the information that each piece represents is rare. Data is _dense_ when the information that each piece of data represents is common. Data is sparse when the dataset has a lot of zero values. So if _yarnit.ai_ has 10 popular yarns that represent almost all of what we sell, the data for any given purchase of one of those yarns is dense—it’s unlikely to teach us very much that’s new. If a single purchase of a popular yarn is readable in one copy of our storage system but not another, the model will be essentially unaffected. On the other hand, if 90% of our purchases are for different yarns, every single purchase is important. If one part of our training system sees a purchase of a particular yarn and another does not, we might produce an incoherent model with respect to that particular yarn, or yarns that our model represents as similar to that yarn. Under some circumstances, consistency is difficult to guarantee, but often if we can wait somewhat longer for the data to arrive and sync, we can easily guarantee this property.

We can eliminate consistency concerns in the data layer in two straightforward ways. The first is to build models that are resilient to inconsistent data. Just as with other data processing systems, ML systems offer trade-offs. If we can tolerate inconsistent data, especially when the data is recently written, we might be able to train our models significantly faster and operate our storage system more cheaply. The cost, in this case, is flexibility and guarantees. If we go down this path, we limit ourselves to indefinitely operating the storage system with these guarantees and we can train only models that are satisfied with this property. That’s one choice.

The second choice is to operate a training system that provides consistency guarantees. The most common way to do that for a replicated storage system is for the system itself to provide information about what data is completely and consistently replicated. Systems that read the data can consume this field and choose to train only on the data that is fully replicated. This is often more complicated for the storage system since we need to provide an API to replication status. It may also be significantly more expensive or slower. If we want to use the data quickly after ingestion and transformation, we may need to have lots of resources provisioned for networking (to copy the data) and storage I/O capacity (to write the copies).

Thinking through consistency requirements is a strategic decision. It has long-term implications for balancing costs and capabilities and should be made with both ML engineers and organizational decision makers involved.

### Version Control

ML dataset versioning is, in many ways, similar to traditional data and/or source code versioning used to bookmark the state of the data so that we can apply a specific version of the dataset for future experiments. _Versioning_ becomes important when new data is available for retraining and when we’re planning to implement different data preparation or feature engineering techniques. In production environments, ML experts deal with a large volume of datasets, files, and metrics to carry out day-to-day operations. The varying versions of these artifacts need to be tracked and managed as experiments are performed on them in multiple iterations. Version control is a great practice for managing numerous datasets, ML models, and files in addition to keeping a record of multiple iterations—i.e., when, why, and what was altered.[9](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn20)

### Performance

The storage system needs fast-enough write throughput to rapidly ingest the data and not slow transformations. The system needs fast-enough read bandwidth to enable us to rapidly train models using the access patterns that fit our modeling behavior. It is worth noting that the cost of slow read performance can be quite significant for the simple reason that ML training often uses relatively expensive compute resources (GPUs or high-end CPUs). When these processors are stalled waiting on data to train on, they are simply burning cycles and time with no useful work done. Many organizations believe they cannot afford to invest in their storage system, when they actually cannot afford not to.

### Availability

The data we write needs to be there when we read it. _Availability_ is, in some ways, the product of durability, consistency, and performance. If the data is in our storage system and is consistently replicated, and we are able to read it with reasonable performance, then the data will count as available.

## Data Integrity

Data that is valuable should be treated as such. This means respecting provenance, security, and integrity.[10](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn21) Our data management system will need to be designed for these properties from the beginning to be able to make appropriate guarantees about the kind of access controls and other data integrity we can offer.

Data integrity has three other big themes in addition to security and integrity: privacy, policy compliance, and fairness. It is worth taking a moment to consider these topics from a general point of view. We need to ensure that we understand the requirements presented by these areas so that we can ensure that the storage system and API that we build offer the kinds of guarantees we require.

### Security

Valuable ML data often begins its life as private data. Some organizations choose to build processes to simply exclude all PII from their datastore. This is a good idea for several reasons. It simplifies access control problems. It eliminates the operational burden of data deletion requests.[11](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn22) Finally, it removes the risk of storing private information. As we’ve discussed, data should properly be considered a liability as much as an asset.

We may have successfully excluded PII from the ML datastore. But we probably shouldn’t count on that for two reasons. On the one hand, we may not have excluded PII as effectively as we think we have. As previously mentioned, it is notoriously difficult to identify PII without a thoughtful analysis so unless there is careful, time-consuming, human review of _all_ data that is added to the feature store, it is extremely likely that some of the data in combination with other data does contain PII. On the other hand, for many organizations, it may simply not be feasible to reasonably exclude all PII from the datastore. Such organizations are obliged to strongly guard their datastore as a result.

Beyond concerns about PII, teams will likely develop particular uses for particular kinds of data. Reasonable use of the datastore will restrict access to certain data to the team most likely to need and use that data. Thoughtful restriction of access will actually increase productivity if model developers can easily access (and only access) the data they are most likely to use to build models.

In all cases, systems engineers should track metadata about which development teams build which models and which models depend on which features in the feature store—effectively an audit trail. This metadata is useful, if not required, for operational and security-related purposes.

### Privacy

When ML data is about individuals, the storage system will need to have privacy-preserving characteristics. One of the fastest ways to transform data from an asset to a liability is by leaking private information about customers or partners.

We can make two architecturally different choices about dealing with private data: eliminate it or lock it down. To the extent that we can still get excellent results, eliminating private data is an extremely sound strategy. If we prevent PII data from ever being stored in the data storage system, we eliminate most of the risk of holding private data. This may be difficult—not only because it is not always easy to recognize private data, but also because it’s not always possible to get great results without private data.

**PRIVACY CHOICES FOR RECOMMENDATIONS AT YARNIT**

Let’s think about a recommendation or discovery system for YarnIt. The general idea is that we would like to show customers items that they might be interested in buying at various stages of their visit to the _yarnit.ai_ website. That might include when they land on the page, when they search for a type of yarn or a brand of knitting needle, when they put something into their cart, or when they check out. Ideally, we will present them with suggestions that they find appealing. So, what pieces of information would we need as inputs to our system to identify products they might also consider?

One of the time-honored approaches is “people who bought _X_ also bought _Y_.” It makes sense and allows us to make reasonable recommendations for a wide set of products for which there is commonality or homogeneity among the customers. If everyone who buys a particular type of mohair yarn also buys a specific type of needles, we should be able to recommend them with no private information at all about the individual user right now. But if there is some variety among our customers, things get more interesting.

For example, what if one customer is considerably more or less price sensitive than other customers? If they have a much smaller budget than the typical mohair yarn purchaser, they may choose not to purchase additional needles, or may buy them only below a certain price. Or what if the system knows that this customer had already purchased those needles in a previous transaction? In that case, recommending more of them is probably a waste of screen space and precious attention. We should recommend things we have reason to predict the customer will actually be interested in buying.

To make those kinds of recommendations, however, we need private data. Specifically, we need individual users’ purchase histories. With that data, we can easily determine things like estimated budget and types of items previously purchased, including specific items already purchased. If we decide that our models can achieve their goals only by having access to private data, we will need to have a serious conversation about the architecture of storing, using, and eventually deleting that private data. The most thorough structural approach generally requires creating per user datastores that are encrypted at rest and unlocked by a key under control of only the customer. This is most common when handling other organizations’ data and less common for individuals running their own training system. Additionally, using data like this, combined with general datastores with data from multiple users, requires federated learning—an advanced topic that’s beyond the scope of this book.[12](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn23) (See [Figure 2-5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#choices\_and\_processing\_as\_data\_moves) for a list of the types of data and access control implications.)

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098106218/files/assets/reml_0205.png" alt="Choices and processing as data moves through at ML system" height="269" width="600"><figcaption></figcaption></figure>

**Figure 2-5. Choices and processing as data moves through an ML system**

Given all of this complexity, it’s substantially better and easier to anonymize data as we ingest it. As previously mentioned, the topic of anonymization is technically complex, but everyone building an ML system needs to know two key facts:

Anonymization is hardIt’s a topic people study and develop expertise on. Don’t try to just muddle through. Take it seriously and do it right.Anonymization is context dependentThere is no guaranteed way to anonymize data without knowing what other data exists and how the two bits of data relate to each other.

Anonymization is difficult but not at all impossible, and when done properly, it avoids a host of other problems. Note that doing so durably will require periodic review to ensure that current anonymization still matches the assumptions made about the data and access permissions when it was implemented, as well as a review every time new data sources are added to ensure that connections among data sources do not undermine anonymization. This topic is covered more extensively in [Chapter 6](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#fairnesscomma\_privacycomma\_and\_ethical).

### Policy and Compliance

Policy and compliance typically flow from requirements originating outside your organization. In some cases “outside the organization” actually means a boss or a lawyer working for YarnIt but implementing an external legal requirement of some kind, and in other cases it means a national government directly intervening. These requirements often have painful and powerful reasons behind them, but these backstories are usually not obvious when looking at the requirements themselves.

Here’s an annoying but powerful example: European regulations about cookie consent in browsers often seem overbearing, intrusive, or silly to web users, both European and non-European. The idea that websites should get explicit consent to store an identifier on the user’s machine might appear unnecessary. But anyone who understands the privacy-violating power of the third-party ad cookie can attest that a really strong rationale lies behind at least some restrictions on cookies. While the “ask every user for every website” approach probably isn’t the most elegant and scalable, it is much more understandable when we know more about how these cookies can be used and how difficult it is to protect against their bad usage.

Policy and compliance requirements for data storage should be taken seriously. But it’s a mistake to read the letter of a requirement or standard without understanding the intent behind it. Often, whole industries of consultants have developed difficult compliance practices when a simple approach might also be in compliance.

Anonymization, as mentioned previously, is one potential compliance shortcut. If private data requires special treatment, there may be a way to avoid those requirements simply by determining (and _documenting_) that we are not storing any private data.

There are two other things to note about policy and governance requirements: jurisdictions and reporting.

#### Jurisdictional rules

The world is increasingly filled with governments that assert control over the handling of data stored in or sourced from their geographic location. While this seems reasonable in principle, it does not map at all cleanly onto the way that the world has been building networked computer systems for the past few decades. It may not even be possible for some cloud providers to ensure that data generated in one country is processed in that country. YarnIt plans to sell globally, even though we may launch with only a few supported countries to start. So we will have to think carefully about what data storage and processing requirements we need to comply with.

For larger organizations, one choice of jurisdiction is more important than any other: the host country of the corporate headquarters. This matters because the government of that country is able to exert authority over data residing in any other country. Picking the country of incorporation can have profound implications to our data management system, but it’s not a factor most people carefully consider. They should.

#### Reporting requirements

Remember that compliance work entails reporting. In many cases, reporting can be integrated into the way we monitor the service. Compliance requirements are SLOs, and reporting includes the SLIs that establish the status of our implementation with respect to those compliance SLOs. Thinking of it this way normalizes this work alongside the rest of the implementation and reliability work we need to do.

## Conclusion

This has been a rapid and superficial introduction (though it probably doesn’t feel like that) to thinking about data systems for ML. At this point, you may not be comfortable building a complete system for data ingestion and processing, but the basic elements of such a system should be clear. More importantly, you should be able to identify where some of the biggest risks and pitfalls lie. To start making concrete progress on this, most readers will want to divide their efforts into the following areas:

Policy and governance

Many organizations start work on ML from the product or engineering groups. As we have highlighted, however, having a consistent set of policy decisions and a consistent approach to governance will be critical in the long run. Organizations that haven’t yet started on this effort should do so immediately. [Chapter 6](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#fairnesscomma\_privacycomma\_and\_ethical) is a good place to start.

To have the most impact in this area, you should identify the likely biggest problems or gaps and address them first. Perfection is not possible, given the current state of our understanding of the risks of using ML incorrectly and the tools available. But reducing instances of egregious violations absolutely is possible and is a reasonable goal.

Data sciences and software infrastructure

If we have started using ML at all, it is likely that our data science teams are already building bespoke data transformation pipelines in various places around the organization. Cleaning, normalizing, and transforming the data are normal operations that are required to do ML. To avoid future technical ML debt, we should start building software infrastructure to centralize these transformation pipelines as soon as is practical.[13](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn24)

Teams that have already solved their own problems may be resistant to this centralization. However, by operating data transformation as a service, it is sometimes possible to entice all new users and even some existing users to transfer allegiance to the centralized system. Over time, we should try to consolidate data transformations in a single, well managed place.

InfrastructureWe obviously need significant data storage and processing infrastructure to manage ML data well. The biggest element here is the feature storage system (often simply referred to as the _feature store_). We discuss the useful elements of a feature store in detail in [Chapter 4](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch04.html#feature\_and\_training\_data).

[1](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn12-marker) Version control for different versions of the data itself might also be warranted if the data is mutable and updated.

[2](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn13-marker) For the data to be useful, it has to be of high quality (accurate, sufficiently detailed, representative of things in the world that our model cares about). And for supervised learning, the data has to be consistently and accurately labeled—that is, if we have pictures of yarn and pictures of needles, we need to know which ones are which so that we can use that fact to train a model to recognize these kinds of pictures. Without high-quality data, we cannot expect high-quality results.

[3](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn14-marker) The case of AOL search logs is the most famous such debacle; see [“A Face Is Exposed for AOL Searcher No. 4417749”](https://oreil.ly/WALx5) by Michael Barbaro and Tom Zeller Jr. This incident is also explained at Wikipedia’s [“AOL search log release” page](https://oreil.ly/cBpOve).

[4](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn15-marker) A complementary approach to Model Cards for datasets is known as Data Cards; see the [Data Cards Playbook site](https://oreil.ly/aaSMr).

[5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn16-marker) See [“Learning to Compose Domain-Specific Transformations for Data Augmentation”](https://oreil.ly/uxLdr) by Alexander J. Ratner et al. for more details.

[6](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn17-marker) There is very little reason to believe this is true, but it’s vaguely plausible and moderately amusing. If anyone implements “source temperature” as a feature and finds it to be valuable, please contact the authors for an autographed book.

[7](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn18-marker) Most common, large data storage and analysis services from large cloud providers are column oriented. Google Cloud BigQuery, Amazon RedShift, and Microsoft Azure SQL Data Warehouse are all column oriented, as is the main data store for data services provider Snowflake. Both PostgreSQL and MariaDB Server have column-oriented configuration options as well.

[8](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn19-marker) Data scientists, data analysts, research scientists, and applied scientists use various data visualization tools and techniques including infographics, heatmaps, fever charts, area charts, and histograms. For more insights, refer to Wikipedia’s [“Data and information visualization” page](https://oreil.ly/DL2B2).

[9](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn20-marker) Some readers might read “version control” and think “Git.” A content-indexed software version control system like Git is not really appropriate or necessary to track versions of ML data. We are not making thousands of small and structured changes, but rather generally adding and deleting whole files or sections. The version control we need tracks what the data refers to, who created/updated it, and when it was created. Many ML modeling and training systems include some version control. MLflow is one example.

[10](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn21-marker) Data durability, discussed previously in [“Durability”](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#durability), is often included as a key concept in data integrity. Since this entire chapter is about data management, durability has been grouped with reliability concepts, and _integrity_ here refers to properties we can assert about the data, beyond its mere existence and accessibility.

[11](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn22-marker) One useful summary can be found in the Databricks article “Best Practices: GDPR and CCPA Compliance Using Delta Lake,” especially the section on [pseudonymization](https://oreil.ly/I5hPt).

[12](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn23-marker) See [“Federated Learning: Collaborative Machine Learning Without Centralized Training Data”](https://oreil.ly/ptj9h) by Brendan McMahan and Daniel Ramage for a general overview and reasonable links to the topic as it was in 2017. Federated learning has, of course, continued to evolve since then.

[13](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch02.html#ch01fn24-marker) The technical debt in ML systems is often fairly different from what we see in other software systems. One paper that explains this in detail is [“Hidden Technical Debt in Machine Learning Systems”](https://oreil.ly/3SV7Q) by D. Sculley (a coauthor of this book) et al.
