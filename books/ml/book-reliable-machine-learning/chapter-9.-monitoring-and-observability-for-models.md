# Chapter 9. Monitoring and Observability for Models

**Contributors/Reviewers:** Ely Spears, Lina Weichbrodt, Tammy Le

**Diagrams:** Joel Bowman

Managing production systems is somewhere between an art and a science. Add the complexities of ML to this hybrid discipline, and it looks less like a science and more like an art. What we do today is very much a frontier, rather than a well-defined space. Despite that, this chapter outlines what we know about how to monitor, observe, and alert for ML production systems, and makes suggestions for developing the practice within your own organization.

## What Is Production Monitoring and Why Do It?

This chapter is about how to monitor systems that are doing ML, rather than using ML to monitor systems. The latter is sometimes called _AIOps_; we are focusing on the former.

With that out of the way, let’s talk about production monitoring generically, without the complexities of ML, so we can make things easier to understand—and where better to begin than with a definition? _Monitoring_, at the most basic level, provides data about how your systems are performing; that data is made storable, accessible, and displayable in some reasonable way. _Observability_ is an attribute of software, meaning that when correctly written, the emitted monitoring data—usually extended or expanded in some way, with labeling or tagging—can be used to correctly infer the behavior of the system.[1](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn96)

Why would you care? It turns out there are lots of reasons. Most urgently, monitoring allows you to figure out whether your systems are actually working. If you are buying and reading this book of your own accord, you probably already understand how important that is. No less a luminary than Andrew Clay Shafer, cofounder of the DevOps movement, [wrote](https://sre.google/workbook/foreword-II), “If the systems are down, the software has no value.” If you don’t accept this is important, or if you understand the arguments but don’t believe them, we encourage you to read James Turnbull’s _The Art of Monitoring_ (2016). For the purposes of the rest of this chapter, though, we assume you understand that you need to monitor (and alert on) the state of systems, and what is up for discussion is how best to do that.

Of course, the situation has more nuance than that. For a start, systems don’t usually behave as a Boolean, either entirely up or entirely down; generally, they can be performing anywhere on a spectrum from superb to very badly. Monitoring obviously needs to be able to handle this situation and represent the reality correctly.

Monitoring is hugely important in and of itself, but an offshoot of monitoring is absolutely crucial: alerting. A useful simplification is that when things go wrong, humans are alerted to fix them, and for the purposes of this paragraph, _alerting_ is therefore both defining the conditions for “things going wrong,” and being able to reliably notify the responsible folks that something isn’t right—e.g., paging. This is a key technique in helping to “defend the user experience.”

Less urgently, but still vital, is monitoring for long-term trend analysis, capacity planning, and general understanding of your service envelope. You use this kind of monitoring data to answer questions like these: Is my service cost-effective? Does it have any unobvious performance cliffs? Is there data distribution drift? How does service latency, for example, relate to user behavior on the weekend versus the working week? All these questions and more cannot really be answered well without monitoring and observability.

### What Does It Look Like?

As we’ve alluded to, to do monitoring, you must have a _monitoring system_ as well as systems to be monitored (here, called the _target systems_). Today, target systems emit _metrics_—a series, typically of numbers, with an identifying name—which are then collected by the monitoring system and transformed in various ways, often via _aggregation_ (producing a sum or a rate across multiple instances or machines) or _decoration_ (adding, say, event details onto the same data). These aggregated metrics are used for system analysis, debugging, and the alerting we mentioned previously_._

A concrete example is a web server with a metric of the total number of requests it received; this metric has a name—say, in this case, `server.requests_total`. (Of course, it could be any request/response architecture, like an ML model!) The monitoring system will obtain these metrics, usually via _push_ or _pull_, which refers to whether the metrics get pulled from the target systems or get pushed from them. These metrics are then collated, stored, and perhaps processed in some way, generally as a _time series_. Different monitoring systems will make different choices about how to receive, store, process, and so on, but the data is generally _queryable_ and often (very often) there’s a graphical way to plot the monitoring data so we can take advantage of our visual comparison hardware (eyes, retinas, optic nerves, and so on) to figure out what’s actually happening.

By extension, an observable system uses these foundational ideas but goes a step further: instead of just getting a counter for the total number of requests, you get _labeled_[2](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#idm45447339809648) data for that metric, and indeed most metrics. Specifically, _labeled data_ means that you don’t just get a counter; you get subdivisions, or slices, of that metric. So, for example, you don’t just have `server.requests_total`; you have `server.requests_total{lang=en}`, which means, “For all requests made where the client requested the page be rendered in English, what is the total number of requests?” Of course, not just `{lang=en}` either—also `{lang=fr}`, `{lang=pt}`, `{lang=es}`, `{lang=zh}`, and so on. A fully observable system allows slicing and dicing of such data on _extremely_ fine-grained boundaries, such that it is possible to construct queries to look at the past 12 days of queries in Romanian that resulted in a HTTP 404 return code after 1200 ms of latency.[3](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn97)

Monitoring in general has many subtleties, particularly around how aggregation is done and how results are used, but it’s a reasonable high-level picture—for _non-_ML system monitoring at least. When you add ML systems as _target systems_ to this picture, you get not just all the issues mentioned previously, but also the special concerns of ML; [Figure 9-1](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#observability\_layers\_and\_system\_require) may help illustrate.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098106218/files/assets/reml_0901.png" alt="Observability layers and system requirements" height="589" width="600"><figcaption></figcaption></figure>

**Figure 9-1. Observability layers and system requirements**

### The Concerns That ML Brings to Monitoring

One important concern is not necessarily the task of monitoring ML itself, but the _perception_ of the act of monitoring by the model development community. What do we mean?

Well, we believe the model development community understands very well that software has inputs and outputs, and should be observed in order to figure out what’s going on. (The whole act of model development can be looked on as the process of metric extraction, control, and optimization, for a start!) What is sometimes missing, though, is awareness and engagement with what happens _after_ a model is developed and goes out into production. What we see as the mindset problem with monitoring ML also partially derives from how the word is used—semantically, _monitoring_ can mean inspection activities applying to model development, or it can mean continual observation of systems in production. In actuality, the term is used in both contexts.

To put it another way, many model developers don’t realize that exactly the same requirements for inspectability _could_ and _should_ apply when a model is running in production as they do in development. This gap is particularly true if your background is in using metrics for optimization, but not for _detection_. Detection turns out to be a hugely crucial use case, and the activity of monitoring should apply across the whole-model lifecycle.

This is not just a question of perception, though. The reality is that ML already struggles with explainability—particularly at the time of execution in production. This is partially because of the nature of ML, partially a function of the way models are developed today, partially the nature of production operation, and partially a reflection of the fact that tools for inspectability are generally aimed just at model development. All of these combine to make monitoring ML more difficult.

### Reasons for Continual ML Observability—in Production

Observability data from your models is absolutely fundamental to business—both tactical operations and strategic insights. We have mentioned, and much has been written about, the negative consequences of not having monitoring and observability, but positive consequences arise too.

One example we like to use is the connection between latency and online sales. In 2008, Amazon discovered that each additional 100 ms of latency lost 1% of sales, and also the converse—so, the faster the better.[4](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn98) Similar results have been confirmed by Akamai Technologies, Google, Zalando, and others. We assert that without observability, there would be no way to have discovered this effect, and certainly no way to know for sure that you were either making it better or worse!

Ultimately, observability data _is_ business outcome data. In the era of ML, this happily allows you not just to detect and respond to outages, but also to understand incredibly important things that are happening to your business. Ignore it at your peril.

## Problems with ML Production Monitoring

ML model development is still in its infancy. The tools are immature, conceptual frameworks are underdeveloped, and discipline is in short supply, as everyone scrambles to get some kind of model—any kind of model!—off the ground as soon as possible and solving real problems. The pressure to ship is real and has real effects.

In particular, model development, which is inherently hard because it involves reconciling a wide array of conflicting concerns, gets harder because that urgency forces developers and data science folks to focus on those hard problems and ignore the wider picture. That wider picture often involves questions around monitoring and observability.

This leads to two important observations we would make about the difference between model development and production serving. One of these is a generic observation about all production environments, which just happens to be particularly complex and difficult in the ML world. The other is a specific situation about model development that is currently broadly true but may not be so forever; nonetheless, it is worth mentioning as a foundation to what follows.

### Difficulties of Development Versus Serving

The first problem is that effectively simulating production in development is extremely hard, even in separate environments dedicated to that task (like test, staging, and so on.) This is not just because of the wide variety of possible serving architectures (model pools, shared libraries, edge devices, etc., with the associated infrastructure that you might or might not be running on) but also because in development you often invoke prediction methods directly or with a relatively small amount of code between you and the model for velocity reasons. Running in production also generally means you don’t have the ability to manipulate input, logging level, processing, and so on, arbitrarily, leading to huge difficulties in debugging, reproducing problematic configurations, etc. Finally—and crucially—the data you have in testing is not necessarily distributed like the data the model encounters in production, and as always for ML, data distribution really matters.

The second problem is a little different. In conventional software delivery, the industry has a good handle on work practices that are known to improve throughput, reliability, and developmental _velocity_. The most important of these are probably the grouped concepts of continuous integration / continuous deployment (CI/CD), unit tests, small changes, and a collection of other techniques probably best described in _Accelerate_ by Nicole Forsgren et al. (IT Revolution Press, 2018).[5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn99) Unfortunately, today we are missing this equivalent of CI/CD for model development, and cannot yet say we have converged onto a good set of (telemetry-related, or otherwise) tools for model training and validation. We expect this will improve over time as existing tools (such as MLflow and Kubeflow) gain traction, vendors incorporate more of these concerns into their platforms, and the mindset of holistic or whole-lifecycle monitoring gains more acceptance.

**AN IMPORTANT NOTE ABOUT SKEW**

_Skew_ is used broadly to describe a variety of data problems. Among common uses of the term are biased distribution shifts in the underlying data, outliers (especially unexpected outliers), semantic violations in the interpretation of data, and missing feature values (especially including values missing in some features but not others). Skew also commonly refers to a failure of correspondence between two variables or data streams that are intended to be synchronized. Note that the way this is used in ML _systems_ only rarely includes the use in statistics (biased distribution shifts in the underlying data).

The skew most likely to cause production problems is _training-serving skew_, which describes any difference between the performance of your model in training and in serving. Common causes are changes in feature definition between training and serving, changes or gaps in the data itself, or sometimes even some kind of a feedback loop between the algorithm and the task. Skew of this kind and other kinds is a common cause of avoidable outages in ML systems, and all of the simple causes of it should be targets for monitoring.

Different models and ML systems are subject to different kinds of skew. Astute readers will therefore note a problem: the techniques for monitoring and detecting skew are model specific (or at least specific to a particular model architecture, configuration, and purpose). There’s no meaningful “just monitor all manner of skew for all models” function that anyone knows how to do yet (though it’s certainly an intriguing idea). In other words, we can detect a difference between training and serving, but not know the underlying causes that difference has, or whether that difference is meaningful. We cannot detect differences in coverage of particular datasets without knowing what constitutes the domain for those datasets and what the expected coverage is.

In terms of impact on monitoring best practices, this implies that the monitoring system has to be general-purpose and flexible, but that individual model and model-family monitoring has to be implemented by production engineers and ML engineers working closely together. This is an area that many people are working on and is expected to improve in the future.

### A Mindset Change Is Required

Though we have many technical challenges today, the organizational and cultural ones that act against holistic monitoring are arguably the most relevant ones here. In particular, model developers don’t generally think in terms of detection of issues _post_-deployment, but instead think in terms of modeling KPI performance _pre-_deployment—and modeling KPIs are not necessarily directly connected to business KPIs\![6](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn100)

This obviously presents a problem for whole-lifecycle monitoring, since both pre- _and_ post-deployment turn out to be important, and like successful software generally, post-deployment often lasts longer than you think. Teams focused on developing and deploying models quickly are often impatient about rigorous delivery frameworks, as though these delivery frameworks will prevent them from organizing training and serving in whatever way is most convenient for them. Which, of course, they do—to some extent. But they do so by providing a set of monitoring and management guarantees in production that would otherwise be difficult to achieve and deployed on only an ad hoc basis.

If we accept this framing, the most important thing we should do is try to have a reasonable, flexible solution for maintaining the broadest useful picture of model behavior throughout its entire lifecycle, and make it adaptable for your own situation. Since the special case tools that today are used for model development (TensorBoard, Weights & Biases, and so on) don’t usually naturally translate into production itself, the particular monitoring system in use, and so on, at the moment we will necessarily have to make some of this up ourselves. Given that, the overall goal for this chapter is to recommend a whole-lifecycle approach to monitoring, and in particular, suggest a default set of things to monitor _other_ than the specific business metrics the model is intended to improve, since they are already well understood.

## Best Practices for ML Model Monitoring

Let’s start off with a few framing assumptions: for the purposes of this chapter, model development is generally done in a loop. We select data, train on it, build a model, do basic testing/validation, tweak, retrain, eventually release to production, learn how the model behaves, and begin the cycle again with ideas for improvement.

Monitoring in serving can be divided into model, data, and service (also known as _infrastructure_). Separation like this is useful because we don’t have to handle every detail at every level, though we acknowledge that some crossover exists.

_Explainability_—understanding what led the model to classify as it did, predict as it did, and so on—is a huge topic and likely to get more and more important as ML plays a larger role in more industries. A detailed explanation of explainability and an overview of best practices is beyond the scope of this chapter, and indeed this book. A handful of the ethical principles are addressed in [Chapter 6](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#fairnesscomma\_privacycomma\_and\_ethical), and most readers should review that.

From the more practical perspective of model monitoring only, however, explainability is important to understand in preproduction and, increasingly, production phases. The particularities of explainability vary according to model type, phase, business strategies, and so on. But the in-production cases are usually where ML is being used in safety-critical or socially significant ways, and where there might be a legal or ethical interest in understanding what led to the outcome. The primary objective here is to find as many ways as possible to “smooth out” the difference between model development and serving.

### Generic Pre-serving Model Recommendations

We talk about this more in [Chapter 3](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch03.html#basic\_introduction\_to\_models), but from a monitoring point of view, it’s most important to keep in mind the business goal attached to the development of the model, and connect its KPIs to exported metrics for monitoring purposes. A model for which you have no business insight but plenty of infrastructural insight would be close to useless, and similarly, a model that you understood to work well in development but for which you had no insight into production behavior would arguably be potentially more harmful than useless.

The most important recommendation is that your business KPIs should correlate with model metrics; you should be able to trace these continuously from development to production. For example, if you are a ride-hailing app company, and you are predicting estimated time of arrival (ETA) for the ride, pickup location is something you’ll want clearly available during the whole period. When monitoring for data integrity, the most important features of the model should therefore be given high priority.

#### Explainability and monitoring

As we’ve mentioned, explainability is a big area, but in particular for monitoring it presents some problems. Like debugging or observability in general, full explainability generally involves more resources (and is slower, more costly, and so on) in production than in development. Yet you often want explainability most urgently in a production context.

People responsible for ML models want explainability for a few reasons: establishing which features should be prioritized for data integrity, investigating a specific prediction or specific slices of predictions, and responding to business requirements for explainability generally. Since it’s so expensive, and since business people may not have all the background required to understand it, being effective here often amounts to having a conversation with them about what they really care about. Often you can respond to their concerns (and potentially even build special monitoring solutions) without having to get into highly specific modeling details, which is a conversation that can sometimes distract from the essentials.

However, in some cases explainability is essential for troubleshooting. Let’s take a lending use case: say a prediction is rejected because the request has a unique value for one feature that is not commonly seen—in this case, an application date of February 29. Application date is typically not one of the top 10 most important features across all of our predictions, but if for some reason our training dataset has only a few applications on February 29 and those end up being poor risks in the intervening years, we can imagine a model that uses the application date to heavily influence the decision to reject.

In such cases, explainability can surface what drove that individual prediction’s output. This could be implemented via regularly logged summaries of predictions, exposing internal mechanisms via LIME and SHAP (essentially surrogate or duplicate models that use a conceptually analogous approach to differential cryptanalysis)[7](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn101), or another more customized approach. You generally find such explanations are not just used by model developers, but also risk and compliance teams, and can help nontechnical users understand systematic issues with models. Unfortunately, this is mostly too expensive to do in production.

### Training and Retraining

In many ways, the training phase is the easiest to handle—from a classic monitoring point of view, anyway. The most important thing for monitoring training is keeping a holistic view, with the most important metric being how long it takes from starting training to producing a (hopefully working) model.

Having said that, other factors are important too—in particular, understanding when to _retrain_—i.e., to build a new model, in the hope or expectation that it might fix a problem for us. Though model rollback is the most common tactic used to resolve production problems, from time to time retraining is used—in this context, you can think of it as being like roll-forward for models: i.e., replacing what’s in production with the latest version of everything.

Retraining is generally used when rollback hasn’t worked or can’t work (see [“Fallbacks in validation”](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#fallbacks\_in\_validation)), or if roll-forward is easier for your infrastructure than rollback.[8](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn102) Equally, retraining has drawbacks in two circumstances. In the first, retraining would execute over the exact same data you used to train the old model (in which case, you would broadly expect the same behavior, since it’s the same input). In the second, retraining takes so long that you can’t use it to tactically resolve an outage. (This is one reason you might want to just run retraining automatically and periodically, if it’s feasible for you to spend resources on this.)

For the first issue, you can _sometimes_ “hack” the situation a little by changing the data you’re training over—either by using an entirely new corpus, replacing some bad data, adding missing data, and so on. (Of course, significant variables for the training process here include changing the range of data we are training over, and whether or not we change any weightings, but hopefully deciding those is relatively quick.)[9](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn103)

Training is important in a monitoring context not only because of retraining, but also because this is where most developers establish their _baselines_, and these are used widely thereafter as a comparison point.[10](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn104)

#### Concrete recommendations

Next, we outline a list of things to monitor to get base-level coverage. Things that we consider minimal and mandatory, we highlight in italics; if you have to start somewhere, start there. Of course, you could put a _lot_ of effort into monitoring, and perhaps in some cases it’s worth it, but in some it’s not. So we recommend you think carefully about the cost/benefit trade-off before implementing all of the following checks. (If you are looking for a more advanced guide to deciding what’s important, we recommend you look at writing an SLO for your training pipelines and models—see [“SLOs in ML monitoring”](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#slos\_in\_ml\_monitoring) for more details.)

* Input data
  * _How large is the input dataset compared to the expected size?_ Compare the training dataset to either the last time we trained this model or use another exogenous metric that can indicate rough size. If the dataset has shrunk by 50% unexpectedly, that’s usually a bad sign.
  * _Are you comparing raw input data and feature data?_ (Some problems emerge only when combining fields into features.)
  * _What are the youngest and oldest pieces of input data? Do they conform to expectations?_ For extra credit, start to look at the histogram of the distribution of ages. In a distributed training situation, sometimes there can be a small number of very old pieces of data while everything else has been processed cleanly and effectively.
  * _What is the cardinality_ (in this case, total number of elements or examples), and _how is the data distributed?_ Is the distribution very different from the expected distribution—either compared to the previous time we trained on this data, or compared to other generated expected distributions?
  * In a batch-processing scenario, _can you enumerate the data and ensure it’s finalized?_ (Can you guarantee that all of the events from a particular day—say, yesterday—have arrived and include all relevant fields? Do you have any outliers from the day before that, or from today?)
  * In a streaming processing scenario, _what is the rate of arrival of incoming data?_ Do you process on receipt of each “bundle” or after a fixed size has been collected (in which case, you should track how often processing is invoked)?
  * If access to the data is mediated, _what are the rates of access? Are there a large number of failures_ (particularly authentication-related ones)?
  * If the data is copied from somewhere else to go into, say, a feature store, and the model is built from a feature store, _does that copying happen correctly?_
* Processing
  * _How many processing jobs are running? When did they last run? Did they complete by the time they should have? What was the rate of restarts and the successful job completion rate? Is there a backlog of unprocessed units? What is the distribution, in age, of the unprocessed units?_
  * _What is the processing rate_ (measured, say, in input data elements processed per second?) _What is the 50th percentile, 90th percentile, and 99th percentile of processing rate?_ (Pay particular attention to long-running shards—i.e., if you split your work across many processing jobs, you often see one of them running long—see the previous comment about input distributions.) How does that compare to the previous iteration? Or for more accurate comparisons, how do you ensure that seasonality effects are properly accounted for in defining the previous iteration?[11](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn105)
  * _How much, in total, of the three axes of CPU, memory, and I/O have been consumed to produce the model?_ (This can be nontrivial to determine, but is crucial for figuring out bottlenecks—in particular, if you add more of any resource, would you know if your system would get faster?)
* Holistic view
  * _How long did the entire run—from marshaling input data to producing the model—take, both in absolute duration and comparative duration?_
  * What is the _size of the output model_? If there is more than one file, is every file present that should be?
  * _Can the model be successfully loaded and make simple predictions?_
  * For those who do testing in a separate environment, _does the model pass the testing process?_
  * _What is the time to get the model into production and serving queries?_
  * For those who do testing in production, _does the model pass exposure to users?_ Are the business or use metrics that you track in production affected in an unexpected way by the new model?
  * _Do you understand what the largest contribution to getting models into production is?_ Is it manual action or automatic action? (You might have to add decoration or annotation information to achieve this: for example, a way to annotate a particular period as being a time of heavy network load.)

Finally, we note that some of these suggestions might also usefully apply to subcomponents of your training pipeline. It’s not uncommon to have, for example, various data landing zones that have some very simple checks applied to them before they get copied for fuller processing elsewhere (think yarn supplier delivery manifests, for example, often sent over relatively inflexible electronic fund transfer, or EFT, processes). Being aware if the size of those dropped by 50% would be useful. Similar arguments might apply to chains of feature preprocessing prior to feature-store assumption, and so on. We can’t describe every possible architecture in advance here, but we can say that business risk analysis can help you to understand where best to place your scarce resources.

### Model Validation (Before Rollout)

Models are generally designed to achieve a certain business impact, also known as _improving a metric_. _Business validation_ can therefore be understood as attempting to understand the business impact of a model,[12](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn106) for example, looking at how it affects profit and loss (also sometimes known as the _profit/loss curve_), or using a confusion matrix, which tries to understand where the model classified no and the answer should have been yes, and so on. Of course, we always need to beware of user behavior being noisy, but the primary goal is to find out whether the model improves a certain metric from a baseline.

The secondary goal is trying to figure out whether the new model is better than what we already have. To do that, we not only have to test versus historical data, which we are probably doing anyway, but also compare two models against each other.[13](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn107) We can choose from at least two good approaches:

* Test in preproduction environments (sometimes called a _sandbox_), and run the models either in parallel or serially, depending on your capacity, to compare behaviors.
* Test in production, with the model getting a small subset of real user traffic (typically, between 1% and 5%), sometimes called a _canary test_.

The second approach also has the good effect of exposing any problem with the model reacting to user traffic before a full rollout. However, this does require you to have some way of engineering traffic such that a particular version gets only a subset of traffic—not all infrastructure supports this feature. But if you have it, it’s really good for allowing safe transitions from old to new models, and overlaps nicely with A/B testing. (General model updates can also fit well within such a framework, and indeed contemporary software deployment makes a lot of use of this approach.)

A hybrid approach is to send some data to the model locally, and the same data to the model running in production (though with this approach, the local model is not enabled to take full production traffic; it is just taking your test traffic). This helps expose feature code differences between modeling and serving, configuration differences, and so on. Finally, some folks send production traffic to a model, but don’t serve the results to end users, which tests many components of the serving path without exposing users to risk. This is called _shadowing_, and provides another place on the spectrum to balance correctness and risk.

#### Fallbacks in validation

For all kinds of reasons, it’s highly advisable to have a _fallback plan_, which is a set of steps you take if the new model fails, the rollout fails, or even the old model is found to have some weird behavior in a particular subset of circumstances. There are two main approaches (as seen in [“Emergency Response Must Be Done in Real Time”](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch10.html#emergency\_response\_must\_be\_done\_in\_real)):

Use an older version of the model (roll back)This implies it’s a good idea to keep the actual binaries of these around, versioned correctly, as well as monitoring the actual versions in production, since stressing your training infrastructure to build a model from old data at a time of production outage is generally the opposite of a good idea.Fall back to a simpler algorithmic or even hardcoded approachFor example, when attempting to supply ranked recommendations for product purchases, instead of broken recommendations, just display the top 10 most popular products on the site—though it won’t be right for anyone, it won’t be too wrong either (in most cases)!

Be very careful in your rollback, since many subtle problems wait in the long grass, waiting to strike: _schema changes_, _format changes,_ or changes in _semantics_—whereby a crucial database, data source, or feature store format changes between releases—are all examples of areas that are easy to overlook in rollbacks. This can sometimes make it effectively impossible or impractical to do so unless the whole suite of dependencies is rebuilt, but might also make it hard to roll forward to a new version unless the actual error is fully understood. In these situations, algorithmic fallbacks can be life-saving. However, algorithmic fallbacks also can themselves fail because of categorical errors.

A nuance here is that even if the schema stays the same, feature transformations applied to one version of the model may not be the same as what’s applied to another version. For example, how to handle a missing value for a ride-hailing app pickup location in one version of the model might be to simply default to where the rider was last seen, and in another version of the model might be the closest previously saved pickup location (e.g. home, office, etc.). These differences in the way the features are transformed might differ among versions and would also complicate rollback. Even worse would be if you roll back to one feature transformation on one “side” of the system and stay with the new one on the other “side.”

#### Call to action

We don’t yet have anything like the CI/CD workflow used in the infrastructure-as-code (IaC) community today that is commonly available for model development.[14](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn108) Different companies and even different teams within the same company could solve the problems differently. That doesn’t mean you can’t do checks, or can’t do them automatically—homegrown approaches have, of course, some utility—but today’s reality involves manual, peer-based review of both code and data, and detailed validation requiring PhDs in statistics.

For what it’s worth, this situation imposes serious friction for ML development as it stands. The industry actively needs to work toward making as much of this as automatic as possible, and this work is too important to be left to the platforms. We therefore call on the industry to move toward a state where IaC approaches are used as widely in ML development as CI/CD and IaC enjoy today in product development. Even if we don’t get that far, being in a situation where everything after manual validation moves the training artifacts safely and automatically to production would be a significant improvement than what we have on average across industry today.

#### Concrete recommendations

Commonly used numerical KPIs are measured to assess an ML model’s performance. These are also covered in some detail and with a deeper theoretical foundation in [Chapter 5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch05.html#evaluating\_model\_validity\_and\_quality), and with specific attention to serving use cases in [Chapter 8](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch08.html#serving-id0000021). We summarize these KPIs here for continuity and ease of access:

AccuracyThe fraction of predictions for which the model is correct.PrecisionThe ratio of true positives to total positives.RecallThe ratio of predicted positives to total positives.[15](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn110)Precision and recall (PR) curveThe space of tradeoffs between precision and recall at different decision thresholds.Log lossSee [Chapter 5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch05.html#evaluating\_model\_validity\_and\_quality) for more detail.ROC curve and AUCA threshold-independent measure of model quality.

Many other possible mathematical measures could be used, some of which you might recognize from early statistics classes (e.g., _p_-values and coefficient effect sizes) and some of which are a little more involved (e.g., posterior simulations). On the whole, though, if you understand the preceding list in terms of your model, and understand your output distribution _shape_, you have a very good handle on what’s going on.

### Serving

Monitoring ML models while they are serving in production has all of the difficulties of observing things in production, combined with the numerous challenges of figuring out _why_ things are happening that are peculiar to ML. Nonetheless, we can start with a set of simple questions:

* What are good metrics to measure my model in serving? (In some sense, you want the more relevant ones; arbitrary metrics have a habit of growing without limit until signal/noise is degraded.)
* Is my model performing as expected? If it’s not performing as well, why, and how can I resolve the issue?

To choose good metrics for measuring model performance, we need to properly understand how a model can fail. Three components are needed to make a model work successfully:

ModelThe model making predictions.DataThe data that is flowing in and out of the model. This includes the features that the model uses to make a prediction and the prediction itself.ServiceThe service that actually renders the model**.** This typically involves the deployment of the model and serving of inferences—infrastructure, in the broadest sense.

Each of these three components needs to be working as expected in order for the model to be successful. A failure in any can have an impact on the overall business KPIs that the model was designed to improve. Therefore, we need to measure _all_ of these to have a complete picture. In the next sections, we examine them in more detail.

#### Model

Starting with the model itself, we again note that measuring model performance _in_ serving is a lot trickier than measuring performance _prior to_ serving. Given those difficulties, we prefer to use the same metrics to evaluate the model in serving as we do in training/validation, since that will give us more confidence that we’re actually measuring in some sense “the same thing.” Of course, doing this requires being able to match the prediction with the corresponding observed reality.

For example, let’s assume a ride-hailing company has a model predicting the ETA of a car for a customer. While evaluating this model in validation, you care a lot about minimizing the error (usually referred to as _root mean squared error_, or _RMSE_[16](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn111)) so that the model’s prediction is close to the actual ETA: in this context, it turns out that customers tend to get more upset if you overpromise and underdeliver than the other way around. So to calculate this same metric in a production environment, there must be some process that can reconcile the predicted ETA with the actual ETA (i.e., calculate the error).

In practice, a few scenarios describe the delay with which the real results (known as _actuals_) arrive, and how we cope with that.[17](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn112)

**Case 1: Real-time actuals**

The ideal ML deployment scenario—and often the only one taught in the classroom—occurs when you get fast actionable and fast performance information back on the model as soon as you deploy your model to production.

Many industries are lucky enough to have this ideal scenario. Probably the most famous is digital advertising: a model attempts to predict which ad a consumer is most likely to engage with. Almost immediately after the prediction is made, the ground truth—whether they clicked or not—is determined. A similar example is food delivery; as soon as the pizza has arrived at the hungry customer’s house, you have real measurements you can compare your model predictions with, and pizza delivery as a business has a strong time limit baked into it (as well as other ingredients)—take too long, and the customer typically no longer wants it.[18](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn113) Ultimately, a strong detection KPI is what you want, if you can get it.

The key thing this quick feedback loop enables is the ability to measure the effectiveness of your models essentially _instantaneously_ (or at least very quickly); of course, once you have this latent ground truth linked back to your prediction event, no matter how long it took to get it, model performance metrics can easily be calculated and tracked.[19](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn114) Tracking such metrics on a regular cadence allows you to make certain that performance has not degraded drastically from when a model was trained, or when it was initially promoted to production.

However, many real-world environments change the way you get access to ground-truth data, as well as the tools you have at your disposal to monitor your models.

**Case 2: Delayed actuals**

In this case, you don’t get the benefit of the fast real-time feedback outlined previously. Indeed, these kinds of situations are arguably more common in business generally than case 1, and certainly a lot more awkward to deal with. Imagine you are trying to predict the likelihood of physical infrastructure failing—say, bridges collapsing—or use ML to perform predictions in the real estate market. Both of these scenarios have durations that could plausibly be measured in years, or even decades. This makes it tricky to ensure that a model developed to help you make higher-quality decisions is behaving as expected.

This delay in receiving ground truth, as well as being very long, might also be _unbounded_. Take, for example, a fintech company trying to classify which credit card transactions are fraudulent. You likely won’t know whether a transaction is truly fraudulent until you get a customer card loss report or charge dispute. This can happen a couple of days, weeks, or months after the transaction cleared—or in the scenario where the transaction goes undetected by the customer, might _never_ happen.

Ultimately, you can still make the first approach work if you get “enough” semi-real-time data, the data arrives reliably enough, and so on, but if you can’t make that approach work, teams may need to turn to proxy metrics. _Proxy metrics_ are alternative signals that are correlated with the ground truth that you’re trying to approximate, but are selected specifically because they arrive more quickly.

In the case of bridge failures, we might look at results of bridge inspections, maintenance schedules, whether a bridge is in an area prone to flooding, and the age of a bridge. For real-estate purchase-price predictions, a common technique is to look at prices for similar houses, but with as few components changed as possible: for example, the same number of bedrooms and bathrooms but in a different area, or different number of bedrooms but in the same area, and so on. For construction timescales of months or years, even a messy composite proxy like this can be better than nothing.

Ultimately, proxy metrics serve as a powerful tool in the face of delayed ground truth since, if you can’t get a strongly correlated metric in real time, you can at least get a more weakly correlated one; often that’s good enough. Don’t forget the mathematical requirement that your proxy metric has statistical significance, however—and proxy metrics may even change relevance over time, so they need to be continually reevaluated.

**Case 3: Biased actuals**

One important thing to note is that not all ground truth is created equal. For example, if we are a fintech company looking at creditworthiness in loan contexts, the central problem with predictions is that declining a loan means you no longer have any information about whether the applicant could have paid you back. In other words, only the people you decide to give a loan to will result in outcomes that you can use to train future models on—a kind of selection bias that could allow bias of other kinds to creep in. As a result, we will never know whether someone the model predicted will default could have actually paid the loan back in full. As described more fully in [Chapter 6](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#fairnesscomma\_privacycomma\_and\_ethical), it is therefore critical to assess our data for potential biases, blindspots, and areas of under-representation.

**Case 4: No/few actuals**

In some ML applications, getting back actuals (responses) in a reasonable time window is just not possible. This could be for a variety of reasons, including these:

* Manual intervention is required to verify the model’s predictions.
* There is no way to attribute the response to the prediction.
* The time window of getting actuals is so delayed it cannot meaningfully inform the modeler that the model’s performance should be looked at.

For example, many image-classification applications require a human in the loop to manually verify that the images were classified correctly. This might require sampling so that only a valuable segment of the predictions is manually verified to improve the model’s future training dataset.

So what does a team do if it doesn’t get back actuals? In these scenarios, it’s not uncommon for ML teams to once again use proxy metrics to give signals of model performance. A weaker correlation may be better than nothing in these extreme cases. It is also common while testing new versions of models in production to A/B test models and compare their impact on product metrics. Monitoring data of the model becomes even more important to know if the model has deviated from what is expected.

**Other approaches**

Quite apart from how a model might handle the relationship with actuals, it is possible to use generic measurements of the behavior of a model that can be useful for figuring out whether things have gone truly awry. Our top selection here is the share of “useless” responses—i.e., empty, incomplete, or with subpar fallback—versus “good enough” responses, though another critical one is our old friend, data distribution.

**Troubleshooting model performance metrics**

Let’s assume that model performance metrics can be calculated. Inevitably, at some point your model will not meet performance expectations. The hard question to answer is _why._ The most common causes are typically undersampling in training data, drift, and data integrity issues impacting the quality of data the model uses to make predictions. A best practice is to look beyond averages and investigate various _slices_ of predictions, i.e., a specific subsegment of predictions, such as everyone in California.

Imagine that your model is predicting likelihood of fraud. You expect the volume of false negatives (your model missed catching a real fraud transaction) to be less than 0.01%. Let’s say you suddenly see false negatives jump to 2%. A natural way to proceed is to see whether this behavior is localized in any particular region or merchant. By doing so, you can surface where the missed classifications happen, and what slices of data to upsample when retraining the model. Of course, understanding the worst-performing slices of the model can provide feedback to model builders for ways to improve model performance.

#### Data

A key component of ML monitoring is monitoring the inputs and outputs of the model. As features are added or dropped, monitoring must be adapted to the schema of the model. We have two common ways to monitor data—drift detection and data quality checks. Drift is better for catching slow changes to the distribution of the data, while data quality checks are better for catching sudden, large changes in the data.

**Drift**

Drift measures change in distribution over time. Models do not perform equally well on every possible input: they are highly dependent on the data they were trained on, perform well when they see data that resembles that, and perform less well when they don’t. Especially in hyper-growth businesses where data is constantly evolving, accounting for drift is important to ensure that your models stay relevant. As a result, measuring the distribution of your data in various dimensions, if only via histogram-style methods, is crucial to understanding what’s going on in production. Some models are resilient to minor changes in input distributions, but accepting infinite resilience does not exist; at some point data distributions will stray far from what the model saw in training, and performance on the task at hand will suffer. This kind of drift is known as _feature drift_, or _data drift_. Conversely, when model outputs deviate from the established baseline, this is known as _model drift_, or _prediction drift_.

It would be great if the only things that could change were the inputs to your model, but unfortunately, that’s not the case. Assuming your model is deterministic, and nothing in your feature pipelines has changed, it should give the same results if it sees the same inputs.

While this is reassuring, what would happen if the distribution of the correct answers, the actuals, change? Even if your model is making the same predictions as yesterday, it can make mistakes today! This drift in actuals can cause a regression in your model’s performance and is commonly referred to as _concept drift_.

**Measuring drift**

Understanding the differences in two distributions is important, but is a sophisticated topic that we cannot go into in detail at this point. So if this material is foreign to you, don’t fret. Just either use it as it is, or go look things up and learn more. (Either way, you’ll probably end up coming across some of these terms if you engage with data distributions at all, never mind actively monitoring them.)

As we talked about previously, we measure drift by comparing the distributions of the inputs, outputs, and actuals between two distributions. The two distributions are typically the production data and the baseline distribution. Commonly used baseline distributions are training datasets, test datasets, or a prior window of time in production. But how do you quantify the distance between these distributions?

Several ways to do this are popular. The _population stability index_ (_PSI_), commonly used across banking and fintechs, relies on bucketizing data for one metric, calculating the percentages residing in the buckets for each distribution, and comparing the log of the divided percentages. This detects shifts within buckets or between buckets. The _Kullback-Leibler divergence_ (_KL divergence_) is similar to PSI, but asymmetric so it can detect distribution order switching. The _Wasserstein distance_ (also known as _earth mover’s distance_) calculates the amount of work required to move one distribution over to another, which is useful for recognizing the amount of shift between buckets—i.e., if things spill from one bucket to the next, the Wasserstein score will be lower than if a more radical jump occurs from one end of the distribution to the other far end.

While each of these distribution distance measures differ in the way they compute distance, they are all doing the same thing: providing a way to quantify how different two distributions are. Especially when actuals are not available, drift is used in the real world to identify changes in model predictions, features, and actuals.

**Troubleshooting drift**

In many ML use cases where performance metrics cannot be calculated directly, drift often becomes the primary way to monitor changes in model predictions. When examining why predictions have started the drift, we usually start with looking at which inputs have also drifted, and we can couple feature drift and feature importance together to determine which features might be more strongly correlated with the change. In turn, that helps us figure out which features might need to be resampled from baseline or could potentially need a data quality fix.

#### Data quality

While drift is focused on _slow_ failures, data quality monitoring for models is focused on the _hard_ failures. Models rely on the input features coming in to make a prediction, and these input features can come from a variety of data sources. Different types of data can be monitored for data quality issues—categorical fields, numerical fields, as well as unstructured data types such as embeddings, and so on. Here, we will dive into common strategies for monitoring structured categorical and numerical data.

**Categorical data**

_Categorical data_ is a stream of selections of a single value from a limited but larger collection of values. Think of categories like the type of pet someone owns: dog, cat, bird, and so on. You might not think much can go wrong here, but a sudden shift in the distribution of categories is always possible, either because of user behavior (this year’s hot Christmas pet is, for example, reindeer) or other failures. Let’s say, for example, your hypothetical model predicting which pet food to buy for your pet supply store sees data suggesting that people own only cats now. This might cause your model to purchase only cat food, and all your potential customers with dogs will have to go to the pet supply store down the street instead.

In addition to a sudden cardinality shift in your categorical data—i.e., literally how many are counted in each category—your data stream might start returning values that are not valid for the category. This is, quite simply, a bug in your data stream, and a violation of the contract (semantic expectations) you have set up between the data and the model. This could happen for a variety of reasons: your data source being unreliable, your data processing code going awry, an upstream schema change, etc. At this point, whatever comes out of your model is undefined behavior, and you need to make sure to protect yourself against type mismatches like this in categorical data streams.

Examples might include the following:

* You were expecting a string for a feature and suddenly received floats.
* You are case-sensitive for a feature (i.e., state inputs) and were expecting lowercase values, but are now receiving uppercase values (e.g., _ca_ versus _CA_).
* You are receiving data from a third-party vendor, and the order of their schema has an off-by-one error. You are now seeing each feature receive another feature’s values.

An unfortunately common—and tricky-to-handle—situation is missing data. This can arise for any number of reasons to do with infrastructure, application, storage, and network failures, or simple bugs. The real question is how to handle it. In a training context, sometimes just discarding the row will allow things to proceed and not be too bad; in a production context, you can throw an error (but not a fatal one; otherwise, that turns an intermittently flaky storage service into fleet-wide death, which is not to be recommended). While these techniques help you compensate for this problem, it’s not really a sustainable solution: if you have hundreds, thousands, or tens of thousands of data streams used to compute one feature vector for your model, the chance that one of these streams is missing can be very high!

**NOTE**

Though this is more to do with robustness than monitoring, it is possible to compensate for missing values in categorical data in a number of ways, a process commonly referred to as _imputation_. You could choose the most common category that you have historically seen in your data,[20](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn116) or you could use the values that are present to predict what this missing value likely is. The complexity of your solution to this problem is entirely up to your application scenario, but it’s important to know that no solution is perfect.

**Numerical data**

A numerical data stream is also pretty self-explanatory. _Numerical data_ is data generally represented by floats (though sometimes integers), such as the amount of money in your bank account, or the temperature in Fahrenheit or Celsius.

To start off our analysis of what can go wrong with numerical streams, one thing you sometimes see is an _outlier_, a value that exists far out of the range of historical values. Outliers are potentially dangerous to your model, as they could make your model mispredict in spectacular fashion.

_Type mismatch_ can also affect numerical data. It’s possible that a particular data stream where you’re expecting a temperature reading instead gives you (say) a categorical data point, and you have to handle this appropriately. It’s possible that the default behavior may be to cast this categorical value to a number which, though now valid, has entirely lost its semantic meaning and is now an error in your data that is incredibly hard to track down. Another possibility is something that maybe doesn’t change type but changes semantics: maybe you’re tracking a total counter (with some arbitrarily large or small wrap), and it suddenly gets changed to a delta. Depending on the data, of course, this might go undetected for quite a while!

Lastly, just as with categorical data, numerical data also suffers from the same missing data problems, but the ordering and span implied by a numerical sequence gives us more options for imputation compared to the pure categorical situation. For example, we can take an average, median, or other aggregate distribution metric to impute a missing numeric value such as yarn weight (in grams); see [Figure 9-2](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ways\_to\_handle\_bad\_production\_data).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098106218/files/assets/reml_0902.png" alt="Ways to handle bad production data" height="369" width="600"><figcaption></figcaption></figure>

**Figure 9-2. Ways to handle bad production data**

**Measuring data quality**

It’s not surprising to ML practitioners today that many models rely on very large numbers of features to perform their task. With training set sizes exploding into the hundreds of millions and even billions, models with feature vector lengths in the tens and hundreds of thousands are not uncommon.

This leads us to a major challenge that practitioners face today. To support these incredibly large feature vectors, teams have poured larger and larger data streams into feature generation. The reality is that this data schema will inevitably change often as the team experiments to improve the model. It’s common to add a feature, drop a feature, or change how it is computed/processed, and platforms such as feature stores are becoming widespread for tackling precisely this management overhead. Concretely, here are the most important checks to do across your features:

* Categorical data
  * Cardinality: Has the number of unique values changed?
  * Missing values: Is this feature missing data?
  * Type mismatch: Has the data type changed?
  * Volume of data: Has the volume of data seen for this feature changed?
* Numerical data
  * Out-of-range violations: Has the value gone outside an appropriate range?
  * Missing values: Is this feature missing data?
  * Type mismatch: Has the data type changed?
  * Moving averages: Have the feature values been increasing/decreasing?

#### Service

For an ML system to be successful, you need to understand not just the data going in and out of the ML system, and the performance of the model itself, but the overall service performance in rendering or serving the model—making its predictions or classifications available. Even if the model performance improves business outcomes and data integrity is maintained, but it takes several minutes for a single prediction, it might not be performant enough to be deployed in a real-time serving system. Similar to other software services deployed to production, the service deploying the model into production and serving the model’s inferences needs to be monitored.

Numerous options exist for serving a model into production: deploying your own APIs/microservices, using an open source framework for model serving (TensorFlow serving, PyTorch serving, Kubeflow serving, and so on), or using a third-party service. Regardless of what is used for model serving, it is important (especially in real-time services) to monitor the prediction latency of the service because the expected prediction should happen immediately after the request is sent. There are two ways to do this:

Model levelReduce the time it takes for the model to make a prediction.Serving levelReduce the time the system takes to service the prediction when it receives the request. This is not just about the model, but also gathering the input features (sometimes precomputing or caching them), and quickly catching the predictions to serve.

**Optimizing performance of the model**

To optimize the model for lower prediction latencies, the best approach is to reduce the complexity of the model. Some examples of reducing complexity could be reducing the number of layers in a neural network, reducing levels in decision trees, or reducing any irrelevant/unused part of the model. Architecture makes a difference too: for example, bidirectional encoder representations for transformers (BERT) is slower than feed-forward approaches, and tree-based models are faster than deep learning.

In some cases, this might be a direct trade-off to the model efficacy. For example, if there are more levels in a decision tree, more-complex relationships can be captured from the data and therefore increase the overall effectiveness of the model. However, fewer levels in a decision tree can reduce prediction latency. Balancing the efficacy of the model (accuracy, precision, AUC, and so on) with its required operational constraints is important to strive for in any model to be deployed. This becomes especially relevant for models that are embedded on mobile or devices.

**Optimizing performance of the service**

To optimize service performance, here are suggestions for areas you could monitor and improve.

First, let’s consider _input feature lookup_. Before the model can even make a prediction, all of the input features must be gathered, and this is often accomplished by the service layer of the ML system. Some features will be passed in by the caller, while others might be collected from a datastore or calculated in real time. For example, a model predicting the likelihood of a customer responding to an ad might take in the historical purchase information of this customer. The customer wouldn’t provide this when they view the page themselves, but the model service would query a feature store or a real-time database to fetch this information. Gathering input features can generally be classified into two groups:

Static featuresFeatures that are less likely to change quickly and can be stored or calculated ahead of time. For example, historical purchase patterns or preferences for a customer can be calculated ahead of time.Real-time calculated featuresFeatures that require being calculated over a dynamic time window. For example, when predicting ETAs for food delivery, this might require knowing how many other orders have been made in the last hour.

In practice, we typically have a mix of user- or application-provided features, static features, and real-time calculated features. Monitoring the lookup and transformations needed for these features is important to trace where the latency is coming from in the ML system.

Next, let’s consider _precomputing predictions_. In some use cases, it is possible to reduce prediction latency by precomputing predictions, storing them, and serving them using a low-latency read datastore. For example, a streaming service might store ahead of time the most popular recommendations for a new user of their service. This type of offline batch-scoring job can vastly reduce latencies in the serving environment because the brunt of the work has been done before the model has even been called.

### Other Things to Consider

Even with all of the preceding considerations, we have by no means exhausted the list of potential concerns you might have with monitoring and observability. Here are some other areas you might want to keep in mind for your monitoring journey.

**SLOs in ML monitoring**

SLOs are a popular and still growing technique used to explicitly decide in advance an appropriate level of reliability for systems—in some sense, determining what the user experience will be—and deciding whether to do feature work or reliability work, depending on which side of the threshold we are on at a given moment. For example, if we have decided the customer should have 99.9% availability, and we move below that at some point, this triggers fixing broken systems until we are trending at 99.9% again. That way, the union set of ML engineers, the overall product team, and the SRE team (if any) cooperate to “bend the curve” of the user’s experienced reliability toward the optimal, decided-on level.

This is a very simple, indeed simplistic, example, and the reality is considerably more complex in multiple key areas. Even though ML is complex, you can still get some value out of doing SLOs. But watch out for the following subtleties:

* As we have already established, you cannot determine the suitability of a model for production by just looking at the model. Therefore, SLOs have a strong case for being scoped to cover data as well (and data distribution, freshness, and so on).
* Availability for request-response serving systems, like the frontend web server for _yarnit.ai_, is a relatively clear-cut issue—it is either serving correct content within a certain acceptable latency, or it isn’t. What does availability for an ML system mean when the things being served might have, for example, classifier confidences ranging from 0.0 to 1.0, and any individual result or set of results might be completely, correctly, and justifiably less than any arbitrary quality threshold? At the very least, SLOs for ML therefore have to encompass the idea of quality or confidence thresholds.
* The best way to do this is, perhaps a little surprisingly, not to focus on the ML system performance, but instead to focus on the business objective that the ML system in question is supposed to deliver. Or in other words, the system as a whole can suffer degradations of various—perhaps temporary—kinds, if the overall user experience is still within the defined limits. For more of this topic, see [Chapter 8 of _Implementing Service Level Objectives_](https://oreil.ly/hZ76d).
* For environments with lots of models, or lots of model churn, a promising approach is a self-service infrastructure so ML engineers can define and enforce per model or per class of model SLOs (generally by comparison to a golden dataset); SREs could develop, offer, and support such a service, thus helping the overall SLO approach scale for everyone. (The same approach can be extended to separate use cases for a single model, too, as long as such tagging can be done deterministically.)
* Finally, as is clear from [Chapter 11](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch11.html#incident\_response), ML is _entangled_ across the business, and it may not be practical to specify SLOs for an ML system without also specifying them for all or most adjacent systems. (Indeed, you’ll have to, for anything other than the smallest systems.) Additionally, since inspectability or explainability is often difficult to obtain, reacting quickly enough to defend a sufficiently demanding SLO (say, 99.5% or more) is not going to be something a human-driven process can achieve on its own: this will be a question of automatic systems acting in concert with the humans. Automatic systems acting on their own in the non-ML case are often very useful, but we should move with caution in the ML case.

If you wish to get more concrete about establishing SLOs for your ML infrastructure, we have a recommendation for where to start. If you look at [Table 9-1](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#behaviors\_or\_metrics\_to\_monitor\_per\_ml), you’ll see our suggestions for what to base your SLOs on, given what you care about. The rows cover the context of what you’re trying to measure, and the columns address the scope. So, for example, if you’re worried about the overall system health of your ML training system, the top-left quadrant is where to go to find ideas on where to start. In essence, the table outlines behaviors, indicators, or example metrics to base SLOs on. Of course, this is a nonexhaustive list.

|                                         | Training                                                                                                                                                                                                                                                                                                                              | Serving                                                                                                                                                                                                                                                      | Application (YarnIt)                                                                                                                                                                                            |
| --------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Overall system health                   | <p>Model build time (excluding snapshots)<br>Number of concurrent trainings<br>Number of failed build attempts<br>Resource saturation (e.g., GPU or I/O throughput)</p>                                                                                                                                                               | <p>Latency (time to respond to request)<br>Traffic (number of requests)<br>Error count (number of failed requests)<br>Saturation (exhaustion of any particular resource: CPU, RAM, I/O)</p>                                                                  | <p>Mostly tied to user journey or session<br>Purchase rate<br>Login rate<br>Page subcomponent failure rate (e.g., cart display)<br>Cart abandonment</p>                                                         |
| Generic ML signals / basic model health | <p>Pretraining:<br>Source data size (i.e., hasn’t grown or shrunk a lot since last build)<br>Training:<br>Training time as a function of model type and input data size<br>Training configurations (e.g., hyperparameters)<br>Post-training:<br>Model quality metrics (accuracy, precision, recall, other model-specific metrics)</p> | <p>Model serving latency (as a fraction of overall serving latency or compared over time)<br>Model serving throughput<br>Model serving error rate (time-out/ empty value)<br>Serving resource use (RAM in particular)<br>Age/version of model in serving</p> | <p>Model-specific metrics (visible on individual page load)<br>Number of recommendations per page<br>Estimated quality (predicted click-through) of each recommendation<br>Similar metrics for search model</p> |
| Domain-specific signals                 | <p>Built model passes validation tests (not only that aggregate filters work, but results on golden set and held-out set tests are of equal or better quality).<br>Recommendations for new products in the store are of similar or greater quality.</p>                                                                               | <p>These metrics are often delayed. Offline signals match served predictions in aggregate and in relevant slices.<br>Number of recommendations served for specific queries matches pre-serving expectations.</p>                                             | <p>Session- or user-journey-specific metrics.<br>Percentage of visits with purchases is not declining.<br>Average sale per visit maintained.<br>Session duration maintained.</p>                                |

Actually specifying an SLO is best left to other texts, but [The Site Reliability Workbook](https://oreil.ly/1lbVy), edited by Betsy Beyer et al. (O’Reilly, 2018) has a reasonable example of a [fully specified, exhaustively detailed SLO document](https://sre.google/workbook/slo-document); it is also possible to be have something as simple as “99% of this data is younger than two hours” and for that still to provide value.

**Monitoring across services**

As you scale out your ML infrastructure further, two of the major things that you’ll probably find yourself doing, which probably won’t be easy, are distributed tracing, and understanding the latency distribution of your systems. (Though we’ve been talking about distributions a lot in this chapter, we are specifically referring to monitoring in this section, rather than ML.)

Distributed tracing, strongly related to observability, enables you to trace the full path of a request as it goes between different services in a distributed architecture. Think of it as labeling a request with “give me a full report on the path this takes.” Every system that handles the request should turn on some degree of extra visibility for it as it cascades throughout the system. Various commercial and free systems implement this—you could do worse than look at [OpenTelemetry](https://opentelemetry.io/) to get a handle on how it works.

Another crucial area is understanding distributions—in this case, not of ML data, but of latency. It’s not necessarily intuitive, but in a sufficiently complicated architectural arrangement, even a surprisingly small proportion of requests that run slowly can end up affecting customer experience significantly.[21](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn117) Most organizations don’t have the storage or compute power to track this for every request, so some kind of _request sampling_ is required—i.e., to track detailed statistics for certain requests, but only a comparatively small proportion of them by default. You can achieve this in various ways, depending on your environment, but we advise you to enable these capabilities early, since the additional insight into customer experience and production behavior generally can be vital.

Finally, one issue that is typically pertinent to only very large organizations is how to scale to very large monitoring setups. Though we can’t cover this in detail here, there are three main issues to consider. First is (1) how to run multiple monitoring entities without duplication of alerts, data, and so on. Usually this is related to (2) finding a way to divide the entities to be monitored in a reasonable way (this usually implies approximately equal partitioning of the monitoring targets). Finally, (3) we must monitor the monitors, to make sure they themselves are running. Different monitoring products support these features to a different extent—Prometheus, for example, can monitor itself and deduplicate alerts. Our experience suggests that deduplication is potentially the most important capability here—if your system can’t perform that, the next time you have a serious outage, you potentially have _N_ × <_number of pages you would receive_>, where _N_ is the number of monitoring instances you have. That could be quite a large number.

Further advice on constructing and running sophisticated monitoring setups is available from a variety of sources, but one particularly good source is [_Observability Engineering_](https://oreil.ly/WFkOm) by Charity Majors et al. (O’Reilly, 2022).

**Fairness in monitoring**

In this chapter, we have covered important metrics for monitoring the service health, model efficacy, and data integrity of the model. However, the topic of monitoring includes other things to consider that we will not be able to cover in this chapter. For example, our deployed models, almost by definition, have real-world effects: deciding who gets a loan, who gets a job, or what purchasing decisions we will make. In the increasingly automated world of such decision making, it is critical we do not codify systemic bias and discrimination through the models. Fairness is obviously a critical topic and is covered in depth in [Chapter 6](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#fairnesscomma\_privacycomma\_and\_ethical). From a monitoring point of view, our major concern is the requirement to facilitate such visibility into model decisions, while not facilitating inappropriate visibility—see the next section for more detail.

**Privacy in monitoring**

A special case of fairness is the question of privacy in monitoring. Almost by default, production monitoring dashboards will display the information that is fed to them. You can therefore understand immediately that if your monitoring stack exposes PII or other sensitive information, that will go right to the dashboard and be viewable to all who can access it at the dashboard level—even if they can’t access it at the production level. This is clearly a violation of privacy expectations, if not some privacy-concerned legal frameworks themselves (though we are not lawyers).

Though every situation has nuances, in general we recommend that if you can’t tolerate the risk of PII escaping, you _either_ tighten dashboard access (and every intermediate point where that information is collected), or you control what happens to the PII on egress from the first point of aggregation—for example, names, addresses, and so on, are fully anonymized and the system has had a thorough privacy review. In general, though controlling access to a dashboard is simpler and easier, it’s really not a strong control point. You almost certainly want (need!) to PII-proof the data at the first point of egress from the monitoring system, or even do it in a staged way, where different kinds of data can be processed or stripped at different ingress points to different systems—a multistage filtering approach. Again, see [Chapter 6](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch06.html#fairnesscomma\_privacycomma\_and\_ethical) for a much more complete treatment of this important issue.

**Business impact**

Another important category we don’t cover is relating the model performance metrics to the business impact of the model. We measure model performance with statistical metrics such as AUC or log loss, but those don’t incorporate the concrete impact that the business has experienced when the metrics degrade. The more representative the metrics are, the easier it will be to make that connection—choose carefully\![22](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn118)

**Dense data types (image, video, text documents, audio, and so on)**

Another important category we won’t be covering is monitoring for dense data types, also sometimes called _unstructured data_—though since these formats are, in fact, highly structured, this is a bit of a misnomer! As ML increasingly uses images, video, and so on as inputs into its models, it’s necessary to monitor data integrity for these nontabular data types too. There aren’t commonly available approaches today, and we call on the industry to actively work toward supporting this. One growing approach is to monitor the _embedding_ outputs of the data itself. ML practitioners use embeddings to map these items (e.g., movies, images) to lower-dimensional vectors where similar items are closer together. Monitoring these lower-dimensional vectors provides a proxy to monitoring dense data types.

### High-Level Recommendations for Monitoring Strategy

[“Serving”](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#serving-id0000040) made many detailed recommendations for how to start monitoring, but we’d like to cover a few high-level recommendations for your overall monitoring strategy here:

Actuals or notIf your model is able to get back actuals in a near real-time way, monitoring model/KPI performance is the best signal, since that corresponds most closely to your concept of what the model is doing. If you don’t have reasonable actuals, you’ll have to build a picture of what’s happening from a set of partial sources, including infrastructural elements as well as model performance. Look at the “Generic recommendations” item toward the end of this list for hints on how to do that.Model performance metricsIn general, you are best served by exposing the model performance statistics we talked about in [“Model”](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#model). Doing this is strongly dependent on your local monitoring situation, ML platform usage, and so on, so we can’t tell you how to do this in detail, but at the very least you should be tracking them. There’s also some value in looking at how the model performance is working at a pure service level. Think of it as a simple request-response service; if the model is unable to make predictions/recommendations, if the model produces predictions with a drastically lower confidence, and if any algorithmic fallback path is being invoked more than expected, these are all good things to know (and you should therefore monitor them).Data concerns (drift)It is vital to track the distribution of input data on an ongoing basis, so computing the various measures of distribution (PSI, KL divergence, Wasserstein distance, etc.) and also surfacing that in your monitoring system is vital. (The distribution of your _output_ data—i.e., the predictions themselves, reconciled with reality—is covered previously.)Data concerns (quality)Track missing data, type mismatches, data corpus size, and related attributes as discussed previously in this chapter. Some of the recommendations from the training section are also hugely relevant in the serving context too—not the duration of training itself, of course, but everything about the availability of data.Service or infrastructure performanceAs a first approximation—i.e., you need a place to start—you can treat this as any serving system. From [_Site Reliability Engineering_](https://oreil.ly/Y1geI), there are four _golden signals_ you can use to look at the high-level state of your serving system: latency, traffic, errors, and saturation. If you establish good levels for these—via, for example, [Chapter 8 of _Implementing Service Level Objectives_](https://oreil.ly/efwiE)—it’s quite hard for something to go seriously wrong in infrastructure and _not_ have it be reflected in those numbers. In some sense, “goodput” is what you’re looking for: questions are arriving at a reasonable rate, and getting answered correctly and quickly enough for the needs of the application.Alerting and SLOs

We haven’t said much specifically about alerting in this chapter, since in general, once you’re monitoring a metric and have established some kind of threshold, alerting is (supposed to be) a fairly mechanical achievement. But it’s important to pay special attention to the questions of deciding what the right metrics to alert on are—definitely not every metric—and what makes for a good threshold. Otherwise, everyone gets drowned in alerts, sucking away valuable cognitive attention, almost all of which are basically irrelevant to the customer experience. For more on this, a good place to start might be [Chapter 8 of _Implementing Service Level Objectives_](https://oreil.ly/Ruf03).

Though many of the conceptual underpinnings of request/response systems apply to ML serving systems, training is different, being primarily either batch or streaming oriented. One potential problem you’ll often come across in monitoring training is the question of how to alert for model building being late. The major challenge here is that since the model building duration can be a large number of minutes, hours, or even days, it doesn’t make much sense to alert on every fluctuation in training performance; otherwise, you’ll be alerting too much.

The quantity that becomes important therefore is _catch-up time_. If your model takes roughly (say) 20 hours to build, and you have a requirement for a new model every 24 hours, what happens if you get 8 hours through building and then stall for 4 hours? Having consumed 12 wall-clock hours, with 12 wall-clock hours to go, if you resume training at your previous rate, you will have 12 hours of learning before you’re finished. So you may already have blown your deadline, and predicting that 12 hours early would be a significant optimization. Therefore maintaining a notion of how long it would take to catch up, and alerting when current time + catch-up time is greater than threshold will bring attention to potentially stuck situations usefully quickly.

Generic recommendationsAn important monitoring and/or debugging technique is to log predictions (ID, value) to a table, or some otherwise easily searchable format—if you’re concerned about speed, head-of-line blocking, and so on, then sampling is also a perfectly tractable approach (though you will then lose guaranteed explainability). For extra points, have a column for the _actual_ value, as opposed to the prediction, so you can backfill and compare. In the world of microservices and/or distributed tracing, it’s also useful to get the client consumer of a prediction to log the prediction ID as well, so you have a chain of joinable events. (Sometimes it’s OK to kick away this kind of scaffolding in the process of moving from development to prediction, but it’s generally handy in both contexts.)Explainability during servingThe gold standard for explainability during serving is what’s called _individual inference level explainability_—in other words, why was this specific transaction granted, or denied, or what led to the classifier making that particular decision in this particular context? As per the preceding “Generic recommendations” paragraph, success here really looks like being able to tie the specific state of the model at the time of specific prediction request with a specific state. In some industries, such as lending, models that predict whether to give an individual a loan may use explainability to surface why an individual was rejected. This is often communicated to the downstream users via reason codes.

## Conclusion

We hope this chapter has provided a useful overview to monitoring your ML systems from birth to happy life in production. To reiterate: the main battle is to realize that you need to monitor at as high a level of fidelity as you can—for explainability, production debugging, and just generally knowing how your business is doing. Once you accept that, you can choose from multiple approaches to implementation, and aggregating the various “Concrete recommendations” sections of this chapter should be of use to you.

[1](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn96-marker) You can’t have observability without monitoring, but you can have the reverse—coarse-level detection without any ability to inspect in greater detail. This is, however, not the direction of travel of the industry.

[2](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#idm45447339809648-marker) Note that this use of “labeled” is distinct from the use of the term “labeled data” in supervised ML. Here, the labels are more like arbitrary key-value pairs associated with a time series.

[3](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn97-marker) Of course, you don’t get this level of detail for free. Product developers have to write code to maintain labeled metrics and export them correctly, and you need a system capable of analysis and display. But it’s worth it.

[4](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn98-marker) _Discovered_ is maybe not quite the right word. For more details, see the Digital Realty blog post [“The Cost of Latency”](https://oreil.ly/qawrq) by Farhan Khan, or the 2018 Zalando study [“Loading Time Matters”](https://oreil.ly/UCN69) by Christoph Luetke Schelhowe et al.

[5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn99-marker) CI/CD is too complicated to describe in detail here but basically means delivering software in a continuous, reliable stream. In addition, to be clear, just because we know what works under certain circumstances doesn’t mean that the industry as a whole does it consistently—just that we have good, evidence-based reasons to believe it.

[6](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn100-marker) Indeed, we often see that modeling KPIs are actively hard to link to business KPIs, and teams can end up doing a series of A/B tests geared not toward safe rollout, but toward understanding the degree of coupling between online business metrics and offline modeling metrics.

[7](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn101-marker) We see an industry trend toward SHAP as the gold standard for model interpretability; LIME is also popular. See [“Idea Behind LIME and SHAP”](https://oreil.ly/yd9zo) by Ashutosh Nayak for more details on SHAP and LIME.

[8](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn102-marker) In the annals of reliability literature, _roll-forward_ is generally considered quite risky and to be seriously considered only when there is genuinely no possibility of a safe and clean rollback. Change is change, and even going back to an older version of a model represents some risk, but doing so may tend to minimize that risk by reducing the number of _untested_ changes.

[9](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn103-marker) Be warned that sometimes if you press the retraining button, things get worse. You could press it at a time when a radically different model might be built than only a short time ago and thereby produce something equally broken. Think of the difference between training over December 24 and December 26 in the Anglosphere, for example. There’s also no guarantee the new version is better, so unless your validation process is significantly automated, you won’t just pay the CPU cost; you’ll pay staff time costs as well.

[10](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn104-marker) Andrej Karpathy’s blog on the software development lifecycle (SDLC) in an [ML context](https://oreil.ly/dtqF7) and the Google Cloud pages on [continuous ML](https://oreil.ly/IKU5a) are also very much worth reading in this context.

[11](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn105-marker) For example, using week-on-week to avoid comparing (say) a Saturday to a Friday, which often have very different shopping patterns. In general, being aware of these patterns is useful for monitoring work.

[12](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn106-marker) [Chapter 5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch05.html#evaluating\_model\_validity\_and\_quality) covers this topic in more detail, but it is necessary to repeat some of the elements, and emphasize a slightly different set, here.

[13](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn107-marker) Of course, that raises the possibility that, given performance may be a multidimensional assessment, there might be no clear winner. In that case, there are good arguments for doing two opposite things: (1) preferring the model currently running on the basis that it changes the least and is well understood, and (2) preferring a new model on the basis that something built over more recent data is probably going to survive change better and will be less painful to transition from. Only you can judge which is best for your circumstances.

[14](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn108-marker) IaC is the act of managing computer infrastructure as if it was code—which is to say, with statements in files, version control, release processes, and so on.

[15](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn110-marker) For those with a search engine background, by way of an example, if you search for a particular term, _precision_ indicates the number of documents you retrieve that contain the term in a relevant way, divided by the total number of documents retrieved, and _recall_ indicates the number of relevant documents retrieved, divided by the total number of relevant documents. So _high precision_ means you don’t give the user irrelevant documents, and _high recall_ means you give them almost every relevant document. The converses are, alas, both easy to intuit the definition of, and easy to generate.

[16](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn111-marker) Much more detail is available on RMSE in many other places; the [C3 AI Glossary page](https://oreil.ly/KqpdH) is probably a good place to start.

[17](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn112-marker) The engineering challenge here is how to join these sets of data, which can often be done with session IDs, user journey tokens, or something similar.

[18](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn113-marker) Though YarnIt has been experimenting with drop-ship capabilities for delivering its products, the feedback loop is still limited by the physical delivery cycle and product review time.

[19](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn114-marker) The best model metric to use primarily depends on the type of model and the distribution of the data it’s predicting over. This should typically match up with the metrics used in training/validation.

[20](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn116-marker) This strengthens the argument for your application keeping distribution state, or at least bucket counters. Other techniques for robustness actually use _dropping_ data in order to improve robustness (which is really a synonym for avoiding overfitting).

[21](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn117-marker) See [“The Tail at Scale”](https://research.google/pubs/pub40801) by Jeffrey Dean and Luiz Andre Barroso.

[22](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#ch01fn118-marker) The business impact of a model is ultimately the only reason it exists. Every other metric is secondary to the purpose for which the model was built. Therefore, direct monitoring of business impact is the holy grail of monitoring, however difficult it might be. Determining the correlation between available monitoring metrics and business value is almost certainly the most important thing for the organization to do in order to extract the most value out of its ML efforts in production.
