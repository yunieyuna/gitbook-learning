# Chapter 10. Continuous ML

Up until now, our discussions of ML systems have sometimes centered on the idea that a model is something we train and then deploy, almost as though this is something that happens once and only once. A slightly deeper view is to draw a distinction between models that are trained once and deployed versus those that are trained in a more continuous fashion, which we will refer to as _continuous ML systems_. Typical continuous ML systems receive new data in a streaming or periodic batch fashion and use this to trigger training an updated model version that is pushed to serving.

Clearly, there are major differences from an MLOps perspective between a model that is trained once versus a model that is updated in a continuous manner. Moving to continuous ML raises the stakes for automated verification. It introduces the potential for headaches around feedback loops and model reactions to changes in the external world. Managing continuous data streams, responding to model failures and corruptions, and even seemingly trivial tasks like introducing new features for the model to train on all increase system complexity.

Indeed, on the surface, it may seem like creating a continuous ML system might be a terrible idea. After all, in doing so, we expose our system to a set of changes that is inherently unknowable in advance because of potential changes in the real external world, and thus may cause unexpected or undesired system behavior. If we remember that in ML, data is code, the idea of continuous ML is to accept the equivalent of a steady stream of new code that can change the behavior of our production system. The only reason to do this is when the benefits of a continuously updating system outweigh the costs. In many cases, the benefits are indeed considerable, because having a system that can learn and adapt to new emerging trends in the world can enable a level of overall system quality that could help us achieve complex business and product goals.

The goal of this chapter is to examine the areas where costs can accrue and problems may arise, with the aim of keeping the overall cost of continuous ML systems as low as possible while retaining the benefits. These include the following observations:

* External world events may influence our systems.
* Models may influence their own future training data through feedback loops.
* Temporal effects can arise at several timescales.
* Crisis response must be done in real time.
* New launches require staged ramp-ups and stable baselines.
* Models must be managed rather than shipped.

Each of these points summarizes a range of underlying complexities, which we will dive into in the bulk of this chapter. The technical challenges are not the end of the story, of course. In addition to the practical and technical challenges that continuous ML introduces into our ML development and deployment processes, it also creates organizational opportunities and complexities.

**TIP**

Models that are continuously improved need organizations capable of managing that continuous improvement.

We need frameworks for generating and tracking ideas for improvements to the model. We need a way to evaluate the performance of various versions of our models over long periods of time rather than focusing sharply on the point of time when we launch one model to replace another. We need to think of modeling as a long-lived, value-generating program that has costs and risks but also huge potential benefits, and discuss the organizational implications of these needs at the end of this chapter.

## Anatomy of a Continuous ML System

Before we look in detail at the implications of continuous ML systems, let’s take some time to take a pass through the typical ML production stack and see how things change in the continuous setting compared to the noncontinuous setting. At a high level, a continuous ML system regularly takes data in from the world in a steady stream, uses it to update the model, and then after appropriate validation, pushes out an updated version of the model to serve new data.

### Training Examples

Rather than existing as a fixed set of immutable data, training data in a continuous ML system comes in a steady stream. This might include things like sets of recommended products from a set of possible yarn products, along with the query that generated these recommendations. In high-volume applications, the stream of training examples may resemble a firehose, with significant amounts of data being collected every second from all regions of the globe. Significant data engineering can be required to ensure that this stream of data is processed effectively and reliably.

### Training Labels

The training labels for our examples also come from the world in a stream. Interestingly, the source of this stream may well be distinct from the stream of training examples itself. For example, say we wish to use whether the user purchased a given yarn product as the training label. We know which products a user is shown at query time, and can log those as they are sent to the user. However, purchase behavior cannot be known at query time—we have to wait for some time to see if they choose to buy—and this information may come from a purchase-handling system that resides in a completely different part of the overall service infrastructure. In other settings, we may see delays in training labels when those are provided by human experts.

Joining together the examples with their correct labels thus requires unavoidable delay, and likely involves some relatively sophisticated infrastructure to process efficiently and reliably. Indeed, this joining is production critical. Just imagine the headache that would be caused if the label information was unavailable because of an outage, and unlabeled examples were sent to the model for training[1](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch10.html#idm45447338049648).

### Filtering Out Bad Data

Whenever we allow our models to learn directly from behavior in the world, we run the risk that the world will send along behaviors that we wish our model did not have to learn from. For example, spammers or scammers may try to interfere with our yarn product prediction model by issuing many fake queries without purchasing in an attempt to make it appear that some products are desired less by users than they actually are. Or some bad actors may try to make our helpful _yarnit.ai_ chatbot learn rude behavior by entering offensive text repeatedly into the chat window. These attacks must be detected and dealt with. Less malicious but equally damaging forms of bad data may be caused by pipeline outages or bugs. In all cases, it is important to remove such forms of bad data from the pipeline before training, so that the model training is not impacted.

Effective removal of spammy or corrupted data is a difficult task. It requires automated methods for anomaly detection, and models whose primary purpose is to detect bad data. Often an arms race of sorts arises between the bad actors trying to inappropriately influence the model and ops teams trying to detect these attempts and filter them out. Effective organizations often have fully dedicated teams devoted just to the problem of filtering out bad data from a continuous ML pipeline.

### Feature Stores and Data Management

In typical production ML systems, raw data is converted into features, which in addition to being useful for learning are also more compact for storage. Many production systems use a _feature store_ for storing data in this way, which is essentially an augmented database that manages input streams, knows how to convert raw data into features, stores them efficiently, allows for sharing among projects, and supports both model training and serving.[2](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch10.html#idm45447338069824) For high-volume applications, it is often necessary to do some amount of sampling from the overall stream of data to reduce storage and processing costs. In many cases, this sampling will not be uniform. For example, we might wish to keep all of the (rare) positives but only a fraction of the (very common) negatives, which means we will need to track these sampling biases and incorporate them with appropriate weighting into training.

Even though the featurized data is more compact and generally more useful, it will almost always be necessary to also keep some amount of logged raw data as well. This is important to have, both for developing new features and for testing and verifying correctness of the feature extraction and transformation code paths.

### Updating the Model

In continuous ML systems, it is often preferable to use a training methodology that allows for incremental updates. Training methods based on stochastic gradient descent (SGD) can be used without any modifications in continuous settings. (Recall that SGD forms the basis of most deep learning training platforms.) To use SGD in a continuous setting, we essentially just close our eyes and pretend that the stream of data shown to the model comes in a stochastic (random) order. If our data stream is actually in a relatively shuffled order, this would be totally fine.

In reality, a stream of data often has time-based correlations that are not really random, and then we have to worry about how much breaking this assumption hurts us in practice. The absolute worst way that our data could be nonrandom is if we had an upstream batch-processing job that ordered, say, all the positives to come in one batch, and then all the negatives to come in another. SGD approaches would fail miserably on data like this, and we would need to create intermediate shuffling of the data to help put SGD on safer, more randomized ground.

Some pipelines enforce a strict policy of training on a given example only once, in the order that it appeared temporally in the stream. This policy simplifies many things, both from an infrastructure standpoint and from a model analysis and repeatability standpoint, and when data is plentiful has no real drawbacks. However, more data-starved applications may need to visit individual examples many times to converge to a good model, so this strategy of visiting each example exactly once in order cannot always be followed.

### Pushing Updated Models to Serving

In most continuous ML settings, we refer to major changes to a model as a _launch_. Major changes might include changes to a model architecture, the addition or removal of certain features, a change in hyperparameter settings like learning rates, or other changes that would motivate us to fully reevaluate the performance of a model before launching it as our production model. Minor changes such as small modifications to internal model weights based on new incoming data are referred to as _updates_.

As the model is updated, we will periodically write out checkpoints saving the current state of the model. These checkpoints are then pushed out to serving, but they are also important for disaster recovery. One way to think about pushing a new model checkpoint out to serving is that it is actually a small, automated model launch, and if we push a new checkpoint four times an hour, then we are doing almost a hundred small, automated model launches a day.

All of the things that can go wrong with a major model launch can also go wrong when we push a new checkpoint to serving. The model may be corrupted somehow—perhaps by bugs, perhaps by having been trained on bad data. The model file itself may be flawed, perhaps corrupted by write errors, hardware bugs, or even (yes, really) cosmic rays. If the model is a deep learning model, there is a chance that in the most recent training step it has “exploded” and the internal model parameters contain `NaN`s, or that we run into the vanishing gradients problem, effectively halting further learning. If our checkpointing and pushing process is automated, there may be bugs in that system. If our checkpointing and pushing process is not automated, and relies instead on manual effort, then the system is probably not ready to be run in a continuous mode.

Of course, lots of things can go wrong with our system if we do _not_ regularly push model updates based on new data, so the point is not to avoid updating models, but rather to point out that validation of model checkpoints is a critical step before they are pushed to serving. Typical strategies use staged validation. First we use tests that can be performed offline without impacting the production system, such as loading the checkpoint and scoring a set of golden set data in a sandbox environment. All of the offline evaluation methods discussed in [Chapter 5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch05.html#evaluating\_model\_validity\_and\_quality) apply here. Then we load the new checkpoint into a _canary_—a single instance that we observe carefully to see if it fails—and allow it to serve a tiny amount of traffic, and then as long as monitoring holds, we slowly ramp up the amount of traffic served by the updated version until it is finally serving 100% of the data.

## Observations About Continuous ML Systems

Now that you’ve gotten a bit familiar with the ways that continuous ML pipelines can differ from their noncontinuous cousins, we can dive into some of their unique characteristics and challenges.

### External World Events May Influence Our Systems

When we look at the API for a widely used class or object, like a `vector` from the C++ standard template library or a Python `dictionary`, they typically do not include a stark line of documentation that reads, “WARNING: behavior undefined during the World Cup.” Thank goodness they don’t, and don’t have to.

In contrast, continuous ML systems have—or should have—exactly this form of warning. It could read something like this:

**WARNING**

Any change in input distributions to the model in production may cause erratic or unpredictable system behavior, because the theoretical guarantees for most ML models really hold for only the IID setting.[3](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch10.html#ch01fn119)

The sources of such changes may be incredibly varied and unexpected. Sporting events, election nights, natural disasters, daylight savings, bad weather, good weather, traffic accidents, network outages, pandemics, new product releases—all of these are potential sources of changes to our data streams, and thus to our system behavior. In all cases, we are likely to have little or no warning about the events themselves, although monitoring strategies described in [Chapter 9](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#monitoring\_and\_observability\_for\_models) might help you to think about what’s required for prompt alerts.

What sorts of things can happen from an external event? Here’s an example. For our yarn store, let’s imagine the impact of having a major political figure appear on national television on a frigid day wearing hand-knit brown woolen mittens. Searches and purchases for “brown wool” spike suddenly. After a short delay, the model is updated on this new influx of search and purchase data, and learns to predict much higher values for brown wool products. Our model is trained with a form of SGD, which ends up overconfident and making scores extremely high for these products. Because of the sudden high scores, these products are shown to nearly all users, and the available stock is rapidly sold out. Once all stock is sold out, no more purchases are made, but nearly all searches are still showing brown wool products because of their high score from our model.

The next influx of data shows that no users are purchasing any products, and the model overcompensates, but because the `brown_wool` products have been shown on such a broad range of queries to such a broad range of users, the model now learns to give lower scores for nearly all products, resulting in no results or junk results for all user queries. This reinforces the trend that no users are purchasing anything, and the system spirals down, until our MLOps team identifies the issue, rolls the model back to a previous well-behaved version, and filters the abnormal data from our store of training data before re-enabling training.

Clearly, this example can be addressed with potential fixes and monitoring at multiple levels, but it illustrates the way that system dynamics can have consequences that are difficult to anticipate in advance.

One subtle danger of knowing that a wide variety of world events can cause unexpected system behavior is that we can end up explaining away changes in observed metrics or monitoring too quickly. Knowing that Argentina and Brazil are playing an important soccer (football) match today may cause us to assume that this is the root cause for an observed system instability, and miss rooting out a pipeline error or other system bug.

What would it look like to have a continuous ML system that was completely robust to distribution shifts? Basically, we would need to have a way to adaptively weight the training data so that its distribution did not depend on changes in the world. One way to do this would be to create a model to do _propensity scoring_, which shows how likely a given example is to occur in our training data at a given time. We would then need to weigh our training data by the inverse of this propensity score, so that rare examples are given greater weight. The propensity scores would need to be updated quickly enough that when a world event caused some examples to suddenly be much more likely than in the past, they were down-weighted accordingly. Most importantly, we would need to make sure that all examples had nonzero propensity scores to avoid divide-by-zero issues when doing inverse propensity scoring.

We need to avoid propensity scores that are too small, so that the inverse propensity weighting does not get blown up by a small number of examples with huge weights. This could be done by capping the weights, except that we would also need these scores to be statistically unbiased, and capping weights would cause bias. Instead, we could use extensive randomization to ensure that no example has too low a probability of being included, but this may well mean exposing users to random or irrelevant data or having our models suggest random actions that may be undesirable. All in, achieving a setup like this is possible in theory but extremely difficult in practice.

**TIP**

The reality is that we are likely to have to find ways to manage instability due to distribution shifts, rather than completely solve them.

### Models Can Influence Their Own Training Data

One of the most important questions to answer for our continuous ML system is whether a _feedback loop_ exists between a model and its training data. Clearly, all trained models are influenced by the stream of data that comes in for training, but some models also, in turn, influence the data that is collected.

To help understand the issues, consider that some model systems have no influence over the stream of data that is collected for retraining. Weather prediction models are a good example. No matter what the weather station might like us to believe, a prediction that tomorrow will be a nice sunny day has no actual influence on atmospheric conditions. Such systems are clean in the sense that they do not have feedback loops, and we can make changes to models without fearing that we might impact tomorrow’s actual chance of rain.

Other models do influence the collection of their training data, especially when those models make recommendations to users or decide on actions that impact what they can learn about next. These create implicit feedback loops, and as anyone who has heard screeching feedback from a microphone knows, feedback loops can create unexpected and detrimental effects.

As an easy case, some systems that rely on feedback loops to learn about new trends might miss them entirely if they never have a mechanism that allows them to try a new thing in the first place. We might reflect on the experience of trying to get a child to try a new food for the first time as a way to think about this effect. As a more concrete example, consider a model that helps recommend wool products to show to users; the model may then be trained on the user response to those selected products, such as clicks, page views, or purchases. The model will receive feedback about the products that were selected to be shown to the user, but will _not_ get feedback about products that were not selected.[4](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch10.html#ch01fn120) It is easy to imagine that a new wool product, such as a new color of organic alpaca yarn, might be something that users would love to purchase, but for which the model has no previous data. In this case, the model might continue recommending previous nonorganic products and be oblivious to its omission.

Not discovering new things is bad, but even worse behaviors can happen. Imagine a stock market prediction model that is in charge of picking stocks to buy and sell based on market data. If an external entity mistakenly makes a large sale, a model might observe this and predict that the market is about to go down and most holdings should also be sold. If this sale is large enough, this will drive down the market, which may make the model even more aggressive about wanting to sell. Interestingly, other models—potentially from completely disjointed organizations—may see this signal in the market and also decide to sell, creating a reinforcing feedback loop that creates an overall market crash.

While the stock prediction scenario is an extreme case, it did, in fact, happen.[5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch10.html#ch01fn121) However, we don’t need to exist in a broad market of competing models to experience these effects. For example, imagine that in our _yarnit.ai_ store, we have one model that is in charge of recommending products to users, and one model that is in charge of determining when the user should be given discounts or coupons. If the product recommendation relies on purchase behavior as a signal in training, and the presence of discounts influences purchase behavior, then there is a feedback loop that links these two models, and changes or updates to one model can influence the other.

Feedback loops are another form of distribution shift, and the propensity-weighting approaches we’ve described can be helpful, although they are difficult to get completely right. The effect of feedback loops can be lessened to some degree by logging model version information along with other training data, and using this information as a feature in the model. This at least gives our model the opportunity to disambiguate whether a sudden change in observed data is due to a change in the real world—such as the holidays are over, and nobody wants to buy wool (or indeed much of anything) in early January—or a change in the model’s learned state.

### Temporal Effects Can Arise at Several Timescales

We create continuous ML systems when we care about the ways that data changes over time. Some of these changes are deeply meaningful to the underlying product needs, like the introduction of a new form of synthetic wool or the creation of automated knitting machines suitable for home use. Incorporating data about these emergent trends as soon as possible would be important for our _yarnit.ai_ store.

Other temporal effects are cyclical, with cycles occurring over at least three major timescales:

SeasonalMany continuous ML systems experience profound seasonal effects. For online commerce sites like _yarnit.ai_, dramatic changes and increases in purchasing behavior can occur as the winter holidays approach, followed by a sudden drop in early January. Warm weather months and cool weather months may have very different trends—and may also vary significantly by geographic region or even by Northern versus Southern Hemisphere. The most effective way to deal with seasonality effects is to make sure that our model has trained on data from more than one full year in the past—if we are fortunate enough to have it—and that time-of-year information is included as a feature signal in the training data.WeeklyJust as data can vary by season, it can also cycle on a weekly basis, based on day of week. Weekend days may have significantly more usage for some cases, like the portion of our _yarnit.ai_ store targeted to hobbyists—or significantly less for others, like the portion of the yarn store that targets business-to-business sales. Locality is strongly tied in here, as weekend days may differ by country, and time-zone effects also matter strongly, as it may be Monday in Tokyo while it is still Sunday in San Francisco.DailyThings start to get nontrivial when we look at the daily effects, based on time of day. At first glance, it is obvious that many systems will experience different data at different times of day—midnight behavior is likely different than early morning, or during the work day. It is also likely clear that locality is crucial here, because of time-zone effects.

The subtlety for daily cycles comes in when we consider that most continuous ML systems actually run continuously behind reality, because of the inherent need for delays in pipelines and data streams waiting for training labels—such as click or purchase behavior that may take some time for a user to decide on—as well as filtering out bad data, updating models, validating checkpoints, and pushing checkpoints to serving and ramping them up fully. Indeed, such delays could add up to 6 or even 12 hours. Therefore, our models may be operating very much out of phase with reality, serving a version of the model that thinks it is the middle of the night when, in fact, it is in the heart of the workday.

Fortunately, fixing such issues is relatively easy, by logging time-of-day information along with other training signals and using these as inputs to our models. But it highlights the importance of thinking through ways that the version of the model we happen to have loaded in serving may be stale or otherwise misinformed about the actual reality it is asked to work with at that moment.

### Emergency Response Must Be Done in Real Time

By this point, it should be clear that while continuous ML systems can provide great value, they also have a broad range of system-level vulnerabilities. It can be argued that a continuous ML system inherits all of the production vulnerabilities of large, complex software systems—which carry plenty of reliability issues on their own—and adds in a whole set of additional issues that can produce undefined or unreliable behavior. In most settings, we cannot rely on theoretical guarantees or proofs of correctness.

When such issues arise in continuous ML systems, they not only need to be fixed, but also need to be fixed (or mitigated) in real time. There are several reasons for this. First, our models are likely mission critical, and having a production fire that impacts our ability to serve useful predictions from our models may impact our organization minute to minute. Second, our model may have a feedback loop with itself, meaning that if we do not address issues quickly, the stream of input data may also be corrupted and require care to fix as well. Third, our model may be part of a larger ecosystem that is difficult to reset to a known state. This can occur when our model is in a feedback loop with other models, or when poor predictions from a model create lasting harm and are difficult to undo, such as an otherwise helpful _yarnit.ai_ chatbot suddenly issuing rude curses at users.

Real-time crisis response requires first detecting issues quickly, which means that from an organizational standpoint, a good litmus test for determining whether we are ready for continuous ML is to examine the thoroughness and timeliness of our monitoring and alerting strategies. It can take time for the full effect of data imperfections to create downstream detrimental effects, because of pipeline delays and slow changes in systems that learn gradually on new data. This makes it especially important to have simple canary metrics that alert on changes to input distributions, rather than waiting for model outputs to change.

Monitoring or altering strategies will be helpful in real time only if they are supported by folks who have the responsibility to respond when the alerts fire. Well-functioning MLOps organizations set up specific service-level agreements about how quickly an alert must be responded to, and set up mechanisms like pager rotations to ensure that the alert is seen by the right person at the right time. Having a global team with teammates in several time zones, should you be in that fortunate position, can help tremendously with avoiding the need for folks to be woken up by an alert at 3 A.M.

Once we receive an alert, we need to have a well-documented playbook of responses that can be carried out by any member of the MLOps team, along with an escalation path for further action.

**TIP**

For continuous ML systems, we have a set of basic immediate responses to any given crisis. These are stop training, fall back, roll back, remove bad data, and roll through.

Not every crisis needs all of these steps, and the choice of which response is most appropriate depends on the severity of the issue and the speed with which we are able to diagnose and cure the root-cause issue. Let’s look at each of these basic crisis response steps for continuous ML systems and then discuss factors for choosing a response strategy.

#### Stop training

It has been said that the First Rule of Holes is this: _when you find yourself in a hole, stop digging_. Similarly, when we find that our data stream is corrupted in some way, perhaps by a bad model, or an outage, or a code-level bug somewhere in the system, a useful response can be to stop model training and halt pushing any new model versions out to serving. This is a short-term response that at least helps ensure that problems will not get worse while we decide on a mitigation or fix. It makes sense to ensure that there is an easy way for MLOps folks to stop training on any model that is their responsibility. Automated systems are helpful here, but of course need to alert sufficiently so we do not discover that a model has silently stopped training three weeks ago.

**TIP**

It is always useful to have the equivalent of a Big Red Button that can be used to stop training manually in a detected emergency.

#### Fall back

In continuous ML systems, it is important to have a fallback strategy that can be used in place of our production model that provides acceptable (even if nonoptimal) results. This could be a far simpler model that does not train in a continuous fashion, or a lookup table of the most common responses, or even just a small function that returns the median prediction to all queries. The key thing is that if our continuous ML system encounters sudden massive failures—what we might describe as “being on fire”—we have an ultra-reliable method that can be used as a temporary replacement without the larger product becoming completely unusable. Fallback strategies are typically less reliable in overall performance than our main model (otherwise, we would not use an ML model in the first place), so fallback strategies are very much intended to be short-term responses that allow for emergency responses to take place in other parts of the system.

#### Roll back

If our continuous ML system is currently in a crisis state, it makes sense to revert the system to a state from before the crisis and see if everything is OK. The root cause of our crisis may have come in through two basic areas: bad code or bad data.

If we believe that the root cause of our issue is bad code, from a recently introduced bug, then rolling back our production binaries to use a previously known-good version may fix the issue in the short term. Of course, any rollback to a previous production binary must be done in a staged ramp-up in case any new compatibility issues or other flaws exist that make the old version of the binary no longer usable. At any rate, it is important to keep on hand a set of fully compiled previous binaries so that rollback can be done quickly and efficiently when needed.

If we believe that the root cause of our issue is bad data that has caused the model to train itself into a bad state, it makes sense to roll back the model version to a previously known-good version. Again, it is important to keep checkpoints of our trained production model on hand so that we have a set of previous versions to choose from. For example, imagine that Black Friday sales in the US cause such a large increase in purchase requests from users to our _yarnit.ai_ store that the fraud detection portion of the system starts to label all purchases as invalid, making it look to our model as though all products are extremely unlikely to be purchased. Rolling back to a version of the model that was checkpointed a week before the Black Friday date would at least allow the model to serve reasonable predictions while the rest of the larger system was fixed.

#### Remove bad data

When we have bad data in our system, we need to have an easy way to remove it so that our model will not be corrupted by it. In the preceding example, we would want to remove the data that was corrupted by the faulty fraud detection system. Otherwise, when we re-enable training to proceed, this data will be encountered by our rolled-back model as it moves forward in time through the training data, and it will be corrupted by the bad data again. Removing bad data is a useful strategy whenever we believe that the data itself is highly unrepresentative of typical data and is unlikely to give the model useful new information, and that the root cause of the bad data is temporary, due to an external world event or to bugs in our system that can be quickly fixed.

#### Roll through

If we have stopped training for our continuous ML system, at some point we need to cross our fingers and enable it to resume training. We typically do this after bad data has been removed and we are sure that any bugs have been addressed. However, if a crisis is detected due to an external world event, sometimes the best response is to just cross our fingers and roll through it, allowing the model to train on the atypical data and then to recover itself as the world event ends. Indeed, it is unfortunately true that this world has few days with no political event, major sporting event, or other major newsworthy disaster happening somewhere, and making sure that our model has enough exposure to atypical data like this from different global regions can be an important way to ensure that our model is generally robust.

#### Choosing a response strategy

How do we choose which events to stop, roll back, and remove, and which to roll through? To answer that question, we need to observe our model’s response to similar historical events, which is most easily done when we have trained the model on historical data in sequential temporal order. Another important question to answer is whether the crisis-indicating metrics we are currently seeing are due to a bad model or to the atypical state of the world. In other words, is our model broken, or is it just being asked to handle much more difficult requests right now? One way to judge this is to observe offline metrics on golden set data, which should be recomputed for our model on a frequent periodic basis for this reason. If the model is actually corrupted, the golden set results may show a sharp decrease in performance, and rolling through is probably not the right approach.

#### Organizational considerations

When a crisis is currently going on, it can be a difficult time to learn new skills, to figure out roles within a team, or to decide on how to implement various response and mitigation strategies. Real-world firefighters regularly train together, refine best practices, and ensure that all of the infrastructure they need to respond to an alarm is in excellent condition and is ready to roll into action at a moment’s notice. Similarly, we do not know exactly when our continuous ML systems will require crisis response, but we can confidently say it will happen and that we need to be well prepared. Creating an effective crisis response team is part of the cost of creating and maintaining a continuous ML system, and must be accounted for when we move in this direction. This is discussed in more detail in [“Continuous Organizations”](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch10.html#continuous\_organizations).

### New Launches Require Staged Ramp-ups and Stable Baselines

When we have had a model running as part of our continuous ML system for a period of time, we will eventually want to launch a new version of that model that creates improvements in various ways. Maybe we want to use a larger version of the model for improved quality and now have serving capacity to handle it, or perhaps a model developer has created several new features that significantly improve predictive performance, or maybe we have discovered a more efficient model architecture that reduces serving costs in an important way. In cases like these, we need to explicitly launch the new version of the model to replace the old version.

Launching a new model most often involves some amount of uncertainty because of the limitations of offline testing and validation. As we describe in [Chapter 11](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch11.html#incident\_response), offline testing and validation can give useful guidance on whether a new version of a model is likely to perform well in production, but often cannot give a complete picture. This is especially true when our continuous ML model is part of a feedback loop, because the data that we have previously trained on was most likely chosen by a previous model version, and evaluation on offline data is limited to data that has been collected based on actions or recommendations made by that previous model. We can imagine this situation as similar to that of a student driver, who is first evaluated by sitting in the passenger seat and being asked to give their opinion about the instructor’s actions in driving the car. Just because they agree 100% with the actions of the instructor does not mean that they will not make some poor decisions when first given the opportunity to steer the car for themselves.

In this way, a new model launch requires some amount of _testing in production_ as the final form of validation. We need to give our new model the ability to demonstrate that it is capable of being in the driver’s seat. But that doesn’t mean we just hand it the keys and expect everything to be perfect. Instead, we will most often use a staged ramp-up, first allowing the model to serve only a fraction of the overall data, and increasing that amount only as we observe good performance over time. This strategy is commonly known as an _A/B test_: we test out our new model A against the performance of our old model B, in a format that resembles a controlled scientific experiment and helps verify that the new model will show the appropriate performance on our final business metrics (which may be distinct from offline evaluation metrics like accuracy).

The difference between a model launch and an ideal A/B test is that in a scientific experiment, A and B are independent and do not influence each other. For example, if we run an experiment in a scientific setting to determine whether cotton sweaters (A) keep people as warm as natural wool sweaters (B), the people wearing wool sweaters are unlikely to make the people wearing cotton ones report feeling any warmer or colder. However, if the wool sweater folks are so warm and happy that they go make tea and hot soup for the cotton sweater folks, this would definitely ruin our experiment.

For A/B experiments comparing continuous ML models, it turns out that A and B may well influence each other when our models are part of a feedback loop. For example, imagine that our new model A does a great job of recommending organic wool products to _yarnit.ai_ users, whereas our previous model B had never done so. An A/B experiment might initially show that the A model is much better in this regard, but then as training data is produced by A that includes many more organic wool recommendations and purchases, the B model (which is also continuously updating) may then also learn that these products are liked by users and begin to recommend them as well, making the two models appear identical over time. If these effects are more diffuse, it can be hard to say whether the benefits of A have disappeared because it was never actually better than B, or if B has itself improved.

We could try to fix this by restricting A and B to each train on only the data that they themselves serve. This strategy can work well when each model serves the same amount of data, such as 50% of the overall traffic each, but can make for flawed comparisons in other cases. If A is looking bad early on, is that because the model is bad, or because it has only 1% of training data while B has 99%?

Another strategy is to try to create a stable baseline of some sort, which can help serve as a reference point in comparisons so that we can figure out whether comparisons between A and B are changing because A is getting worse or because B is getting better—or indeed, if both are getting dramatically worse in lockstep. A stable baseline is a model C that is not influenced by either A or B, and is allowed to serve a certain amount of traffic so that we can use those results as a comparison. The basic idea is to then look at (A-C) against (B-C) rather than A against B directly, with the idea that this will allow us to see any changes more clearly.

The four general strategies for creating stable baselines have different advantages and disadvantages:

Fallback strategy as baselineWhen we have a reasonable fallback strategy that does not involve continuous retraining, this is useful not only for crisis response but also as a potential independent datapoint. This can work well if the quality of the fallback strategy is not too much worse than the main production model. If the difference is very large, however, statistical noise may overwhelm any comparisons between A and B using this as a reference point.Stop trainerIf we have a copy of our production model B and halt training on it, then by definition it will not be influenced by any future behavior of A or B. If we allow it to serve a small amount of traffic, this can provide a useful stable baseline C, with the caveat that the overall performance of a “stop trainer” model will degrade slowly over time. It can be useful to run independent experiments to observe how much degradation can be expected and whether this strategy will be useful.Delay trainerIf we expect our overall launch process to take, say, two weeks, then a reasonable alternative can be to run a copy of our production model that is set to update continuously, but at a two-week delay. This has the advantage over the stop trainer in that the relative performance is unlikely to degrade, with the drawback that after it has been running for a length of time equal to its delay, it will begin to become influenced by A and B and will lose its utility. Thus, a two-week delay trainer model will become useless after two weeks.Parallel universe modelAn approach that maintains strict independence from A and B but does not have a limited shelf life is a parallel universe model that is allowed to serve a small fraction of overall data and learns only on the data that it serves itself. A and B do not train on this data, keeping these data universes completely separate.

Why is this useful? Imagine that the act of putting B into production changes the overall ecosystem in some way. We could imagine stock prediction market models operating this way—perhaps pushing the overall market up or down in some special cases. In this case, both A and B might end up strongly increasing or decreasing their median predictions, but the difference A-B might be small and appear stable. Having this third point C allows us to detect whether the changes between models are isolated to differences in A and B themselves, or are due to a broader impact.

Parallel universe models often take time to stabilize after being set up because of the restricted amount of training data and the overall distribution shift. But after this initial period, they can provide a useful independent evaluation point—again, up to the limits of statistical noise when comparing evaluation metrics.

### Models Must Be Managed Rather Than Shipped

Overall, model launches require particular care because at these times our systems are most vulnerable to crisis. If we are in a model launch process with A and B both serving roughly half the traffic, we have just doubled the potential sources of error and doubled the amount of work needed to address any emergency that may arise. Like crisis response, model launches are done best when they rely on well-communicated, well-practiced processes.

Some products are like brick walls: they take a lot of planning and effort to get right, but once completed are more or less done and require only occasional maintenance. The default state is that they just work. Continuous ML systems are at the opposite end of the spectrum and require daily attention. Problems that arise with a continuous ML model may be difficult to solve in a complete or permanent way. For example, if one of our overall product issues is that we would like to do a better job of recommending wool products to users in warm climates, this is one that is likely to require a variety of approaches, and the utility of those approaches may change over time as tastes and fashions adapt season to season and year to year.

In this way, a continuous ML system is something that requires regular management. Managing a model effectively requires daily access to metrics that report the model’s predictive performance. This can be done through dashboards and related tooling that allow a model manager to understand how things look today, how trends might be changing, and where problems might be arising. Any dashboard’s utility is upper bound by the amount of attention paid to it, so there needs to be a clear owner who regularly spends time with it. A useful metaphor is that it is useful to have a cup of coffee with our model every day, just to get to know it by working with a dashboard to understand how it is doing that day. And just as a people manager provides regular performance evaluation, a model owner should provide regular reports about model performance to upper levels of the organization to share knowledge and visibility.

When we learn something useful about our model, a strong best practice is to write it down in the form of a short writeup. A writeup, which can be just a few paragraphs accompanied by a screenshot of a dashboard or similar supporting evidence, can help build organizational knowledge, and is of the most benefit when the observation is accompanied by a short summary about what it means about model behavior. Such writeups have historically proven extremely useful, both for helping guide future model development and for understanding and debugging unexpected behaviors seen during crises.

Lastly, when we do encounter a crisis, it is important that organizationally we extract as much learning from the experience as possible by creating postmortem documents that describe in detail what happened, how the issue was diagnosed, what the damage was, what mitigation strategies were applied and how successful they were, and finally recommendations for how to make improvements to either reduce recurrence of the issue or enable more effective response in the future. Creating these postmortem documents is useful both in the short term to help identify fixes, but also in the long term as a repository of organizational knowledge and experience that can be referenced over time.

## Continuous Organizations

At this point, it should be clear that an organization that is taking on a continuous ML system is committing to a long-term responsibility. Like a puppy, a continuous ML system requires daily attention, care, feeding, and training. Structuring our organizations to be well equipped to handle the responsibilities of managing a continuous ML system requires numerous structures to do well.

Determining evaluation strategies is a key leadership responsibility. Evaluation strategies allow us to assess the health and quality of our models, both in terms of long-term business goals and short-term decisions, such as whether to include or exclude a given feature from the model. As noted in [Chapter 9](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch09.html#monitoring\_and\_observability\_for\_models), it can be tempting to reduce this problem to determining metrics, but a given metric (such as revenue, clicks, precision, recall, or even latency) is meaningless without a reference point, baseline, or distribution. Decision making in continuous ML settings often requires some amount of counterfactual reasoning, thinking through the impact of feedback loops, or wrestling with noise and uncertainty that makes effective decision making challenging. We can help reduce the difficulty of these challenges to some degree by having clearly defined and documented standards and processes in place for evaluation.

Organizationally, making investment decisions is similarly challenging. How much should we invest in more compute to create a larger and potentially more powerful model, and will that investment pay for itself in terms of improved product outcomes relative to opportunity cost? How should we trade off investing in model quality improvements versus ML system-level reliability? Organizationally, how do we direct the time and effort of our constrained human experts to best benefit the overall mission? These are fundamentally hard problems, in no small part because different parts of our organization may have different or even incompatible viewpoints on priorities.

We have two main strategies available for dealing with these kinds of issues. The first is to ensure that we have organizational leadership with enough scope and context to effectively weigh the differing needs of, for example, improving model monitoring and crisis response handling with that of improving model accuracy. Ensuring that the lessons learned from each part of the organization are well communicated across the entire organization can be one way to help different parts of an organization understand one another’s challenges and pain points. This is best done by regularly sharing postmortems from incidents—and also by proactive “pre-mortem” discussions identifying potential weaknesses and failure modes. The second strategy is to invest in infrastructure that ensures that alerts and other warnings are propagated well in the full chains of producers and consumers. This can require a serious organizational commitment, but can also pay off over time in terms of reducing the human burden of verification within complex systems.

Organizationally, understanding that a continuous ML system relies on a steady stream of incoming data to determine system behavior makes clear that the data pipeline itself requires serious, dedicated oversight and management. In addition to simply ensuring that data is flowing and the pipeline is functioning well, key questions around which kinds of data to collect, how long to store data, and how our data pipelines should interact with upstream producers and downstream consumers are all critical strategic questions for organizational leaders to address. Deeper questions of privacy, ethics, data inclusivity, and fairness all also play important roles and must be part of the overall organizational strategy.

As we’ve noted, the launch process for ML systems and further improvements necessarily requires a staged ramp-up procedure in the continuous ML setting. A critical role of leadership is providing the oversight and review for the results of each stage, and making approval decisions to move to the next stage or determining that we are not yet ready to proceed or must even ramp down if things are not looking as intended. These decisions require high-level oversight because the consequences can be far reaching and interactions can occur with multiple producer or consumer systems, especially in the presence of feedback loops or other complex system-to-system interaction points. The process for assessing the wider impact of various launch stages and ensuring stability before proceeding must be well established and rigorously followed for a continuous ML organization to be effective in the long run.

Finally, when incidents or crisis moments occur, we need to have a process in place for responding effectively. For continuous ML systems, we have an advantage and a disadvantage in handling incidents. The advantage is that we almost always have a slightly (or somewhat) older version of the current model that we can roll back to in serving while we take a look at what went wrong. This can be incredibly helpful when we need a quick mitigation for something that went disastrously wrong with the current model. The disadvantage is that the entire system is constantly evolving, making it difficult to isolate root causes or breaking changes. Our model’s change might have been motivated or required by other concurrent changes in the system (such as new data added, or new integrations using the model). In this context, troubleshooting exactly what has gone wrong can be much more difficult in a continuous ML environment. For concrete examples of this, see [Chapter 11](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch11.html#incident\_response).

The most important prework for an organization running a continuous ML system is to pre-negotiate outage consequences and handling. This goes beyond just determining, in advance, who will fill which roles—although you need to do that too. ML production engineers shouldn’t be determining the urgency of resolving a particular incident while in the middle of it, and model developers shouldn’t be guessing at the costs and consequences of a given outage while it is ongoing. Each person should know their role, their authority, and their path to escalate a decision to someone else because these should be worked out in advance. But where possible, the whole organization should also have agreed upon some general service reliability standards. Examples might be the following:

* This model can be up to 12 hours old without serious consequences, even though we generally prefer that it be based on data that is no more than about 1 hour old.
* If this model has quality metrics worse than a particular known threshold, the approved fallback is to roll back to an old model.
* If the model is worse than an additional threshold that is predetermined, waking up the following business leaders is appropriate…

And so on. The general idea is to predetermine the parameters of various kinds of outages so that incident responders have the maximum ability to take action and minimum latency to a decision about how to respond. Many organizations tend not to work out these decisions in advance until they experience a few outages, but after that, it becomes quite reasonable to pre-decide what to do in the case of really bad ML problems.

## Rethinking Noncontinuous ML Systems

We have talked about a range of issues for continuous ML systems in this chapter. But in addition to giving what we hope are useful mitigation strategies for creating robust and reliable systems even in the face of the difficulties, we would like to make a broader recommendation:

> All production ML systems should be treated as continuous ML systems.

We should think about every model that is a key part of a production system as one that is trained continuously, even if it is not actually updated on new data every minute, every hour, or even every day or week.

Why would we make such a recommendation? After all, continuous ML systems are full of complexity and vectors for failure. One reason is that if we apply the standards and best practices from continuous ML systems to all production-grade ML systems, we will definitely be ensuring that our technical infrastructure, model development, and MLOps or crisis response teams are set up to meet challenges as they arise. If we assume that our ML system is a continuous ML system and plan accordingly, we will be in a good spot.

Is this overkill? If a model is trained only once, applying the standards and best practices from continuous ML may be seen as a waste of resources. But in reality, no production models are trained only once. In our experience, we have seen that every production-level ML model will eventually be retrained or have a new version launched—maybe in a few months, or next year, as new data becomes available or models are developed. This can be done in an ad hoc fashion, every few weeks or months, but this irregular approach is likely to lead to failures and oversights. Our strong recommendation is that effective MLOps teams ensure that their models are updated on a regular schedule, be it daily, weekly, or monthly, so that validation procedures and checklists can become part of the organizational culture. From this standpoint, then, the recommendations for continuous ML systems are applicable to every ML system.

## Conclusion

In this chapter, we have laid out sets of procedures and practices that can form the foundation of an organizational playbook for the ongoing care and oversight of continuous ML systems. These systems offer a remarkable range of benefits, enabling models that adapt to new data over time and allow for responsive learned systems that interact with users, marketplaces, environments, and the world.

It is obvious that any system that is both highly impactful and easily influenced requires considerable oversight. In our experience, the requirements for oversight in these cases go far beyond what can be expected of any individual, no matter how capable, and cannot be left to intuition or improvisation. Any organization managing a continuous ML system needs to think of this as an ongoing high-priority mission, with special care at times of launches or major updates, but also with continual monitoring and contingencies in place to allow fast response to emergent crises.

We covered six basic insights about continuous ML systems that we hope you will come away with:

* External world events may influence our systems.
* Models may influence their own future training data through feedback loops.
* Temporal effects can arise at several timescales.
* Crisis response must be done in real time.
* New launches require staged ramp-ups and stable baselines.
* Models must be managed rather than shipped.

And finally, we ended the chapter with the idea that all ML systems should likely best be thought of as continuous ML systems, as all models are eventually retrained, and having strong standards in place will benefit any organization in the long run.

[1](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch10.html#idm45447338049648-marker) See [“5. Ad Click Prediction: Databases Versus Reality”](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch15.html#fivedot\_ad\_click\_prediction\_databases\_v) for an example where this happens.

[2](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch10.html#idm45447338069824-marker) See [“Feature store”](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch04.html#feature\_store-id0000011) for an in-depth treatment of feature stores.

[3](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch10.html#ch01fn119-marker) In statistics and ML parlance, the IID assumption is that data is drawn independently from an identical distribution—that is, that our test sets and training sets are randomly drawn from the same sources in the same way. This is covered in much more detail in [Chapter 5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch05.html#evaluating\_model\_validity\_and\_quality).

[4](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch10.html#ch01fn120-marker) Those familiar with different subfields of ML will note that this is technically a contextual bandit setting, and thus subject to an explore versus exploit trade-off. The main idea in this setting is that when our system learns only about the things it intentionally selects, it is important to sometimes randomly change our selections to explore the world a little more and make sure we do not lock our system into a self-perpetuating loop of beliefs. Exploring too little leads to missing out on the very best; exploring too much is a waste of time and resources. Parallels to any of our own life choices are, of course, completely coincidental.

[5](https://learning.oreilly.com/library/view/reliable-machine-learning/9781098106218/ch10.html#ch01fn121-marker) ​​See [_The Flash Crash: A New Deconstruction_](https://oreil.ly/sAkYH) for an in-depth treatment of the events that lead up to the flash crash, including consideration of the triggering incident as well as the facilitating market conditions. One point to take away from this discussion is just how hard it can be to do root-cause analysis on systems that involve multiple models interacting with one another.
