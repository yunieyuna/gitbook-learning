# 2. Structuring and Delivering AI Projects

Congratulations, the board of directors wants to move forward with AI. The senior managers chose you as their AI program and project manager. Now it is your time to deliver results. You had the vision; now you can make your vision become a reality. Just do not forget: some truths apply to all projects, AI or Non-AI ones. The corporate world expects projects to deliver results cost-efficiently, timely, and reliably. Most companies cannot and do not want to pay ten PhDs and wait for eight months to get a perfect solution. Academia is patient and invests so much time and effort. Companies, not so much. As an AI project manager, your challenge is to work on a solution with two or three IT professionals and deliver the first results in six or eight weeks. With such a context in mind, this chapter provides answers for the following AI project management challenges (see Figure [2-1](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig1)):

* What are the different layers on the AI technology stack which enable companies to innovate with AI?
* How can AI project managers scope their projects to meet the expectations of their business stakeholders and sponsors?
* AI models are the core deliverable of AI projects. How do, for example, neural network or statistics-based AI models look like?
* What is a development methodology which works for AI projects? What are vital efficiency factors?
* How can you integrate AI models in an existing corporate application landscape?

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig1_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig1_HTML.png" height="179" width="1721"><figcaption><p>Figure 2-1</p></figcaption></figure>

### The Four Layers of Innovation

Managing AI projects and teams has some peculiarities. Team members tend to be more technology-affine and have a stronger computer science and math background but less experience with business problems than legacy application developers. There are fewer patterns and established toolchains known from traditional software engineering. Thus, AI project managers have to provide more directions to ensure delivering concrete business benefits. They have to challenge their data scientists constantly to prevent the latter from doing AI research and reinventing the wheel.

Structuring a technology area in layers and defining a technology stack is a long-known and widely practiced approach. When looking at the example of Java, the Java backend application runs code in Java VMs. A Java VM runs on a hypervisor, which runs on an operating system, with the operating system running on physical hardware. There are similar layers in AI (Figure [2-2](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig2)). The lowest layer is the **hardware layer**. Neural networks, for example, require many, many matrix multiplications. Regular CPUs can perform them, but there are faster hardware options: Graphical Processing Units (GPUs), Field Programmable Gate Arrays (FPGAs), or Application Specific Integrated Circuits (ASICs). Innovation on this layer comes from hardware companies such as AMD or NVIDIA. Companies buying such innovative products are server and workstation manufacturers or public cloud providers. These customer groups need hardware for their data centers that run AI-rich workloads efficiently.

The **AI frameworks layer** forms the next layer. It represents frameworks for machine learning and AI, such as TensorFlow. TensorFlow distributes the workload for training massive neural networks on large and heterogeneous server farms, thereby taking advantage of their computing power. TensorFlow abstracts for the data scientists from the underlying hardware. It parallelizes autonomously computing tasks. Data scientists can run the same code and learning algorithms on a single laptop or a cluster with hundreds of nodes with hardware tuned for AI and benefit from the cluster. Data scientists do not have to change their code for different hardware configurations, saving them a lot of work and effort.

Companies use AI frameworks to develop new AI algorithms and innovative neural network designs. Suppose a company wants to develop the next big thing after GPT-3 or a fundamentally new computer vision algorithm. In that case, their data scientists use an AI framework to engineer and test new algorithms. In other words: nearly all data scientists do not innovate on this layer but work on top of the existing frameworks.

There are only two scenarios for innovation in this layer: improving an existing framework such as TensorFlow or developing a completely new one. These activities are usually part of academic research. For companies, it makes sense only if they have an outreach to a massive number of data scientists. The latter applies to public cloud providers or AI software companies. They need innovation to provide “the best” AI framework to lure data scientists into using their AI and/or cloud platform.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig2_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig2_HTML.png" height="821" width="1304"><figcaption><p>Figure 2-2</p></figcaption></figure>

Next is the customized AI layer. Its specifics are easy to understand after the purpose of the top layer is clear, the **AI services layer** . AI services enable software engineers to incorporate AI into their solution by invoking ready-to-use AI functionality. The engineers do not need any AI or data science know-how. One example is Visua/LogoGrab. The service detects brand logos in images or video footage. It measures the success of marketing campaigns, for example, by checking how often and for how long the brand of a sponsor is on TV during a sports event. It finds counterfeits and look-alikes of brand products on the Internet and electronic marketplaces as well. LogoGrab is an example of a highly specialized AI service.

There are also ready-to-use AI services with a broader focus, for example, from AI vendors or the big cloud providers Microsoft Azure, Google Cloud, and Amazon AWS. Examples are AWS Rekognition or Amazon Comprehend Medical. The latter service analyzes patient information and extracts, for example, data about the patient herself and her medical condition. AWS Rekognition (Figure [2-3](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig3)) supports use cases such as identifying particular generic objects on pictures such as a car or a bottle. Do you see more customers drinking beer, white wine, or red wine? Marketing specialists can feed festival pictures to the AWS service, let the service judge what is on the image, and prepare statistics showing the results. Knowing that 80% of the festival visitors drink wine, 15% champagne, and just 5% beer might be a red flag for a brewery when deciding whether to sponsor this festival.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig3_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig3_HTML.png" height="569" width="1712"><figcaption><p>Figure 2-3</p></figcaption></figure>

Companies can innovate quicker when building on existing AI services such as LogoGrab. They integrate such a service into their processes and software solutions. They do not need an AI project or data scientists. There is no risk of project delays or failure due to issues with the AI model.

In many cases, generic AI services are not sufficient for concrete scenarios. What should a marketing specialist do if she needs to understand whether there is Swiss wine on a picture, other wine, or other drinks? She needs a neural network tailored and trained for her specific niche.

As stated, AWS Rekognition comes with a set of standard object types the service can identify, for example, on pictures. However, AWS Rekognition is more powerful. Engineers can train their specific neural networks to detect object types specific to their needs. The engineers have to provide sample pictures, for example, for Swiss wine bottles. Then, **AWS Rekognition Custom Label** trains a machine learning model for these customer-specific object classes.

This AWS service is just one example of services forming the **Customized AI layer** . They train and provide ready-to-use customer-specific neural networks based on customer-delivered customer-specific training data. In Figure [2-4](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig4), a marketing specialist for Swiss wine might be interested in understanding whether festival visitors prefer Swiss wine, other wine, or other drinks. So, she prepares training and test data with pictures labeled for these three types of drinks. When pushing the “train model” button, AWS generates the neural network without any further input and without requiring any AI knowledge.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig4_HTML.jpg" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig4_HTML.jpg" height="886" width="1713"><figcaption><p>Figure 2-4</p></figcaption></figure>

Customized AI is intriguing for companies that want to optimize specific business process steps to gain a competitive advantage. Thanks to customized AI, they achieve such goals without a large data science team. They can use cameras to check products on the assembly line for flaws in the production process. Therefore, they collect sample pictures for products that can and cannot be shipped to customers, let the customized AI service train a neural network, and apply this AI model to images coming from a camera on top of the assembly line. All typical suspects offer customized AI services: cloud providers plus analytics and AI software vendors.

AI Services, Customized AI, AI Frameworks, and AI-specific hardware – AI innovation comes in many forms. For innovation, companies and organizations rely primarily on university-trained data scientists. These data scientists know the AI frameworks layer exceptionally well. However, this layer is more for academic researchers but not in the focus of (most) companies. Thus, managers have to communicate their strategy clearly: on which layer(s) is the company innovating with AI? AI Services, Customized AI, AI Frameworks, or Specialized Hardware? This four-layer AI technology stack can act as a communication tool in any AI project and for any AI manager.

### Scoping AI Projects

A clear project scope is a crucial success factor for any project; this is my personal experience from managing and restructuring IT projects. This rule applies to AI projects as well. Senior managers fund them because they want or have to achieve specific goals, and AI might help. Thus, first, AI project managers must understand these goals and deliver what is expected. Their second supporting goal is to make sure the project team focuses on these goals and does not spend time and effort on optional or irrelevant topics.

The six-step AI project scoping guidelines (Figure [2-5](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig5)) helps AI project managers to reach this goal. Depending on the setup, the project manager works on these topics himself (potentially together with a solutions architect and an AI specialist). Alternative and depending on the organizational context, he might delegate these topics to the traditional roles of business analysts and architects or to the newer role of an AI translator that we discuss in a later chapter. What matters are a good understanding of business topics and AI technologies combined with social skills. Once the project has completed these six steps, the technical implementation starts, that is, working on the model and integrating the overall application landscape, hopefully following a structured, methodological approach.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig5_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig5_HTML.png" height="542" width="1721"><figcaption><p>Figure 2-5</p></figcaption></figure>

#### Understanding the Business Goal

The first step is a rather generic project management task: understanding what the business wants to achieve. It is a clear sign of trust in AI’s innovation potential when the business spends half a million or three million euros on implementing an AI solution. Then, they obviously believe that AI helps them run their business better. However, to prevent disappointments, a business case must answer the following questions:

* What is the exact goal of the business? Why do they invest money?
* What are the criteria to decide whether the project delivered everything they expected?
* How does the project relate to strategic and tactical business goals?
* What is the expected timeline?
* Is there already a budget? How much is it?

The answers to these questions help when writing the actual business case. The AI project manager can verify whether the expectations of the management and how the actual high-level project planning after the scoping phase match. Do the project’s direction, deliverables, and timeline match the senior management’s expectations?

#### Understand the Insights Category

The second scoping step translates the business problem into an AI problem. Is supervised or unsupervised learning the solution? Which subcategory is needed? Clustering, association, or dimensionality reduction for unsupervised learning or prediction and classification for supervised learning? Does the training data consist of tables, text, audio, video, or other data types? Figure [2-6](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig6) provides a first overview.

**Supervised** learning algorithms get training data with input and the corresponding correct output. It is always a pair such as a word in English and the proper German translation: \<red, rot> \<hat, Hut>. One subcategory for supervised learning is **classification**. A classification algorithm puts data items or objects in one of the defined classes. They work similarly to the Sorting Hat in the Harry Potter books and movies. The Sorting Hat determines for each new student in which of the four dormitory houses he or she fits best in and should live in the following years. A typical classification use case is image recognition: Is it a cat in the picture, or is it a dog?

**Prediction** is the second category of supervised learning insights. How much sugar and cream should we order to produce enough ice cream tomorrow? What will be tomorrow’s ice cream sales based on last year’s numbers, today’s weather, and the weather forecast for tomorrow? Supervised learning is often directly actionable. Thus, they are beneficial for transforming operational business processes in companies and organizations aiming to become data-driven and AI-empowered.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig6_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig6_HTML.png" height="842" width="1717"><figcaption><p>Figure 2-6</p></figcaption></figure>

**Unsupervised** learning structures the input data. A **clustering** algorithm looks at all data points. It comes up with, for example, three groups of data points that are pretty close together: status-oriented customers, price-sensitive customers, and nouveau-riche customers. Clustering provides additional insights to the sales and marketing departments and product managers without directly triggering any actions. **Associations** identify relationships between data elements, whereas **dimension reduction** simplifies complex, high dimensional data, for example, to remove noise from images.

In general, unsupervised learning algorithms unveil hidden structures of/in your data, but (often) do not directly tell what to do. While the book uses mainly supervised learning algorithms as examples, many of the methodologies work for unsupervised learning algorithms as well.

When a project has identified the needed insights category, the next step is to understand the data types used for training the model – and where to source them. **Database** **or** **CSV/Excel tables** are common formats, though the exact format or file type does not matter much. What matters is that the machine learning algorithms get structured, table-liken input.

**Images and videos** are other types of input data. They help with camera images during the production process and check whether the produced items have a good quality or video streams from CCTVs to check for persons entering restricted areas. **Speech and sound** (e.g., engine noise) or **text** in the form of documents or emails are additional options.

The exact data types do not impact whether AI can come up with the solution per se. It is more to make a better effort estimation (in short: tables are less work than speech) and elaborate the timeline more precisely. Furthermore, the AI project manager can validate how experienced the data scientists are with the particular area of AI needed for this business question. No project manager wants to realize at halftime that his project misses essential skills, that it is unclear whether suitable AI code libraries exist, or that they must identify a particular external service to build the solution.

Having answers for the questions addressed in the second scoping step brings the project closer to deciding which AI methods and algorithms to use. However, this decision still needs clarity regarding the explainability question.

#### “Prediction Only” vs. “Prediction and Explanation”

Two similar questions require slightly different approaches and potentially different AI learning algorithms. Suppose a TV streaming platform’s sales department wants to boost sales. First, they could ask which customers they should call to sell as many subscription upgrades as possible within one week. Second, they might be interested to understand how their customer base differs from the overall population. For example, the product manager could realize her customers are mainly in the German-speaking part of Switzerland. Adding a French and an Italian language offering might be an option to foster growth in Switzerland.

The first option with lists of potential buyers is operational. The sales managers need a list of which clients to call or mail. They do not care whether the list comes from a statistical model, a 100-layer neural network nobody understands, or from a fortune-teller using a crystal ball. They want a list with accurate predictions of who might buy.

The second question asks about buyers’ characteristics. Again, data scientists need a model that predicts who is likely to buy what. This time, though, understanding and explaining the model is essential. The attributes and their values matter. They help to distinguish buying customers from others. A fortune-teller or a 100-layer neural network might be highly accurate with predictions, but less accurate statistical models are often better for understanding characteristics. “Explainability” is a prominent topic in one of the later chapters.

Consequently, AI project managers or data scientists can provide a “prediction only” AI model or must deliver a “prediction and explanation” one. It is like painting a house white or pink. Both are possible. You should just clarify very clearly what your customer expects to prevent surprised reactions later.

#### The Training Set Challenge

Training AI models require good training data. The better the training data, the better the trained model’s accuracy. The AI training algorithm is undoubtedly essential, but – as a rule of thumb – an AI model cannot be better than the quality of its input training data. Five top biologists discussing which plant on a particular picture can prepare better training data than a drunken monkey doing the same. Suppose the idea is to be “better than a human” in the decision-making. There are three explanations when and how this can happen:

* Imitating a better-than-average human expert (“five top biologists”).
* Automatically derived real-world training data, for example, from shopping baskets or server logs.
* Humans get tired and, thus, make more mistakes, AI components don’t.

So, automating human judgment, for example, in quality assurance or incident analysis, requires a knowledgeable person to prepare the training set.

For supervised learning, the training data depends on sample input values plus the expected, correct output. Training a model for distinguishing between tomatoes and cherries requires many sample images with a label stating whether a concrete image is a cherry or a tomato. Getting such labeled training data is a challenge for many projects.

In general, there are four main approaches: logs, manual labeling, downloading existing training sets from the web, and database exports. **Existing training sets** enable researchers to develop better algorithms for common problems such as image classification. They often help to validate algorithms for challenges for which many solutions exist. Thus, in general, they benefit more academia. Companies should check whether already (pre-trained and) ready-to-use models exist. Not surprisingly, commercial AI projects often use **logs** (e.g., the shopping history of customers or click logs showing how customers navigate a web page) or time-consuming **manual data labeling** (e.g., humans mark whether an image contains a cherry or a tomato) for building training data sets. Building training sets from **database and data warehouse exports** works often for AI scenarios in the business or commercial domain such as sales, potential in combination with logs.

#### Model Update and Usage Frequency

A significant cost block for AI projects is the **integration** of AI components in the overall application landscape. It requires considerable time and engineering resources to make AI components and existing and new applications work together smoothly. Closely related is **automating**, collecting training data from various source systems and cleansing and transforming the data to create an AI model.

AI project managers must understand to which extent their projects need work-intensive integration and automation functionality. Key decision parameters are model usage and model update frequency. The **model usage frequency** indicates how often the AI model, for example, classifies pictures or predicts customer behavior. Is the model for a one-time, very specific mailing campaign and never used again? Is it for a webshop that calculates daily shopping items to present prominently? The more often the model is used, the more beneficial is integrating the AI model into the application landscape. Integration reduces manual work by automating the transfer of files and data from AI training environments to production systems. This technical integration minimizes the operational risks of manual data transfers, such as copying wrong files or forgetting to upload new data. The practical implications of such mistakes can be severe. A webshop might propose items not matching customer needs. Male teenage customers might not spend more money with a webshop that tries to sell them expensive pink handbags. Thus, a frequently used model calls for a well-tested integration of the AI components with other applications to reduce manual work and operational risks.

Closely related to model usage frequency is the **model update frequency** . It refers to how often data scientists retrain the AI model. Retraining means keeping the model structure, the data sources, the output data structure, etc., and “just” update the model’s parameter values so that they reflect the latest data. The retraining frequency depends on how often the real world and its reflection in data changes. When a sports fashion shop trained a model for the winter and skiing season, the shop should not use the same model next summer when it is super hot, and everyone spends the weekends and evenings on the beach or at outdoor pools. Suggesting pink skis as an add-on to a pink bikini might get the webshop in the press but not generate extra revenues. We observe a similar pattern as for the usage frequency: if data scientists have to perform the same process – this time, the one for updating the model – frequently, it is time to automate collecting the data and preparing it for retraining the model.

The conclusion for AI project managers is simple. During their project scoping phase, they must understand how deep the integration of the AI model and the AI training environment has to be with the rest of the IT application landscape. The answer heavily influences project costs and timelines.

#### Identify the Suitable AI Layer

The last scoping question aims to clarify how much the AI project can build on existing external AI solutions and services. This topic relates to our discussion of the AI technology stack at the beginning of this chapter. A project can build on ready-to-use AI Services. In such a case, the project engineers a solution with AI functionality without requiring any AI skills in the project team. Then, there is the area of customizing pre-trained AI models. The pre-trained model, for example, detects cars, but the project wants it to distinguish a BMW from a Citröen. Finally, AI projects can use, for example, TensorFlow and architecture neural networks best suited to their needs.

This last scoping question about the AI layer requires more AI knowledge than the previous ones. A project might even have to research the market to understand whether suitable external services or tools provide the envisioned AI functionality. These questions have a massive impact on the project and its business case. They determine whether and how many data scientists the project needs.

#### From Scoping Questions to Business Cases

The scoping questions help AI project managers to clarify many details before inviting senior data scientists and software architects (or external vendors) to work on a high-level project plan.

A project plan at this stage identifies the main milestones and potential valuable intermediate deliverables such as a minimal viable product. It states when which milestones can be roughly achieved and lists the needed resources. Resources cover how many engineers with specific skills are necessary, potentially highlighting internal candidates for critical roles. Resources cover as well the financials for external staff, licenses, and computing infrastructure, whether internal servers, powerful laptops, or public cloud resources.

Preparing a project plan and the needed resources is standard for project managers for non-AI and AI projects. Often, they have to follow company-specific mandatory rules and processes and fill out standard templates.

A business case just covering a project plan and expenses would be incomplete. The management wants to understand the benefits and the potential return of investment as well. The previous chapter covered potential benefits of AI projects already in more detail.

Eventually, the big day comes. A senior manager or a management committee decides whether you, as an AI project manager, get the funding for the project based on your business case. You maximize your chances with a clear scope, a convincing high-level project plan, and target-specific elaborated business benefits. If you get the funding, the data scientists are happy to prepare for and start working on the envisioned AI model, their core project deliverable.

### Understanding AI Models

AI models aim to solve particular, narrow problems. A model might get CCTV footage as input and provide as output whether a “moving thing” on the camera is a harmless cat or a potential burglar. Over more than half a century, research and engineering in AI came up with various approaches to generate such insights. They fall into two main categories: computational intelligence and symbolic artificial intelligence.

Solutions based on **symbolic artificial intelligence** represent knowledge explicitly, for example, using first-order logic or a formal language. The systems have formal operations or rules that operate on their symbolic knowledge. Such systems were popular when I went to university in the late 1990s. Thus, some readers might be familiar with the names of some of the concepts and languages in this area, such asstate-space, means-end analysis, blocks world, expert systems, or Prolog and Lisp.

An easy-to-understand example is the following Prolog code fragment. The system knows some facts about who likes whom in a small group:/\* Let's start with some facts ... \*/likes(peter, john) /\* Peter likes John\*/likes(peter, andrea) /\* Peter likes Andrea\*/likes(peter, sophie) /\* Peter likes Sophie\*/likes(john, peter) /\*John likes Peter\*/Interference rules generate new knowledge and drive the system’s reasoning. In this example, a rule defines being friends as two persons who reciprocally like each other./\* A rule to define friendship ... \*/friends(x, y) :- likes (x,y), likes (y,x)Posing a query to Prolog starts the reasoning process. The system tries to prove or disprove a statement (Are Andrea and John friends?) or determine values for which the query is correct (Who are Peter’s friends?)./\* Let's pose queries to let the system reason a bit ... \*/?- friends(john, andrea)No?- friends(peter, X)X = john

Today, the second big group of AI methods, **computational intelligence** , is much more popular. It approaches knowledge and facts representation and reasoning and insights generation entirely differently. Computational intelligence methods store information or facts as numbers. There is no explicit knowledge representation. Insights generation means multiplying and manipulating big matrixes.

Figure [2-7](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig7) provides an example. The input picture is a big matrix with numbers representing color and brightness. The system then implements matrix operations – the reasoning and insights generation. The figure contains the very first matrix operation, which averages cell values to reduce the matrix size. After some more matrix operations, the system states whether the image is a car, truck, boat, or plane.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig7_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig7_HTML.png" height="338" width="1721"><figcaption><p>Figure 2-7</p></figcaption></figure>

Computational intelligence algorithms are at the heart of the ongoing AI revolution in enterprises. While data scientists know which algorithm and method to use in which context, AI project managers need a basic understanding to contribute to the “definition of done” or quality assurance discussions. Readers interested in the multitude of algorithms find them covered in detail in one of the various algorithmic-centric AI textbooks.

We focus in the following on two examples: an old-fashioned statistics method (some might not even consider it to be AI) and deep learning neural networks. For the latter, we present basic neural network concepts and discuss advanced topologies. These algorithms require some entry-level math, though understanding the essence is possible even without.

#### Statistics-Based Models

Mathematicians and statisticians developed various algorithms over the last decades and even centuries. Many work perfectly for table-based Excel-like input data, typically available in ERP or core banking systems. We focus on one algorithm and just look from a high level: linear regression. Suppose an online marketplace for secondhand cars wants to incorporate a “fair price” feature. It should make deals smoother by fostering trust between car dealers and customers by suggesting a “fair price” to the contract partners. Therefore, the marketplace needs algorithms to estimate, for example, the market price for a Porsche 911 3.8 Turbo Cabrio.

In the first step, the model might consider only one input parameter for price estimations: the car’s age. Since we use linear regression, the mathematical function to determine the car price is a linear function:![\$$ Price\left( Age\\; of\kern0.17em the\\; Car\right)={a}\_1\ast age+{a}\_0 \$$](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372\_1\_En\_2\_Chapter/516372\_1\_En\_2\_Chapter\_TeX\_Equa.png)Once we know the price of two car sales – for example, 153,900 EUR for a 12-month-old car and 124,150 Euro for a 28-month-old one – we put these two data points in a diagram and draw a line through them. The line is a linear function for our car-price estimation (Figure [2-8](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig8), left).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig8_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig8_HTML.png" height="575" width="1717"><figcaption><p>Figure 2-8</p></figcaption></figure>

However, a good pricing model needs hundreds or thousands of data points. Figure [2-8](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig8) (right) illustrates this. The example contains many data points, meaning the model accuracy should be good. However, it is (nearly) impossible to draw a line going through all data points if you have more than two. So, how can linear regression help? Basically, there are two needs:

* An error metric measuring how good a function reflects reality. A metric makes two or more estimation functions comparable. We can measure which of them estimates sales prices better. A widely used metric for that purpose is “least squares” (see Figure [2-9](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig9)).
* An algorithm that systematically or randomly tries out various values for parameters such as a0 and a1 in the example. Using the error metric, it calculates the quality of a specific parameter setting. It readjusts the parameters and calculates the error again – until the algorithm decides to terminate the optimization and returns the best values for a0 and a1.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig9_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig9_HTML.png" height="854" width="1421"><figcaption><p>Figure 2-9</p></figcaption></figure>

We already mentioned two impact factors for model quality: the learning algorithm and the size and quality of the training data set. Similarly important are which attributes the model incorporates. In our example, the fair-price estimation function for cars considers just one attribute: the age of a vehicle (v1). The function is insufficient even if trained with millions of sales data points. The reason: various other factors impact the price of a car as well significantly, for example, the number of driven kilometers (v2), whether the vehicle was involved in a heavy accident (v3), has air conditioning (v4), or a high-end music system (v5). As a result, we get a more complex price estimation function to optimize:![\$$ Price\left({v}\_5,{v}\_4,{v}\_3,{v}\_2,{v}\_1\right)={a}\_5\ast {v}\_5+{a}\_4\ast {v}\_4+{a}\_3\ast {v}\_3+{a}\_2\ast {v}\_2+{a}\_1\ast {v}\_1+{a}\_0 \$$](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372\_1\_En\_2\_Chapter/516372\_1\_En\_2\_Chapter\_TeX\_Equb.png)

The underlying assumption for such a linear regression function is that the nature of the problem is linear. However, used car price estimation functions are **not linear**. The value of a car drops heavily when a car dealer sells it the first time to a customer. The price decline slows down afterward, and, usually, the sales price does not get negative. Depending on the problem’s nature, replacing the linear estimation function with a different (more complex) mathematical function results in better estimations.

Another algorithm similar to linear regression is **logistic regression** . Logistic regression helps for classification tasks such as deciding for specific credit card payments whether they are fraudulent. Usually, a function such as the sigmoid one is applied in the end to provide a classification value between 0 and 1 (see Figure [2-10](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig10), left). If the value is 0.5 or higher, it might be a yes, otherwise a no.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig10_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig10_HTML.png" height="513" width="1704"><figcaption><p>Figure 2-10</p></figcaption></figure>

#### Neural Networks

Neural networks serve a similar purpose as the statistical regression algorithms. They allow for more complex and accurate prediction or classification logic. Thus, they are especially beneficial for tasks such as image recognition and natural language processing.

Neural networks date back to the 1950s when Rosenblatt developed the Perceptron for detecting patterns in (small) images. Key algorithms are from the 1980s and 1990s. Their big breakthrough was in the 2010s resulting in their widespread use in industry today. Enabling factors have been the explosion of computational power and available data such as text, images, audio, or video. The cloud is the most recent catalysator for the adoption of neural networks. Cloud providers offer highly scalable computing resources and ready-to-use AI services.

The human brain, with its neurons and synapses, is the inspiration for artificial neural networks. Neurons are the processing units. Synapses connect neurons to exchange information, which drives the information processing of the neural network. A neuron fires and is activated, depending (also) on the activation status of the incoming neurons.

An artificial neural network implements the **reasoning** of a single neuron in two steps. First, the neuron calculates the weighted sum of all incoming activations from the feedings neurons. In Figure [2-11](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig11), the neurons A, B, C, and D propagate their activations _aA_, _aB_, _aC_, and _aD_ to neuron N. Neuron N calculates the weighted sum of these activations using the weights _wAN_, _wBN_, _wCN_, and _wDN_. So, the calculation is: _aA_\*_wAN_ + _aB_\*_wBN_ + _aC_\*_wCN_ + _aD_\*_wDN_.

When the weights are positive, a connection supports that neuron N fires. If the weight is negative, this suppresses the firing of neuron N. Besides the already mentioned weights, a value named “bias” is typically added to the calculation, resulting in the following term: _aA_\*_wAN_ + _aB_\*_wBN_ + _aC_\*_wCN_ + _aD_\*_wDN_ + _bN_.

The bias shifts the graph on the x-axes. The purpose of this becomes clear when looking at the second processing step of the neuron: the neuron applies an activation function on the weighted sum, resulting in the activation of this neuron. A widely used activation function is the Rectifier (ReLU) function (see Figure [2-10](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig10), right). It returns the weighted sum of the activation sum if positive; otherwise, 0. This activation is then fed to other neurons, such as neurons X, Y, and Z in Figure [2-11](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig11).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig11_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig11_HTML.png" height="1088" width="1409"><figcaption><p>Figure 2-11</p></figcaption></figure>

Neural networks consist of large numbers of neurons grouped into layers (Figure [2-12](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig12)). The first layer is the input layer. Its neurons represent the outside world, such as pressure sensors’ values in a turbine or pixels of a camera. The last layer is the **output layer**, which delivers the artificial neural network’s reasoning. It consists of one or more neurons, depending on the neural network’s purpose. For binary classification problems – does an image contain either a dog or a cat – one neuron is sufficient. A value close to 1 indicates a dog, a value close to 0 a cat. Suppose a neural network classifies animal pictures, for example, whether the image is a dog, a cat, a dolphin, or a manta ray. In that case, the output layer consists of various neurons for all the different animals.

The neural network can have zero, one, or multiple **hidden layers** between the input and the output layers. In the most straightforward neural network topology, all neurons of one layer feed their activation to all neurons of the following layer – a fully connected neural network.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig12_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig12_HTML.png" height="1079" width="1504"><figcaption><p>Figure 2-12</p></figcaption></figure>

When **training a neural network**, the first step is to choose a data item from the training set as input. We calculate the neural network’s prediction or classification for this item and compare the result of the neural network with the correct output. We know the correct result since the data item is from our training set. The difference between the correct and the actual result is the input for step 2, which readjusts the weights. For this readjustment or learning step, data scientists commonly use the backpropagation – a gradient-descent – algorithm. The backpropagation algorithm starts with optimizing the output layer’s weights and then works its way back layer by layer.

The weights are parameters of the neural network architecture representing the brain or intelligence of the artificial neural network. The parameters that control the learning process are called hyper-parameters. The neural network architecture, such as the number of layers and neurons, is a hyper-parameter, and so is the learning rate. The learning rate influences how much weights are adjusted if the actual and the correct output differ for a training data item. A high learning rate enables the neural network to learn quickly in the beginning. Over time, the learning rate must drop to ensure that the neural network converges to stable weights.

#### Advanced Neural Network Topologies

Research on neural networks made significant progress over the last years. Data scientists improved and adjusted the neural network architecture for dedicated application areas, such as computer vision and image recognition. They relax the topological rule that a neuron propagates its activation value to exactly all neurons of the next layer and nowhere else. The relaxation is the base for three significant neural network topology variants: convolutional neural networks, recurrent neural networks, and residual neural networks. A **residual neural network** does not only propagate its neurons’ activations to the next layer. It can also feed the activations to layers further down. In Figure [2-13](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig13), for example, layer 3 forwards its activation not only to layer 4 but also to layer 6.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig13_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig13_HTML.png" height="450" width="1425"><figcaption><p>Figure 2-13</p></figcaption></figure>

**Convolutional layers** are another topological variant. They differ from fully connected neural network layers in two ways (2-14). First, neurons of a convolutional layer do not consider the activation of all neurons of the previous layer. They consider only specific collocated neurons, such as neighboring pixels of an image. For example, only a 3x3 matrix of neurons in a layer feed their activations as an input of a specific neuron in the next layer. Second, the neurons of a convolutional layer calculate their activation differently. They use a filter (named convolutional kernel) with weights. The filter is a kind of window moving over the matrix, which has the activation values of the previous layer.

In Figure [2-14](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig14) (right), the left upper cell of the kernel has the value “1”. When calculating the value of the black cell, the left upper value of the activation matrix with the value “0.9” is weighted with the value “1”. Then, “0.7” is multiplied with “0” and “-1” with “0.1”. This calculation continues for all other cells of the filter matrix. For calculating the value of the grey cell, we start with “0.7”, which is multiplied with “1”.

Data scientists build on neural network topologies with convolutional layers for dedicated AI application areas such as computer vision with their complex problems. They consist of twenty or thirty layers, a mix of fully connected layers, residual layers, and many convolutional layers.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig14_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig14_HTML.png" height="675" width="1721"><figcaption><p>Figure 2-14</p></figcaption></figure>

In time series prediction or natural language and speech processing, **recurrent neural networks** are a popular neural network topology choice. The topology reflects that the ordering of the input is essential. When understanding a text, “Heidi kills Peter” and “Peter kills Heidi” have a different meaning. Recurrent neural networks connect nodes within one layer to reflect the ordering aspect (Figure [2-15](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig15)). Sophisticated recurrent neural network topologies base on the well-known neural network architectural patterns such as Long Short-Term Memory Units (LSTM) or Gated Recurrent Unit (GRU). They introduce the concept of a “memory” for previous state information. The relevance of memories is easily understood when thinking about crime stories. The book might start with the sentence “Heidi killed Peter.” This sentence contains information still relevant two or twenty sentences later when the police arrest Heidi.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig15_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig15_HTML.png" height="913" width="704"><figcaption><p>Figure 2-15</p></figcaption></figure>

AI project managers benefit from a basic understanding of advanced neural network topologies, even though they do not design or optimize the topology. They should be aware that various best practice topologies exist. Plus, they should be sparring partners for data scientists suggesting designing and optimizing a company-specific and use-case-specific neural network topology (and investing months of work) instead of using best practice topologies.

The fact that best practice topologies exist impacts project teams. First, it is an extra motivation to use existing, well-trained AI models, for example, from cloud providers. Second, suppose projects embark on the journey to design and implement their own neural network. In that case, the project manager should ensure that he has an experienced data scientist in his project team. Ideally, he can convince a senior data scientist with know-how in the concrete application area. Otherwise, he should try to get at least temporary support from such a person.

### Developing AI Models

The last decades taught the IT industry and IT professionals that unstructured and uncoordinated engineering or unsystematic experimentation is the short road. Its only drawback: It leads to disaster, not to success. Thus, AI project managers should ensure that the engineering of “his” AI models results from a hopefully creative and systematic engineering process. In the following, we highlight three topics each project manager should know:

* Jupyter Notebooks as the most widely used AI tool
* CRISP-DM (CRoss Industry Standard Process for Data Mining) as a methodology for structuring AI projects from the idea to the delivery
* Approaches for improving the efficiency of AI teams

#### The Jupyter Notebook Phenomenon

Jupyter Notebooks are a fascinating piece of software. I learned about them in my first deep learning and AI courses. I never heard about them in my two decades in data management, often with hands-on SQL programming. As Google Trends illustrates, the interest in Jupyter notebooks started to rise from around 2014/2015. It almost doubled every year in the beginning and is still increasing today (Figure [2-16](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig16)). Every manager in AI will hear data scientists mentioning them on a nearly daily basis. If data scientists have issues with their Jupyter Notebooks, this slows down their work. In general, the primary use of Jupyter notebooks is developing Python scripts. However, they are not simply a kind of TOAD-like tool or SQL Developer for Python. They are a new way to work with data.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig16_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig16_HTML.png" height="738" width="1721"><figcaption><p>Figure 2-16</p></figcaption></figure>

Jupyter Notebooks are an **interactive development environment** . The commands support, for example, downloading data and cleansing or transforming the data, and training AI models. As an interactive environment, data scientists can write some lines of code and execute them immediately. They can choose whether to rerun the complete script or just run new lines of code. This choice is essential. Single commands for data preparation or for generating the model can take minutes, hours, or days. Rerunning the complete script for testing some new code lines often takes too long.

Besides the interactive mode, the second differentiator is that **commenting code is fun** with Jupyter Notebooks. It is different from old-style Java code comments and feels more like making notes on a paper notebook. Before (or after) writing a new command, adding a comment helps to remember its exact purpose. Otherwise, data scientists might not know anymore five or ten commands later the reason and actual context of every single line of their code. Jupyter Notebooks allow formatting comments nicely (e.g., bold, italics) and integrating images and sketches. These options indicate that comments are not just an add-on but an integral part of the model development process.

To illustrate the concept of Jupyter Notebooks better, Figure [2-17](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig17) contains a screenshot. This Jupyter Notebook is a script for downloading images from the web and storing them in an array for later processing. On the top is a cell with comments (“markdown”). Then, there is a cell with code. The first commands define and instantiate variables, followed by a for-loop for downloading images. The last commands write to the console. Below the script with the code, there is the output the Jupyter notebook wrote to the console when executing this script.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig17_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig17_HTML.png" height="929" width="1717"><figcaption><p>Figure 2-17</p></figcaption></figure>

#### CRISP-DM for Model Development

With or without Jupyter notebooks, unstructured experimenting and uncoordinated engineering seldom deliver a successful AI model or a working AI-driven software solution. The bigger and more relevant the AI projects get, the more vital are methodologies. In AI, the most popular one is the Cross-Industry Standard Process for Data Mining (CRISP-DM) . Released at the turn of the millennium, it is still state of the art with its six phases (Figure [2-18](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig18)).

The first phase is **Business Understanding** . What does the business want to achieve – and what is the corresponding AI project goal? An example of a _business goal_ is to reduce the churn rate by 50%. In other words: 50% fewer customers change the bank when they have to renew their mortgage. A matching _AI project goal_ is to provide a list of the 1000 customers potentially not prolonging their mortgages up for renewal next month. The first phase also covers _assessing the resource situation_. Projects need expertise and adequate staffing, data sources, engineering and AI tools, and training and production environments. There might be legal constraints to be clarified or implicit critical underlying assumptions never written down. Understanding and improving the resource situation is part of the way to elaborate a _project plan_.

Thus, the first phase of the CRISP-DM covers aspects related to the business case before the action project starts. We covered such elements in detail with the scoping questions earlier in this chapter and the business benefit discussion in the previous chapter.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig18_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig18_HTML.png" height="992" width="1004"><figcaption><p>Figure 2-18</p></figcaption></figure>

The second phase is **Data Understanding** . It starts with getting access and potentially copying data from the sources identified in the previous phase. It covers tasks such as getting accounts, opening firewalls, or initiating file transfers. Data scientists describe and work on understanding the data from a high-level perspective. They document the meaning of the attributes, their syntax (e.g., string vs. number fields), their statistical distribution, and their quality. Typical quality issues are, for example, empty fields or tables that contain only partial data (e.g., all customers worldwide, but excluding China or the US).

In phase three – **Data Preparation** – data scientists transform and prepare the final data set for the subsequent model creation. They identify potentially helpful tables and attributes. The rule of thumb is to provide as many attributes as possible and let the AI training algorithms determine which attributes improve the AI model quality. When trying to understand which customers might decide not to prolong their mortgage, everything we know about the customer can help for training the model: account balances, number of contacts with a bank advisor, inflow and outflow of money – everything that widely relates to the customers, its mortgage, or her behavior and actions. Data preparation also covers feature engineering. Not all potentially helpful input values exist in a database. A data scientist might have to extract all customer contacts from client advisors from the company’s calendar solution and determine the number of contacts per customer. The net in- or outflow of assets requires summing up the positions today and one year ago.

Data preparation also covers cleansing the data: skipping rows with empty attributes, setting empty fields to a default value, or estimating missing values based on external information. During this phase, data scientists have to bring such data together from various tables and databases. All information about customers must be in one table, requiring merging and joining tables from one or multiple databases. The core database might provide the account balances. The number of meetings with the bank advisor and the customers comes from a table in the customer relationship solution. Finally, sometimes the syntax or format of the data has to be changed: Boolean values ‘Y’ and ‘Y’ might have to be mapped to the numbers ‘0’ and ‘1’.

The fourth CRISP-DM phase is **Modeling**. It is the core task and the one data scientists love most: creating an AI model. They decide whether to train, for example, a neural network or linear regression model. They define how to measure and evaluate the model quality and perform this task after the model creation. Finally, they document the model accuracy and compare potentially competing models.

The **Evaluation** phase leads to the final decision of whether the model is ready for production usage. The stage covers the quality assurance as well as deciding whether the model is good enough for production. In contrast to the modeling phase, which looks at the model from an IT/AI perspective, the evaluation phase challenges whether the model really helps reaching the business goals. It can even mean applying and trying out the model in reality. One bank branch might call customers with a high risk of leaving. After one month, the project compares the numbers for this branch with those of a similar one that did not put in extra effort to convince potentially leaving customers. Such a comparison allows validating the benefit of AI.

The sixth and final phase is **deployment** . The project plans how the model is used, elaborates a monitoring strategy to detect a degenerating model, and ensures the maintenance of the AI. It covers, furthermore, generating a final project report and having a lessons-learned workshop.

Projects usually do not go linearly and in waterfall-style through these phases (see arrows in Figure [2-18](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig18)). Sometimes, going back one step allows reaching the final goal quicker and/or better. For example, assume an AI project detects that the bank mainly lost German customers last quarter. Then, compliance and sales managers might suddenly remember that they forced them to leave – and did not tell the AI project team. Another iteration becomes necessary. Indeed, the AI project benefits heavily from the data and insights in a second iteration.

In general, in the absence of detailed company-specific data, projects should plan three iterations. A second iteration typically needs around 50% and a third around 25% of the effort of the first. Also, practical experience indicates that the actual creation and training of the AI model takes only 10–20% of the overall time budget. It is a small fraction – and the reason why IT organizations try to improve their data scientists’ productivity. They want them to work on their core tasks and on new models that create business value.

#### Improving the Productivity of Data Scientists

Making data scientists work more efficiently is a big topic, though the exact motivation differs between organizations. Tech companies and start-ups often cannot recruit enough experts with specific technology or domain know-how. Other companies – especially in the IT service or consulting sector – face another dilemma. They (or the managers of their customers) have great ideas about innovating with AI. Most of the time, the data is available. However, my experience from brainstorming workshops is that financial gains and savings and the actual project costs often do not match. No business managers want to invest CHF 100,000 in an AI project to get CHF 30,000 savings per year. AI projects are cool but unfamiliar and, thus, “risky.” Hesitating managers might expect bullet-proof business cases. When AI organizations can increase their productivity, costs go down, and the business value remains the same. AI projects pay off quicker and deliver higher savings or returns.

Going through the CRSIP-DM process is a structured way to identify improvement potential (Figure [2-19](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig19)). The first phase is “business understanding.” The broad existing literature about **business analysis** and requirements analysis covers all methodologies and tools for understanding the goals and requirements of users, customers, and managers. Besides this general methodology know-how, there are AI-specific chances to optimize the first phase. This book elaborated them in the **scoping** section at the beginning of this chapter and the business value discussion in the previous chapter.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig19_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig19_HTML.png" height="975" width="1721"><figcaption><p>Figure 2-19</p></figcaption></figure>

Scoping and business analysis do not reduce the effort data scientists have for cleaning and preparing training data and to train a concrete model. However, a good business understanding reduces the risk that data scientists do not fully understand the business challenge. An insufficient understanding of the expected goals can result – after weeks of intense work – in the verdict that an AI model is correct and well-trained, but does not help anyone.

The second CRISP-DM process phase, “data understanding,” covers data collection and acquisition, understanding the actual data, and verifying its quality. Two relevant optimization options exist for this phase: managing training data and setting up or introducing a data catalog. A **data catalog** compiles information about the various data sets stored in the organization’s data lake, databases, and data warehouses. It helps data scientists find relevant training data. The benefits are twofold. First, data catalogs reduce the time data scientists need to identify training data. Second, the AI models get potentially better because the data scientists could use a wider variety of available, relevant training data. We take a closer look at data catalogs in a later chapter.

Data catalogs help to find existing training data. Sometimes, data scientists work on AI solutions for which no useful training data exists. Suppose an AI-driven application should decide based on images whether a steering wheel on an assembly belt can be packaged and shipped to the customer. Training such a model requires having steering wheel images labeled either as “good quality” or “insufficient quality.” Collecting and labeling images are time-consuming, tiring, and expensive because training sets have to be large. How long would you need to take 1000 pictures of steering wheels on an assembly line? Some have to be suitable for shipment; others must not. Regarding the defective steering wheels, you need to take at least 20 images for each quality issue.

The good news for data scientists and AI projects is: they can optimize the labeling task with services such as **AWS SageMaker Ground Truth**. The service innovates labeling in multiple ways:

1.  1\.

    Out-tasking the labeling to AWS-managed workers or third party service providers. This out-tasking frees data scientists from labeling large data sets themselves or having to identify and coordinate internal or external persons helping them. Out-tasking works if the labeling does not require too specialized skills. Also, data privacy constraints and the protection of intellectual property can limit such out-tasking.

    &#x20;
2.  2\.

    Managing the workforce: The service compiles batches of texts or pictures to be labeled and assigns them to the various internal or external contributors – no need to ask for the status by mail and phone or to organize project meetings.

    &#x20;
3.  3\.

    Increase the labeling quality by letting more than one individual contributor label the same pictures or texts.

    &#x20;
4.  4\.

    Reduce human labeling effort: AWS can distinguish between “easy” training data items. It can label itself and challenging training data items that a human contributor has to label.

    &#x20;

The next phase is “data preparation.” It is the tiring, time-consuming, cumbersome work of getting the initial data set in a helpful form for training an AI model. Data scientists can perform this work more efficiently with AI-specific **integrated development environments** (IDEs) such as the already discussed Jupyter notebooks. They speed up the development process by enabling to execute only selected commands of a long script, by easing writing comments and documentation and collaboration.

Auto machine learning (AutoML) promises to automate data preparation, choosing a training algorithm, setting the hyperparameters, and training an AI model. AutoML understands these tasks as (high-dimensional) optimization problems, which a clever AutoML algorithm solves autonomously without human intervention. You provide a table – AutoML returns you a ready-to-use trained AI model.

It sounds like science fiction, but various offerings on the market prove the opposite. Examples are Google’s GCP AutoML Tables, SAP Data Intelligence AutoML, or Microsoft Azure Automated ML. It is not clear yet whether and under which circumstances AutoML outperforms or constructs similar good models like the ones “hand-crafted” by experienced data scientists. Independent of that, AutoML is already a game-changer. It speeds up and simplifies creating AI models without needing large data science teams. The Pareto principle (80/20 rule) applies to data science projects as well. It is often better to have a relatively good model quickly with low effort than spending months for the perfect model.

Finally, AI organizations can optimize the deployment phase as well. This phase consists mainly of generic IT engineering tasks. Thus, general improvements such as CI/CD pipelines bring benefits. The AI models become part of the application code or run on a separate AI run-time server. Fewer frictions equal fewer potential errors. Other engineering teams contact the data scientists less often to help them in debugging or fixing integration issues.

Not all presented optimization options help every AI project. The higher on the AI technology stack, the fewer areas do projects and organizations have to optimize. When companies rely entirely on existing, ready-to-use **AI services** ,they neither train machine learning models nor deal with training data. They outsourced everything. Nothing is left that can or should be optimized.

Companies relying on **Customized AI** provide training data but delegate the actual training and model creation to external partners. Organizations following this strategy can optimize their work with data catalogs and improved data labeling processes.

Many data scientists and organizations (still) prefer to train the machine learning models using **AI Frameworks** such as TensorFlow. They benefit from data catalogs, improved data labeling processes, or IDEs. They can also use AutoML to improve and (semi-) automate specific data science tasks.

In general, optimizing an AI team’s efficiency has two facets. The most important one is to go for a high layer of the AI technology stack if possible. Then, additional optimizations are possible that are relevant for the chosen AI technology layer (Figure [2-20](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig20)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig20_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig20_HTML.png" height="504" width="1721"><figcaption><p>Figure 2-20</p></figcaption></figure>

### Integrating AI Models in IT Solutions

Online shoppers are familiar with the phenomenon. Webshops offer their customers products that perfectly match the contents of the shopping cart: a pair of black socks with a black suit? Click! Pink shoes for the customer with the pink handbag? Click! AI algorithms run in the background. Their aim? Increase sales! To make it work, however, an AI model alone is insufficient. We have to integrate the AI model into a “real world” application such as the webshop.

Data scientists new to big organizations and with a long and extensive academic background tend to underestimate the integration challenge. They start experimenting with a commercial or free AI software package or cloud-based AI services. They want to figure out whether and how to benefit from AI by trying out some use cases. At this moment, they do not (yet) require an interface between the AI component and one or multiple applications. The situation changes once the benefits are evident. The business and the IT management want AI to become an integral part of complex software solutions. For example, an online fashion shop has many features that have nothing to do with AI. The webshop presents fashion; customers can search for items, add them to a shopping cart, and pay. An AI component is simply another feature that “only” determines products for each customer that might be of particular interest to these customers. The AI component might suggest black socks for customers with a suit in their shopping basket and transmit the suggestion to the webshop application. The customer then sees black socks in the browser as a product proposal.

The collaboration between the AI component and the rest of the application is the goal. The starting point for AI – and software solutions with AI functionality – is historical data. Data scientists need them to create AI models, for example, for customer buying behavior. However, it would be a bad idea if data scientists perform their work on a production system. When data scientists query large data sets and run complex, resource-intensive statements, they potentially interfere with daily operations. Thus, decoupling their work by copying or extracting data from the production system and putting the data into a training environment with sufficient computing power is essential.

As soon as the AI model is ready, data scientists or IT engineers deploy the model in production systems. No model is there for eternity. A newer model will replace it after a day or some weeks or months based on more recent data (Figure [2-21](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig21)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig21_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig21_HTML.png" height="917" width="1709"><figcaption><p>Figure 2-21</p></figcaption></figure>

The exact deployment or integration can follow various patterns. The three primary variants are:

* Precalculation
* Reimplementation
* Encapsulated AI component

For **precalculation** , a data scientist develops and creates the AI model, which, for example, determines for each customer the product she most likely buys next. Next, the data scientist or an IT engineer uploads this information to the application. Now, the application knows which items to suggest to the customers when they visit the online shop again later. The application works with the result. It does not know the model. There is no need for integration between any AI component or environment and the application. Model development and evaluation take place independently in the AI training environment, and the evaluation or interference bases always on the most up-to-date model (Figure [2-22](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig22)).

Precalculation has some limitations if the frequency of model updates and the “real” world changes do not match. For our webshop example, the webshop continues to suggest a neon-green cocktail dress to a customer after she bought one till the next data upload. In such a case, the ads do not generate additional sales – and let the customer feel that the webshop cannot provide a shopping proposal matching her needs.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig22_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig22_HTML.png" height="609" width="1704"><figcaption><p>Figure 2-22</p></figcaption></figure>

In the second and third variants – the reimplementation pattern and the encapsulated AI component variants – the AI component feeds real-time customer shopping data to the model and always gets up-to-date suggestions for online shoppers. Suppose the data analyst creates the model on April 30 in his AI training environment. However, applying the model on user data is deferred to when a customer visits the webshop. Then, her up-to-date shopping history is fed into the model to get product proposals. If she bought a green dress yesterday and extravagant sunglasses an hour ago, the AI component considers this. When a customer visits the webshop on May 17 in our example, the application uses the model created on April 30 and customer data from May 17.

The second and the third variant differ regarding the technical implementation. The **reimplementation** **pattern** means to program the code again for the software application a second time. Software engineers take the model from the training environment and implement the same model now, for example, in Java. The engineers put the neural network parameters and weights into a configuration file, such that parameter updates do not require a code change. Ideally, a CI/CD pipeline automates the integration. Such a pipeline automatically collects the various sources from all developers and data scientists, creates a deployable software component, and installs it on development, testing, and/or production systems. Thus, when a customer visits a webshop, the webshop contains the neural network, such as any other application feature or code (Figure [2-23](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig23)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig23_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig23_HTML.png" height="479" width="1721"><figcaption><p>Figure 2-23</p></figcaption></figure>

Alternatively, the application landscape might have a dedicated **AI runtime server** in the encapsulated AI component pattern (Figure [2-24](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_2\_Chapter.xhtml#Fig24)). The latter runs the AI models for all the company’s applications relying on AI functionality. RStudio Server is such a product. When a customer visits a webshop, the webshop invokes the model on AI runtime server, passing this customer’s shopping history. The AI runtime server pushes the shopping history data through the neural network to get a prediction of the customer preferences. Then, the AI runtime server returns this prediction to the webshop, which presents and highlights relevant product(s) for this concrete customer.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_2_Chapter/516372_1_En_2_Fig24_HTML.png" alt="../images/516372_1_En_2_Chapter/516372_1_En_2_Fig24_HTML.png" height="496" width="1721"><figcaption><p>Figure 2-24</p></figcaption></figure>

For this pattern, the AI team has to set up, run, and maintain an AI runtime server to which every data scientist deploys his models.

As a side remark, “models that continuously optimize themselves” sound obvious and visionary at the same time. They are technically feasible. For example, Google’s GCP ML Pipeline follows this idea. However, fully self-optimizing and completely autonomous models can have harmful side effects. Suppose AI only suggests extravagant ties with a high margin since the optimization goal is increasing the margin. In that case, the profit might increase in the short term. But at some point, the fashion webshop is perceived as a tie shop, and only customers who want a tie come. Humans notice such trends better than fully automated optimization processes. Even in a world of AI, there is still a need for the human brain.

### Summary

This chapter explained every AI organization’s potentially most apparent and vital task: delivering an AI project. It elaborated the various facets and layers of AI innovation, scoping AI projects, and how AI models such as statistical or neural network models – the core delivery of every AI project – look in practice. Further, the chapter elaborated on the development process of AI models and the integration in a corporate IT application landscape. With this knowledge, the next big challenge for AI project managers is obvious: How can AI project managers validate whether an AI model behaves as expected and provides the needed prediction or classification quality?
