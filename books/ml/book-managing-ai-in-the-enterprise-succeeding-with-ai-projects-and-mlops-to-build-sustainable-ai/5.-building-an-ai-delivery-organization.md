# 5. Building an AI Delivery Organization

When AI shall rock your company, a few projects here and there won’t do the trick. You need a stable AI organization always ready to step in, help, and drive forward the transition towards a data and AI-driven company. This aim has implications for data scientists and the overall AI organizations. They have to widen their focus and shift it away from projects and models towards a kind of nation-building for AI within organizations. The task: build an AI organization that lasts.

Building an AI organization starts with identifying (internal) customers and securing funding. Customers mean that there are managers within your organization with a challenge they can and want to solve with AI. You have to convince them that your AI organization is technically competent, understands their business challenge, and helps them achieve their ambition. An AI organization cannot just focus on technology, or only within very few very large corporations with a high division of labor. From understanding the business problem to providing a solution for the exact context, this end-to-end perspective is the key to success with customers and for funding.

When new to corporate politics, it is especially crucial to be aware of two types of managers who support you. A _sponsor_ helps you fund your service. She has a budget, or she can help you get funding from someone else. On the other hand, a _non-material patron_ tells you how great you, your ideas, and your entrepreneurial spirit are. However, she has no money, and she does not lobby for any budget for you.

Once an AI organization ensured funding, the team must deliver the promised AI projects and sort out and run its AI infrastructure. Spending months on evaluating technologies and detailing all technical concepts seldom matches corporate expectations. Instead, sponsors and senior managers want to see that the AI organization delivers substantial business value.

This chapter helps managers to succeed with these challenges by elaborating on three aspects:

* Shaping an AI service, that is, discussing typical elements of (AI) services and what choices exist.
* Looking at the organizational challenges of AI organizations running projects for creating superior AI models.
* Detailing the organizational needs of AI operations that run an AI model for years, for example, as part of the solution code or on a dedicated AI runtime server.

This topics combination helps managers prepare their service proposals and implement the actual service organization once senior management has approved the concept and ensures funding.

### Shaping an AI Service

Personally, I always enjoy working with teams and managers to shape services within IT departments. You discuss a lot with engineers, managers, and customers, thereby deepening your knowledge about the topic and the organization. It is like Goethe’s Faust, aiming to «… understand whatever binds the world’s innermost core together, see all its workings, and its seeds.»

#### IT Services Characteristics

“Service” is one of those terms most people have more of an intuitive than a precise understanding. However, an accurate understanding helps to design AI (and other) services. Nearly twenty years ago, Joe Peppard elaborated on the main characteristics of IT services. They are: intangible nature, human factor, simultaneous provisioning and consumption, and no upfront demonstration.

The **intangible** nature of services reflects that services do not produce anything physical you can touch. Instead, a service is about someone doing something with something, which might change a physical or digital asset.

AI teams typically create, deploy, and maintain digital assets: AI models, inference results in the form of lists when applying and using the models, code for data cleaning and preparation, or interface configurations when integrating models in complex solutions. Additionally, AI teams might provide communication-intense consulting (e.g., helping internal customers formulate the question AI should solve) or support for technical incidents related to the model and its integration.

The **human factor** reflects that people interact and collaborate. Handling customer interactions professionally is as essential to avoid unhappy customers as successfully solving tasks and problems. Data scientists often have more academic merits than many of their customers. Still, they must patiently explain topics and communicate in an easy-to-understand language, value their internal customers’ business know-how and experience, and show interest and understand the application context.

A second often overlooked aspect is the need for the customer side to contribute constructively. Suppose the integration between an AI runtime server and a sales campaign management system does not work. The owner or manager of the campaign management system cannot simply blame the AI team and refuse debugging on their side as well. Seniority and experience working with customers help handling such situations.

**Simultaneous provisioning and consumption** mean that service provisioning and consumption usually take place at the same time. If a physician examines me, I have to be present. The same applies to AI services. When AI models or interfaces fail, the AI team has to start working on the issue as soon as possible. Such incidents cannot wait, and such a support service cannot be prepared in advance and “place into stock.” Depending on the exact request, the simultaneity is relaxed. Creating an AI model is a multi-week task. You might not want to generate a list in August with clients to target with Christmas tree coupons in early December. You have to do this in November or December, though one day or even a week earlier or later does not make a big difference.

The simultaneous nature has two implications for the AI organization. First, the management has to ensure that the AI team always has an adequate amount of work – not too much, not too little. Second, while proactive quality control for digital assets such as AI models is possible, proactive quality control is impossible for human interaction. Training data scientists on how to handle customers is an important measure. Still, when a data scientist yells at a customer, you cannot “quality control” the interaction before it impacts the customer. You can only do “damage control” afterward.

**No upfront demonstration**. It is usually impossible to demonstrate an IT service before the contract is signed and the service is established. You cannot test-drive an IT or AI service. An AI manager or data scientist can explain the sales team’s success based on a new AI model. Still, such storytelling is no full equivalent to a live demonstration, for example, of the benefits AI can bring for a team optimizing an engine.

IT and AI service characteristics have clear **implications for an AI organization**. As mentioned, AI managers must carefully balance potential staffing needs and work gaps due to canceled or delayed projects due to the simultaneous provisioning and consumption of AI services. Managers with a consultancy or IT service provider background find this obvious. It is a new aspect for employees with a more academic background. The two other implications are similar – obvious for consultants, more surprising for others. AI team members require interpersonal skills for fruitful daily interactions, especially team members interacting with customers. Finally, when the AI organization requires identifying new projects, acquisition skills for AI managers are essential due to AI services’ intangible and impossible-to-demonstrate-beforehand nature. However, the exact requirements and needs differ from project to project and service to service – and especially between companies with different corporate cultures and organizational structures.

#### AI Service Types

Creating an AI model for identifying the best city to build the next shopping mall vs. running and developing an AI component for a webshop suggesting customers the next best item to buy – these are two completely different services. One is an AI project (service); the other requires a stable AI operations service.

Projects – **AI projects** and others – have clearly defined goals and deliverables with set due dates and transparent effort estimations. Delivering “on time and in budget” is the aim of all project managers.

The core deliverable of an AI project is one (or more) AI model(s) complemented by secondary deliverables. One typical secondary deliverable is an inference result, such as a list for bank advisors as to which customers they should call to sell a new investment fund. The list gets created by applying the AI model to the customers’ buying history. Another type of secondary deliverable is an analysis and explanation of an AI model pointing out and listing the (input) parameters with the highest impact on the overall result, for example, the carbon content or the exact heat when modeling steel production in a blast furnace.

AI projects need strong collaboration between AI specialists and employees from the line organization in the business to succeed. The latter have the required business know-how; they have to share it and must be willing to discuss and answer questions from the data scientists about the semantics of data or the exact business need and the business meaning of results.

The second service type consists of the **AI operations services**, which come in two variants. The first is integrating an AI component into a software solution. The software components trigger inference by the AI model whenever needed. There is no human involvement. The second variant of AI operational service requires data scientists or IT specialists to generate AI inference whenever needed or periodically, for example, every month. An operations engineer might generate a list of potential buyers of neon-orange handbags not only once but every month.

The AI organization has to balance the costs for automation with the costs for manual inference execution and handling customer requests – plus consider the operational risk of, once in a while, mixing up whom to send coupons for neon-orange handbags and whom for Ascot ties. Furthermore, even if the goal is complete automation, the second variant fits the minimum valuable product (MVP) concept: getting the solution to work and providing first results before working on advanced features such as full automation.

In any case, and in contrast to projects, AI operations services are a long-term commitment for many years. An AI operations service requires running and maintaining an AI platform, self-developed code, and interfaces and monitoring the model quality. From time to time, retraining the model is necessary, requiring short projects. Thus, an AI operations service needs (some) AI knowledge, but in general, running and maintaining an AI component is a standard IT operations topic. Data scientists are essential for model-related challenges. However, IT operations and application management specialists can take over all other tasks.

AI operations services have different collaboration patterns compared to project services. Projects require close interactions for efficiency. In contrast, operations services minimize interactions for cost reasons. They prefer asynchronous, highly structured, and standardized interactions: ticketing and workflow systems, not emails, phone calls, and in-person support. We discuss this topic in more detail later. Furthermore, Table [5-1](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Tab1) summarizes AI project and operations services’ characteristics and compares them.Table 5-1

Comparing AI project services and AI operations services

|                        | AI Project Services                                                                                                                           | AI Operations Services                                                                                                                                                                       |
| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Goals and Deliverables | Create an AI model and either apply it once (or a few times) or analyze the model                                                             | Ensure that an AI team or an AI component continuously delivers AI inference results                                                                                                         |
| Duration               | Limited. Projects end once all deliverables are provided                                                                                      | Long-running, ongoing service, usually a few years                                                                                                                                           |
| Duration               | Single, defined project with clear deliverables,                                                                                              | Typically several years                                                                                                                                                                      |
| Scope                  | <p>Create an AI model</p><p>and</p><p>Apply the model on data once or a few times</p><p>or</p><p>Analyze the insights the model provides.</p> | <p>Frequent manual inference</p><p>or</p><p>Running and maintaining an AI component with an AI model</p><p>Monitoring the AI model for degeneration</p><p>Trigger an AI model retraining</p> |
| Interaction Pattern    | Close collaboration between business and AI specialists                                                                                       | Mostly ticketing system                                                                                                                                                                      |

#### Characterizing AI Service Types

Is your AI projects and services organization similar to a fast-food or a fine dining experience? When you go to a fancy restaurant, a waiter will do some small talk, figure out what kind of food you prefer, and help you choose the menu. A sommelier pairs your meal with the perfect wine whereby considering your unique wine preferences. Then, the chef prepares the meal taking your allergies in mind. It is a different experience than ordering a Big Mac at McDonald’s over a touch screen and getting a bag with your meal two minutes later over the counter. These are two different experiences, two different market segments, and two different price tags. Similarly, any AI department should be aware of what their customers expect and what the management funds.

Peppard’s **IT Services Matrix** (Figure [5-1](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig1)) is a simple tool to validate customer expectations and the AI organization’s actual service proposition. Such a matrix considers two dimensions for assigning services to one of the four service types “service factory,” “service shop,” “service mall,” and “service boutique.”

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig1_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig1_HTML.png" height="954" width="1504"><figcaption><p>Figure 5-1</p></figcaption></figure>

The first discriminating criterion asks how much **customization** different customers need. The second criterion concerns the **interaction** between service employees and the user. How often and for how long do they communicate or collaborate to achieve a good result?

The “de-luxe” model is the **service boutique** : many interactions and much collaboration between the AI department and the customers and users, plus the option to place highly individual requests. Generating strategic insights for management advisory falls into this category. The top management has enough resources to fund dedicated AI projects – and they make sense to support high-impact strategy decisions. Also, unique product portfolio enhancement initiatives might request such services from the AI department. For example, a medical diagnostics company develops an AI-supported solution for diagnosing specific illnesses.

Besides C-level AI projects, regular AI projects for creating a new (non-trivial) AI model fall into the service boutique category. For example, data scientists have to analyze every detail when creating the first AI model supporting a marketing campaign. However, synergies might emerge later. Data scientists can reuse data sources or data preparation and cleaning logic when creating a second, third, or fourth model. It is a gradual development. Forward-looking AI managers foster and encourage such a trend.

The **service factory model** fits services for which customers have none or limited contact with the AI service organizations and get a standardized service fulfilling most needs without much individualization. Typically, an AI organization acts as a service factory when running AI models on an AI runtime server. The server has to run, the deployed AI models have to be operational, and the interfaces to the applications consuming the predictions and classifications must be up. However, there is no need for direct personal interaction with customers. Indeed, adding, improving, or retraining AI models means that the service switches (temporarily) to a service boutique style.

The category of a **service shop** covers requests with little interaction and still very much customer-specific tailoring. Software development projects fall into this category, so does designing a company’s IT network infrastructure. However, they are less typical for AI organizations with their tendency to require much interaction between data scientists and domain or business specialists. When AI managers realize that they try to set up and run such a service, they should analyze the overall situation. Is it truly a service shop use case – or does a project sponsor try to save money by making a project look like a service shop instead of a service boutique project? Such ideas save money on paper, but tend to result in spectacular failures.

The **service mall** category provides services with low individualization or customization but a high interaction and involvement with the customer. AI departments operate in this category if they provide contractor-style internal AI consulting. In such a case, the AI organization has a pool of data scientists who help other teams with AI-related tasks. These data scientists need excellent social skills and the ability to tailor solutions for individual customers. There is much interaction with the customer, but the customization is limited. It is like renting a car. You can rent a car with two or four doors – you can either get an AI consultant with Python or with R know-how, respectively, if your team needs AI support.

The different models – service factory, service mall, service shop, and service boutique – are of particular interest to AI organizations. They have implications on which service attributes are essential for concrete AI services.

#### Understanding Service Attributes

A well-managed AI organization focuses its efforts on service attributes that matter. Philip and Hazlett’s PCP attribute model helps AI organizations for getting priorities right. Their model distinguishes three types of service attributes: pivotal, core, and peripheral (Figure [5-2](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig2)).

Most important are the **pivotal service attributes** . Pivotal elements for AI services and projects are typically the AI model and its evaluation and usage in the customer context. A concrete example is an AI model for churn prediction for a phone company, together with a concrete list of customers potentially terminating their contract next week. Such deliverables require AI know-how, including statistics and machine learning know-how, experience with tools and with bringing together all relevant data for superior models.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig2_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig2_HTML.png" height="725" width="1729"><figcaption><p>Figure 5-2</p></figcaption></figure>

**Core service attributes** are essential for long-term customer satisfaction. They cover people, processes, and organization and how these elements interact and create a service experience. They should ensure a reliable and friendly service. For example, how do customer touchpoints look like and how do customers accept them? Is there phone support, and can the hotline be reached as announced? Are there ticketing tools that are convenient to use and safe money compared to phone support? Can the project organization deliver on time? In various IT departments in entirely different areas, I observed that failing to manage such core service attributes results in escalations and frustration.

Finally, there are **peripheral service attributes** for the wow-effect. They excite customers, managers, and users. For example, are reports and lists designed well? Do team members present at conferences and are, thus, interesting to collaborate with in a project? Are there community events to foster relationships between the AI service team and the customers – and between customers? They excite and get you some goodwill when things go wrong once, but they are useless if the AI organization fails to deliver pivotal and core attributes adequately.

Understanding, categorizing, and designing service attributes is an important task when setting up an AI service. AI managers must understand and/or define them. Staffing, resource allocation, and management attention must reflect the importance of the various service attributes.

#### Designing (for) and Measuring Service Quality

You work hard and go the extra mile. Still, your customer, boss, or a friend of yours is dissatisfied with the result and expresses his or her emotions brutally direct. It is frustrating in private life as well as in a business context. It can happen as well with AI teams and their internal customers and stakeholders. However, AI managers can reduce the risk of friction when understanding the various service quality layers (Figure [5-3](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig3)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig3_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig3_HTML.png" height="1071" width="1729"><figcaption><p>Figure 5-3</p></figcaption></figure>

Customer satisfaction for AI (and non-AI) services depends on the perceived service quality regarding

* _what_ the service delivers (pivotal elements) and
* _how_ it is delivered (core and periphery elements)

Indeed, the **expectations** depend on **objective needs** and requirements, for example, to boost sales for a specific mobile subscription. The image and reputation of the AI team impact the expectations, so do experiences from previous interactions. These are **subjective influence factors**. Customers and users form an expectation, based on these various factors, what they expect from the service. They compare the _expected_ service with the one they _perceive_ being delivered. Customer satisfaction does not (only) depend on how well the AI team delivers a service based on _their_ standards and guidelines. It depends on customer expectations and customer perception, too.

On the service delivery side, there are three different **service quality layers**. The starting point is the _perception_ of customer needs and wishes. The next step is to _design_ a service that meets these customers’ needs. Then, the designed service is implemented and _delivered_ by the AI team. Misunderstandings can happen in each step, impacting the overall adequacy of the service to meet customer expectations. As mentioned before, there is the customer perception of the delivered service, too. And, as always, some customers see a half-full glass, others a half-empty one.

The implication of this layered model based on various decades-old research on service quality from Grönsroos, Parasuraman, and Philip and Hazlett are threefold:

1.  1\.

    A stringent service design and experienced data scientists creating high-performing AI models are crucial.

    &#x20;
2.  2\.

    Excellent PowerPoint slides about how the service works are not sufficient. An AI manager, an AI translator, a team lead, or consultants must work together with the delivery team to implement the concepts in a working organization. They have to check whether the service concept works in reality.

    &#x20;
3.  3\.

    Managing stakeholder and customer expectations is a crucial part of the job for AI managers. Gaps are easier to handle before a project has started and while it is in progress than after burning all the budget.

    &#x20;

These three guiding principles sound familiar, trivial, and evident for seasoned IT service managers and professionals with an IT or management consulting background. However, they help highly technical experts being in a lead role the first time and helps them make all painful experiences themselves. Running a service is a combination of communication and commitment. Based on my service design experience, writing down the key points of the service for potential customers helps. What do you do, and what are the limitations? It does not make additional communications obsolete. It fosters discussing needs and expectations – and prevents wrong assumptions on the customer side.

### Managing AI Project Services

AI projects train AI models and derive actionable insights such as a target list of potential buyers for sales or typical buyers’ characteristics for product managers. Managing an organization running many such projects requires awareness of the distinct workload patterns, understanding cost structures for preparing budgets, and communicating the results to the customers. First of all, however, an AI organization has to understand the needed capabilities.

#### The Capabilities Triumvirate for AI Project Services

The term “capability” comes from enterprise architecture. It helps to express what an organizational unit can do. It does not mean that an organizational unit has just the theoretical knowledge to take over tasks. Having a capability means that skilled staff, tools, process definitions, and methodologies are in place. The organizational unit, a team, or the AI organization must be able to perform or performs already the tasks successfully today.

Following the CRISP-DM methodology and going through all the stages for creating an AI model (i.e., without the deployment phase that is more an operations and less of a project task), we can identify all capabilities an AI project services organization needs (Figure [5-4](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig4)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig4_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig4_HTML.png" height="904" width="1729"><figcaption><p>Figure 5-4</p></figcaption></figure>

The **core capability** is **data science** know-how. It is the base for creating an AI model in the “modeling” phase and assessing the AI model quality in the “evaluation” phase. It is the most apparent capability for AI organizations. Often underestimated are the two additionally needed capabilities of “AI translator” and “data engineering,” though they require more time and effort in nearly all projects.

AI organizations need the **data engineering capability** for the “data understanding” and “data preparation” phases for performing the following tasks:

* Organizational challenges such as getting permissions of data owners to use their data
* The data transport: users and permissions, network bandwidth, transfer tools
* Understanding the data model and the semantics of the data
* Analyzing the data quality

The distinction between data engineering and data science capabilities is essential for staffing. Superheroes that can do everything are rare and expensive. Most companies have specialists who know SQL and one or more of the company’s databases. They are highly efficient in preparing data exports. They can take over data cleansing and preparation without in-depth math, machine learning, or AI know-how. They are helpful in each AI organization, especially if they know data sources well. While they need to interact with data scientists to perform their tasks, they help the AI organizations by freeing up data scientists so that they can focus more on creating and optimizing AI models.

The third and last **capability** needed for AI projects is the AI translator. He understands the business question and formulates an AI task and presents the results to and discusses them with the business. He understands the types of insights AI models provide and should have a basic understanding for making some rough effort estimations. He does not have to be able to create and optimize models. The role is similar to business analysts in classic software engineering: understanding commercial challenges, asking the right questions when talking with users and customers, presenting results to the management, and writing high-level specifications. No software project sends its Java gurus to the C-level to discuss the strategic goals for the software development project. Neither should an AI manager send highly skilled and specialized data scientists.

Data science, data engineering, and AI translator form the capabilities triumvirate needed to support strategic decisions in an organization with one-time-use AI models. Figure [5-5](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig5) puts them in a broader context. Data engineering is close to the data providers and data sources. They ensure the AI organization can integrate as much of the data stored in any of the company’s databases. The AI translators work with managers from various departments to identify which strategic decision-making can benefit from AI. In other words, creating which AI models could help analyze strategic and operational challenges by predicting individual or group behavior?

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig5_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig5_HTML.png" height="838" width="1725"><figcaption><p>Figure 5-5</p></figcaption></figure>

Data scientists work with both of them, with AI translators to identify which AI models benefit the business and with data engineers to get the data they need. Indeed, the picture is not complete without external providers. For example, consulting companies might take over complete tasks or projects; contractors can augment internal teams. Plus, there are tool vendors (including public cloud providers) which the organization uses to improve its efficiency.

To prevent misunderstandings: Figure [5-5](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig5) lists many capabilities but does not imply that AI organizations should hire 2–3 persons for each of them. It is more like a list of tasks to be assigned to the team members. In a big AI organization, you and five other data scientists might work in a dedicated data science team. If the AI organization can fund two data scientists or engineers, the two together are responsible for everything. So, any team development strategy or process for hiring new specialists should consider all capabilities needed in general and the current team’s strengths and weaknesses.

#### Workload Pattern

When an AI organization runs (mostly) AI-based advisory projects, the workload has a specific pattern. It is typical for IT or management consultancy but potentially new to AI managers and data scientists with a different professional background. Being in the **project business** means acquiring and delivering project after project after project. Where should the company open the next new branches of a supermarket chain? Can we improve an engine the last time before presenting a new car to the press and the media? The projects follow the CRISP-DM methodology. Within a few days or months, the data scientists create a model and explain its implications to the customers. The project comes to an end. There are no follow-up tasks for the data scientists and no additional project costs. Instead, the benefit for the business might just start, for example, more revenues or lower costs (Figure [5-6](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig6), left). When one project ends, it is time for the data scientists and engineers to move to the next. Potentially, they might even use new and different technologies. It is the perfect working style for experienced data scientists always looking for a new challenge.

The implications for AI managers are the need to acquire new projects constantly. Only new projects prove the AI team’s relevance to the company’s top management and ensure funding. Continuous savings due to a finished AI project are not sufficient for an AI organization to stay relevant. If there is no next project, the CIO can shut down the AI organization without impacting the organization.

An AI organization (or a part of it) usually has a few or many parallel projects because there are always new ones starting, whereas older ones come to an end. Figure [5-6](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig6) (right) illustrates this aspect.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig6_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig6_HTML.png" height="559" width="1721"><figcaption><p>Figure 5-6</p></figcaption></figure>

#### Budgets and Costs

Impressive titles alone, such as Head of AI, are insufficient for AI managers to establish a flourishing AI team with data scientists, data engineers, and AI translators. They need a budget to fund their team and pay for the tooling and infrastructure of their AI projects. For budgeting, they have to understand the cost structure of AI project services. We look first at an individual AI project’s cost structure. Afterward, we broaden the view to the overall AI organization’s one.

Two **basic cost dimensions** are essential for the understanding: inevitable costs vs. costs-by-design and fixed vs. variable costs. Inevitable costs are costs you cannot avoid. Costs-by-design means that the management makes an explicit decision where the organization invests more than the bare minimum. Two or three screens per data scientist workstation instead of one is an illustrative example.

The second dimension distinguishes fixed and variable costs. When you buy servers for machine learning, these are fixed costs. They have to be paid whether or not AI model training takes place. In contrast, on-demand-charged VMs in the cloud is an example of variable costs.

AI projects have the following cost structure consisting of four categories (as shown in Figure [5-7](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig7)):

* Staffing costs for employees and contractors
* Infrastructure such as workplace for staff and compute and storage for machine learning training
* License fees for AI platforms such as SAP or SAS
* Centralized functions

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig7_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig7_HTML.png" height="509" width="1725"><figcaption><p>Figure 5-7</p></figcaption></figure>

When working with internal employees, the **staffing costs** are fixed costs for the AI _organization_. The company has to pay their employees even if they have not enough work or them. When looking at _project_ costs, staffing costs are variable. Projects typically only pay internally for employees to the extent they ask for their support. A mismatch between the overall work in projects and available staff in the AI organization is a challenge. Too much work causes project delays due to missing resources. Not enough work can result in cost-cutting activities. Finding the right balance is a crucial task for AI managers. One option to deal with workload fluctuations is to varabilize parts of the staffing costs by augmenting internal employees with contractors that scale up and down more quickly.

**Infrastructure costs** (besides employee workplaces) and**license costs** are fully variable for AI projects and AI organizations if the projects use cloud services. Alternatively, suppose an AI organization builds its own AI platform for its project. In that case, these costs are part of the **centralized service costs** . The latter also covers dedicated ticketing systems for interacting with customers or a Sharepoint to store documents. Therefore, project-focused AI organizations should carefully control costs for centralized services. If they are fixed costs, they can be a financial burden making the internal service potentially more expensive than externally sourced consulting services.

Once AI managers understand their cost structure, they have to ensure adequate **funding** . Details vary from company to company, and it can be a highly political issue. I just want to briefly explain two typical patterns: internal service providers and **prioritizing sponsors** (Figure [5-8](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig8)). The last one is the most convenient. A senior manager (or some of them) provides the funding. The consequence: He selects the projects on which the AI organization works. He who pays the piper calls the tune. If he is in marketing, the AI organization most probably focuses on marketing and sales topics. If the sponsor is the head of an R\&D department working on submarines, the AI team will optimize submarine-related challenges. Figure [5-8](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig8) illustrates a typical interplay between AI organizations and prioritizing sponsors.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig8_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig8_HTML.png" height="779" width="1721"><figcaption><p>Figure 5-8</p></figcaption></figure>

The second option is the internal **service provider budgeting** pattern (Figure [5-9](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig9)). It is typical for shared service organizations. The AI managers have to find customers in the organization who fund (at least) partially the costs. Variable, project-specific costs are easy to charge, whereas financing the fixed costs can be challenging. Adding a certain percentage to the variable costs to distribute the fixed costs is a typical mechanism.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig9_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig9_HTML.png" height="779" width="1721"><figcaption><p>Figure 5-9</p></figcaption></figure>

With this brief introduction, AI managers can discuss with their line managers and with their customers about costs and funding without needing a business degree.

#### Selling Results: Data Story Telling

Reaching your audience is vital for book authors, journalists, comedians – and AI organizations. Unfortunately, today’s air pressure in Guadeloupe is of no relevance for most of us. And for those few who live there, they care less about the air pressure and more about whether it will be warm and sunny or hot and rainy with thunderstorms tomorrow afternoon. It is the perfect example that what you can predict (air pressure) might not be directly helpful for the audience. Instead, they want to know whether it rains where they are tomorrow.

Similarly, data scientists have to know their audience as well. They are experts in AI and machine learning algorithms, predictive analytics, and statistics to find unseen and unexpected correlations in enormous data collections. However, project results must meet four criteria, especially for projects helping with strategic decisions:

1.  1\.

    **Relevant** for the audience. The marketing department is interested in customer insights and sales, not so much in optimizing support functions.

    &#x20;
2.  2\.

    **New insights** for the audience. What do they know after the AI project presented its results? What is new? A prediction that there is no snowfall in the Sahara desert tomorrow at noon might not be considered a groundbreaking new insight.

    &#x20;
3.  3\.

    Be **actionable**. The results should allow for some improvements and optimization or efficiency gains. Obviously, the benefits must be higher than the investments in the project.

    &#x20;
4.  4\.

    **Compelling presentation**. The message must make the audience curious and interested.

    &#x20;

These tasks fit nicely into the CRISP-DM methodology (Figure [5-10](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig10)). Checking the relevance of a topic is part of the business understanding phase when planning what to do and is repeated in the evaluation phase. Before presenting the results, AI project teams must check whether they provide relevant new insights and whether they are actionable. Certainly, presentations and written documentation should fascinate the audience as well. All these activities are highly creative and analytical. They are perfect tasks for AI translators who understand the customers and users and their business context in-depth.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig10_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig10_HTML.png" height="884" width="1509"><figcaption><p>Figure 5-10</p></figcaption></figure>

I want to illustrate this explanation based on a concrete example of turning a model into an awaking story. A Swiss newspaper in July 2020 presented a perfect example of how to present data in a dull and meaningless fashion. They reported that the Federal Office of Public Health (BAG) announced 108 new Corona infections in Switzerland and Liechtenstein. These are more than twice as many as the previous day. There were 43 newly confirmed cases on Monday, 99 on Sunday, and 110 on Saturday.

When going through the four criteria, we see that the news fulfills the first criterion. The piece of information is relevant for readers in Switzerland. But does it provide new insights? No. Readers do not know afterward whether the numbers are comforting or alarming. They heard for weeks and months that the numbers from one day to the next could not be compared directly, because they are higher on certain weekdays. Thus, a piece of news just providing today’s number and the information from three previous days is worthless. The number for each weekday from last week gives a much better indication. This Tuesday, we have 108 cases; one week ago, only 70. Yesterday, we had 43, the week before 63. While this might not sound alarming, this is different if the seven-day average indicates an increase of 12% per week.

Still, this information is too abstract for most readers to be relevant for them. The absolute numbers are too low (726 out of 8 million). It is more likely that you get 5 + 1 correct with the Euromillions lottery. However, the yellow press would make it relevant for everyone in Switzerland by connecting the numbers with the traumatic first lockdown in March of the same year. The number of daily infections on that day was 1063. With a 12% increase per week, Switzerland would have reached the same level again next winter. A catchy headline in big letters for the front page would be “2nd lockdown expected around Christmas.” Suddenly, an abstract number is relevant for everyone and even actionable for politicians – a perfect example of turning simple numbers into a dramatic story catching everyone’s attention.

To prevent any misunderstandings: This is and was not an actual forecast. The underlying data and the statistical models used are inadequate. The example highlights what communication can (over)achieve. When AI managers want to innovate the organization, their results must catch the attention of the business and reach their audience. Stakeholders or (business) managers and customers might have no AI background. They can be too impatient and busy to get their own understanding. In such (typical) cases, it is up to the AI organization to let AI translators help out or ensure that some data scientists have outstanding communication skills.

### Managing AI Operations Services

When AI organizations create models and integrate and run them in larger solutions, the set-up and working style of (this part of) the AI organization evolves further. Capabilities, financials, skills, and cost-drivers differ from a consulting-style project-focused AI organization. For AI operations services, new topics emerge, such as target operating models and clear support touchpoints to the outside world or model management. The AI organization needs six instead of just three capabilities.

#### The Six AI Capabilities

When looking at the needed capabilities for AI operations services, there are core tasks for an operations service – and tasks an AI operations organization has to perform, too. Indeed, it has to be able to run at least small-scale AI projects, such as creating (less complex) initial AI models and retraining models. Thus, an AI operations organization also needs the AI projects capabilities triumvirate: data science, data engineering, and AI translator. (Figure [5-11](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig11)).

When looking at the additional capabilities, especially relevant for AI operations organizations is the **integration engineering capability** . It covers installing and patching software components and configuring interfaces the AI organization takes care of afterward. Integration engineering includes connecting the AI components’ interfaces with other solutions to enrich traditional business processes with AI logic. An excellent example in the AI context is installing software such as RStudio Server or integrating RStudio with LDAP, which comprises typical **application management** (AM) and **IT operations** tasks: solving issues, installing patches, and coordinating releases with other applications and the underlying infrastructure. It might also cover integrating the AI components with the IT department’s central monitoring solution, which looks for aborted processes or errors and warning messages in logs to detect issues, ideally before they impact users.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig11_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig11_HTML.png" height="838" width="1725"><figcaption><p>Figure 5-11</p></figcaption></figure>

The **support capability** reflects that components do not always work as expected, requiring a support specialist to analyze the situation and fix issues. Thus, AI operations implies the need for a support team or at least for clearly defined specialists who take over user and customer interactions and ensure prompt reaction. They are the face to the users and customers, for example, if users miss a result, an interface breaks down, or interface configurations have to be changed. In addition, in an AI operations organization, they might perform routine AI tasks, for example, in the context of retraining models.

Involving application management and support specialists in routine AI tasks increases the AI organization’s resilience. These specialists demand mature scripts and clear documents when they take over a model from AI projects and data scientists. The benefit for the organization is obvious: From now on, when a data scientist gets sick, not everything breaks apart because he is not the only one anymore knowing all the scripts and tricks to run and retrain an essential AI model for online sales.

Based on my experience, one single person can take over support and application management in small and focused organizations with not more than a handful of user interactions per day and a few components and not-too-complex systems and interfaces. Still, these are separate tasks and capabilities. One focuses on reactive user interactions and solving easier challenges, including maloperations by end-users (support capability). The other, that is, application management, is more proactive maintenance and analyzing the trickier issues when the AI components or interfaces fail. The combination makes sense due to the similar know-how and social competencies needed. Obviously, however, if a role is filled with just one person, deputies are mandatory.

One capability might or might not be needed: **Report Provisioning, Presentation, and Visualization**. AI often delivers lists, but – especially when explaining models are at the core of a project or an AI organization – more complex visualizations help. Also, when the AI organization provides new lists daily or weekly, sending these lists out as CSV or Excel files attached to emails is questionable. It is work-intensive. Plus, operational mistakes likely happen over time. The AI organization sends out old lists to sales staff, resulting in a drop in revenues. Also, someone might send such an email by mistake to persons outside the organization, potentially disclosing sensitive data. Thus, a platform for providing reports is essential – and the more users, the more important is a stable platform with a defined support model. Ideally, an AI organization feeds its results into the company standard solution such as a Business Intelligence (BI) solution or an Enterprise Resource Planning platform.

#### Workload Pattern

Recurring business and flattening the (workload) curve are two slogans on how AI service organizations work with support and application maintenance tasks.

**Recurring business** means that the AI organization provides ongoing services. A support organization, software maintenance, and application management are necessary for users and customers of AI models or components. The AI model generates business benefits or operational efficiencies, but only as long as the AI service organization works. If the top management shuts down the AI organizations, the benefits and efficiencies are gone from that moment. Figure [5-12](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig12) illustrates this spendings and benefits pattern over time.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig12_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig12_HTML.png" height="692" width="1404"><figcaption><p>Figure 5-12</p></figcaption></figure>

The “flattening the workload curve” slogan reflects that operations services can, up to a certain degree, shift workloads to different days, weeks, or even months. Application managers and support staff can work for several teams and applications in parallel. The example in Figure [5-13](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig13) illustrates this. The AI service organization supports and maintains AI models and interfaces for three applications. Additionally, they run one project, which goes live soon. If there are separate teams for each of them, the App X team requires three specialists, App Z three and two for App R and one for Project 13. Altogether, the organization has to pay for nine specialists. When sharing, they need four – and if they manage to shift one workday from day three to another day, then only three. Many maintenance tasks can be moved by a few days or weeks, which is the key to flattening the workload curve and the organization’s efficiency.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig13_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig13_HTML.png" height="984" width="1725"><figcaption><p>Figure 5-13</p></figcaption></figure>

#### Understanding and Managing Costs Drivers

The big challenge for managers of AI operations services is to invest sufficiently in preventive measures for stabilizing the solution and in the availability of support and engineering staff to handle incidents and support cases. The intentional investment into running the service does not generate profits. Not spending enough, however, results in service negligence costs and harms the business. Two radical approaches illustrate the challenge. The support desk can be one intern reading emails and helping with wrongly used GUIs on Monday morning, Wednesday, and Thursday afternoon. Alternatively, a 7/24 phone support hotline provides access to three senior engineers on call, plus professional monitoring and alarming, making incidents even less likely. The costs differ, so does the impact on the rest of the organization.

The **cost for running the service** is the money the organization (willingly) spends for the support capability and ongoing application management. Therefore, the first cost domain is **support and problem management** with the following three primary cost drivers (Figure [5-14](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig14)):

* **Customer interaction design**. Phone, drop-in service-desk, email, ticketing systems, FAQs, one-page documentation leaflets, trainings – there are many ways to provide customer and user support. However, the right approach depends on the volume and complexity of the requests and impacts the time spent with users and customers.
* **Support and reaction times**. 7/24 support requires more engineers and generates higher costs than if the support only works from 9 am to noon on weekdays. Also, if tickets have to be answered within five minutes and not within four hours, staffing needs and costs explode.
* **Diversity of the service portfolios**. What technologies are in use, such as Python, SAS, SAP Analytics, or R? How different are the implementations and business aims of the implemented services? Even just helping customers use the GUI requires the support staff to understand the technologies. Much more technical know-how is needed when having to solve support cases requiring real engineering. The underlying question is whether one engineer per shift can handle all requests knowledge-wise – or whether there is a need for two or three engineers. It is a vital question before adding any new technology to the service portfolio.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig14_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig14_HTML.png" height="596" width="1729"><figcaption><p>Figure 5-14</p></figcaption></figure>

The latter aspect reflects the impact of technology and technologies. The **complexity of the IT infrastructure** impacts the service costs as well. First, this covers more **integration**-related elements such as the number and stability of the interfaces. For example, is a messaging middleware in place, or is the communication and interaction self-coded? Are the applications the AI components work together stable, or are there always incidents and discussions which solution is the root cause for issues? These questions relate to the interplay between production systems. However, an AI organization must also maintain its **internal** hardware and software **infrastructure** with CI/CD pipelines, especially if integrated into pipelines from other software development teams, runtime servers, AI platforms, etc.

**Business continuity management** and **monitoring** represent additional cost blocks. Details depend on whether there is a solution and processes in place by the IT organization. So again, they do not help you generate business value; they “only” detect problems earlier or reduce the impact of issues.

All these mentioned costs for support and application management are directly visible. Controlling specialists understand precisely the money spent, but not the benefits. Such constellations are an invitation for budget cuts during cost-optimization initiatives. One person less? You should not see any impact. We could cut one engineer without any service impact; why not cut a second position as well? It is like the story told about a university. To save money, they reduced the room temperature in winter to lower the costs for heating. You can repeat this exercise year after year – until your electricity bills explode because your staff brings electric radiators once offices were too cold to work. It is similar to application management and support costs. They can be cut more or less to zero. The impact is severe: negligence costs within the AI organization and business impact outside the AI organization.

**Negligence costs** refer to costs and efforts generated within an AI organization because support and application management do not function properly. The first impact is a disruption of ongoing AI (project) tasks. Improper, unprofessional handling of adequate or even inadequate user and customer requests can result in escalations. Escalations are time-consuming for the management, but for data scientists and engineers as well. The latter have to collect data, search for old email communication, etc. These efforts have to be paid and hinder ongoing projects. In case there is no support organization, data scientists answer the simplest user questions. In other words: the data scientists get disturbed with their project work on new AI models if first-time users do not understand the GUI or push the wrong button. Also, if systems stop working, the time pressure for patching or reconfigurations is much higher. Thus, more persons have to work on the issue than performing measures preventively in a less hectic setting. Furthermore, every issue you detected with monitoring before users call or open tickets reduces the workload for the support channel.

Finally, the **business impact** and **potential losses** reflect the non-realized business opportunities or savings because an AI is not working. For example, suppose the AI component generates upselling and cross-selling recommendations accounting for 20% of the revenues of the webshop. In that case, these revenues are gone when the AI component is down or starts making bad suggestions. If the business managers are not aware, AI managers should bring this on the table in budget discussions when senior managers challenge the costs for model quality and service stability.

#### Model Management

“It is mine, I tell you. My own. My precious. Yes, my precious.” The famous quote from “Lord of the Rings” is a seldom heard expression of care and obsession about an artifact. AI organizations do good taking similar care for their AI models and treating them as valuable corporate assets and intellectual property. That is the task of model management.

Model management has only limited real IT or AI challenges. Still, it is the most technical of all AI operations challenges. It covers three main aspects. The first relates to quality assurance during the CRISP-DM model engineering process. The second aspect is monitoring and managing the model quality after deployment. The third and last aspect is how the organization handles models as artifacts during their lifecycle (Figure [5-15](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig15)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig15_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig15_HTML.png" height="659" width="1721"><figcaption><p>Figure 5-15</p></figcaption></figure>

A previous chapter was completely dedicated to quality assurance during the model creation with some brief remarks about post-deployment quality measures. We highlight a different perspective in the following.&#x20;

A **reactive approach** is an **automated model scoring** **and** **KPI monitoring** looking for changes (“drift”) in the input data and the derived predictions or classifications (Figure [5-16](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig16)). Does the input data distribution change, or more concrete, for example, its average or lower and upper values? Does the output distribution change over time, or the business impact change as a drop in the conversion rate? When the monitoring detects that a KPI drops below a threshold, it can generate an alarm for the data scientists such that they are prompted to retrain the AI model. The simplest option is to train the same neural network architecture with the most up-to-date data (which might still might contain older data if still relevant and helpful). The most comprehensive retraining approach is to repeat the complete model development process, including feature development and hyperparameter optimization.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig16_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig16_HTML.png" height="267" width="1109"><figcaption><p>Figure 5-16</p></figcaption></figure>

It is a reactive approach since the retraining starts only if a detectable decay of a KPI signalizes a need. On the other hand, a **proactive** approach is to retrain even if the KPIs still look good. The **champion-challenger pattern** falls into this category. The model currently in production is the champion model. Though the model (still) meets the quality requirements, data scientists already prepare a new model named the challenger model. Data scientists rely on the latest data or try more sophisticated optimizations and compare the champion and challenger models’ performances. If the challenger model outperforms the champion model, the challenger model becomes the new champion model. Otherwise, the data scientists discard the challenger model and start working on the next one. Figure [5-17](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig17) illustrates this champion-challenger race. In general, the champion-challenger pattern is a kind of A/B testing trying to find the best solution.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig17_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig17_HTML.png" height="534" width="1504"><figcaption><p>Figure 5-17</p></figcaption></figure>

Besides the quality aspect, looking at **models as artifacts** and optimizing their handling and, hopefully, smooth progress through corporate processes is another model management topic. Data scientists work with them, share them, document them, store and archive them, or version them. Many tasks are familiar from software engineering; others are new. However, three aspects are especially relevant (Figure [5-18](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig18)):

1.  1\.

    Tool-based **managed deployment processes** **orpipelines** enforce corporate procedures and reduce operational risks and hurdles. Classic IT organizations tend to implement sophisticated approval processes for signing off requirements, quality assurance, and deployments using tools such as Jira or ServiceNow. This old-world approach focuses on auditability. It is mandatory for many organizations and potentially binding for AI organizations and how they create and deploy models. The newer trend is that developers and data scientists automate the release building and deployment using CI/CD pipelines. They make manual work redundant. They save time and reduce the risk of deployment mistakes (wrong environment, wrong model, wrong parameters, etc.). AI managers should check whether their processes are fully integrated into the company’s change management processes, rely on the same tooling, and fulfill the audit requirements.

    &#x20;
2.  2\.

    **Code conversion** functionality helps data scientists and software developers when they work with different technologies. One of the options discussed in previous chapters for integrating an AI model into a software solution is to (re)code the model in the programming language the solution developers use. Data scientists or software engineers can perform this task manually, obviously an error-prone activity. Alternatively, code conversion packages automate this step. The more development languages and the more different AI tools a company uses, the more critical is this aspect.

    &#x20;
3.  3\.

    Data scientists must store models as valuable corporate assets in a central **repository**. A repository guarantees that models do not get lost or deleted by mistake, enables collaboration between data scientists, and ensures auditability.

    &#x20;

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig18_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig18_HTML.png" height="984" width="1300"><figcaption><p>Figure 5-18</p></figcaption></figure>

The repository needs some additional explanation. AI organizations can, for example, use the IT department’s code repository that they set up for software engineering. An AI organization would store, obviously, the AI model’s parameters and hyperparameters. Furthermore, additional information is vital for the deployment process, for searching for models, or for surviving audits. Such potential additional information is:

* The data preparation and model training **code**, accompanied by the actual **data or a reference to the data**. Only this combination enables data scientists to recreate precisely the same model at a later time.
* **Data lineage** documenting how the data got from the source to the point it became training data. For example, does the financial data come from the enterprise data warehouse or an obscure Excel created by a trainee in the housekeeping department and copied and modified by five teams afterward?
* **Scores** measuring the model performance. They document the rigidness of the quality assurance activities and prove the adequacy of the model.
* **Audit logs** documenting crucial activities such as who changed the training code or data preparation or deployed the model to which environment.
* **Tags** storing additional information. Examples are the data scientists that worked on the projects, the exact purpose of a model, or why a data scientist used a particular hyperparameter value.

Storing AI models in a repository together with additional (meta-)data, enforcing a rigid deployment process, and monitoring the after-deployment performance of models – these are the three pillars of model management. Of course, model management is only a sandwich layer between the trendy and hype data science work and the glamorous management level. Still, model management is the glue needed for a sustainable AI initiative. This glue allows corporations to benefit from AI initiatives for years, not only for weeks or a few months. Finally, model management gives future data science projects a kick-start with all the model repository information and artifacts – reuse cannot get more comfortable.

#### Organizing an AI Organization

Congratulations! You shaped your service, analyzed the necessary capabilities, prepared a budget, got funding, and hired the right talent. However, to kick-start your AI service, your staff has to know how to work on which tasks, how to collaborate within the AI organization and the rest of the company, and how to handle and complete user and customer requests.

**AI project services** are forgiving if the AI organization is not super structured – they (should) have explicit goals. The AI management just assigns engineers and data scientists to projects and clarifies how much time they can spend on which project. The CRISP-DM methodology structures the daily work of the AI experts (Figure [5-19](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig19), left).

**AI operations services** require a more sophisticated organizational setup. Just assume that an online shop AI component suddenly proposes ski equipment to customers searching for beachwear. In that case, the AI organization is under high pressure to fix the issue – calmly, speedily, professionally.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig19_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig19_HTML.png" height="846" width="1417"><figcaption><p>Figure 5-19</p></figcaption></figure>

The big difference to projects is that most operations organizations serve multiple (internal) customers, applications, or services in parallel. They react to incidents and dedicated customer requests. Typical tasks are restarting components, fixing interfaces problems, or configuring a new interface. They schedule preventive maintenance such as installing patches when vendors provide new ones. Such tasks keep engineers busy for a few hours or days, not for months or years. To keep the support specialists’ work pipeline full, they take care of multiple applications and interfaces as illustrated in Figure [5-13](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig13) – or they work in the context of one super-complex application.

The challenge for service organizations: Everybody wants to benefit from the lower costs of a shared service, that is, specialists work not only for one interface or one small component. Still, everybody expects superior service, even if other customers and users have issues simultaneously, and compete for the attention of the support organization.

When preparing an AI operational team for such daily challenges, organizing the actual service delivery (such as implementing changes on systems) is just one task. Crucial for success is also the **customer touchpoint** as part of the core and peripheral service attributes. Here, users, customers, and the AI operations organization interact. Answering phone calls from first-time users or sending emails forward and backward to get all details for a service request is time-consuming and keeps AI operations specialists busy. Having to sit in meetings and workshops can be worse. No manager wants to pay 20 hours for a two-hour task due to time-consuming workshops, notes being made, and employees exchanging many emails. AI organizations have to prevent such situations.

But costs are just one aspect. Making sure that AI operations experts can focus on tasks without being interrupted by customer requests is a second need. Working on critical updates requires uninterrupted attention for some time. Flattening the workload over time is a third crucial element for cost efficiency. Non-urgent tasks are delayed to low times, even if customers think their needs are the company’s most important and urgent ones.

AI managers have to design the customer touchpoint carefully. FAQ pages and good documentation reduce the need for reaching out to the AI operations team. A widely followed service-design option is to prevent direct contact between AI operations specialists or engineers and users from other components. Group mailboxes are a starting point; the only long-term solution is request forms or a ticket system such as Jira. They ensure transparency in which teams required how much support. Furthermore, if another team member has to step in due to holidays or sicknesses, she can access the information about all incidents. There are cases when users and customers need a specific contact person. In that case, this requires additional staffing in the AI organization – and someone has to fund such premium service.

The main challenge for the **service delivery** is to ensure a smooth processing and routing of requests and a good collaboration within the AI organization and with other teams. All involved specialists must know which activities they are carrying out, which interim results they receive from their colleagues, what they have to deliver as results themselves. The solution? Clearly defined and agreed-upon processes.

For an AI organization, four processes or process areas are of particular interest:

* Incident process: Something that worked before is suddenly not working anymore and has to be fixed. An example is an AI runtime server abruptly discontinuing to reply to requests.
* Change Request: Something that works should be changed or moved to the next level. For example, there should be an alarm for model degeneration. In the future, the alarming should take place when less than 20% of the customer buy proposed items instead of 30% now.
* AI Requests: questions, wishes, and requirements related to the actual AI models, for example, retraining a model or clarifying questions about the model.
* General inquiry: standard input window for other questions and not time-critical aspects.

A **process definition** clarifies who is doing what, when, and using which tools and systems. Managers and experts with a strong IT or science background tend to define processes that cover all possible cases. This approach is super-expensive, identifying every conceivable case is often challenging or impossible, and the process definition distracts the users from the core ideas, that is, what operations and support specialists should focus on to deliver an excellent service.

The preferable approach for defining processes is to focus on modeling and training “happy flows.” These are the cases where the process goes through smoothly. Only frequently occurring error cases are modeled, not every exception. Exceptions are seldom an issue in practice for independently acting, well-trained support and operations specialists. They find a solution. If the situation is beyond their area of expertise and comfort, they contact the management. Just one warning, this assumption might not be correct for every company culture and each cultural context.

A **swim lane process model** (Figure [5-20](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig20)) is a straightforward process modeling technique useful for documenting AI operational processes. Each swim lane represents a department, a team, or a specific role. A process consists of different steps. Their placement in a column or swim lane defines who is performing the task. If the employees work with a specific IT system to fulfill a particular task in the process, the system used should also be specified. Swim lane process models (in contrast to, e.g., UML Sate Machines) can only visualize processes without too many case distinctions or loops. If a swim lane model gets too complex, splitting it into two or three can help. Otherwise, the process is probably too complicated for everyday usage by a team. Once an AI organization has defined all its core processes (e.g., the four mentioned above) and trained all team members, the organization is ready to work.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig20_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig20_HTML.png" height="863" width="1721"><figcaption><p>Figure 5-20</p></figcaption></figure>

A guiding principle for an AI organization designing and implementing its processes is **compatibility with the company’s other processes**. If other teams have to perform a specific step, these teams must also agree on the processes and tools. In many cases, IT departments define their processes based on the de-facto standard, the **Information Technology Infrastructure Library**(**ITIL** ). However, just knowing the standard does not help an AI organization much. IT departments tailor ITIL processes heavily. Who is precisely approving requests coming from which system using which forms? What are the tools for which procedures? The details are company-specific but crucial for working AI processes.

Many AI solutions come with integrated workflow engines helping an AI organization to structure its work. It is a convenient option, but AI organizations should clearly understand the limitations: they might have to use, in parallel, other systems and copy tickets from one system to another. Suppose a database team should export and hand over some data. Will they accept tasks assigned to them by a workflow system of the AI organization? Maybe, but for sure not in large IT departments. What happens when the marketing team notices that the “customer who buys dresses by also …” feature does not work? Will they open a ticket in the company-wide **incident management** system and let the IT department figure out the details? Or would they love to figure out themselves whether it is an AI or a webshop issue, search where the right incident ticketing system is, and open a ticket in this specific tool of the AI organization? Apparently not. AI organizations have to ensure that their tooling for processes and requests follows the company standard. Otherwise, they look for trouble when their systems have any relevance to the business.

While processes are crucial, many organizations nowadays talk about **(target) operating models** (TOMs) that define how an organization works. TOMs describe an intended future operational model. In contrast, the term “Current Operating Model” refers to the current status quo. An AI organization providing operational services usually needs a TOM. The purpose is not so much for keeping the operational processes up. The real purpose is more to discuss strategic directions for the AI organization, elaborate on how to transform the organization, and clarify its possibilities and limitations.

A (target) operating model cover the following core aspects (Figure [5-21](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_5\_Chapter.xhtml#Fig21)):

* **Service pledge** with the value proposition. Here, the AI organization’s business value and shaping the service as presented at the beginning of this chapter help.
* **Processes**: Which activities are processed in which order by whom and using which tools? We discussed this a few paragraphs up.
* **Staff and organization** taking over the tasks defined in the processes.
* **IT** with the infrastructure and applications (and their parameterization) as a core “enabler” for (efficient) process execution.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_5_Chapter/516372_1_En_5_Fig21_HTML.png" alt="../images/516372_1_En_5_Chapter/516372_1_En_5_Fig21_HTML.png" height="954" width="1413"><figcaption><p>Figure 5-21</p></figcaption></figure>

AI managers can specify the first three topics and write them down quickly based on this book’s input. Many supporting topics, including KPIs, capacity planning regarding IT resources, staff, or souring strategies (e.g., internal employees, contractors, consulting companies) require, for sure, more clarifications. The fourth important aspect of an operations model is the needed IT infrastructure. It is the topic of the following chapter.

### Summary

This chapter elaborated on the specifics of managing an AI organization and not just any team within an IT department. In particular, it covered three topics: shaping an AI service, managing the AI project services for creating new AI models, and ensuring reliable operations when it comes to running and maintaining (the infrastructure) for AI models and inference based on them.

Shaping a service is about understanding the collaboration patterns between business and AI organizations. These patterns directly impact funding needs and how an AI organization presents its services to potential internal customers to manage their expectations properly. It also guides the AI managers in clarifying which aspects of a service are pivotal, core, or more peripheral as a base for investment decisions.

AI organizations that want to run AI projects need data science know-how, but also data engineering capabilities to efficiently make use of existing data and AI translator capabilities to bridge the gap between business needs and concrete requirements what an AI model should deliver – our capabilities triumvirate. Understanding the cost drivers and the budget structure were additional aspects, so was pointing out the main challenge of project-focused funding: balancing project acquisition to prove the organization’s relevance and ensure funding vs. available resources to execute the acquired projects.

When AI organizations want to run and maintain AI models, they need additional capabilities: support and application management, integration engineering, and reporting and visualization. They have to manage the AI models and monitoring the model performance as two additional AI-specific operations tasks. Finally, process definitions and target operating models help AI organizations establishing a structure and ensuring smooth, day-to-day operations.

This knowledge is what distinguishes an AI manager from a conventional IT manager or a technology- and algorithms-focused data scientist.
