# 6. AI and Data Management Architectures

The AI capability in an IT landscape is more than just the icing on the cake. It is a core capability for forward-thinking, data-driven, innovative companies. While not just the icing, it is also not the whole cake. An AI organization has to integrate its components into the overall corporate application landscape – and its own system landscape typically consists of more than just a few Jupyter notebooks. In other words: This chapter looks at the big architectural questions any AI organization faces that is more than just a one-man show.

The architectural challenges relate to three topics:

* Architecting AI environments for model training and ongoing execution and inference
* Integrating AI environments into the corporate data management architecture
* Understanding the impact of the public cloud on AI environments

### Architecting AI Environments

When it comes to an organization’s efficiency and its capabilities, tooling is a central enabling factor. The tooling impacts and determines the services an AI organization can deliver and how efficiently they can work.

We looked already at Jupyter notebooks in an earlier chapter. They help to create and optimize AI models. Here, we broaden the perspective and look at the other enabling systems an AI organization needs to work efficiently (Figure [6-1](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_6\_Chapter.xhtml#Fig1)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_6_Chapter/516372_1_En_6_Fig1_HTML.png" alt="../images/516372_1_En_6_Chapter/516372_1_En_6_Fig1_HTML.png" height="992" width="1734"><figcaption><p>Figure 6-1</p></figcaption></figure>

#### Ingestion Data into AI Environments

Training AI models and applying them to actual data for inference require adequate data. The data might come from operational systems, data warehouses, physical sensors, log files, or other data sources. The challenge is to build an architecture that delivers all potentially relevant data to the AI training environment or to the components performing AI inference in the production system using the trained AI models (Figure [6-2](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_6\_Chapter.xhtml#Fig2)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_6_Chapter/516372_1_En_6_Fig2_HTML.png" alt="../images/516372_1_En_6_Chapter/516372_1_En_6_Fig2_HTML.png" height="884" width="1400"><figcaption><p>Figure 6-2</p></figcaption></figure>

The training and the inference use cases are similar regarding setup and technical solutions, though two nuances differ. For **training data**, the rule of thumb is: **more is always better**. More attributes per data point increases the chance that more attributes turn out to be relevant for and contribute to the model. The model accuracy gets better, no matter whether the model predicts or classifies. More rows stabilize the model, and more tables allow for a broader range of use cases. If input variables are irrelevant, they simply do not become part of the final model.

**Inference data** serves as input for the trained, ready-to-use AI model. Such data describes the current situation for which the AI model makes predictions or performs classifications. For example, the data might describe the status of a chemical reactor, and the model predicts whether the reactor will blow up within the next hour. Such an AI model has a set of input parameters. Data scientists or automated tools must provide the current values precisely for all these input parameters. More data does not bring a benefit. Suppose the model needs the reactor’s pressure five minutes ago. In that case, the prediction does not improve if we also know the pressure two days or one minute ago – or even tomorrow’s weather forecast for the Sahara desert. The model cannot incorporate such uncalled for data into its predictions and classifications.

While not-asked-for data is useless for inference, architectural flexibility is advisable. The next version of the AI model after retraining might have two more attributes. Ideally, data scientists have an architecture that allows removing attributes or adding new ones from an already connected component with a reconfiguration, that is, without changing code.

A second nuance between training and inference data is the different **data delivery frequency**. Model retraining is an activity data scientists perform from time to time, for example, every few weeks. Inference takes place more often. For example, a marketing department might run an AI solution twice a week that personalizes newsletters by predicting which articles and which ads work best for which customer. Even real-time inference is possible, for example, when targeting customers with specific ads and items while customers browse an online shop.

The **technical implementation** for the data delivery or data ingestion can build on existing data integration patterns (Figure [6-3](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_6\_Chapter.xhtml#Fig3)). Most companies run one or more suitable system already. The characteristics discussed in the next few paragraphs help to communicate the needs to internal data integration teams or external vendors and integration partners. Certainly, it helps as well if the AI organization has some understanding of the potentially possible patterns.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_6_Chapter/516372_1_En_6_Fig3_HTML.png" alt="../images/516372_1_En_6_Chapter/516372_1_En_6_Fig3_HTML.png" height="479" width="1729"><figcaption><p>Figure 6-3</p></figcaption></figure>

The probably best-known database-related integration pattern is the **database federation pattern** . A federation defines how database schemata from various databases relate to each other. The pattern allows querying tables from multiple databases and even joining them without knowing and caring in which database they reside. When considering the two AI data ingestion use cases, the federation pattern does not help to get data from operational databases to training environments or production systems for inference.

**Bi-directional patterns** keep redundant data in two (or more) systems consistent. For example, a solution has one database in Zurich and one in Singapore to reduce data access latencies. Both store the same data. Bi-directional update patterns enable applications and users to write to their regional databases, not just one global master. Bi-directional patterns, such as “synchronization” or “correlation,” propagate the changes done in one database to the other database and update the data there – entirely transparent for the users and applications.

Bi-directional patterns for training or inference data would promptly forward new or updated data from operational databases to the training and inference environment. However, they also have unwanted and potentially catastrophic consequences: Suppose a data scientist deletes the table with all payments of the current month in the training environment. Since every update is synchronized to the other database, the data disappears from the operational database. Not a good idea! Thus, bidirectional patterns do not fit the data ingestion use cases. We need a pattern that ensures that updates and data flow from the operational databases (or sensors, data warehouses, logs, etc.) to the AI environment, not vice versa.

Three **one-directional patterns** are of particular relevance. The first one is the **one-time copy pattern**. A database administrator or a data scientist copies data manually from the source systems to the training environment to train an AI model and run inference afterward. This pattern avoids investments into automating data copies and requires manual work. Typical use cases are a proof-of-concept, AI models supporting unique strategic decisions (expand to Latin America or the Pacific region?), or AI models requiring infrequent retraining with potentially changing source systems.

The second one-directional pattern is the **Extract, Transform, Load (ETL) process** pattern. It is well-known from data warehouses. The pattern has three steps: extract, transform, and load. They represent extracting the source system’s relevant data, transforming the data to match the target schema, including data cleansing and consolidation, and loading the data into the target system. Data scientists or engineers implement these steps typically in SQL scripts. An orchestration component invokes and executes them, ensuring a stable and reliable execution, for example, every Sunday at 2:30 am.

The ETL process pattern is a typical example of batch processing. You collect data over time; then, the system processes all the collected data together at once. The pattern fits AI use cases that require data for periodical retraining or precalculating inference results. To give an example: A bank wants to know the product a client most likely buys additionally. At the beginning of each month, the bank updates the training data with the newest customer behavior and data and retrains its AI model. Then, the bank (pre-)calculates the next best product for each customer. When the customer logs in on his mobile app or contacts the call center, the system or the call center agent proposes this product to the client. It takes the information from the precalculated data, not requiring any online inference.

The ETL integration pattern is probably the most relevant for an AI organization supporting and optimizing business-critical operational processes and decisions. Calculating every week or month which customers might cancel phone plans is not the most innovative use case. Still, these are the ones ensuring the funding for many AI organizations since their business value is clearly measurable – and for such use cases, the ETL pattern works perfectly.

The third one-directional pattern is the **event-driven architecture** pattern. Changes in the physical or virtual world – a new sensor value or a customer clicking on a fashion item – trigger an event. Events flow through interconnected applications and components using technology such as Apache Kafka. The event routing is part of the business logic. It determines which systems see and process which events. There are two relevant differences to the ETL process pattern. First, the (near) real-time processing. Second, the publish-and-subscribe communication style. Components put events into channels such as “customer actions” or “weather sensor CDG.” Other components subscribe to channels to receive and process these events. Potentially, they put the result into another channel to trigger follow-up processing. It is an utterly decentralized computing model.

Event-driven architectures allow for real-time processing. There is no benefit in delivering _training_ data in (near) real-time since retraining does not occur so often. However, some use cases benefit from real-time _inference_.

The choice of the integration pattern and technology, however, should be well thought out. Switching the pattern or technology costs time and money. It is a complex undertaking for larger organizations. Thus, the AI organization should incorporate the life-cycle aspect as well. Which integration patterns are available in two or three years? What does the IT department plan for ETL tools? Event-driven architectures are seldom necessary, but using them can be a strategic move to protect investments since IT departments move in this direction. Ideally, the AI organization ensures it is aligned with the IT department’s integration architecture.

So, to sum up, one-directional patterns are the solution for ingesting training data into training environments and for delivering data for inference to AI models. Choosing a fitting pattern for training and inference data ingestion is a child’s play after answering three questions:

* How often do you build or retrain a model in your training environment?
* Is the AI model used for real-time inference, or are the inference activities performed as batch jobs, for example, at the beginning of each month?
* What is the integration architecture’s roadmap for integration patterns and tools over the next years?

#### Storing Training Data

Training data ingestion patterns deliver data into the AI training environment, which has to store this data. A multitude of traditional and new technologies exist for storing data (Figure [6-4](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_6\_Chapter.xhtml#Fig4)). AI organizations should make an informed decision on which ones to use in their training environment.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_6_Chapter/516372_1_En_6_Fig4_HTML.png" alt="../images/516372_1_En_6_Chapter/516372_1_En_6_Fig4_HTML.png" height="438" width="1729"><figcaption><p>Figure 6-4</p></figcaption></figure>

The most popular form for organizing, processing, and storing structured, table-like data are **SQL databases,** under which we also subsume **data warehouse** technology.

Data warehouses are optimized for executing complex queries against large data sets most efficiently (OLAP – online analytical processing). Architects and database administrators rely on specialized schemata and table structures for more efficient query execution (“star” and “snowflake” schema). In contrast, OLTP – online transaction processing – optimized databases support higher transaction rates, that is, reading, writing, and updating one or a few rows. For most AI use cases, the difference between OLTP- or OLAP-optimized databases should not have a considerable impact (at least if there are not too complex queries for extracting data). Most probably, most data processing takes place in Jupyter notebooks, not in the database.

An SQL database for storing (some of) the training data is a must for an AI organization, though other technologies might be needed as well. The relevance of SQL databases bases on the fact that most (business and/or corporate) data reside in SQL databases in tables. Tables have the data structure AI training algorithms need as input. Thus, transforming data into a different structure to store the data usually causes costs without providing any benefits.

SQL databases have additional advantages. First, staffing projects is easy because SQL know-how is widely available on the market. Second, most IT departments have database administrators that can help with complex technical questions. The AI organization does not need specialized know-how. Finally, an AI organization needs just a plain-vanilla SQL database without advanced features. So, license-cost-free databases such as Maria DB are perfectly suitable.

While database and data warehouse technologies are from the last millennium, the rise of **No-SQL** – Not only SQL – **databases** are a phenomenon of the 2010s. The world of No-SQL databases is heterogeneous from focusing on scalability (by reducing on transactional guarantees) over schema-on-read (we see this with data lakes as well) up to different data structures. The latter is the focus here: key-value stores, document databases, and graph databases. They all store data differently.

The data model of **key-value stores** is truly minimal – pairs of a key and a value, such as <114.5.82.4, 20.07.2020 09:32>. Retrieving the value (e.g., the last time the IP address connected to a database) requires the key (e.g., an IP address). AI organizations can store such key-value pairs easily in SQL databases. Thereby, they prevent setting up a technology zoo.

**Document databases** store – surprise – documents. Documents are semi-structured, with the most popular formats being XML and JSON. Documents allow nesting attributes and do not enforce a fixed attribute structure. A typical JSON document looks as follows:{  "article": {      "title": "Mobile Testing",       "journal": "ACM SIGSOFT Software Engineering Notes"      "author": {          "firstname": "Klaus",          "lastname": "Haller"      }  \}}

An AI organization should challenge the need for setting up one or more dedicated document databases in their AI training environment, even if source data comes from document databases. There are good alternatives, though SQL databases are usually none. Due to the semi-structured nature of documents, SQL databases with strict schemata are generally not a good fit. In contrast, data lakes and object stores are potential alternatives we discuss later in this section.

**Graph databases** make modeling and querying complex interconnected topics and relationships intuitive. Social networks are an excellent application area. A graph consists of nodes and of edges that connect nodes. Nodes can, for example, represent persons or topics. Both nodes and edges can have attributes to store additional information. Figure [6-5](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_6\_Chapter.xhtml#Fig5) contains various node types: individual persons, a band named Spice Girls, their albums, and some of their songs. The edges represent the relationships like being part of the band, having released a particular album, or a song being part of an album.

While applications based on graph databases can retrieve exciting results and insights, there is no technical requirement to perform such an analysis in a graph database rather than an SQL database. Graph databases cannot store data or data relations you cannot put into an SQL database. Graph databases are more an optimization. They ease writing queries for specific scenarios or allow for quicker query execution. Should an AI organization add a graph database to its technology stack? The answer depends on the circumstances. Does the AI organization have to run a graph database itself, or does the IT department have an expert team running them, or can they use a software-as-a-service graph database service freeing the AI organization from installation, maintenance, and patching? Is this data widely worked with, and does the graph database store important, relevant information? How is the intention to use the graph-structured data for training AI models? Are there performance issues in SQL databases that cannot be solved otherwise? Graph databases require a business case to validate that they bring a financial benefit compared to not using them in the AI environment.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_6_Chapter/516372_1_En_6_Fig5_HTML.png" alt="../images/516372_1_En_6_Chapter/516372_1_En_6_Fig5_HTML.png" height="959" width="1729"><figcaption><p>Figure 6-5</p></figcaption></figure>

A **data lake** is a suitable choice for massive amounts of data stored in files, even petabytes. In contrast to a simple file system, data lakes allow searching for the content of files and aggregating information. For example, count the number of “Potential Breach” log entries in the folder structure 2021/07/\* and its subfolders to understand whether there are more incidents than the month before. Data lakes implement a schema-on-read functionality but operate – in contrast to document databases incorporated into applications – on massive data sets covering data from various applications. There is no need to define a data structure or schema, just where and how to look for relevant attributes within a file. Thus, the data lake continues executing a query without throwing an exception if not finding the attribute, implying that there is no direct feedback if specific attributes do not exist – other than in case of SQL databases.

AI organizations earlier or later have to deal with semi- or unstructured data. They have to store such data somewhere, and, typically, SQL databases are not adequate. Is a Hadoop data lake the right choice? It requires effort to set up and maintain. If you are a big organization, maybe. Smaller organizations can use a data lake service in the cloud.

Finally, **object storage** is a storage option worth mentioning as well. Since the launch of AWS S3, the relevance of object storage is getting bigger and bigger. What was a filesystem in the past is object storage technology today: the place to put files and documents. It is the “standard storage type” in the cloud. In contrast to traditional files, manipulating files is not possible, just replacing them, which does not impact the work of an AI organization. However, it makes sense to invest some time to elaborate on how to deal with unstructured (and semi-structured) data.

#### Data Lakes vs. Data Warehouses

Data lakes are a popular choice for AI organizations to capture and store massive amounts of data, including text, audio, video, and sensor data from various source systems. At the same time, companies also run substantial data warehouses, which are especially strong for reporting and analyzing large amounts of (business) data. While data lakes and data warehouses might look like redundant concepts, they are not. AI organizations benefit from both, though technical processes and the commercial aspects differ.

What ETL – Extract, Transform, Load – is for data warehouses is ELT – Extract, Load, Transform – for data lakes. The steps are the same; their order differs (Figure 6-6). Deferring the transformation is a game changer cost-wise. The transformation step is the most time-consuming and expensive one. It covers cleansing plus ensuring consistency. Ensuring consistency is an analysis-intense, manual task, especially if several databases and reports contain similar, but slightly differently defined key performance indicators. Discussing with various experts and managers in different business teams and achieving a consensus can take several weeks. The benefits: A data warehouse has “gold standard” data quality on which engineers and business users can rely, and that is why managers fund the data warehouses. Thus, avoiding the cost-intensive transformation step before loading data into the data warehouse is no option. As a result, adding single attributes is already an investment, limiting the option to add data quickly. The management has (always) a say in whether and what data the data warehouse teams add.

In contrast, data lakes store uncleansed raw data. There is no discussion about single attributes; there is a decision on which database to add or which folders with massive amounts of log files. Adding data incurs no high costs, neither for analysis in the project nor for storage afterward. These low costs are the success factor for data lakes. Engineers can just add data based on a vague hope someone might have some kind of idea somewhere in the future to use the data. This someone, then, pays for the transformation, including cleaning and preparation.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_6_Chapter/516372_1_En_6_Fig6_HTML.png" alt="../images/516372_1_En_6_Chapter/516372_1_En_6_Fig6_HTML.png" height="913" width="1725"><figcaption><p>Figure 6-6</p></figcaption></figure>

#### Data Catalogs

A crucial prerequisite for high-quality AI models is adequate training data. Which customers are likely to terminate the contract in the following weeks? Which screws on the assembly belt have a defect? Data scientists can answer (nearly) all questions of the world if they have access to the necessary data for training their machine learning model.

Data warehouses provide large amounts of well-structured, well-documented consistent data. In contrast, data lakes collect a multitude of what data warehouses store, but without similar documentation – not what data scientists love, but an integral part of any data lake business case, as elaborated some paragraphs earlier. Plus, operational databases and other data storage means can contain additional data of interest for data scientists. A data catalog contains **information about data** in the various data sources, be it operational databases, data lakes, or other data. It helps AI projects **finding** potentially relevant **training data** they do not know yet. A data catalog makes the difference between a useful data lake and a useless data swamp. It is an enabler and speeds up the work of AI projects.

Data catalogs can provide information on the data attribute and the table level. Table level information consists of three elements (Figure [6-7](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_6\_Chapter.xhtml#Fig7)). First, a data set has a name and a unique identifier. Second, a data set has a description of what this table is about, typically enhanced with category information based on a company’s classification system and keywords that allow potential data users to find data quickly. Third, there is additional meta-data such as publisher, when it was published, by whom, data lineage info, etc.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_6_Chapter/516372_1_En_6_Fig7_HTML.png" alt="../images/516372_1_En_6_Chapter/516372_1_En_6_Fig7_HTML.png" height="488" width="1721"><figcaption><p>Figure 6-7</p></figcaption></figure>

The data set description is the most crucial information in the data catalog. It enables data scientists to search for and identify potentially useful data sets that help training an AI model they work on currently.

Data catalogs also provide information on the attribute level, including the technical data type (String vs. Integer) and, ideally, the domain data type (e.g., whether a string stores employee names or a passport ID). The data catalog might even provide minimum and maximum values for each column. Figure [6-8](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_6\_Chapter.xhtml#Fig8) illustrates these two levels.

**Domain data types** of attributes and data set descriptions reflect one typlical data catalog use case: Finding all the columns with IBANs, customer name, or patient records, which is essential for legal and compliance teams (e.g., in the context of GDPR), but not really helpful for data scientists.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_6_Chapter/516372_1_En_6_Fig8_HTML.png" alt="../images/516372_1_En_6_Chapter/516372_1_En_6_Fig8_HTML.png" height="913" width="1721"><figcaption><p>Figure 6-8</p></figcaption></figure>

Data warehouses come with data catalogs and glossaries – and everybody knows that they are time-consuming to set up. So, can AI organizations or IT departments reduce costs and manual work by automating creating data catalogs data lakes with **data loss prevention (DLP) tools**, for example, with Symantec or Google Cloud DLP?

The short answer: **legal and compliance** teams benefit, data scientists not. DLP tools are great for finding domain data types. They identify which tables or files of databases and data lakes contain, for example, customer names, patient records, or religious affiliation information. Thus, they are perfect for identifying GDPR-relevant information. They do not help to understand the more sophisticated **semantics** of files and tables. Are these all customers with outstanding payments, or are these really important Swiss customers? Was the radiation measurement from yesterday or a week after the Chernobyl catastrophe? Collecting and compiling such information requires human work. Stringent enforcement of maintenance processes is the only way to ensure everybody does her tasks in this context. IT departments must technically prevent any upload of data to a data lake or the AI training data collection if the uploader does not provide a description with all information relevant for data scientists.

From Excel and MS Access to dedicated commercial software, AI organizations and IT departments have many options for data catalogs. Reasons for opting for sophisticated and more expensive solutions are their integration of crowd-intelligence features and their support for **governance processes** – the latter matters, especially in times of GDPR or with ethical discussions becoming more critical. Sophisticated processes and adapters to systems enable specialists to work more efficiently, though this is more a compliance than an AI topic. **Crowd-intelligence** mimics advice from senior experts who help new colleagues. A senior expert would direct her younger colleagues to data sets they need for their work. She knows where to find the data when the bank clients went to which branch. She knows where to find the clients that clicked on the details page for mortgage products in the mobile banking portal. Data catalog features mimicking such advice base their advice on statistics about frequently used data sets and attributes or typical queries submitted in the past. Such advice does not need direct human input. A less high-tech approach to crowdsource catalog information is to let data scientists (or other data users) rate the relevance and quality of data sets. The latter one, for example, is implemented in Qlik’s Data Catalog using three maturity levels: bronze for raw data, silver for cleansed and somehow prepared data for data analysts to work with, and gold for reports directly useful for business users. Incorporating crowd-intelligence is still at an infant stage, though potentially soon a game-changer for the productivity of junior data scientists or data scientists new to a company.

To sum up: Data catalogs are crucial for benefitting from all data in a data lake and from less-known data sources in a company. They are a matter of discipline and stringent processes when establishing and maintaining the data catalog. They are a big help for compliance and governance issues as well – and the upcoming crowd-intelligence functionality is a great chance to increase data scientists’ productivity.

#### Model and Code Repositories

Repositories are a must-have technical capability for AI organizations aiming for operational smoothness. We introduced them briefly earlier. They reduce the risk of having no backup copy of a crucial model or mixing up AI model variants and versions. Plus, they ease collaboration when multiple data scientists and engineers work on the same or similar models.

Repositories act as a hub for all AI models and additional data and store the following:

* The model’s **purpose**, that is, a description of what the model achieves
* The **AI model** itself, for example, a Jupyter notebook file, including its previous versions
* **Other code** with relevance to production usage (interface code from integration engineering) or for reproducing the model (data preparation and cleaning scripts)
* **Experimentation history** documenting the model quality during the training phase, including architectures and (hyper)parameter settings
* **Approvals** for production usage, for example, a workflow action like an “approval click” or an email uploaded to the tool

Repositories for AI models can have various levels of sophistication, from just using GitHub to integrated MLOps platforms from public cloud providers such as the Microsoft Azure Machine Learning MLOps or specialized AI vendors such as Verta AI with fancy dashboards and CI/CD deployment pipeline integration. Since models are business-critical intellectual property, securing and protecting the repositories is mandatory.

#### Executing AI Models

Superior AI models are great for improving a company’s operational efficiency if integrated into the organization’s operational processes. In other words, applications invoke AI models for predictions or classifications. Besides precalculation without any integration, we discussed two approaches for technical integration earlier: **AI runtime server** and **integrating models into the application’s code-base** by reimplementing them. We already discussed these topics from an integration engineering and a testing perspective – here, we focus on more architectural aspects.

Integrating the model by reimplementing them as part of the application code means that the architectural responsibility is not within the AI organization. The software architect and the software development team are responsible. The same applies to running their software solution.

There are various implementation options for an AI model runtime environment, mainly AI platforms from commercial vendors, open-source solutions, and the unique setup of edge computing/edge intelligence.

**Commercial AI Vendor Platforms** such as SAS or Dataiku or public cloud providers have a common and straightforward sales proposition: convenience, user-friendliness, and high productivity. They provide AI runtime environments easing not only the model training but also deployment and usage. Customers pay for these convenience and productivity benefits. Plus, they implicitly accept a vendor-lock-in, for example, if data scientists cannot transfer the data preparation and cleansing on a new platform.

One important remark. There are de-facto commercial platforms looking like “free” or marketed with providing “open source” components. They might even not charge any fees for software or integrated development environments and marketing using open-source components and industry standards. Still, they can generate a (cloud) vendor lock-in. You might not pay software license fees, but you pay by not being able to shift your workload quickly to other (cloud) vendors. When opting for “free” or “open-source” technologies in the cloud, AI organizations should check this detail because accepting a vendor lock-in opens up many more opportunities when choosing an AI platform.

Architecting an **open source-based AI platform** means putting together various open-source components to build an environment tailored to the needs of a concrete AI organization. For example, training models in Jupyter notebooks, using GitHub as a repository and pushing the results packaged as a Docker container to a public cloud provider for scalable execution or to run them on an internal cluster. The AI organization (and their internal customers) can discuss who runs and monitors the models or whether they become part of the actual code-base.

Finally, there is **edge intelligence** . Edge intelligence means that inference does not occur only in the company’s data center or a cloud data center, but that there are edge servers deployed in all regions taking over the AI inference (Figure [6-9](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_6\_Chapter.xhtml#Fig9)). The physical proximity solves latency issues, for example, when devices in the Australian outback invoke a prediction service on a server in Iceland.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_6_Chapter/516372_1_En_6_Fig9_HTML.png" alt="../images/516372_1_En_6_Chapter/516372_1_En_6_Fig9_HTML.png" height="634" width="1721"><figcaption><p>Figure 6-9</p></figcaption></figure>

Edge intelligence makes sense for IoT solutions with devices with limited compute and storage capacity. The assumption is that these devices cannot run AI interference locally. Over time, probably a second use case will gain more importance: protecting your AI models as crucial intellectual property. Companies might avoid deploying AI models to IoT devices to prevent competitors from getting access to a model by buying or stealing a device running AI models on them.

AI organizations can deploy an edge intelligence pattern themselves. However, relying on a cloud provider eases the deployment, primarily when potentially benefitting from other features, for example, integrating IoT devices and using the AI or data management features. To prevent from getting too naïvely into a cloud provider lock-in situation, having a transparent AI and cloud strategy helps – for IoT and edge intelligence and your AI platform.

### AI and Data Management Architectures

While the previous section was about AI environments, we look now at how the AI environment fits in and benefits from an organization’s overall data management architecture – and how AI relates to similar other services. Figure [6-10](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_6\_Chapter.xhtml#Fig10) provides a high-level overview with all relevant components and concepts, including, obviously, data warehouses.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_6_Chapter/516372_1_En_6_Fig10_HTML.png" alt="../images/516372_1_En_6_Chapter/516372_1_En_6_Fig10_HTML.png" height="934" width="1725"><figcaption><p>Figure 6-10</p></figcaption></figure>

#### AI and Classic Data Warehouse Architectures

Databases and data warehouses are not only helping to store data in AI environments. Companies use them already for decades. A pre-AI architecture for data-driven companies consists of an operational database layer and a data warehouse layer.

**Operational databases** serve business applications by storing and processing data. For example, they store master data, product descriptions, bank transfers, or point of sales records.

Business users, potentially with the support of database administrators, can submit ad-hoc queries against the data. Did our marketing campaign targeting golden-agers result in more customers from this age category signing new contracts last month? It is an example of an ad-hoc query with a clear analytics focus.

Querying operational databases comes with two **limitations**. First, there is a potential impact on operations. Long-running complex analytical queries can impact the database stability and performance and, thereby, the overall system. It might not be suitable to submit such a query against the database of an online shop. The second limitation is the focus of operational databases. They (typically) store only data of one application or one specific domain. Combining data from different databases (e.g., one with online shop data and one with retail store data) is work-intense for ad-hoc queries and requires high efforts to keep them stable over months and years. Opening firewalls or dealing with service accounts and certificates are just some sample tasks. Thus, nearly all companies run one or more data warehouses to collect and combine data from various operational databases.

A typical **data warehouse** has three layers: the staging area layer, the consolidated data warehouse layer, and the curated data marts layer. The **staging area** is an intermediate storage place for copied data arriving from operational databases or other data sources during the ETL process. The data warehouse software extracts data from operational warehouses, transforms, and loads the data into the consolidated data warehouse layer during the ETL process. The transformation also includes aggregating data, cleansing the data, and sorting out inconsistencies. The third data warehouse layer provides curated **data marts**. They provide main user groups such as accounting, sales, or operations reports with the data relevant for them in a user-friendly way.

From an **AI organization’s perspective**, a data warehouse with its data marts and the tables in the consolidated layer is a big opportunity. It is a source of high-quality training data with company-wide agreed-upon semantics and from all areas of a company. Building a similar solution is often a multi-million, multi-year endeavor any AI organization should avoid. However, data warehouses have two clear limitations. The data is not real-time data. ETL processes typically run overnight to minimize the impact on operational databases during office hours. Thus, data is at least one day old, but companies might not refresh certain data more than once per month. Second, data warehouses are inflexible when it comes to adding new data. The rest of the company expects the data warehouse to deliver consistent data, requiring an in-depth analysis before adding an attribute – or one or multiple tables. Still, data warehouses are an excellent chance for AI organizations to kickstart activities in new areas with some initial, high quality, and “investment-free” training data.

In such an ideal scenario, the AI organization builds its solution as a new top layer, basing on the data marts or the consolidated data warehouse layer, as illustrated in Figure [6-10](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_6\_Chapter.xhtml#Fig10). The less the AI organization deals directly with operational databases, the more cost-efficient and agile it can operate.

#### Self-Service Business Intelligence

Naming self-service business intelligence (SSBI) tools the biggest fraud in recent data management and analytics history might be a slight overstatement. The problem is not their functionality, but what they are sold for to customers. Many vendors try to make their customers believe that the tool enables them to perform similar analytics as data scientists and AI algorithms can do – and this is not correct.

As the architecture in Figure [6-10](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_6\_Chapter.xhtml#Fig10) illustrates, SSBI tools are on top of data warehouses or data marts (as AI components are as well). The tools **enable business users** to self-define and create tables and views. In contrast to data warehouse views and tables, business users might have a very narrow perspective, not looking left or right or whether a similar performance indicator already exists. Speed, convenience, no effort are the usual foci of business users. Besides all criticism, SSBI tools solve a business need and ease the collaboration in corporate functions such as controlling in worldwide organizations. When a job is about copying data together from various Excels and cleaning and consolidating this information, SSBI tools can greatly help due to their collaboration features. However, when AI organizations consider using data from SSBI tools, they should be aware that the data quality might be less reliable than data from a data warehouse or an operational database.

SSBI tools become an issue for an AI organization and the company’s innovation potential if it **hinders the adaption of AI**. SSBI tools are gold-plated Excel alternatives with more features to play around with, but not enabling business users to manually generate insights and models on the same level with the result of AI projects (and providing AI algorithms without explanation might also not result in useful models). The issue with SSBI tools is not their features, but if business users insist on “analyzing themselves” AI problems using SSBI tools instead of letting a statistical or machine learning algorithm do that much better or trying to create AI models without understanding the needed methodologies. This is a big risk or disadvantage with SSBI tools, and it might cause issues with the standing of an AI organization within the company. AI managers should beware.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_6_Chapter/516372_1_En_6_Fig11_HTML.png" alt="../images/516372_1_En_6_Chapter/516372_1_En_6_Fig11_HTML.png" height="925" width="1725"><figcaption><p>Figure 6-11</p></figcaption></figure>

#### Pantheistic Intelligence

Pantheistic intelligence reflects that neither data warehousing teams nor AI organizations have a monopoly on collecting massive amounts of data and generating additional insights with AI methods. Software **vendors enhance conventional products** with AI features, whether they develop core banking systems, building control applications, or network management and monitoring solutions (Figure [6-11](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_6\_Chapter.xhtml#Fig11)).

For example, a network team might want to speed up the resolution time for customer incidents by identifying a root cause quicker. A typical scenario: a WLAN router crashes, causing hundreds of devices being not reachable anymore, and resulting in hundreds of incidents from monitoring. This flood of incidents hinders network support engineers from identifying the crashed router as the root cause.

In such a situation, the AI organization could offer help to train a model for identifying root causes when there are several “real” incidents causing massive amounts of secondary, non-root-cause incidents. AI specialists train a model based on very few cases. They get the needed training data from a monitoring team, or by creating interfaces to all network and hardware devices. But there is a quicker solution: The network management buys a network monitoring and management solution that comes with interfaces for devices from all important manufacturers and contains an AI component besides the classic network management and monitoring features. Moogsoft is such a software provider. You have to provide a lot of data and do a lot of training, but still, would you go for the ready-to-use standard software or choose self-development if you are the head of networking in your organization?

Pantheistic intelligence is a reality. More and more software vendors incorporate AI functionality in their products. Thus, AI organizations might gravitate to strategic applications with a direct customer and business impact, for which the company wants to integrate data from various sources to build a superior model to achieve a competitive advantage.

#### New Data Categories

Master data and transactional data, this was what most systems could store some years ago. Today, new data categories are standard such as log and behavioral data, IoT data, or external data.

It was a long time ago when online fashion shops just kept your shipping address and order history. Companies today are data-hungry and store and analyze **behavioral data**. How do customers navigate in an online fashion shop? Which items do they scroll over, and which things make them pause for one – or ten – seconds? Often, such data comes from **log files**. **IoT devices** deliver sensor data such as pressure and temperature information – or pictures and video streams. They are core building blocks for digitizing new business sectors such as logistics, manufacturing, or production – or the basis for innovating the core products of technology and industry companies.

If companies do not have the needed data internally, they rely more and more on **external data** providers. External data can be “paid for” data, for example, regarding customer creditworthiness or validating shipping addresses. Many countries and public offices also make their data publicly available for free, for example, as part of “open data” initiatives. Such external data often comes in an easy-to-use form, such as tables in CSV files. When AI organizations identify such valuable external data to be beneficial, they can easily integrate the data. In contrast, behavior data, log file data, and IoT data typically require more data preparation until an AI organization can integrate the data and use it for training purposes. Such investment might not pay off for each small AI case. Still, it is an excellent chance for optimizing AI models that have a considerable impact on corporate success, be it customer understanding or optimized logistics.

### Cloud Services and AI Architecture

The public cloud providers open a gigantic innovation potential, especially for SMEs, to integrate highly innovative, ready-to-use technologies without upfront investment. They change how IT organizations work, how they think and approach AI, and are the most significant influence factor on AI architectures for the 2020s.

Cloud computing builds on three main pillars:

* Infrastructure as a Service (IaaS)
* Platform as a Service (PaaS)
* Software as a Service (SaaS)

Best known are IaaS and SaaS. Everyone has been using **Software-as-a-Service** solutions already for years: Google Docs, Salesforce, and Hotmail are well-known examples. They are convenient for users – and the IT department. SaaS takes away the burden of installing, maintaining, and upgrading software. Using an AI solution in the web, for example, managed Jupyter notebook environments, makes own installations obsolete – though the effort for integrating them with other company systems should not be underestimated.

IaaS – **Infrastructure-as-a-Service** – thrives as well. Many IT departments migrated some or all their servers to the cloud. They rent computing and storage capacity from a cloud vendor in the cloud provider’s data center. Thus, IT departments do not have to buy, install, and run servers anymore – or care about data center buildings and electrical installations. IaaS brings major benefits for AI organizations because they solve the issue with highly fluctuating processing needs. AI organizations do not have to buy hardware with enough capacity for extreme loads for training large AI models once a month. Instead, they rent as much as necessary and when necessary. They get immediate and unlimited scalability, high reliability, or any combination of that.

IaaS and SaaS revolutionize IT service delivery. Pushing a button gets you a VM. Office O365 makes installing software patches on company servers obsolete. Quicker and cheaper – but neither IaaS nor SaaS enables you to build revolutionary new services and products for your customer. PaaS – **Platform-as-a-Service** – opens up this opportunity. PaaS is a game-changer for fast-paced innovations. Every AI department should closely monitor the innovation pipeline of the cloud providers.

With PaaS, software developers and AI specialists assemble and run applications and services with ready-to-use building blocks such as databases, data pipes, or tools for development, integration, and deployment without dealing with complex installations or IT operations. The real game-changer is the massive investment of the big cloud service providers into production-ready AI services from computer vision to text extraction, aiming to make state-of-the-art AI technology usable by developers without AI background. Plus, they have exotic-obscure offerings such as ground station for satellite data or for building augmented reality applications – and they allow third party providers to make their software available, hoping for a network effect as known from the iStore or the PlayStore.

These PaaS services also impact AI organizations, and not every data scientist might be happy about every change. If a cloud provider offers image recognition, there is no need and opportunity to train generic AI models anymore themselves. Application developers use these services instead of asking the AI organization to train an AI model. For example, they need as much time to integrate AWS’s Rekognition service as for writing a printf-command – and as a result, they know whether a person smiles on a picture and whether there are cars on the same image. Commodity AI services will come from the cloud. Consequently, AI organizations might have to move to more complex, company-specific tasks and build and maintain more extensive AI solutions – or lend out engineers with knowledge of, for example, Azure’s AI functionality.

On the other hand, AI services from the public cloud require some governance. IT departments and AI organizations should not be too naïve. First, cloud services are not free. Business cases are still needed, especially if you are in a low-margin market and need to call many expensive cloud services. Second, the market power changes. Some companies pressed their smaller IT suppliers for discounts every year. Cloud vendors play in a different league. Third, using specific niche services – the exotic ones which help you to design unique products and services to beat your competitors – result in a cloud vendor lock-in.

The cloud-vendor lock-in for platform-as-a-service cannot be avoided. Enterprise, solutions, and AI architects must manage this unpleasant reality. A simple measure is separating solutions and components based on their expected lifetime. “Boring” backend components run for decades. IT departments must be able to move them easily to a different cloud vendor with little effort. Then, there are innovative, short-lived services, mobile apps, or web-frontends. They have a life expectancy of less than a decade – plus, every new head of marketing or brand officer demands fundamental changes. For such solutions, vendor lock-in is of less concern. You can change the technology platform anyhow every few years when developing the next version.

The implications for an AI organization are crystal clear:

* Data preparation and data cleaning activities are probably the artifacts that have the most extended lifespan. Being able to reuse them when changing the platform is essential.
* The platform for “do-it-yourself” AI model development does not matter too much. All platforms support linear regression and neural networks. Retraining these models is less of an issue if the data preparation works.
* PaaS offerings such as labeling support, object detection, or speech recognition result in a high vendor locking. They are not standardized; functionality and interfaces differ.

Vendor lock-ins are painful. Some vendors are feared for their creativity to play around with licensing and pricing every year. On the other side, organizations cannot prevent any form of lock-in, especially if the alternative is missing out on the quick wins of AI PaaS services. Being attacked by more innovative competitors using the power of PaaS is not a viable alternative. IT departments might ignore cost-savings and agility improvements IaaS and SaaS promise, but AI organizations should be careful when rejecting AI PaaS services as innovation opportunities.

### Summary

The first architectural challenge for AI organizations is to organize their AI environment for training AI models and, potentially, for inference using the trained AI models. There are apparent components such as training areas with Jupyter notebooks, plus data storage such as databases or data lakes for the training data. However, only well-maintained data catalogs actually lift the data treasures in data lakes – and without data ingestion and data integration solutions in place, updating the training data is a nightmare. Model management helps keep track of the existing AI models and their versions, preventing chaos such as losing models or deploying outdated ones. AI organizations might also manage AI runtime environments for inference – or solutions teams take over the models, integrate them in their code, and run the application without the involvement of the AI organization.

AI organizations can only efficiently operate if they get their hands on the necessary data. Thus, it is vital to be aware of the options such as directly exporting data from operation databases, IoT devices, data lakes, or external sources. Still, the best sources are data warehouses: well-documented, well-maintained, consistent data covering many domains. A data warehouse enables AI organizations to create AI models for various new areas or business questions quickly.

In the coming years, more and more AI environments will move to the cloud. Paying only for compute resources when training a model is a significant booster for resource effectiveness. However, the biggest change to AI organizations is the AI PaaS services making training standard services such as plain-vanilla image detection obsolete and freeing up data scientists to work on the tough company-specific topics.
