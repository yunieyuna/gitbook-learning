# 7. Securing and Protecting AI Environments

Once upon a time, IT and information security were simple. IT security specialists deployed malware agents and set up a firewall – and the organization was safe. Today, information security is much more exciting and complex, requiring everyone’s contribution. Even data scientists in AI organizations must be aware and contribute to security – and AI managers might suddenly get accountable and responsible for proper security measures for their organizations’ AI infrastructure. The pressure is ubiquitous, but not the necessary know-how – especially not in AI organizations. To complicate matters, many senior IT security and risk professionals do not have an in-depth understanding of AI methodologies and techniques they assess. A dilemma for which this chapter provides the solution by addressing four topics:

* The CIA triangle describing the central aims for IT and information security
* Collaboration patterns and responsibility distribution between IT security and the AI organization
* AI-specific security threats and risk scenarios, including potential attackers and assets under attack
* Concrete technical and procedural measures to mitigate AI-related risks and improve the overall level of security

Figure [7-1](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig1) visualizes their interdependencies. The following pages elaborate the topics in detail.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_7_Chapter/516372_1_En_7_Fig1_HTML.png" alt="../images/516372_1_En_7_Chapter/516372_1_En_7_Fig1_HTML.png" height="621" width="1209"><figcaption><p>Figure 7-1</p></figcaption></figure>

### The CIA Triangle

Information security professionals frequently refer to two fundamental principles of information security: the need-to-know principle and the CIA triangle or triad. CIA is an abbreviation for confidentiality, integrity, and availability (Figure [7-2](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig2)). These three terms make it clear: information security is more than protecting your network with firewalls and running anti-virus software. Thus, they apply to and impact AI organizations and their infrastructure and platforms, too.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_7_Chapter/516372_1_En_7_Fig2_HTML.png" alt="../images/516372_1_En_7_Chapter/516372_1_En_7_Fig2_HTML.png" height="717" width="817"><figcaption><p>Figure 7-2</p></figcaption></figure>

**Confidentiality** means keeping secrets secret. Users can only access data and systems they need for their work (need-to-know principle). Typically, employees get necessary access based on their roles. Other access rights require someone to grant access explicitly. A data scientist working for HR needs access to HR-related data stored in the data lake. In contrast, a data scientist working on sales topics does not have access to HR data.

The second aspect of the CIA triangle is **integrity**. Integrity means that data and information in databases, files, and data lakes are correct. Applications, users, and customers can rely on them. For example, training data is highly critical for creating a usable AI model. Removing all data from a country or related to particular minorities impacts the correctness or quality of the model. Integrity does not require making all data manipulations and tampering technically impossible. Ensuring integrity requires detecting manipulations, for example, by using and validating checksums. Thus, integrity relates closely to non-repudiation. A user or engineer can only change data, applications, or configurations after authenticating herself. Authentication is a prerequisite for logging who did what. Needless to say: It is essential to store these log files tamper-proof.

The third and final concept of the CIA triangle is **availability**. Data and information stored in files, databases, and software solutions deliver only benefits if one can access the data. When a building access system relies on an AI component to identify employees, the system must be operational 7/24. If the system is not available, nobody can enter the building. Typical technical measures for availability are backups or redundancy. The architecture must accompany these measures with additional protection against denial of service attacks or unintentional application shutdowns.

### Security-Related Responsibilities

For technical and AI-focused specialists, security is often just a sideshow. Most engineers and architects enjoy putting the various AI-related components and systems together. When projects face the issue of who takes over a particular task, there is usually a solution within the project team, even if there is a shortage of engineering capacity. The challenge is more to keep the systems secure for the years they run. Who periodically inspects logs for irregularities? Who checks whether vulnerabilities emerge and vendors provide patches – especially forzero-day attack vulnerabilities?

When AI organizations run training environments themselves, they are responsible for runtime servers and might have model repositories or data lakes. The AI organization must ensure that these systems are secure. It is often unclear which tasks a security organization takes over and which ones the AI teams have to organize themselves. A particular challenge for AI managers is understanding a typical double role of IT security organizations: security services and security governance (Figure [7-3](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig3)). Both help a company and an AI organization to secure their IT infrastructure and their application landscape. Approach and aims, however, differ.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_7_Chapter/516372_1_En_7_Fig3_HTML.png" alt="../images/516372_1_En_7_Chapter/516372_1_En_7_Fig3_HTML.png" height="671" width="1734"><figcaption><p>Figure 7-3</p></figcaption></figure>

The company’s **security governance** function’s core tasks are elaborating policies, performing assessments, and managing IT-related operational risk emerging from derivations of actual systems implementations and procedures from policies. The security governance function is just one governance function. There are others such as legal and compliance or data governance impacting AI organizations as well, which other chapters address.

Writing security policies is like writing a wish list for Santa Clause. You write them down; the actual delivery and implementation are not your concern. Policies contain procedural and technical requirements. One example of a procedural requirement is demanding penetration tests before software developers or data scientists make a web service available for the rest of the company. An example of a technical requirement is demanding to encrypt all network traffic with TLS 1.3. Governance specialists elaborate policies often from standards and frameworks such as the ISO 27000 family and ISAE 3402 or best practice documents from the software vendors or public cloud providers.

The IT security governance teams do not provide any technical services to other teams. They guide the organization to a more secure setup with their policies. Plus, they support senior management to understand the company’s overall IT security and risk situation. The rest of the organization, including all IT and AI teams, act as Santa Clause. They have to fulfill the wishes and needs laid out in policies. Each **application owner** must ensure that his application is secure and complies with the security policies. AI organizations must fulfill the same requirements for their training environments, runtime servers, or model repositories as any other application or system handling large amounts of sensitive data. These tasks are usually with the AI organization’s application management and operations specialists providing internal services for the data scientists, data engineers, and AI translators. Figure [7-3](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig3) refers to these components as **managed AI platforms and infrastructure**.

Besides the governance function, many security organizations providesecurity services – application owners inside and outside the AI organization benefit from them. One service type comprises **centralized security services** . These services address and solve specific, generic tasks for the company. They relate to protecting the company’s perimeter – firewalls, data loss prevention tools, scanning incoming files for malware – or checking for patching-related issues or (OS-level) vulnerabilities.

In contrast, **collaborative security services** require close collaboration between an IT security delivery team and application owners. Identity and access management (IAM) or web application firewalls (WAF) are examples. Both require per-application customization. An IAM system can only manage access to an AI platform if it knows the roles and if the AI platform can interact technically with the IAM solution. Sophisticated WAFs need training for fine-tuning their rules to learn all the legitimate requests an AI runtime server might receive.

In other words, in contrast to centralized security services, collaborative security services require the AI organization to perform dedicated tasks to secure the AI systems. The integration engineers, who set up the managed AI platforms and infrastructure, are responsible for securing them. After the go-live, the application management and operations experts of the AI organization have to keep their systems secure for the years to come. The AI management can ensure that data scientists and engineers are not distracted by security-related tasks to keep up the AI organization’s productivity. There is just one big exception: personal unmanaged installations and applications. When AI specialists install systems, they have to secure them, whether the systems run on servers or in the cloud.

**Experimental installations** pose a particular risk, though there can be a need for them. From time to time, data scientists or engineers try out new tools. They install the tools on their laptops or a VM and perform some use cases, potentially with actual company data. A typical misconception is that such systems are not relevant to the organization’s overall security posture because they are just experimental. The contrary is true, especially (but not only) when they contain actual data. Experimental installations are often not secured as well as production systems. Thus, hackers might take over such experimental systems more quickly and misuse them for attacking other applications and extracting confidential data. If protecting such systems seems not feasible, the only solution is setting up a sandbox environment in a locked-up network zone.

AI specialists and managers must never forget two reasons why to secure their systems. The first one is to ensure that their data remains confidential and to guarantee data integrity and their systems’ availability. They are responsible, even though they can or have to build on some security services.

The second reason why AI organizations must harden their systems is to prevent “**spilling over**” effects. A successful attack, for example, on a single (web) application, must never put the attacker in the position to take over effortlessly also the AI runtime server or the AI training environment – or experimental systems. No security organization can guarantee that. It is the ultimate responsibility of every member of the AI organization.

We conclude the discussion about responsibilities with a final remark about AI organizations using **public cloud services** . It is a booming topic, which means that governance and responsibilities are less elaborated than for traditional data centers. The AI organization should clarify with the security organization whether they are allowed to use the intended cloud services. Furthermore, they should also clarify whether the security organization provides the standard security services for the envisioned cloud environment. Otherwise, the AI organization might have to set up and manage the cloud security all by themselves – or risk that auditors shut down their environment.

### Mapping the Risk Landscape

The rise of AI and its contribution to business-critical processes transform organizations – and creates new risks. What if attackers manipulate AI features or steal data or models? AI is a new domain for many risk and security specialists, but the methodology remains the same. First, understand the threat actors and their motivations, aims, and capabilities. Second, identify assets attackers might attack or try to steal. Third, understand scenarios and what attackers aim for. Fourth, and finally, filter for likelihood and potential impact to identify the relevant risks. Figure [7-4](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig4) illustrates these steps. We take a closer look at them in the following.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_7_Chapter/516372_1_En_7_Fig4_HTML.png" alt="../images/516372_1_En_7_Chapter/516372_1_En_7_Fig4_HTML.png" height="546" width="1729"><figcaption><p>Figure 7-4</p></figcaption></figure>

#### Threat Actors

Script kiddies, professional cybercriminals, and foreign intelligence services: a diverse and colorful crowd threatens today’s IT systems and AI environments. Threat actors aim for stealing intellectual property, blackmail for ransom, or try to crash crucial servers. They threaten AI organizations, AI-related assets, and AI-driven business processes and control systems. The threat actors differ regarding motivations and capabilities, technical and financial-wise.

The hurdle for engaging in cyberattacks today is lower than ever. Attack tools are widely available, enabling **script kiddies** to pursue not-so-much-recommended leisure activities. **Cybercriminals** have more personnel and financial resources for attacks. They outsource tasks to criminal service providers when they miss specific know-how or run large-scale attacks. Cybercriminals have clear monetary motives, such as extracting ransom payments. For example, they encrypt all company data, including backups – and decrypt the data only after payment. They are a challenge for small and medium enterprises, but even for well-funded IT security teams. Even worse are **state-sponsored attackers** and intelligence services. They have nearly unlimited resources. Their focus is mainly on critical infrastructure, enabling them to bring down another country’s infrastructure. Alternatively, they try to steal business and military secrets from governmental offices, the military, or highly innovative companies and institutions of strategic interest. AI models in antiaircraft systems might be of high interest to rivaling countries. So are the newest insights about autonomous cars or biotech innovations.

There are three more types of external attackers: activists, partners, and competitors. **Competitors** are natural threat actors since they benefit directly from illegal activities such as stealing intellectual property or harming ongoing operations. However, it depends on the business culture of the industry sector and the countries in which companies operate, produce, and sell.

Often overlooked are **partners** as threat actors. Some might have a hidden agenda. For example, they might want to shift the contracts’ balance to increase their profitability or secretly plan to enter the market as competitors. Then, there are the **activists** . They do not aim for financial benefits but fight against what is, in their eyes, injustice. They want to expose an organization as unprofessional or prove injustice based on the AI models, for example, if the models discriminate against society subgroups.

When organizations mistreat their employees or employees try to make quick money, some turn against their employers and harm them. **Criminal employees** with access to significant data sources (e.g., data lakes) are particularly dangerous. Without logging, forensic experts cannot understand what happened or pursue criminal investigations. If everyone can copy large amounts of data, for example, to personal USB sticks, this further eases illegal activities.

Besides the risk of criminal attacks, there is the risk that good-willing **employees** make mistakes due to negligence. They might not understand security procedures; the procedures might be inconvenient or – from their perspective – not make sense. Such situations can result in employees following security procedures halfheartedly, potentially resulting in vulnerabilities.

Which threat actors are relevant for a company? More or less, every company is a potential target for script kiddies and cybercriminals. For example, cybercriminals often demand “only” five-digit ransom payments. Small and medium enterprises can pay such sums – and limited IT defense capabilities make them viable targets, especially if attackers can automate attacks. The risk of the other threat actors has to be looked at for each company individually. For example, a French corporation might come under attack from foreign activists, for example, due to tensions between France and Turkey.

After understanding potential and relevant threat actors, the next step is to look at what the attackers might target. In other words: What are the assets of an AI organization?

#### Assets in AI Organizations

“Asset” is a widely used term. It refers to software, hardware, and data – everything needed for information processing. Four asset types are essential for AI organizations:

* **AI models**. For example, they help optimize business processes, oversee assembly lines, control chemical reactors, or predict stock market prices. AI models can be an integral part of the application’s codebase or run on AI runtime servers.
* **Training data** is essential for data scientists. They need the data to create and validate AI models.
* **AI runtime servers** are the software and hardware systems on which the AI model runs and inference takes place. It can be an RStudio Server, the model can be part of the actual application code, or the application implements an edge intelligence pattern.
* The **training areas** on which data scientists create, optimize, and validate the AI models, for example, using Jupyter notebooks.

Various additional systems and assets are of relevance for an AI organization that aims to work efficiently. They primarily relate to the training environment:

* Internal **upstream systems** and their data such as SAP, a core banking system, or Salesforce.
* **Pretrained models, libraries**, and reference **data** from the Internet, speeding up the training process or helping to get better models.
* The **data ingestion and distribution** solution, for example, an ETL tool, loading data from the data sources into the training data collection and, potentially, loading it as well into the training areas.
* The **AI repository** storing the model descriptions and training history of the models – and the models themselves.
* A **data catalog** indexing and describing the data available for training models.

Any AI-related threat or attack relates to one or more of these assets (Figure [7-5](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig5)). When successful, attacks compromise one (or more) information security CIA triangle properties: confidentiality, integrity, availability. Some assets might impact production, others are “only” needed for efficient training. Understanding the details is part of the threat analysis.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_7_Chapter/516372_1_En_7_Fig5_HTML.jpg" alt="../images/516372_1_En_7_Chapter/516372_1_En_7_Fig5_HTML.jpg" height="965" width="1725"><figcaption><p>Figure 7-5</p></figcaption></figure>

#### Confidentiality Threats

AI-related systems need and work with large amounts of data to generate new insights. These systems are of high interest for threat actors trying to **steal intellectual property** ,particularly **training data** or **AI models**. They are a gold mine for competitors time-wise and money-wise. For example, Waymo, a self-driving car company, prides itself on more than 20 million self-driven miles and more than 15 billion simulated miles. When competitors get this training data or the AI model built with this data, they save millions of investments and years of work. Likewise, a stolen underwriting model in the insurance sector enables competitors to lure lucrative customers, while staying away from problematic ones.

Just by looking at Figure [7-5](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig5), it becomes obvious where training data resides. Attackers can steal it from the original **data sources** and databases. Still, the most extensive data collection is, in most cases, the AI organization’s **training data collection**/data lake, comprising all available data. In contrast, the actual training areas, for example, a single Jupyter notebook, provide more limited data. Attacks on the data ingestion and distribution component are another way to collect data.

Besides stealing data, it is also an option to steal the final **AI models** to use them – or to understand how a company acts, for example, on the market and to undercut profitable offers. AI runtime server or other production systems, the AI model repository, and training areas store or have access to one or many of them. The AI **model repository** is the component storing all AI models, even with explanations and documentation.**AI runtime servers**, if used, contain an extensive collection of AI models. Other systems or components, such as the **training areas** or application source code – if AI models are reimplemented and part of the code – provide only one or a few AI models. However, for any insurance, the loss of even a single model for one large or important customer segment can be already catastrophic.

Loss of intellectual property might not be an issue for all companies. However, a second confidentiality threat is relevant for many companies: stealing data to extort a **ransom**. Hospitals store which customers are HIV positive. Insurance companies store salary-related data about their customer companies’ employees if they handle sickness allowances. No hospital or insurance company wants to see such information on the web. Neither do their customers and patients. It is the perfect setting for blackmailing all of them if cybercriminals get their hands on such sensitive data.

In the past, **stealing files and data** required sophisticated attacks – at least for data from not-Internet-facing servers. Still, an AI organization would act foolishly in ignoring such risks, though there are more severe and more uncomplicated-to-perform attacks today.

With the advent of the public cloud, companies train AI models there. Training data and trained models are in the cloud. If the company or AI organization sets up the cloud security properly, assets are safe. However, wrong configurations happen quicker, and (weakly protected) Internet-facing public cloud environments are an invitation for attackers. Many companies had to learn it the hard way in the last years.

The most effortless and potentially most challenging to detect confidentiality attacks involve **employees**. In many organizations, they can transfer easily and risk-free any file – including models or training data – out of the company.

Stealing a model, however, is also possible without access to it, just by **probing**. Suppose an attacker wants to mimic a “VIP identification” service identifying stars and starlets on images. The first step is to crawl the web to collect sample images (Figure [7-6](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig6), left). At this point, it is not clear whether pictures show VIPs. So it is not possible to train an AI model with the data. Thus, the second step is to submit these pictures to the “VIP identification” web service. This service returns which images contain which VIPs (middle). Now, the attacker can build a training set for training his own neural network mimicking the original one with this information gained from probing and use it in production (right).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_7_Chapter/516372_1_En_7_Fig6_HTML.png" alt="../images/516372_1_En_7_Chapter/516372_1_En_7_Fig6_HTML.png" height="579" width="1721"><figcaption><p>Figure 7-6</p></figcaption></figure>

A final remark about non-financially motivated threat actors: **activists** can be interested in AI models. Their motivation would be to validate whether a model discriminates subgroups of the society systematically, for example, by pricing them differently than mainstream customers.

#### Integrity Threats

AI-related integrity threats relate, first of all, to the **model quality**. When threat actors modify a model or interfere with its training, the model classifies images incorrectly or makes unprecise predictions. Consequently, a factory might produce unusable parts, an aircraft carrier assumes that a harmless passenger jet flies to it and not an enemy bomber, or an insurance company assumes that 20-year-old potential clients have a 90% probability of dying next year. Thus, the insurance stops selling them life insurance policies. Extreme cases of faulty models sound frightening, though organizations quickly notice them. More dangerous are subtle manipulations. They are much harder to identify. How can the business detect a manipulated risk and pricing model that gives customers in southern Germany a 7% too low price while rejecting 10% of lucrative long-term customers in Austria? Such manipulations are often not apparent, but highly effective. They drive away loyal Austrian customers while generating direct financial losses in Germany due to unnecessary and unjustified discounts.

An external threat actor **manipulating** a specific AI model in the AI model repository or production systems sounds unlikely, at least for companies not in the focus of intelligence services. But even companies not in the spotlight of global cybercrime have to be aware of the threats of less-than-perfect AI models coming from the inside. Data scientists feeling mistreated (or being approached by competitors) can manipulate models much more effortlessly. Plus, even highly professional data scientists with goodwill make **mistakes**, resulting in mediocre or completely wrong models. Consequently, the internal quality assurance procedures for AI are crucial. AI-related quality assurance is often more in a highly agile startup mode and less rigid and sophisticated than for traditional software engineering.

The last remark already shifted the focus from manipulating the model to the overall creation and training process. **Lousy training data** implies bad models, no matter what quality metrics indicate. What would James Bond do if he does not want to be recognized by surveillance cameras? He would ingest a few hundred pictures of himself in a training set, tag himself as a bird, and retrain the AI model. From now on, surveillance cameras ignore him because he is a bird and not a burglar.

**Training data mistakes or manipulations** are severe and challenging to detect. They can happen at various places: in the original operational databases, in the data ingestion scripts, in the training data collection or data lake, and in the training area during the model training. If data scientists use external training and validation data sets, these are also potential ways for attackers. Who looks through millions of pictures to look for wrongly tagged James Bond images? Who understands why a command during test data preparation performs a left outer join and not a right outer join? Why are some values multiplied by 1.2 instead of 1.20001? And why does an upstream system discontinue sending negative values? Is it a mistake, sabotage, or improved data cleansing? Creating training data is highly complex, even without external threat actors.

All the threats and attacks discussed yet require access to training environments, production systems, AI models, or training data within an organization. Other threats do not require any of these. They succeed without touching any asset of the targeted organization.

Data scientists want to move fast and incorporate the newest algorithms of this quickly evolving field. They **download** pre-trained models, AI and statistics libraries, and publicly available datasets from the Internet. These downloads are the basis for building better company-specific models. At the same time, the downloads are a backdoor for attackers that can provide manipulated data or models, especially for niche models and training data.

Another attack type not requiring touching any assets are **adversarial attacks**. The idea is to manipulate input data so that a human eye does not detect the manipulation, but that the AI model produces the wrong result. For example, small changes in a picture or some new points on the street as illustrated in Figure [7-7](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig7) can result in the AI model not detecting a traffic light or getting a wrong understanding of whether to continue on the street to the right or whether to go full throttle straight and fall down the slope. Such attacks work only for AI models that are complex neural networks, not for simple linear functions.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_7_Chapter/516372_1_En_7_Fig7_HTML.png" alt="../images/516372_1_En_7_Chapter/516372_1_En_7_Fig7_HTML.png" height="646" width="1721"><figcaption><p>Figure 7-7</p></figcaption></figure>

#### Availability Threats

The last threat scenario covers a potential non-availability. First of all, there is a risk of an **AI logic in production** or a **training area** outage. In the latter case, data scientists cannot work. If production systems are down, this impacts business processes. Non-availability is the consequence of attacks against **servers**. Typical attacks are crashing the applications with malformed input or flooding the server with (distributed) denial of service attacks.

An aspect with particular relevance for AI is the threat of **losing intermediate results or models**. In data science, processes tend to be more informal than in traditional software development regarding documentation and version management of artifacts. Such laxness can cause availability risks. For example, when a key engineer is sick, on holiday, or has left the company, his colleagues might not be able to retrain a model. Instead, they have to develop a completely new one, which takes a while and incurs costs.

#### From Threats to Risks and Mitigation

The difference between a conspiracist and a risk specialist is the consideration of probabilities. For example, the NSA could have manipulated this book by adding five sentences and removing an illustration with a dancing pink elephant from the previous page. Most probably, they can perform such an attack. However, why should they invest time and money in such an attack? Understanding the motivation and aims of threat actors and their technical and financial capabilities is crucial to address the highly relevant security risks and improve the overall security posture.

The confidentiality, integrity, and availability threats we discussed are generic. In an IT security risk assessment, risk assessors and AI experts take a closer look at which attacks are possible and rate the likelihood of attacks and their potential impact on the concrete AI environment. The result: a risk matrix as illustrated in Figure [7-8](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig8).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_7_Chapter/516372_1_En_7_Fig8_HTML.jpg" alt="../images/516372_1_En_7_Chapter/516372_1_En_7_Fig8_HTML.jpg" height="751" width="1717"><figcaption><p>Figure 7-8</p></figcaption></figure>

The sample risk matrix contains two threats. Risk A in Figure [7-8](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig8) reflects the possibility that an unhappy employee steals training data consisting of (anonymized) past customer shopping behavior. The financial impact rating is “minor,” meaning between 50,000 and 250,000. The limited financial impact is due to its strong brand and unique logistics capabilities, making it challenging to copy the business model. The likelihood rating for such an attack is once every five years.

Based on the risk assessment and a risk matrix, the management has to decide which risks to accept and which to mitigate. **Mitigation** means investing in organizational measures or technical improvements. Later in this chapter, we discuss some more complex mitigation actions in detail. First, however, we introduce some typical mitigation actions in the context of AI for a better understanding already here. Examples are:

* Ensure that **access control and perimeter security** are in place for the training environment and production system. These measures reduce the risk of unwanted transfer of data or models to the outside and unwanted manipulations. In addition, they are essential if AI functionality resides in a public cloud.
* Enforce stringent quality processes and **qualitygates**. The negligent data scientists are, in most cases, one of the highest risks for poor models. Four-eyes-checks and proper documentation of critical decisions reduce the risk of deploying inadequate models to production, though having a structured and comprehensive quality assurance process is even better.
* Ensure that external data, code libraries, and pretrained models come **from reliable sources**. Experimentations with such downloads are essential for data scientists. Still, some governance process is necessary when external downloads contribute to AI models deployed to production.

These three measures reduce the potential threat and attack surface of organizations. However, they do not make a proper risk assessment obsolete. A risk assessment takes a closer look at all threat scenarios and threats discussed above. AI is fun, and AI brings innovation. At the same time, organizations have to understand and manage their AI-related security risks. Defining mitigation actions only helps if there is clarity on who manages and addresses which risks. In the following, we take a closer look at potential actions.

### Securing AI-Related Systems

For AI and analytics, data is what fuel is for your car: the more you have, the further and faster you can go. There is just one big difference: you burn the oil, but the data remains in your systems. An AI team piles up massive amounts of data within just a few months – a security nightmare. A scrupulous competitor has to turn only one data scientist against you – and she can extract and transfer most of your commercial data and intellectual property to the competitor. Consequently, this section has one ambition: elaborate on reducing this security risk without blocking the daily work of the data scientists and the AI organization.

Three traditional measures are the ticket to success together with three measures that require innovative thinking beyond standard IT security. The measures are:

1.  1\.

    Generic system hardening

    &#x20;
2.  2\.

    Governance Processes

    &#x20;
3.  3\.

    Compartmentalization

    &#x20;
4.  4\.

    Advanced Techniques for Sensitive Attributes

    &#x20;
5.  5\.

    Probing Detection

    &#x20;
6.  6\.

    Cloud-AI Risk Mitigation

    &#x20;

AI and security specialists apply them to the various AI components as Figure [7-9](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig9) illustrates.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_7_Chapter/516372_1_En_7_Fig9_HTML.png" alt="../images/516372_1_En_7_Chapter/516372_1_En_7_Fig9_HTML.png" height="988" width="1729"><figcaption><p>Figure 7-9</p></figcaption></figure>

#### System Hardening

The generic system hardening covers the traditional IT security measures such as network zones and firewalls, integrating security services such as Internet and access management (IAM) solutions or web application firewalls (WAF), and configuring systems securely, that is, restricting IP ranges.

These are standard activities known from “normal” software. They apply as well for any AI system or component, be it a training area, repositories, data lakes, or anything else. The aim: making sure that unauthorized persons do not get access to the systems plus enabling data scientists and engineers to perform their work.

#### Governance

The distinction between adequate and necessary vs. unneeded and risky access to training data requires human judgment. The outcome of the judgment should be transparent and repeatable. Similar situations should result in similar decisions and requesters want to understand who is involved in the decision process and how far the decision-making proceeded. Governance processes help – and AI organizations benefit from setting up two especially relevant ones for data uploads and for data usage (Figure [7-10](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig10)). The data upload governance process verifies whether the data loaded into the training environment is adequate. The usage governance process verifies whether the data can be used for a particular use case.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_7_Chapter/516372_1_En_7_Fig10_HTML.png" alt="../images/516372_1_En_7_Chapter/516372_1_En_7_Fig10_HTML.png" height="767" width="1721"><figcaption><p>Figure 7-10</p></figcaption></figure>

The separation of the two processes is a cost- and throughput-time optimization. So, a first project needing telemetry data from a new turbine implements the data transfer and preparation and goes through the upload approval process. Ideally, ten or twenty later projects can reuse the data. They only need approval for using the data for their purposes. They can skip the upload approval and reuse the implemented and approved data copy and preparation infrastructure, saving time and money.

The **data upload governance process** looks, from an approval perspective, whether data can and should be copied into an AI training environment. Data owners, data privacy officers, legal teams, and IT security typically drive the decision-making. Obvious concerns relate to data privacy when AI training environments are in a different jurisdiction than the humans whose data the company analyzes. A second concern relates to companies in highly competitive and sensitive industry sectors or governmental agencies. They might hesitate to put every piece of information in every public cloud, no matter how good its AI features are.

While the data upload governance process has a control function, it also helps make training data more valuable by easing (re-)use. This governance process can act as a checkpoint for documentation in the data catalog. It can enforce that data is only put into the training environment when the metadata is complete. This means: Data scientists must describe the content of their data, have assessed and documented the data quality, clarified potential usage restrictions, and analyzed the data lineage. This governance process is the shortcut to high-quality data catalogs for all data sets, at least within the AI training environment.

However, there is also the option to discuss and implement measures to **make data less sensitive**. For example, shopping cart information related to individual customers is more problematic than anonymous shopping cart data. Obviously, the natural point for anonymization is before the data upload to the training environment.

Ideally, the usage-related governance process is part of the organization’s management and approval processes for new (AI) projects. Following the data upload and before an actual use case implementation and AI model training, it focuses on approvals, not on enablement. It questions whether the intended data usage or the envisioned AI model violates data privacy laws, internal ethics guidelines, or any other policy or regulation. Again, data owners, data privacy officers, or legal and compliance teams are the natural decision-makers.

#### Data Compartmentalization and Access Management

Compartmentalization balances two contrary wishes: the wish for fine-granular data access control plus the wish and need for manageability. Achieving both at the same time is possible for **repositories** and the AI model training areas. If a specialist works on a project, he gets access to the specific training data; otherwise, not. Also, figuring out which application has to get models from a particular repository is straightforward. The challenge is the access management for data lakes or the **data collection** of the AI organization within their AI model training environment.

Access to all data for every data scientist is not an option. Neither is managing data access separately for thousands of data sets and data classes. The latter might be a surprise. Super fine-granular access rights appeal, at least at first glance. However, data scientists and data owners cannot work with them on a daily basis. The complexity is too high for the human mind. As a result, they would stop following the need-to-know principle to prevent the AI organization from being blocked. They move to a “grant access that is not obviously unnecessary” model, approving much more data access than needed. So, super fine granular access control looks good on paper, but fails in reality.

Suppose a data owner from the business knows his data and the data model very well. He can handle ten or twenty subsets, not one hundred and not thousands. AI organizations need a compartmentalization approach to limit the number of data access control roles and adjust them to their organization’s actual size and complexity. A starting point is following a three-dimensional compartmentalization approach (Figure [7-11](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig11)). It works for medium enterprises and scales even to the largest corporations of the world.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_7_Chapter/516372_1_En_7_Fig11_HTML.png" alt="../images/516372_1_En_7_Chapter/516372_1_En_7_Fig11_HTML.png" height="934" width="1504"><figcaption><p>Figure 7-11</p></figcaption></figure>

The first dimension is **sensitivity**, typically categorized based on four levels: public, internal, confidential, and secret. Public data is (or could be made) available on the web, for example, statistics published by public health organizations or product descriptions in marketing brochures. Then, there is internal data. It means every employee (with a good reason) can access it. Customer contact information is one example. So is a categorization whether a customer is a top or a standard customer, or a customer frequently causing trouble. Complex offerings in a B2B context or a WHO tender are examples of confidential data. Deal or no deal can have a significant impact on the company’s bottom line. And any potential disclosure of the offer to competitors might result in losing the deal for sure. Finally, there is secret data. Passwords or (master) keys for encryption and signing files belong to this category. Information potentially impacting a corporation’s stock exchange price might – before disclosure – fall into this category. The latter examples also illustrate that data sensitivity can change over time. For example, once such stock-price relevant information is out, it becomes public data.

Additional standards and regulations impact the data sensitivity classifications in certain industry sectors. Examples are the already mentioned GDPR, Health Insurance Portability and Accountability Act (HIPAA), or Payment Card Industry Data Security Standard (PCI-DSS). Organizations can model the impact in different ways. The regulatory standards can influence to which of the four categories data belong. For example, Singapore and Channel Island bank customer data are “secret,” other customer data are “confidential.” Alternatively, organizations can introduce flags for each standard and regulation in addition to the four sensitivity levels. These flags state whether data fall into an additional dedicated category. For example, a customer record might be tagged as “category confidential” and “sensitive personal data (GDPR).”

The second dimension is the **data or business domain**. Are they HR, marketing, or R\&D-related data? Data owners are responsible for one or more domains. The domain helps to involve the right persons in the governance processes. It is also the dimension on which the company size has the most significant impact. The size determines the approval granularity. Is HR one big domain, or does an organization work on a sub-domain level distinguishing various types of HR data, such as recruiting, payroll, insurances, performance management, and training? Larger organizations tend to have a more granular approval level. They have more staff to handle requests, more sensitive data, and a more complex data model.

The third dimension reflects the company’s **organizational structure and various jurisdictions** in which the company operates and to which data might belong. Different subsidiaries might have different internal rules. Different jurisdictions regulate data storage and usage and artificial intelligence differently.

To conclude, domains, organizational structures, and jurisdictions are natural ways to divide work, manage access rights, route requests in governance processes, and determine quickly which rules and regulations apply. The data sensitivity dimension allows for assessing quickly whether a request requires a more profound analysis or can take a shortcut.

#### Advanced Techniques for Sensitive Attributes

Certain data types stand out from the rest of the training data, because they pose a high risk if lost or exposed. First, these are data items allowing to identify individuals, such as Social Security Numbers. Second, there are sensitive personal and protected classes such as union membership or race. Here, it makes sense to consider different approaches beyond typical access control. The options are:

* Not copy them to the AI environment. In case protected classes and sensitive personal data must not be used for AI model training, why copy such data? The same applies to identifiers. If not needed, one can remove them when importing the data into the AI environment as training data.
* Anonymizing data means making it completely impossible to identify individuals to which data belongs, even not using additional information and tables. If you replace names with “XXXX,” the result is anonymous data (at least if there are no other columns, e.g., with IDs). AI organizations have really to check in detail in case anonymization is mandatory. Often, this term is also used for the third option, veiling, as well.
* Veiling data, that is, making it more challenging and time-consuming to determine to which individual the data belong. It reflects situations where, for example, a table contains synthetic names and account numbers, but in the end, there is still a table for mapping synthetic names and numbers to the real ones, for example, to ease fixing issues.

#### Probing Detection

Partners, competitors, or customers: these are potential attackers trying to reengineer or misguide automated ratings or decision making. A classic bank with brick branch offices might want to understand the pricing model of an online bank to target customers not getting “good” online offers with high-margin credits (Figure [7-12](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig12), left). A car repair shop might want to understand which kind of picture of a damaged car lets the insurance pay the highest compensation for the repairs. Is it a long shot or a detailed picture, should it be with intense colors – or are black and white images more profitable (Figure [7-12](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig12), right)?

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_7_Chapter/516372_1_En_7_Fig12_HTML.png" alt="../images/516372_1_En_7_Chapter/516372_1_En_7_Fig12_HTML.png" height="859" width="1721"><figcaption><p>Figure 7-12</p></figcaption></figure>

If attackers cannot directly access an AI model, they can – if exposed to the Internet – probe it and send many requests with different value combinations such as income, property, age, child support, or other parameters for our bank example. The attackers collect the replies and use the data to train a copycat AI model behaving similarly to the bank’s original one. The attackers have no access to the original model, but they can analyze their copycat AI model instead. Thus, AI organizations and IT security specialists have to prevent the probing of an AI model – without disturbing “normal” customers’ requests. Potential approaches are:

* Restrict the number of requests per hour for each IP address – or even confuse attackers with wrong results.
* Restrict anonymous access to the systems and require registration before providing any estimations.
* Use Captchas to prevent at least automated probing.
* Do not provide the results of the estimation directly online, but only per email, letter, or phone call.

Companies have to balance the risk of losing potential customers that might find, for example, the process too inconvenient of having to wait for a letter against the risk that competitors or partners probe the model.

To conclude: Companies exposing AI models to the public or to partners, for example as part of the business process, should monitor the processes and requests for anomalies and hints for fraud or espionage.

#### Cloud-AI Risk Mitigation

Many AI organizations are ahead of their time when it comes to moving to the cloud, which brings them into trouble. Massive amounts of data, highly fluctuating compute needs, and the availability of sophisticated AI frameworks in the public cloud foster a quick cloud adoption by data scientists and AI specialists. The rest of the company and the IT security organization might be on the journey to the cloud. Most AI organizations are already there. As a consequence, AI organizations have to take over more security-related tasks compared with traditional on-premise environments. Thus, AI managers, data scientists, and engineers benefit from a basic understanding of the main security risks.

Before elaborating on the risks of being _in_ the cloud, we look at the unique **compliance risks** when moving _to_ the cloud, especially to the big global players. First, public cloud providers offer many different services, from compute and storage to very specific niche services for IoT or the blockchain. Not every service is available in each region – and even if the actual configuration of the data scientists and engineers determine whether the data end up in a data center in an adequate **jurisdiction**. One wrong click during configuration and data scientists or data engineers transfer EU data to the US, often a big issue. Second, the **Cloud Act** means that US authorities might get access even to data in non-US data centers. These data transfers or the potential data exposure can be a no-go for regulators. Third, there is the **sanctions** risk. In such a case, a company might have to switch to a new provider in a different juristiction immediately. In an on-premise world, the software continues to run when a company is put under sanctions, even if security patches are withheld. In contrast, access to all data and software is blocked immediately in the cloud – a real risk even for smaller companies and governmental offices in OECD countries as Sassnitz harbor in Germany exemplifies. The port handles 2.2 Mio tons of freight per year, nothing compared to 469 Mio in Rotterdam. It is owned by the local city and a German federal state. Economically irrelevant, solid owners – and in the middle of a pipeline controversy resulting in the US threatening to sanction the port. What would happen with such a company if its logistics, for example, depend on AI frameworks in the public cloud?

Once the AI organization decides to move to the cloud, particular security risks emerge and become newly relevant. Figure [7-13](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig13) provides an overview of the risk areas requiring cloud-specific **technical security measures**. They are important if the AI organization has an extra cloud or a separate tenant, but only to a lesser degree if running in the same tenant as the rest of the company.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_7_Chapter/516372_1_En_7_Fig13_HTML.png" alt="../images/516372_1_En_7_Chapter/516372_1_En_7_Fig13_HTML.png" height="1009" width="1725"><figcaption><p>Figure 7-13</p></figcaption></figure>

The first risk area relates to user access. Data scientists and engineers have to connect and log in to the cloud (Figure [7-10](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig10), 1). If the AI organization does not use the same cloud environment as the rest of the organization, they might bypass the company’s central **user and access management** and Active Directory. So, managing new employees or dealing with leavers or ones moving to new positions becomes an issue. There is one thing more inconvenient than having an employee guided out of the building by a security guard: if this former employee has access to the cloud for the next weeks and months allowing for revanche because the Active Directory is locked, but not a cloud account “in the wild.” Furthermore, most companies enforce **multi-factor authentication**. Logging in just with a user name and password is frowned upon these days. Employees need RSA keys or special apps on their mobile phones. The public clouds provide these features as well, but they are not enforced by default.

**Service accounts** are the second area, quite a similar topic (Figure [7-10](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig10), 2). The on-premise world or the other company’s cloud environments have to interact with the AI organization’s cloud tenant:

* Training data must get to the AI environment.
* A trained AI model might have to be moved to the on-premise world if the runtime environment is there or if the model becomes part of the application code.
* A trained model might rely on the cloud as an AI runtime environment. Then, various applications of the company have to be able to invoke the AI models on the cloud for interference.

The challenge with service accounts is preventing issues with stolen passwords or access keys. It would be a catastrophe if a data engineer leaves the company, copies a password, and continues to have access to the cloud for weeks and months.

**Internet-exposed platform-as-a-service services** are the third risk area (Figure [7-10](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig10), 3) – and they are unique to public clouds. Databases or object storage are web services. Applications – and attackers – can reach them potentially from the public Internet. Both can invoke AI models and many more services and resources in the same way. Engineers should configure them such that only authenticated users coming from specific IP addresses have access. However, misconfigurations can result in public exposure. The majority of cloud data leakages were the result of such misconfigurations, not of sophisticated attacks.

For clarification, the risk is not so prominent for infrastructure-as-a-service services such as VMs. Accidentally opening up a VM to the Internet requires many more misconfigurations than in the PaaS world.

The fourth topic is **security operations** in the cloud. Who takes care of security incidents, inspects logs, and analyzes warnings? (Figure [7-10](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig10), 4)

The cloud-risk areas are not specific to AI organizations. Every engineer should be aware of them. However, at the moment, most companies are in a transitional phase. AI organizations might not have the usual support from IT security. Suppose the AI organization decides unilaterally to set up an AI cloud. In that case, they are responsible for security engineering and security operations. They cannot assume that the IT security organization helps them. AI managers might realize quickly: auditors do not sympathize with an AI cloud with large amounts of data and insufficient IT security know-how. Finally, to make things worse, the auditors expect that the AI cloud follows the same norms and standards as the rest of the organization, including, for example, the ISO 27000 information security standard.

#### The ISO 27000 Information Security Standard

Standards are not a creative’s – or an IT specialist’s – darlings. Most engineers and data scientists prefer working in sectors without heavy regulations and strict norms. Even if there are engineering or organizational standards such as Scrum, SAFe, Cobit, ITIL, or TOGAF, they allow heavy tailoring and interpretation. One broadly relevant exception to the rule of ease is the ISO 27000 standard. If the senior management decides to take this norm seriously, the IT security department and auditors get persistent. IT norms did not impact traditional statisticians’ teams. The situation differs for AI organizations and data scientists writing code, providing web services, and running and hosting software. They fall under the same rules as any software engineering and systems integration team.

The ISO 27000 standard is a **standard family** consisting of a variety of documents with different purposes (Figure [7-14](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_7\_Chapter.xhtml#Fig14)). ISO 27000 is the vocabulary standard. It provides a basic overview and defines the terminology. Then, there are binding standard documents: ISO 27001 and ISO 27006. ISO 27006 is irrelevant for AI teams since it addresses auditing organizations only. In contrast, ISO 27001 is the bible for any certification. It states all certification requirements. The standard has a main section and appendix A. The main section defines a general information security framework. Topics such as top management involvement or the need for an incident management system are more relevant for senior managers and IT security departments than for AI teams and software engineers. Appendix A lists concrete security topics (“controls”) to be implemented. Some apply directly to AI organizations, including the needs for:

* Identifying and classifying assets such as code, data, or documentation and defining access control mechanisms
* Securing development environments and test data
* Performing acceptance and security tests
* Change control procedures in development and production to prevent unwanted changes and clear rules who can do which installations

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_7_Chapter/516372_1_En_7_Fig14_HTML.png" alt="../images/516372_1_En_7_Chapter/516372_1_En_7_Fig14_HTML.png" height="900" width="1721"><figcaption><p>Figure 7-14</p></figcaption></figure>

ISO 27001 compliant companies often translate these requirements into internal guidelines and directives, such that many organizations implement the requirements without engineers and AI specialists being aware.

As mentioned, the ISO 27000 standard family comprises many more norms and documents. Though the ISO 27001 standard is the only normative binding document for IT organizations, ISO 27002 is worth taking a closer look at since it helps to set up the controls of appendix A of ISO 27001.

**Additional guidelines** focusing on specific aspects. ISO 27003, for example, looks at information security management systems, and ISO 27005 at risk management. Sector-specific guideline standards are also available, for example, for the financial services industry (now withdrawn), telcos, or the energy sector. However, most companies focus on ISO 27001/27002 and do not pay much attention to the additional documents.

A number behind the standard reflects the publishing year and allows to distinguish different versions, for example, the ISO27001:2013 version from the older version, ISO27001:2005. At the moment, there is a peculiar situation with a new draft version of the ISO 27002 standard out, but not a new ISO 27001 draft, which would be the actual binding norm. However, at this moment, there seems not to be much change impacting AI organizations.

For most AI organizations, it is more an **emotional shock** than a big issue to adhere to ISO 27001, at least if they are in contact and aligned with the IT security organization beforehand.

#### Summary

Securing an AI environment means ensuring confidentiality, integrity, and availability of the AI environment and its training data and AI models. While every company has a Chief Information Security Officer with a smaller or larger team, AI organizations have to contribute substantially to securing their AI environment, especially if they rely on cloud infrastructure the rest of the organization is not using (yet). Correctly configuring their AI components and implementing suitable access control mechanisms are just two sample responsibilities of an AI organization.

AI organizations and their assets face various risks and threats. First, AI models and training data should not leave the organization since they are often sensitive, critical intellectual property. Second, an attacker (or unhappy employee) might manipulate the training data, resulting in poorly working AI models. Finally, if the AI organization runs an AI runtime server which executes all the company’s AI models, a non-availability of the AI runtime server impacts all processes somehow depending on requesting AI model inference services on the non-available server.

As a consequence, AI organizations should conduct a risk assessment together with the security organization to identify weaknesses and implement technical and procedural best practices for securing their AI environment and their assets.
