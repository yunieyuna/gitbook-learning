# 3. Quality Assurance in and for AI

With AI influencing important decisions in companies, organizations, and our private lives, the quality of AI models becomes a concern for managers and citizens alike. Sales success depends on targeting customers with personalized and fitting products. In the medical area, researchers want to identify Covid-infected persons by letting an AI model analyze how persons cough into a microphone. These two simple examples prove: AI passed the tipping point from harmless, academic experimentation to real-life solutions. The time has come that AI projects require auditable quality assurance and testing. Errors can impact financial results severely or threaten human lives. Data scientists, test managers, and product owners cannot continue treating AI models as magical, always correct black boxes created by brilliant-minded, impeccable, and sacrosanct specialists.

Consequently, this chapter elaborates on how to structure and manage preventive quality assurance by taking a closer look at the following topics:

* **Model Quality Metrics** measuring the quality of AI models and making models comparable. They prove or disprove whether the models are as good and valuable as the data scientists promise.
* **QA Stages** defining a structured process when and in which order projects must calculate quality metrics based on which data. These stages ensure a smooth progression of AI projects towards ready-to-use AI models. They avoid wild, unfocused, and untransparent experimentations.
* **Model Monitoring** continuously checks whether AI models deliver reasonable prediction and classification quality weeks and months after the deployment.
* **Data Quality** reflecting the high impact of good data on the prediction and classification quality of the trained AI models.
* **Integration Testing** looking at whether the AI model and the rest of the application landscape interact correctly.

Figure [3-1](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig1) summarizes and illustrates the dependencies of these topics. The following sections look at the respective topic boxes in detail.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_3_Chapter/516372_1_En_3_Fig1_HTML.png" alt="../images/516372_1_En_3_Chapter/516372_1_En_3_Fig1_HTML.png" height="600" width="1725"><figcaption><p>Figure 3-1</p></figcaption></figure>

### AI Model Quality Metrics

A basic understanding of AI quality metrics enables AI project managers to talk “data science” with their team members with postgraduate degrees in math, statistics, or computer science and give them the impression of understanding them. Does the equation y=67\*x6+54\*x5+23\*x4+54\*x3-9\*x2+69\*x1+42 provide an adequate price estimation for used cars? Or should we replace 54 with 53 or with -8? Just looking at a linear equation or thousands of weights of a neural network does not allow us to judge whether an AI model works appropriately in reality.

#### Performance Metrics for Classification

The best known and most widely used quality assurance metrics for classifications – does a customer buy a bag or is this a cat image – is the **confusion matrix** . A confusion matrix allows comparing the quality of models trained with the same algorithm, for example, two logistic regressions basing on differently prepared data. Also, it enables comparing models trained with different algorithms, for example, one with a neural network and one with a logistic regression algorithm.

Figure [3-2](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig2) illustrates an AI model for classification that takes images as input and tries to determine whether an image contains a dog. Four of the five images classified as dog images are indeed truly dog images. Thus, the confusion matrix has the value four in the _true positives_ cell. The one image wrongly classified as “true”/“dog” results in the value 1 for _false positives_ in the confusion matrix. The AI model did classify various images in Figure [3-2](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig2) not to be dog images. That is correct for four cases (_false positives_) and wrong twice (_false negatives_).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_3_Chapter/516372_1_En_3_Fig2_HTML.png" alt="../images/516372_1_En_3_Chapter/516372_1_En_3_Fig2_HTML.png" height="984" width="1629"><figcaption><p>Figure 3-2</p></figcaption></figure>

The confusion matrix is the foundation for three advanced metrics:

*   **Accuracy** is the percentage of correctly classified images. Basically, this is the number of images with dogs correctly identified as such, plus the number of images correctly classified not to contain a dog divided by the overall number of images. In the example, the calculation goes as follows:

    Accuracy = (4 + 4) / (4 + 4 + 1 + 2) = 8 / 11 ≈ 73%
*   **Precision** reflects how many of the images classified (correctly or incorrectly) as dogs are really dogs. In our dog example, the calculation is:

    Precision = 4 / (4 + 1) = 80%
*   **Recall** focus on how many of the “true” cases – images with dogs in our example – the AI model identifies. In our example, the AI model identifies four out of six images with dogs:

    Recall = 4 / (4 + 2) = 67%

Obviously, the higher precision and recall – the lower the number of false negatives and false positives, respectively – the better the AI model. Optimizing both, precision and recall, is sensible, desirable, and necessary. However, ultimately, projects have to decide which of the two is more important. The choice depends on the usage scenario. It is often a trade-off once the model reaches a good prediction quality and should be improved further.

Sample scenario one is a credit card company. They process millions of payments per day. Their earnings are 0.1% of the transaction volume. So, a CHF 2000 payment generates CHF 2 revenue. A fraudulent payment means that the credit card company has to write off the total amount. Thus, letting one fraudulent payment slip through has a massive impact on revenues. In such a scenario, the credit card company prefers a high recall rate and accepts a lower precision rate. They block as many potentially fraudulent payments as they can identify. Obviously, there are limitations. Blocking too many payments reduces the income, and customers change to competitors if they cannot use their cards adequately in their daily lives.

Scenario two – determining stocks that potentially outperform competitors – is an example of optimizing precision. A hedge fund might prefer an AI model that identifies five stocks outperforming the market over a model that proposes twenty stocks if share prices plunge next week for half of the twenty.

These two sample scenarios illustrate that project managers need a vision of how they position the project in the precision vs. recall continuum. They can even provide a formula for the data scientists in an ideal world, for example, five false positives are equally harmful as two false negatives. Data scientists feed this information as input to the AI model training algorithms.

#### Classification and Scoring

The discussion about classification model performance gets another nuance when understanding a classification not as a true/false decision but as a **scoring** and **ordering** task. A logistic regression typically returns a value between 0 and 1 for each image. A value over 0.5 means a dog; a lower value indicates that it is not a dog. However, we can also work with the original data between zero and one and interpret the values as an order for their likelihood to contain dogs. To prevent misunderstandings: the values are not percentages or probabilities, just scores!

Figure [3-3](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig3) does exactly such an ordering of the images known from Figure [3-2](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig2).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_3_Chapter/516372_1_En_3_Fig3_HTML.png" alt="../images/516372_1_En_3_Chapter/516372_1_En_3_Fig3_HTML.png" height="567" width="1721"><figcaption><p>Figure 3-3</p></figcaption></figure>

An excellent way to understand this ordering is to think about bubbles in champagne. If you fill a champagne flute and look at the champagne, the bubbles make it one by one to the top – without adding new bubbles. How quickly an AI model moves relevant elements to the front is also a performance metric.

Figure [3-4](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig4) illustrates the process. The set of images consist of 6 dog images and 5 other images. When randomly choosing an image, we have a roughly 50% chance to select a dog image. If we choose two images, we can expect to retrieve one dog image; if we choose four, we expect two. The dotted line shows how many dog images we can expect when randomly selecting images.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_3_Chapter/516372_1_En_3_Fig4_HTML.png" alt="../images/516372_1_En_3_Chapter/516372_1_En_3_Fig4_HTML.png" height="979" width="1725"><figcaption><p>Figure 3-4</p></figcaption></figure>

In contrast, the filled background symbolizes how many dog images we retrieve when selecting images based on their score, starting with the highest score. The indication for the performance of an AI model is how many “dog” images are over the dotted line, which represents the expected performance for randomly selecting images.

In Figure [3-3](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig3), the first three images are all dogs – quite an impressive result for the AI model. The probability of getting such a good result when randomly choosing images is only 12%.

#### Additional Performance Metrics

True positives, false positives, accuracy, precision, recall – these terms and concepts work well for binary classifications. Does the picture contain a dog? Does the client buy a new white suit if we send him a discount coupon by email? We need more varied metrics for other types of insights and AI models with more complex outputs. In the following, we look at metrics for three more scenarios:

* Linear Regression
* Multiclass Classification
* Object Detection, that is, which objects are where on a picture

Suppose you are an AI project manager and run a status meeting. Suddenly, data scientists talk about **R2**, also named _R squared_ or _coefficient of determination_. A good AI project manager has to figure out quickly how to react. Is it time to cheerfully congratulate his team for their breakthrough – or do they need emotional support after a bitter defeat against the world of math? R2 is a metric reflecting how well a linear regression model reflects reality or the data items in a training data set for prediction problems. So, whereas for classifications, the output is “dog” or “cat,” the result for predictions is “We expect 5.7 incoming calls per minute in our call center in the next hour.” and not 20.3 and not just 1.

The calculation of R2 is simple, but the underlying rationale requires some thinking. The first step is calculating the difference between the predicted and mean values and squaring the result for all data points. After summing up the squares, the result is called the sum of residual squares (SQR).

Second, you calculate the difference between the actual value and the mean value and square the results. Again, you sum up the squares over all data points. The result is the sum of squares total (SQT). Then, R2= 1 – (SQR/SQT).

What AI project managers should understand is that R2 is between 0 and 1. The closer to 1, the better the model. High values are possible for technical and physical processes with clear and known variables; for predicting single persons’ individual (!) behavior, even 0.1 can be good. However, when looking at larger groups of humans (not at the level of a specific individual), values between 0.4 and 0.8 or even more can be realistic.

An AI project manager’s (or AI translator’s) task is to put such metrics and numbers into a business context. What advice could the data scientists give to the business? What should the business, for example, do, and how confident are the data scientists that this is the right decision? Increasing sales by 5% might be more than sales managers hoped and dreamed of achieving. On the opposite, good predictions and models might be useless. Do you know the famous forecast from the stand-up comedian George Carlin: “Weather forecast for tonight: dark. Continued dark overnight, with widely scattered light by morning.” Sophisticated AI models predicting the obvious are useless. If companies invest in complicated AI models, the models must be better than a naïve and obvious guess.

As always, there are more prediction performance metrics. Worth mentioning is especially the **Adjusted R2** **Metric**. It also considers the complexity of the prediction function. The metric punishes, for example, if the prediction function is not just linear (y=ax+b), but also contains quadratic (y=ax2+bx+c) or is even a higher-level polynomial. It punishes incorporating many input attributes such as y=ax0+bx1+cx2+dx3+ex4+f instead of selecting the most relevant ones only, such as y=ax0+bx3+c. The rationale is that complex functions improve the prediction quality but potentially cause overfitting (the latter is an issue this chapter addresses later).

**Multiclass classification** means that the classification result is not just a yes/no, but can be a dog, cat, turtle, or giraffe. In a business setting, such an AI model might predict the next best product for a customer. Should the bank advisor try to sell the client an equity fund, a mortgage, or a golden credit card?

Multiclass classifications need an expanded confusion matrix, as Table [3-1](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Tab2) illustrates. Each class requires a matching column and row. Again, the matrix allows comparing predictions and reality.Table 3-1

Confusion matrix example for multiclass classification

|                                      | <p>Predicted:</p><p>Credit Card</p> | <p>Predicted:</p><p>Mortgage</p> | <p>Predicted:</p><p>Equity Fund</p> | Predicted: Savings Plan |
| ------------------------------------ | ----------------------------------- | -------------------------------- | ----------------------------------- | ----------------------- |
| Actual Bought: Credit Card           | 189                                 | 5                                | 8                                   | 7                       |
| <p>Actual Bought:</p><p>Mortgage</p> | 12                                  | 25                               | 5                                   | 4                       |
| Actual Bought: Equity Fund           | 12                                  | 13                               | 58                                  | 24                      |
| Actual Bought: Savings Plan          | 7                                   | 1                                | 51                                  | 125                     |

For multiclass classifications, the metrics definitions change slightly – and more metrics variants exist. Here, we explain two: (multiclass) accuracy and balanced accuracy. Comparing both gives project managers an idea of what different purposes or foci metrics can have.

**Accuracy for multiclass classifications** is the portion or percentage of correctly classified or predicted items. In the example, we divide the sum of the items in the diagonal by the sum over all cells:![\$$ {Acc}\_{multiclass}=\frac{189+25+58+125}{189+5+8+7+12+25+5+4+12+13+58+24+7+1+51+125} \$$](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372\_1\_En\_3\_Chapter/516372\_1\_En\_3\_Chapter\_TeX\_Equa.png)The predictions of the frequent classes dominate the result. Suppose some classes appear more often than others. In that case, the classification quality for the scare classes has a low impact on this metric. For example, different cancer forms are illnesses not many have at a defined point in time. An algorithm that predicts “no cancer” for each and every patient performs close to 100%. The very few cancer patients have (nearly) no impact on multiclass accuracy. Multiclass accuracy is worthless for this scenario. A metric that deals better with scare but important classes is the **balanced multiclass accuracy** metric. It averages the metrics of each class:![\$$ {Acc}\_{balanced}=\frac{\frac{189}{189+5+8+7}+\frac{25}{12+25+5+4}+\frac{58}{12+13+58+24}+\frac{125}{7+1+51+125\}}{4} \$$](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372\_1\_En\_3\_Chapter/516372\_1\_En\_3\_Chapter\_TeX\_Equb.png)

In other words: All classes have the same weight in case of this metric, no matter how frequent any of them is. The performance on multiple infrequent classes outweighs issues on one or two dominant classes for this metric.

Multiclass and balanced multiclass accuracy are two examples of quality metrics for multiclass classifications. None of them is per se better than the other. The driving factor is the business question the AI model wants to solve. AI project managers must understand which metrics the data scientists use and why. It is essential for delivering business value. Choosing inadequate metrics means that the created AI model might not deliver the needed business value. However, it is not always easy to make the right choice, especially for a complex business question. Still, it is a good idea to start with established metrics. There is a reason why they are widely used: they are often helpful.

Measuring the performance of **object detection** goes much further than simple multiclass classification. First, images can contain more than one object, for example, a glass, a bottle, and a chair (see Figure [3-5](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig5)). Second, the AI model also determines bounding boxes for each object. The boxes mark the location of the identified objects on the image. Typically, an object counts as correctly detected if bounding boxes overlap at least 50%.

In Figure [3-5](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig5), the AI model identifies the bottle, the glass not. While the object class “glass” is correct, the overlap of the shapes is too low. Finally, the chair is not detected, and there is no car where the algorithm detected one. So, how good is the AI model? One metric is defined as dividing one by four, resulting in a value of 25% (Intersection over Union).

To conclude, the examples for performance metrics for predictions, multiclass classification, and object detection illustrate the variety of existing metrics for the different AI challenges. Some are more complicated than others, but AI project managers invest their time wisely when they insist on understanding what their data scientists optimize. The AI project success depends on whether the metrics the AI model optimizes match the relevant business performance metrics.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_3_Chapter/516372_1_En_3_Fig5_HTML.png" alt="../images/516372_1_En_3_Chapter/516372_1_En_3_Fig5_HTML.png" height="1075" width="809"><figcaption><p>Figure 3-5</p></figcaption></figure>

### QA Stages in AI Model Engineering

When data scientists present their AI project model and the precision and recall or R2 values, should he run to his sponsors and tell them they made it? Is the model ready for production deployment?

The answer is a clear “no.” A single metric is never enough. The project manager needs at least two to three metrics and confidence that the training data was used adequately to produce the metrics. Figure [3-6](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig6) outlines a process of how to structure the quality assurance for AI models. A good model is not the lucky result of random parameter or weight changes if the metrics are not as good as needed or expected. Getting a good model means starting with multiple candidate models. The project drives them through the various QA stages. Eventually, the chosen one makes it through all stages because it is the first good model getting there or a candidate model superior to all others.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_3_Chapter/516372_1_En_3_Fig6_HTML.png" alt="../images/516372_1_En_3_Chapter/516372_1_En_3_Fig6_HTML.png" height="117" width="1729"><figcaption><p>Figure 3-6</p></figcaption></figure>

#### Perfect but Worthless Model Metrics

Projects should avoid worthless metrics, that is, metrics that do not help to understand the model performance. Some car manufacturers impressively perfected the concept of useless metrics, which turned out to be a bad idea. They optimized car engines a little bit too much. All cars met the regulatory emission benchmarks because the car adjusted the engine when noticing to be in an emission test. Emissions under normal traffic conditions were completely different and much less eco-friendly. It was not for the good of the involved managers and engineers. AI projects should avoid – even unintentionally – producing too-good-to-be-true metrics as the car manufacturers. Such issues happen when mixing up the data for training and for assessing an AI model.

To add a second nuance: Do you know the fastest way to implement a 100% perfect model if using the same data for training and quality assurance? Program a component that checks whether the model input data corresponds to an entry in the training set. If so, take the correct answer from the training set. Otherwise, return a random number. Your model achieves 100% for precision and recall on the training set – and is entirely useless. The only way to avoid running unintentionally into such issues is to split your training data into three data sets: training, validation, and test data sets – and to follow a strict quality assurance procedure this chapter explains.

#### The Training, Validation, and Test Data Split

The purpose of a **training dataset** is to build and train a machine learning model with this specific data. There are many ways to optimize and fine-tune AI models, such as trying out various mathematical function(s) or changing the number of hidden layers for neural networks. Data scientists perform these optimizations using the training data (only). In contrast, the purpose of the **validation dataset** is to compare multiple candidate models and to check for overfitting. The final test before actually deploying a model is a final assessment using the **test dataset**.

The split of the dataset depends on the context. Datasets often consist of several hundred or a few thousand data points for predictive analytics and statistics scenarios. The typical **split ratio for training/validation/test datasets** for them is 60%/20%/20%. In big data, machine learning, and neural networks, there are often millions of data points. Then, a 98%/1%/1% split ratio makes sense. Validation and test datasets of around ten thousand data points are sufficient. Dividing the existing historical training data into these three subsets is essential before starting with the model training.

#### Assessing the AI Model with the Training Dataset

The first check of a newly trained AI model is how it performs on the training set. If AI models fail directly here, they are unlikely to perform better with live data in production in the “real world.” So, this is the first real QA step for a new AI model.

Assessing the training model with training data means feeding the training data set into the model and check whether the model’s predictions are correct. Does the model predict the bank customers to be interested in a credit card that got the product? The data scientists calculate the precision, recall, and accuracy metrics for classifications (or R2 for predictions) and verify whether the value is acceptable for the business. If the company needs a 95% recall to make a business case a success and the model reaches only 85% on the training set, moving forward with this model is a waste of time and money. The AI project team has to fix and improve the model first.

The visualizations in Figure [3-7](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig7) illustrates potential root causes for failing models. The curve represents the AI model, which divides the area into two subareas or classes. All cat images are in the upper left and middle part, all dog images in the lower area – 100% precision and recall. In Figure [3-7](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig7) (a), the curve separates the data set(s) entirely correctly. It is the perfect case, but usually, the situation sketched in Figure [3-7](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig7) (b) is closer to reality. The illustration helps understanding two reasons why AI models might not even perform well on their own training data.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_3_Chapter/516372_1_En_3_Fig7_HTML.png" alt="../images/516372_1_En_3_Chapter/516372_1_En_3_Fig7_HTML.png" height="479" width="1717"><figcaption><p>Figure 3-7</p></figcaption></figure>

The first potential root cause is **missing crucial information** and attributes. A picture of a car in a parking lot is not sufficient to determine whether the vehicle has enough gas to drive thirty kilometers. Thus, even if a distinction of two classes is possible in theory, models fail for specific data if they do not have all information. The solution is obvious: add attributes to the data points of the training data and retrain the model.

The same figure also represents a second reason for an AI model failing on training data: The underlying **model** function is too **simplistic** (**“underfitted model** **”**). Linear regression does not work well for complex, non-linear challenges. In our illustration, the AI model draws (only) one straight line to separate classes. Only a curve can separate the areas correctly (Figure [3-7](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig7), c). If the mathematical function does not match the problem, the AI model performs poorly. Data scientists fix such issues by moving from linear regression to a neural network or adjusting or extending the neural network and then retrain the model.

To conclude: quality issues causing insufficient AI model quality on training data can have two root causes, underfitting and missing information. Data scientists have to dig deeper and potentially experiment to understand the details. But once models work well on the training data, it is time for the next QA stage, which takes advantage of the validation data sets.

#### Assessing the AI Model with the Validation Dataset

When the AI model performs well on training data, the next step is testing it with the validation dataset. We do precisely the same as in the previous assessment, just with different data. This time, we feed the validation data set. The validation data is data _not_ used for training the model. This measurement aims to detect **overfitting** or a **high bias** in the training data. The symptoms are the same – good performance (e.g., recall, precision, R2) on the training set, poor performance on the validation set.

In the case of **overfitting**, the model does not generalize well. The AI model reflects all the oddities of the sample data but does not generalize, which is needed to perform well on unknown data. For those with a strong math background: If you have a training data set with 1001 data points, you can fit a 1000th-degree polynomial as a prediction function. The function works perfectly for the training data but is completely erratic everywhere else. Figure [3-7](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig7) (d) visualizes this phenomenon. The curve separates the dog and the cat images well for the training data. However, the border between the areas is so erratic that the model classifies images similar and close to training images differently.

Figure [3-8](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig8) provides another perspective on the underfitting/overfitting challenge. On the left side of the figure, the model is underfitted and too simplistic. The model error on the training and the validation data set is high. When the model gets more complex, its performance increases, that is, the error drops for the training and the validation data. When the model gets more complex, the model error gets lower and lower for the training data, whereas the model error on the validation data rises. We are overfitting the model. The challenge for data scientists is getting close to optimal model complexity, the tipping point between under- and overfitting. Regularization is one option to ease their work. It means penalizing more complex models when comparing model quality.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_3_Chapter/516372_1_En_3_Fig8_HTML.png" alt="../images/516372_1_En_3_Chapter/516372_1_En_3_Fig8_HTML.png" height="738" width="1721"><figcaption><p>Figure 3-8</p></figcaption></figure>

**Biased data** is the second potential root cause for good model performance on training, but not on validation data. Biased data means that training and validation data (and potentially the reality) have different data distributions and characteristics. In our cat-dog example, one data set might consist of a well-crafted set of dog and cat pictures. A second and a third data set might result from crawling the Internet and Wikipedia for images related to cats and dogs. Obviously, training a model with one of the data sets results in a good performance on this specific data set, but not necessarily when testing with any of the other sets. Without a budget for manually reviewing images, automatically collected training data probably contains images labeled as a cat that contain cat food or litter boxes for pets. All three approaches are legitimate to get data for training and validation and test data sets. The bias issue emerges when not mixing the images from the different data sets, but one becomes the training, the next the validation, and the last the test data set. The characteristics of the image sets most likely differ too much.

Figure [3-7](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig7) (e) illustrates the bias issue. A vertical line separates the two classes quite well when training a model with the circles. However, suppose the distribution of the validation data – symbolized with the rhombs – differs. In that case, the model does not classify them correctly.

Hopefully, your AI project trained one or more candidate AI models which perform well on the validation test data set. Then, the AI model is ready for a final check with the test data before production deployment.

#### Assessing the AI Model with the Test Dataset

The final assessment is with the test dataset. An AI model in this stage passed the assessments successfully with the training and validation data. Still, it can fail on the test data with poor metrics values. Then, the model is probably overfitted to the validation data under the assumption that validation and test data sets are similar.

This explanation might surprise. The purpose of the validation stage is to detect overfitting, yet we face this issue in the test stage. The reason is simple: The validation stage detects overfitting to training data, not to validation data. Since the model is now in the test state, it performed well (also compared to other candidates) with validation data. Obviously, an overfitted model has a good chance to perform well and to get to the test data stage. How to fix this shortcoming? Increase the validation data set to make overfitting less likely, repeat the validation test, and choose a different model for the final assessment with test data.

### Monitoring AI Models in Production

With the deployment of an AI model to production, research-oriented textbooks stop. What could you expect more than a high-performing model? In practice, the business expects a well-performing model not only for a day but for weeks and months. If the quality drops, retraining becomes necessary. At deployment time, the first question might be whether the model provides the classification and prediction quality required for the expected business benefit. Furthermore, the business might pose a second question: How can we ensure that the model offers improvements and savings over the long term. Even if they do not ask right now, they will clearly communicate their expectations if business benefits vanish and sales figures plump due to a not-so-good-anymore AI model.

A forward-looking AI project manager addresses such uncertainties and obvious necessity by implementing monitoring functionality as part of the project. It is a paradigm shift compared to the traditional QA approach in IT departments. The traditional QA strategy tests rigorously before going live. Afterward, the rule is: never touch a running system. AI models need a different approach. They might degenerate and become outdated within hours, days, weeks, or months. It is often a gradual process, but there are clear, measurable indicators:

* A significant change in the input values, for example, changes in the mean value or the distribution of input variables. Do shopping baskets contain 80% swimwear instead of 80% products of the winter clothing category?
* A significant change in the output values ​​of the statistical model. Does the distribution of the classification output change, for example, from 30% dog pictures to 45%? Will only 20 products be proposed to all customers over one day instead of 1000 different products per day previously?
* Declining forecast quality: Are 20% fewer customers adding the product proposed by the AI component to their shopping cart than a month ago?

Of course, an application must collect these key figures so that business users or data analysts can recognize changes or an automatic alarm is possible.

An (more complex) alternative to monitoring indicators is retraining the actual model from time to time or even every night. A new model is generated with the then-current data and compared with the previous model. In the event of significant deviations, data scientists replace the older model with the new one. How often do you generate a new model for control purposes – and whether you can automate this model creation – is ultimately a commercial question.

### Data Quality

The problem with cheating in exams by copying your neighbor’s answers is simple. Typically, the least-prepared students do not sit next to the best and brightest. Their neighbors might also not have a clue – and replicating wrong answers and building on them usually does not end well.

Data scientists face similar situations. They create AI models, sometimes too naïvely, assuming that the training data is in good shape. A recent study found publicly available training sets used in academia and research to have issues with 0.15 to 10.1% of the training set elements. These numbers should be a warning for data scientists and AI project managers to take the topics seriously during the CRISP-DM phase “data understanding.” Projects benefit from looking at data quality right from the beginning.

The literature is full of good ideas and frameworks. Implementing them and ensuring good data governance practices is often a real challenge. Luckily, this is not the responsibility of data scientists. They are not the ones who should get involved in extensive data cleaning exercises. However, they might have to do some data preparation with limited improvements themselves. From an AI or data science perspective, data scientists and AI projects can restrict their focus on three main data quality aspects (Figure [3-9](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_3\_Chapter.xhtml#Fig9)):

* Technical correctness
* Match with reality
* The reputation of the data

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_3_Chapter/516372_1_En_3_Fig9_HTML.png" alt="../images/516372_1_En_3_Chapter/516372_1_En_3_Fig9_HTML.png" height="800" width="1550"><figcaption><p>Figure 3-9</p></figcaption></figure>

#### Technical Correctness

Data is correct from a technical perspective if it matches the defined data model. Mandatory fields have values, expected relationships and links between data sets exist, and data types and actual data match. IT specialists from the database world sometimes forget one fact: not all training data for AI models intelligence comes from tables with strictly enforced schema constraints such as NOT NULL or UNIQUE. Log data, JSON files, or manually filled Excel sheets often are in a less strict format. Plus, some documents are broken for whatever reason, or attributes are missing or filled with placeholders such as “xxxxxx” if not known when creating the row. Humans get pretty creative when forms do not match their needs and reality.

Just skipping – instead of improving – technical inaccurate training data can have unexpected severe side effects. Suppose you create a model for customer behavior in the EU. The various source systems together deliver 500,000 data items for your training set. 35,000 miss multiple attributes. The data scientists suggest skipping them. What should you say or do as an AI project manager under pressure?

Skipping 35,000 from 500,000 data points is not a significant quantitative loss. Still, the impact on the model quality can be severe. Suppose the 35,000 are not equally distributed over the data set, but are the complete German market data. If training a model without such German data, the model probably works nicely on the training data. The metrics might look perfect even for the validation and test data when this data also comes from data that skipped the incomplete rows. However, once in production, the model probably performs poorly for German customers. Thus, addressing technical data quality by removing data requires a second thought and at least a rudimentary analysis before removing the rows from your data set. Otherwise, you might skip essential information.

#### Data Matches Reality?

About 100 million AUD$ just for extra legroom on a flight was what the Qantas app showed a passenger as the price. In the end, he was charged 70 AUD$ on his credit card. It is an extreme example of discrepancies between data in two systems. Similar, though usually less severe, differences can exist when comparing data in a training set and the reality.

AI project managers and teams cannot validate each and every data item. Still, they can verify various aspects by checking individual data items or looking for hints for quality issues. They could, for example, look at the following:

* Is the provided data, such as street names or sales figures, correct?
* Is the data complete and free of redundancies, for example, there is precisely one booking for each customer purchase – and not two or none?
* Is the data consistent? If, for example, two attributes store the same information, such as the tax location, are they always the same?
* Does the distribution of the training data reflect the reality – or do we have in our training data much more pictures of expensive cars than of the various standard entry-level cars?
* How quickly does a change in the real world result in an adaptation of the data? Does it show up in the training set one hour later, or only next month if customers make purchases?

Measuring such objective criteria helps understand when to rely on specific data sources and to detect if sources have discrepancies and (timely/delay-caused) inconsistencies.

#### Reputation of Data

The third dimension differs from the first two. It puts human judgment in the foreground. What do recipients of the data (or the recipients of your AI model) think about the data quality of the training data sources? They might not have a short-term alternative if they question your data sources and, thus, the models you create. Eventually, however, they find other options – other teams, doing something “on top” with Excel, or getting a service from an external provider. I learned this lesson on my very first client engagement. We had a consulting mandate within a large IT service provider. I was trying to draw a diagram with all the systems storing and processing data. One system responsible proudly told me that his data is of top quality and always up to date. Well, one of the recipient teams implemented another solution to clean the data before using it.

The system responsible was neither a liar nor negligent. With different needs and usages, the same data can be anything from “not useful” to “excellent.” An attribute “sold items” can count the number of items which customers put in their shopping basket – or the numbers of shipped items and not returned. If you are in the controlling department, your focus might be more on correct numbers than on valid shipping addresses. Data quality has a recipient-specific subjective component. This aspect is vital for AI projects. It can impact the acceptance of the AI model if you use known-to-be-good or known-to-be-often-wrong sources for your training data.

### QA for AI-Driven Solutions

AI models can support one-time strategic (management) decisions – or optimize and improve repetitive, operational processes. In the latter case, a good model alone is not enough. It must work as part of an AI-driven software solution. Thus, quality assurance must cover three areas:

1.  1\.

    Testing the software solution without the AI component. Is the GUI working? Are calculations correct?

    &#x20;
2.  2\.

    Quality assurance of the AI model. Do predictions and classification models work?

    &#x20;
3.  3\.

    Verification of the integration of the AI model into the software solution.

    &#x20;

The first point relates to standard software testing. Are all features correctly implemented according to the specification? Do the shopping cart feature and the payment process work in the fashion shop app? Is the app not crashing? The IT literature covers testing broadly. Companies have standards and guidelines and established test organizations with experienced testers and test managers. Thus, the first point can be assumed to be solved nearly everywhere, whereas the previous pages covered the second point, QA for the AI model. This last section focuses on the third aspect: potential issues specific to integrating and coupling an AI model into a complete software solution. We have the fashion app, and we have an AI model recommending items to customers – do they work properly together?

Quality assurance and testing measures for the integration depend on the chosen integration pattern. The previous chapter presented three main patterns. The first and most simple is **precalculation**. This pattern does not require any technical integration between the AI component and the software solution. Instead, data scientists or IT operations specialists feed up-to-date data into the AI model, for example, the shopping history of the fashion app customers. Then, the AI model predicts for each customer individually the fashion items she is most likely to buy. These results are written in a CSV or Excel file. A data scientist or IT operations specialist uploads the file, for example, to the CRM tool. The CRM tool sends out personalized push messages to the customers. The messages aim to make the customers curious to look (and buy) the suggested fashion items.

The integration itself does not require any testing, just some validation of whether the generated files match the format expectation of the CRM tool. Since this is a manual process, checklists or a four-eyes principle reduce the operational risks, especially for repetitive processes.

The second integration pattern is **model (re)implementation**. Data scientists provide the model, for example, in Python or R. Then, developers of the actual software solution take over. They add the code as it is or recode the same logic in Java or C#. The model becomes part of the software solution’s source code. This pattern requires dedicated test cases to address two concerns.

The first concern relates to _mistakes in the reimplementation_. Recoding a code fragment in a different language poses the risk of mixing up parameters when invoking a function. Also, overlooking and forgetting to reimplement single lines of code is a risk, so is mixing variable names. Test cases with sample input/output data pairs address this risk. An example is to have a test case based on a specific dog image. Suppose the output classification value was 0.857 in the AI model training environment. In that case, a reimplemented AI model in the software solution must produce precisely the same result.

Also, checksums on configuration, metadata, and weight parameters help when replicating neural networks. The sum of all parameter values – a semantically irrelevant number – must be the same. More advanced metrics detect swapped parameter values or switched rows.

The second concern relates to the actual connection or _interface_, especially regarding the parameter usage. Is the first parameter the last bought product ID or the quantity – or is it the other way around? Does “1” mean that the image contains a dog – or does “0” represent the dog information? Simple mistakes result in a useless software solution and ruin, for example, customer experience and sales numbers of the fashion shop app. An AI project manager should double-check if the testers of the software solution also address these quality risks.

The third integration option is an **AI runtime server** such as RStudio Server. The idea is to provide one server on which all AI models in the company run. When a software solution wants to integrate an AI model, it invokes the specific model on the AI runtime server. Potential testing needs are, again, whether the parameters are used correctly for service invocations. Then, the model management becomes more relevant. Is the correct model invoked or deployed, that is, dog detection, and not the model for cats? Is the most recent winter fashion items catalog the base for the model in use – or the beachwear fashion catalog from two years ago? When implementing this integration pattern, software developers and data scientists can work entirely independently and (nearly) without communicating with each other. Thus, checking whether the software solution invokes the correct model with the proper parameters becomes even more crucial in such settings.

### Summary

AI offers many organizations new optimization opportunities. At the same time, there are new challenges for testing and quality assurance. Confusion matrixes or the R2 value – metrics are the base for determining an AI model’s quality. They are at the core of any proof-of-readiness assessment. Measuring an AI model’s performance against training, validation, and test data provides a structure for the model training and enables AI project managers to track the project’s progress.

In contrast to traditional software with a “never touch a running system” attitude, AI models require monitoring. Are they still adequate some weeks or months later, or do data scientists have to train a new and improved version? The key for any convincing AI model is, however, correct training data. AI projects have to take not only the data quantity but also its quality seriously.

For sure, there are also additional non-functional-requirements-like necessities such as regulatory needs, ethics, or explainability. We cover these quality aspects in the next chapter.
