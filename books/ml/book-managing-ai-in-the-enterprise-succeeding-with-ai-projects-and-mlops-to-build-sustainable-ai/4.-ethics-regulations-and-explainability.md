# 4. Ethics, Regulations, and Explainability

The laissez-faire years for big tech companies, innovative start-ups, and companies exploiting the benefits of IT innovations are coming to an end – also when it comes to AI. Policymakers discuss regulations already for years. But now, they take action. AI organizations and companies have to ask themselves – for ethical reasons – whether they should create the AI models they work on and whether their projects are even legal. They can no longer focus just on developing superior AI models, verifying their quality – and ignore everything else.

While AI ethics sounds like an ivory tower topic, it is not. First, most companies fear negative press and public outrage. They avoid reputational risks, especially when some experts play around with technology without clear business needs or apparent benefits. Second, AI regulations are in flux. Today’s AI ethics discussions are the source of tomorrow’s AI regulatory necessities. These discussions allow AI organizations to forecast what regulators might demand next. It does not need a fortune-teller to foresee that AI regulation will be a dominant topic in the 2020s.

Consequently, this chapter discusses regulatory and ethical topics. A third topic – explainability – complements these two topics. It is a concept or an AI model property helping to meet specific AI ethics and regulatory demands.

### AI Ethics

Suppose you want to buy a new apartment. You ask your bank for a mortgage. They decline. The banker tells you their AI calculated a 75% probability that you get divorced within the next two years. So, no mortgage for you. Would you consider this prediction as ethically adequate? In more than a decade working with Swiss banks, I learned that the big pink elephant in the room is the divorce risk. No banker talks about the topic – especially not with customers – though rumors are clear: divorces are the most crucial reason for mortgage problems. It is a big dilemma for banks. Like most other companies, organizations, and corporations, banks aim to be seen as reasonable and responsible, not as the source of family disputes or escalating relationship problems. Most companies have higher ethical standards than “Don’t be evil.” These pledges have implications for their AI organizations. They have to be aware of three potential areas where ethical questions might interfere with their daily work and long-term business or IT architectural ambitions.

#### The Three Areas of Ethical Risks

Ethical risks for AI fall into three categories (Figure [4-1](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_4\_Chapter.xhtml#Fig1)): unethical use cases, unethical engineering practices, and unethical AI models.

**Unethical use cases** such as calculating divorce probabilities are not AI topics. They are business strategy decisions. Data scientists can and should require a clear judgment on whether a use case is acceptable or not. This judgment is with the business, not with the IT or AI side of an organization.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_4_Chapter/516372_1_En_4_Fig1_HTML.png" alt="../images/516372_1_En_4_Chapter/516372_1_En_4_Fig1_HTML.png" height="554" width="1725"><figcaption><p>Figure 4-1</p></figcaption></figure>

**Ethics in engineering** relates to the development and maintenance process. For example, Apple contractors listened to and transcribed Siri users’ conversations enabling Apple to improve Siri’s AI capabilities. This engineering and product improvement approach resulted in a public outcry since society sees the approach as an invasion of the users’ privacy.

Finally, the **ethical** questions related to the **AI model** and its implications for customers, society, and humans are the true core of any AI ethics discourse. When algorithms decide who gets the first Covid vaccination and governments outsource the AI model development to a controversial IT service provider, AI decides about life and death – and AI ethics is no longer an ivory tower topic.

#### Handling Ethical Dilemmas

Ethical dilemmas are on a different dimension than inconvenient management decisions. Should a social media web page post statements of a well-known agitator to attract clicks? Should a start-up harvest public domain images on a large scale to save money instead of making their own photos? These are inconvenient questions for managers, who have to balance ethical behavior and standards vs. profit, loss, and risk. However, real ethical dilemmas are more severe. They relate more to the AI models themselves. Training an AI model means measuring how good a system is. It means quantifying results and optimizing or minimizing loss functions. These loss functions are at the core of **ethical dilemmas**, challenging situations for which no undisputable choices exist. The classic example is the **trolley problem**. Ahead of a trolley are three persons on the track. They cannot move, and the trolley cannot stop. There is only one way to stop the trolley from killing these three persons: diverting the trolley. As a result, the trolley would not kill the persons on the track. It would kill another person that would not have been harmed otherwise. What is the most ethical choice? If the “damage” of killing someone on the track is “-5”, what should be the penalty for killing an innocent person – also “-5” or better “-10”?

The trolley problem is behind many ethical dilemmas, including autonomous cars deciding whether to hit pedestrians or crash into another vehicle. When AI directly impacts people’s lives, it is mandatory to understand how data scientists train the AI models to choose an action in such challenging situations.

Data scientists can generally design an AI component dealing with such situations in two ways: top-down and bottom-up. In a bottom-up approach, the AI system learns by observation. Since ethical dilemmas of this type are pretty rare, it can take years or decades until a driver faces such a dilemma – if at all. Luckily, most of us never had to decide whether to overrun a baby or a young boy. Observing the crowd instead of working with selected training drivers is an option to get training data quicker. The disadvantage is also apparent: the crowd also teaches bad habits such as speeding or cursing.

Alternatively, data scientists can follow a top-down approach with ethical guidelines guiding through critical situations. The best-known example are Asimov’s laws for robots:

1.  1\.

    A robot may not injure a human being or allow a human being to come to harm.

    &#x20;
2.  2\.

    A robot must obey orders given to it by human beings except where such orders would conflict with the First Law.

    &#x20;
3.  3\.

    A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

    &#x20;

Asimov’s laws are intriguing but abstract. They require a translation or interpretation for deciding whether a car hits a pedestrian, a cyclist, or a truck when an accident is inevitable.

In practice, however, laws and regulations clearly define acceptable and unacceptable actions for many circumstances. If not, it gets tricky. Is the life of a 5-year-old boy more important or the life of a 40-year-old mother with three teenage daughters? An AI model making a definitive decision on whom to kill is not a driver in a stressful situation with an impulse to save his own life. It is an emotionless, calculating, and rational AI component that decides and kills. For AI systems, society’s expectations are higher than for a human driver. What complicates matters are contradicting expectations. In general, society expects autonomous cars to minimize the overall harm to humans. There is just one exception: vehicle owners want to be protected by their own vehicle, independent of their general love for utilitarian ethics when discussing cars in general.

Implementing top-down ethics in AI systems requires close collaboration of data scientists or engineers with ethics specialists. Data scientists build frameworks that capture the world, control devices, or combine ethical judgments with the AI component’s original aim. It is up to the ethics specialists to decide how to quantify the severity of harm and make it comparable – or how to choose an option if there are only bad ones. However, the good news for AI specialists is: ethical dilemmas are exceptional cases.

#### On Ethical AI Models

Most data scientists have to deal with AI ethics when the question emerges whether their AI models are ethically adequate. These models impact people’s lives, be it a credit scoring solution or a face detection model for automated passport controls at airport customs. There is a lot of ongoing research, but two concepts stand out for AI organizations in an enterprise context: bias and fairness (Figure [4-2](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_4\_Chapter.xhtml#Fig2)).

**Bias** was already a topic in the quality assurance chapter. The concept looks at the training data and its distribution. We call training data biased if its distribution deviates from the one in reality. If a society consists of roughly 50% male and 50% female persons, the training data distribution should be similar. If the training data for face recognition consists of 99% male pictures, the created AI model most probably works well on them. In contrast, the model is most likely unreliable for females.

What makes avoiding bias an easy-to-get-support-for measure is that removing bias and raising ethical standards brings a business benefit: the AI model gets better.

**Fairness** is a concept demanding that the trained AI models behave similarly well for all relevant subgroups, even for small ones. Suppose a company has 95% female employees. Such a gender imbalance impacts the AI model training. The training could optimize the accuracy for female employees and ignore the male ones and still achieve high accuracy. When 95% of the employees are female, and the algorithm identifies 99.9% of the female population correctly, but only 10% of the remaining 5% male employees, overall, the AI model identifies 95.4% correct – not bad, at least if you are female, not male. However, the ethical concept of fairness rejects such a model. It would be unthinkable that every male employee must show a passport at the reception, whereas all female employees can use their badge to get in.

In an enterprise context, fairness and bias have one crucial difference. In contrast to bias, fairness causes costs for the business. Fairness means not using the best AI model but a model that works equally well for all relevant subgroups, for example, by overrepresenting small relevant subgroups in the training set. To illustrate this, we extend the example from above. The best AI model might identify 99.9% of the female employees but only 10% of the male ones. The best fair (!) AI model might have an accuracy of 90% for both male and female employees. Such a “small” change of the overall accuracy can be cost-intensive. Using a 95% accurate automatic passport controls at an airport requires the customs staff to check the passport of one out of 20 travelers. An accuracy of 90% means checking 10% of the travelers, requiring to double the customs staff.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_4_Chapter/516372_1_En_4_Fig2_HTML.png" alt="../images/516372_1_En_4_Chapter/516372_1_En_4_Fig2_HTML.png" height="738" width="1725"><figcaption><p>Figure 4-2</p></figcaption></figure>

#### AI Ethics Governance

AI ethics is a topic with many stakeholders, from data scientists, AI managers to business specialists needing AI models and corporate social responsibility teams. It is a highly emotional topic, too. Organizations risk bitter fights about what is adequate and what is morally acceptable and who decides what if not working out crystal-clear who or which board makes which decisions concerning the three areas: (un)ethical use cases, (un)ethical engineering practices, and (un)ethical AI models

The Google/Timnit Gebru controversy illustrates an easy-to-oversee pitfall for AI organizations. Gebru’s reputation bases on earlier work where she unveiled that IBM’s and Microsoft’s face detection models work well for white men but are inaccurate for women of color. In other words: She found flaws in the model caused most probably by biased training data. Such flaws are embarrassing, but knowing these shortcomings enables data scientists to build better models.

While with Google, she co-authored a paper that criticized Google’s work on a new language model for two reasons. First, Google trained its model with texts available online. Such texts are obviously (partially) biased, discriminating, or even openly racist. When the training set also contains such texts, the model reflects them partly as well. UN’s Autocompletion Truth campaign illustrated this in videos. They let Google Search’s autocomplete function suggest which words should come next. If “women cannot” results in “women cannot drive,” this reflects prejudices in our use of the language. The autocomplete prediction works correctly, but the result is questionable. Gebru was and is right: if you use online texts, you replicate today’s injustice and unethical tendencies. The underlying demand of such criticism is: language models should not reflect our language use today, but a better, purified version of ourselves and our language – free from prejudices and discrimination.

This criticism, at the same time, questions the engineering method. Should Google use online texts from the web, which do not incur costs and reflect our language, or should you start filtering and preparing training texts to exclude potentially problematic texts? Suddenly, the criticism impacts the speed of the AI model development and the business case due to the potentially high effort needed to filter and rework texts used as training data.

Her second criticism addresses the massive energy consumption of the language model. This criticism has nothing to do with the ethical properties of AI models, such as bias or fairness. It questions whether Google should develop an AI model that the management sees as essential for Google’s future business. Weighting business strategy and its impacts on ecology is a valid ethical question, it is just more on the business side than at the core of AI ethics. Figure [4-3](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_4\_Chapter.xhtml#Fig3) illustrates this shift in Gebru’s research.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_4_Chapter/516372_1_En_4_Fig3_HTML.png" alt="../images/516372_1_En_4_Chapter/516372_1_En_4_Fig3_HTML.png" height="804" width="1725"><figcaption><p>Figure 4-3</p></figcaption></figure>

Companies and their AI organizations can learn from this controversy. Which manager or board decides about which (of the three) ethical aspects in the AI sphere? What are the competencies of corporate social responsibility or business ethics teams? What does the top management decide?

AI ethics is essential from a moral perspective, for the corporate culture, and for the organization’s reputation – not to speak of the already existing or expected to come regulatory demands. Thus, a frictionless integration in the company and its corporate culture is necessary right from the beginning.

### AI and Regulations

Adhering to ethical standards is highly recommended, though optional. Adhering to regulations is a must. And when it comes to regulations, policymakers around the globe passed laws over the last years and are preparing additional ones that impact AI organizations directly. These regulations come in two forms:

* **Data protection laws** defining which data companies can store, how they can use the data, and which additional conditions they have to meet (e.g., transparency, right to be forgotten).
* **AI algorithm regulations**, such as the EU’s AI act proposal, impacting how algorithms can work with data and which use cases are acceptable.

The EU reacted to the explosion of data and the AI-driven revolution in data exploitation by introducing new laws. The following section looks at the two EU’s landmark regulations: the General Data Protection Regulation (GDPR) and the new EU proposal for an AI act. Many other countries follow this model, at least for GDPR.

In contrast, many relevant regulations in the US are a reinterpretation of existing laws and rules in the light of AI. Thus, as a third example, we look at the Federal Trade Commission’s approach of applying existing principles for the AI context.

A remark before looking at the regulations: Readers should keep in mind that regulations do not only reflect the academic discourse on AI ethics. Regulations also want to foster innovation and ensure that a country’s economy can compete globally.

#### Data Privacy Laws: The GDPR Example

Today’s worldwide gold standard for data privacy is the European Union’s General Data Protection Regulation (GDPR) from May 2018. The GDPR is so well known (and feared) around the globe for combining, first, fines of up to 20 million euros or 4% of a company’s global turnover, with, second, its global applicability if – simply speaking – companies process EU citizens’ data. It does not matter where the company has its domicile or where they store and process such data.

The GDPR defines seven principles for data processing – obligations and restrictions companies and their AI organizations must follow. They are the easiest (and least expensive) to implement when adhered to in the design phase for an AI organization or concrete software solutions. “**Privacy by design**” or “privacy by default” are terms reflecting the ambition to consider privacy right from the beginning instead of trying to get it somehow somewhere into the code late.

The first of the seven principles demands **lawfulness, fairness, andtransparency**. Any processing requires a legal base such as customer consent, legal obligations, or a necessity. Fairness demands not to misguide users, for example, with whom they are in contact or what they approve and accept.

The second principle is **purpose limitation**. When companies collect data, they have to be clear and transparent about the purpose they want to process the data for later. The stated purpose restricts and limits future data processing and usage for training AI models (if there is no other lawful base). Third, **data minimization** requires collecting and storing only data needed for the defined purpose.**Accuracy** demands personal data to be correct and not misleading – and to correct it otherwise. **Storage limitation** is about the duration of the data storage. Companies must not store personal data longer than needed.

The **security principle** (also termed integrity and confidentiality) requires that you secure your data. IT security organizations handle this in many companies. Furthermore, a later chapter looks at how to secure training data and environments and trained AI models.

Finally, there is the **accountability** principle , which requires you to take responsibility for handling personal data.

AI organizations must follow these principles, even if they generate extra work. In particular, AI organizations must understand where data comes from and for which purpose data scientists can use it (data lineage). Moreover, just stating to adhere to these principles is a time bomb, even if AI organizations really do. They need documentation as **evidence for audits**.

GDPR states further obligations. For example, a company has to confirm to natural persons on request whether they process any data relating to them and to provide a copy of all this data. Natural persons also have the right to get additional information such as how long a company stores their data, where the data comes from, plus the recipients of the data, or the purpose of processing (**data subject access request**). Companies need to have a comprehensive **catalog** of all applications, processing, and external data transfers. You cannot provide a requestor all his personal data if you do not know where you store which data. There is also the need for **Data Protection Impact Assessments** for high-risk data processing, for example, when data processing involves people tracking and surveillance or sensitive personal data. Thus AI organizations have to make sure that they follow company guidelines laid out by the IT department, the data protection officer, or legal & compliance teams to be ready for the next audit – or the next request from a data subject.

#### The EU’s “AI Act” Proposal

On April 21st in 2021, the European Commission released a **proposal** for an Artificial Intelligence Act to foster the adoption of AI while addressing the risks associated with the new technology. The proposed legislation regulates AI algorithms broadly, from statistical approaches, over logic or knowledge-based systems, including expert systems, to supervised or unsupervised learning, deep learning, and other methodologies. The proposed AI act applies to AI systems within the **EU’s jurisdiction and** AI systems **outside** if their output is used within the EU – and to companies, public agencies, and governmental offices, but excluding systems for military use. Violations of the AI act can result in **fines** of up to 6% of a company’s global revenues, compared to “only” 4% in the case of GDPR violations.

The proposed regulation categorizes AI systems into four categories: systems with unacceptable risks, with high risks, with limited risks, and systems with minimal risks only (Figure [4-4](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_4\_Chapter.xhtml#Fig4)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_4_Chapter/516372_1_En_4_Fig4_HTML.png" alt="../images/516372_1_En_4_Chapter/516372_1_En_4_Fig4_HTML.png" height="1004" width="1504"><figcaption><p>Figure 4-4</p></figcaption></figure>

The planned AI regulation lists forbidden AI use cases considered as **unacceptable risks**. These are, first, systems using subliminal techniques or exploiting specific vulnerabilities of individuals resulting in harming the person herself or others physically or psychologically. Second, the proposed act forbids AI systems for social scoring by or on behalf of public authorities using data collected in different contexts or if the scores have a disproportional impact on individuals. Third, real-time remote biometric information systems in publicly accessible spaces are forbidden unless in the context of very specific crimes.

**High-risk AI systems** form the second category. Safety and health-related AI systems and components fall into this category, for example, systems for cars and trains or for medical devices. Also, AI systems for the following eight usages:

* Real-time or deferred biometric-based identification of persons
* Managing and operating critical infrastructures such as water supply
* Education and vocational training use cases deciding about the admission and assignment to educational institutions
* Recruitment processes, promotion or termination decisions, task allocation, and employee performance evaluation
* Decision-making about public assistance benefits, for dispatching and prioritizing emergency first response services, and for credit scoring
* Certain law enforcement use cases such as predicting whether persons will do an offense (again) or detecting the emotional state of a person
* Migration, asylum, and border control management, including polygraph use cases, assessing whether persons pose security risks or verifying the authenticity of documents
* Administration of justice and democratic processes with use cases such as researching facts and the law plus applying the law to case-specific facts

The proposed EU regulation does not forbid high-risk systems. Instead, the regulation lays out **specific requirements** for them. Companies have to identify and evaluate known and foreseeable risks in accordance with the intended usage purpose, thereby taking reasonably foreseeable misuses into account. A one-time assessment when developing or setting up such systems is not sufficient. Companies and their AI organizations have to continuously manage these risks over the system’s lifecycle, considering insights gained during the production usage of the models.

There are additional detailed requirements for high-risk AI systems such as technical documentation, logging critical events, communicating the system’s accuracy or robustness, and allowing for and ensuring human oversight.

The third category contains systems with **limited risks**. They have to follow specific transparency requirements. AI systems interacting with humans must unveil their non-human nature. When systems incorporate emotion recognition or biometric categorization systems, they have to inform the users. Finally, any system manipulating images, audio, and video content (“deep fakes”) must make this transparent.

The **minimal risk AI systems** form the final fourth category. According to the EU, the majority of today’s use cases fall into this category. The act does not regulate them but proposes establishing codes of conduct for them.

The proposed AI act results from a more than three-year process. Still, additional changes are likely until the act comes into force. Furthermore, the EU works on additional regulations impacting AI organizations, for example, related to liability questions. Most probably, other countries will put similar laws in place in the years to come, with this proposed EU AI act potentially having a significant impact and acting as a spiritus rector. Thus, AI organizations from outside the EU benefit from understanding these EU concepts, too.

#### The Federal Trade Commission’s Approach in the US

The Federal Trade Commission (FTC) bases its regulation of AI systems on existing laws. The US agency’s responsibility covers enforcing antitrust laws and fostering customer protection by preventing unfair, deceptive, or anti-competitive practices. The agency understands AI as a new technical approach for automated decision-making. So, the FTC does not focus on algorithm types to decide whether it is a “relevant” AI system; the FTC focuses on whether a system makes automated decisions. It regulates AI systems based on existing, older laws: the Federal Trade Commission Act (Section 5), the Fair Credit Reporting Act (FCRA), and the Equal Credit Opportunity Act (ECOA) . Based on them, the FTC developed rules and guidelines for AI systems. The most relevant ones are:

* Transparency about the use of AI. For example, imitating human behavior without telling customers can be illegal.
* Transparency about the data collection, that is, no secret or behind-the-customer’s-back collection of data without consumers being aware.
* Transparency whether automated rules change the deal, for example, by automatically lowering credit lines based on spending patterns.
* Right of customers to get access to their data stored and correct the wrong information. Depending on the exact circumstances, companies are obliged to have up-to-date, accurate data about their customers.
* Ability to provide customers the most relevant influencing factors resulting in the company denying a particular customer a specific request. For scores, a company has to be able to explain the underlying concept and to name the four key factors that impact an individual’s specific score most.
* No discrimination on what represents protected classes in the US: race, color, religion, national origin, sex, marital status, age, or whether someone gets public assistance. Ensuring non-discriminating models requires adequate input data plus verifying the resulting AI model. The discussion on AI ethics provides helpful background information.

Not all of the rules apply to each kind of customer or contract. However, they exemplify how existing laws regulate new AI organizations – and can be quite a challenge. A data scientist might have the perfect AI model for credit scoring beating everything the banks have in place yet. However, if he cannot list the four main factors influencing an individual’s credit score, the bank must not use the model. Thus, when looking at these rules, it is not surprising that the ability to understand and explain how AI models work recently got much attention. The terms used for this trend are “explainability” and “explainable AI.”

### Explainable AI

The need to build explainable AI model for complex prediction or classification tasks is a real challenge. Data scientists can build (deep) neural networks. Neural networks are black boxes solving difficult prediction and classification challenges – but they are black boxes. Trained models consist of hundreds or thousands of formulas and parameters. Their behavior is, for humans, impossible to comprehend – a not-so-desirable option. Alternatively, data scientists can rely on easy-to-understand models such as logistic and linear regressions or random forest. However, they do not perform well for complex AI tasks – another non-desirable option. The recent and ongoing work on the topic of Explainable AI (XAI) wants to overcome this catch-22 challenge.

#### Scenarios for XAI

XAI wants to help with three **scenarios** :

* The model – and not the application of the model on new data – is the deliverable.
* Justifications and documentations of model behavior for ethical and regulatory needs.
* Debugging and optimization.

All these scenarios require to understand how a trained AI model works.

It is well-known in object detection that neural networks do not always detect the actual object in the picture but react to its context. Even competition-winning neural networks have or had such flaws. In one case, the neural network did not recognize horses on images as everybody assumed. Instead, watermarks triggered this classification since only horse images in the training set had them. Suppose data scientists know – for example, in object recognition – which regions of a concrete image decide which type of object the neural network detects. Then, they can **debug and optimize** neural networks more efficiently. They detect more flaws similar to the watermark issue before models get into production and hit the real world.

The second XAI usage scenario reflects regulatory and ethical needs elaborated earlier in this chapter: checking for fairness and ensuring non-discriminating AI models. Customers and society do not accept the outcome of AI models as God’s word: always correct and not to be questioned. Companies and organizations have to **justify** decisions, even if made by AI. As discussed above, the US Equal Credit Opportunity Act demands that banks communicate reasons when rejecting a credit. Otherwise, the bank must not use the AI model.

Third, the **model might be the core deliverable,** not its application on new data. A product manager wants to understand the buyers’ and non-buyers’ characteristics. The model has to tell him. It is not about applying the model to some customer data; it is about understanding what characterizes customers with high buying affinity based on the model.

The fourth, often-mentioned reason for the need for XAI **user trust**. The rationale: If humans understand how AI components work, they more likely trust the AI system’s output. An XAI system for sentiment analysis could, for example, highlight the positive and the negative words and phrases to explain a text’s sentiment rating (Figure [4-5](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_4\_Chapter.xhtml#Fig5)). While it is intuitively clear that such additional information should foster user trust, studies proved the opposite. In one study, user confidence dropped when the AI component provided insights about its reasoning process, especially and shockingly when the AI component was sure to be correct and was actually right.

The conclusion is not to abandon the idea of boosting user trust with XAI. The conclusion is that projects have to validate this assumption empirically with real-life users. Humans often behave less rationally than we expect and wish.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_4_Chapter/516372_1_En_4_Fig5_HTML.png" alt="../images/516372_1_En_4_Chapter/516372_1_En_4_Fig5_HTML.png" height="334" width="1509"><figcaption><p>Figure 4-5</p></figcaption></figure>

XAI is not a challenge for simple models such as linear and logistic regression. Everyone can comprehend the following formula and understand the impact of the parameters when looking at the estimation formula for monthly apartment rents in a specific city:![\$$ AppRent=0.5\ast sqm+500\ast NoOfBedrooms+130\ast NoOfBathrooms+500 \$$](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372\_1\_En\_4\_Chapter/516372\_1\_En\_4\_Chapter\_TeX\_Equa.png)

In contrast, neural networks predict and infer using hundreds or thousands of equations and weights. They deliver better results. However, any approach to presenting and understanding them in their complete baroque grandeur and complexity must fail. Thus, explainable AI chooses one lens to highlight one aspect of the neural network’s behavior. Well-known are **two** of such **lenses**: global explainability and local explainability.

#### Local Explainability

Local explainability aims to understand the parameters which influence a single prediction. Why is this house so pricey? Because of the number of bathrooms or the garden size, or the pool? Local explainability does not aim to understand what – in general – impacts home prices in the market. It seeks to understand what influences the price of one single specific home most. One approach for local explainability is creating and analyzing a simple(r) and explainable model. This model describes the neural network behavior close (!) to the point of interest well but ignores the rest.

In more detail, the starting point is a neural network that approximates reality. Data scientists build the model, for example, using a data set with last year’s home prices (Figure [4-6](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_4\_Chapter.xhtml#Fig6)). The neural network might predict house prices well but is not human-comprehensible. When looking at the neural network and its weights, we do not understand what impacts the model most or whether small size variations matter for homes with 80 or 200 sqm (A and B in Figure [4-6](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_4\_Chapter.xhtml#Fig6)). Thus, to achieve local explainability, data scientists perform three steps for each prediction value they aim to understand:

1.  1\.

    **Probing**. Data scientists need training data for creating an XAI model. Thus, they probe the neural network model around the points of interest. In Figure [4-6](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_4\_Chapter.xhtml#Fig6), they would look for the house prices the model predicts, for example, for 79 sqm, 80 sqm, 80.5 sqm, and 82sqm (case A), and for 185 sqm, 203 sqm, and 210 sqm (case B), respectively.

    &#x20;
2.  2\.

    **Constructing an explainable model**. Data scientists use the data probes from step 1 to train a new model. This new model estimates the neural network behavior around A and B. The core idea is to use now a simple estimation function such as linear regression. The prediction quality is lower, but a linear function is easy to understand. Thus, in Figure [4-6](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_4\_Chapter.xhtml#Fig6), data scientists create two separate linear functions for the two points of interest, A and B.

    &#x20;
3.  3\.

    **Understanding and explaining**. The linear approximation functions from step 2 allow understanding the local situations around A and B. Changes in the home size impact the price heavily for situation A, but not much for B. For models with more input features than just the living space size – garden size, number of bathrooms and carports, or whether the house has a balcony or an outdoor pool – the explanation gets, obviously, richer.

    &#x20;

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_4_Chapter/516372_1_En_4_Fig6_HTML.png" alt="../images/516372_1_En_4_Chapter/516372_1_En_4_Fig6_HTML.png" height="1029" width="1504"><figcaption><p>Figure 4-6</p></figcaption></figure>

A remark for readers with a strong math background: It is essential not to calculate just the gradient but to probe around the points of interest. This approach reduces the impact of local disturbances and noise precisely at the point of interest that might not exist elsewhere in the nearby region (Figure [4-7](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_4\_Chapter.xhtml#Fig7)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_4_Chapter/516372_1_En_4_Fig7_HTML.png" alt="../images/516372_1_En_4_Chapter/516372_1_En_4_Fig7_HTML.png" height="629" width="1000"><figcaption><p>Figure 4-7</p></figcaption></figure>

To conclude: local explainability identifies – for concrete predictions – which parameters to change for changing the prediction outcome most. It helps house owners to figure out for their individual houses whether adding a swimming pool or a car park increases the home values more.

#### Global Explainability

In contrast to local explainability, **global explainability** does not look at single predictions. It aims for the impossible: explaining an inherently too-complex-to-understand neural network and its overall behavior to humans. One well-known algorithm is permutation importance. Permutation importance determines the impact of the various input features on a given model’s predictions.

The algorithm iterates over all input columns. Each iteration permutes the values of one particular column in the table (Figure [4-8](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_4\_Chapter.xhtml#Fig8), ➌). The more the accuracy drops, the more relevant is the permutated single feature. By comparing, for example, the accuracy after shuffling the sex and the zip columns, a data scientist understands that the sex has a higher impact on customers’ shopping habits than the ZIP code (Figure [4-8](https://learning.oreilly.com/library/view/managing-ai-in/9781484278246/html/516372\_1\_En\_4\_Chapter.xhtml#Fig8), ➍).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484278246/files/images/516372_1_En_4_Chapter/516372_1_En_4_Fig8_HTML.png" alt="../images/516372_1_En_4_Chapter/516372_1_En_4_Fig8_HTML.png" height="959" width="1725"><figcaption><p>Figure 4-8</p></figcaption></figure>

Explainable AI is a highly dynamic field with many new ideas coming up every year. However, due to regulatory pressure and societies expecting companies to behave ethically, commercial products today implement XAI functionality already as all the big cloud providers Google, Microsoft Azure, and Amazon’s AWS proof. It is fascinating to see niche research becoming mainstream and relevant for many companies so quickly.

### Summary

Three questions drive ethical discourse about AI usage in an enterprise context:

* Is the envisioned use case ethically acceptable? Should and do we want to do create and use this AI model?
* Do we engineer the model following ethical standards? Or do we, for example, use data in a way we should not just to get results faster?
* Is the AI model ethical? In particular, is the model bias-free and fair?

While bias-free AI models tend to align with business goals, other ethical aspects require balancing the ethical wishful with the commercial necessary – and having clear procedures in place about the responsibilities reduces the risk of bitter fights between teams and individuals.

The ethical discourse on AI, however, left the spheres of academia. Policymakers are actively working on regulating AI. AI regulation is not just about ethics. Economic aspects and fostering innovation and global competitiveness are relevant. However, ethical factors heavily influence AI regulation.

Data privacy laws are in place for years, though the EU’s GDPR revolutionized the regulation of how we store, manage, process, and use data. Brussels has an AI Act in its pipeline, which will most likely impact how we develop and apply AI models in the future. In the US, the FTC reinterprets existing laws for the new world of AI.

The FTC requires, for example, that banks name the four key factors that result in declining a credit. A simple task in the traditional world with rule-based decision-making becomes a challenge if using neural networks with thousands of nodes and weights. It is no surprise that “Explainable AI” is such a hot topic with academia and the big cloud providers. Global explainability, for example, identifies the most important input factors for the overall model behavior. On the other side, local explainability looks at the factors influencing an individual prediction most. It is an eye-opener to observe how FTC’s requirements, academic research, and big tech AI service innovations aim for the same: understanding AI model-based decision-making better.
