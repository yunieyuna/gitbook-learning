# 7. Dense Neural Networks

## Chapter 7. Dense Neural Networks

> \[I]f you’re trying to predict the movements of a stock on the stock market given its recent price history, you’re unlikely to succeed, because price history doesn’t contain much predictive information.
>
> François Chollet (2017)

This chapter is about important aspects of _dense neural networks_. Previous chapters have already made use of this type of neural network. In particular, the `MLPClassifier` and `MLPRegressor` models from `scikit-learn` and the `Sequential` model from `Keras` for classification and estimation are dense neural networks (DNNs). This chapter exclusively focuses on `Keras` since it gives more freedom and flexibility in modeling DNNs.[1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#idm46319975934968)

[“The Data”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#dnn\_data) introduces the foreign exchange (FX) data set that the other sections in this chapter use. [“Baseline Prediction”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#dnn\_baseline) generates a baseline, in-sample prediction on the new data set. Normalization of training and test data is introduced in [“Normalization”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#dnn\_normalization). As means to avoid overfitting, [“Dropout”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#dnn\_dropouts) and [“Regularization”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#dnn\_regularization) discuss dropout and regularization as popular methods. Bagging, as another method to avoid overfitting and already used in [Chapter 6](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#ai\_first\_finance), is revisited in [“Bagging”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#dnn\_bagging). Finally, [“Optimizers”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#dnn\_optimizers) compares the performance of different optimizers that can be used with `Keras` DNN models.

Although the introductory quote for the chapter might give little reason for hope, the main goal for this chapter—as well as for [Part III](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/part03.html#part\_statistical\_inefficiencies) as a whole—is to discover statistical inefficiencies in financial markets (time series) by applying neural networks. The numerical results presented in this chapter, such as prediction accuracies of 60% and more in certain cases, indicate that at least some hope is justified.

## The Data

[Chapter 6](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#ai\_first\_finance) discovers hints for statistical inefficiencies for, among other time series, the intraday price series of the EUR/USD currency pair. This chapter and the following ones focus on foreign exchange (FX) as an asset class and specifically on the EUR/USD currency pair. Among other reasons, economically exploiting statistical inefficiencies for FX is in general not as involved as for other asset classes, such as for volatility products like the VIX volatility index. Free and comprehensive data availability is also often given for FX. The following data set is from the Refinitiv Eikon Data API. The data set has been retrieved via the API. The data set contains open, high, low, and close values. [Figure 7-1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#figure\_dnn\_01) visualizes the closing prices:

```
In [1]: import os
        import numpy as np
        import pandas as pd
        from pylab import plt, mpl
        plt.style.use('seaborn')
        mpl.rcParams['savefig.dpi'] = 300
        mpl.rcParams['font.family'] = 'serif'
        pd.set_option('precision', 4)
        np.set_printoptions(suppress=True, precision=4)
        os.environ['PYTHONHASHSEED'] = '0'

In [2]: url = 'http://hilpisch.com/aiif_eikon_id_eur_usd.csv'  

In [3]: symbol = 'EUR_USD'

In [4]: raw = pd.read_csv(url, index_col=0, parse_dates=True)  

In [5]: raw.head()
Out[5]:                        HIGH     LOW    OPEN   CLOSE
        Date
        2019-10-01 00:00:00  1.0899  1.0897  1.0897  1.0899
        2019-10-01 00:01:00  1.0899  1.0896  1.0899  1.0898
        2019-10-01 00:02:00  1.0898  1.0896  1.0898  1.0896
        2019-10-01 00:03:00  1.0898  1.0896  1.0897  1.0898
        2019-10-01 00:04:00  1.0898  1.0896  1.0897  1.0898

In [6]: raw.info()
        <class 'pandas.core.frame.DataFrame'>
        DatetimeIndex: 96526 entries, 2019-10-01 00:00:00 to 2019-12-31 23:06:00
        Data columns (total 4 columns):
         #   Column  Non-Null Count  Dtype
        ---  ------  --------------  -----
         0   HIGH    96526 non-null  float64
         1   LOW     96526 non-null  float64
         2   OPEN    96526 non-null  float64
         3   CLOSE   96526 non-null  float64
        dtypes: float64(4)
        memory usage: 3.7 MB

In [7]: data = pd.DataFrame(raw['CLOSE'].loc[:])  
        data.columns = [symbol]  

In [8]: data = data.resample('1h', label='right').last().ffill()  

In [9]: data.info()
        <class 'pandas.core.frame.DataFrame'>
        DatetimeIndex: 2208 entries, 2019-10-01 01:00:00 to 2020-01-01 00:00:00
        Freq: H
        Data columns (total 1 columns):
         #   Column   Non-Null Count  Dtype
        ---  ------   --------------  -----
         0   EUR_USD  2208 non-null   float64
        dtypes: float64(1)
        memory usage: 34.5 KB

In [10]: data.plot(figsize=(10, 6));  
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO1-1)

Reads the data into a `DataFrame` object

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO1-3)

Selects, resamples, and plots the closing prices

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0701.png" alt="aiif 0701" height="1528" width="2519"><figcaption></figcaption></figure>

**Figure 7-1. Mid-closing prices for EUR/USD (intraday)**

## Baseline Prediction

Based on the new data set, the prediction approach from [Chapter 6](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#ai\_first\_finance) is repeated. First is the creation of the lagged features:

```
In [11]: lags = 5

In [12]: def add_lags(data, symbol, lags, window=20):  
             cols = []
             df = data.copy()
             df.dropna(inplace=True)
             df['r'] = np.log(df / df.shift())
             df['sma'] = df[symbol].rolling(window).mean()
             df['min'] = df[symbol].rolling(window).min()
             df['max'] = df[symbol].rolling(window).max()
             df['mom'] = df['r'].rolling(window).mean()
             df['vol'] = df['r'].rolling(window).std()
             df.dropna(inplace=True)
             df['d'] = np.where(df['r'] > 0, 1, 0)
             features = [symbol, 'r', 'd', 'sma', 'min', 'max', 'mom', 'vol']
             for f in features:
                 for lag in range(1, lags + 1):
                     col = f'{f}_lag_{lag}'
                     df[col] = df[f].shift(lag)
                     cols.append(col)
             df.dropna(inplace=True)
             return df, cols

In [13]: data, cols = add_lags(data, symbol, lags)
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO2-1)

Slightly adjusted function from [Chapter 6](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#ai\_first\_finance)

Second, a look at the labels data. A major problem in classification that can arise depending on the data set available is _class imbalance_. This means, in the context of binary labels, that the frequency of one particular class compared to the other class might be higher. This might lead to situations in which the neural network simply predicts the class with the higher frequency since this already can lead to low loss and high accuracy values. Applying appropriate weights, one can make sure that both classes gain equal importance during the DNN training step:[2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#idm46319972990776)

```
In [14]: len(data)
Out[14]: 2183

In [15]: c = data['d'].value_counts()  
         c  
Out[15]: 0    1445
         1     738
         Name: d, dtype: int64

In [16]: def cw(df):  
             c0, c1 = np.bincount(df['d'])
             w0 = (1 / c0) * (len(df)) / 2
             w1 = (1 / c1) * (len(df)) / 2
             return {0: w0, 1: w1}

In [17]: class_weight = cw(data)  

In [18]: class_weight  
Out[18]: {0: 0.755363321799308, 1: 1.4789972899728998}

In [19]: class_weight[0] * c[0]  
Out[19]: 1091.5

In [20]: class_weight[1] * c[1]  
Out[20]: 1091.5
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO3-1)

Shows the frequency of the two classes

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO3-3)

Calculates appropriate weights to reach an equal weighting

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO3-6)

With the calculated weights, both classes gain equal weight

Third is the creation of the DNN model with `Keras` and the training of the model on the complete data set. The baseline performance in-sample is around 60%:

```
In [21]: import random
         import tensorflow as tf
         from keras.layers import Dense
         from keras.models import Sequential
         from keras.optimizers import Adam
         from sklearn.metrics import accuracy_score
         Using TensorFlow backend.

In [22]: def set_seeds(seed=100):
             random.seed(seed)  
             np.random.seed(seed)  
             tf.random.set_seed(seed)  

In [23]: optimizer = Adam(lr=0.001)  

In [24]: def create_model(hl=1, hu=128, optimizer=optimizer):
             model = Sequential()
             model.add(Dense(hu, input_dim=len(cols),
                             activation='relu'))  
             for _ in range(hl):
                 model.add(Dense(hu, activation='relu'))  
             model.add(Dense(1, activation='sigmoid'))  
             model.compile(loss='binary_crossentropy',  
                           optimizer=optimizer,  
                           metrics=['accuracy'])  
             return model

In [25]: set_seeds()
         model = create_model(hl=1, hu=128)

In [26]: %%time
         model.fit(data[cols], data['d'], epochs=50,
                   verbose=False, class_weight=cw(data))
         CPU times: user 6.44 s, sys: 939 ms, total: 7.38 s
         Wall time: 4.07 s

Out[26]: <keras.callbacks.callbacks.History at 0x7fbfc2ee6690>

In [27]: model.evaluate(data[cols], data['d'])
         2183/2183 [==============================] - 0s 24us/step

Out[27]: [0.582192026280068, 0.6087952256202698]

In [28]: data['p'] = np.where(model.predict(data[cols]) > 0.5, 1, 0)

In [29]: data['p'].value_counts()
Out[29]: 1    1340
         0     843
         Name: p, dtype: int64
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO4-1)

Python random number seed

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO4-2)

`NumPy` random number seed

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO4-3)

`TensorFlow` random number seed

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO4-4)

Default optimizer (see [_https://oreil.ly/atpu8_](https://oreil.ly/atpu8))

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO4-5)

First layer

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO4-6)

Additional layers

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/7.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO4-7)

Output layer

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/8.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO4-8)

Loss function (see [_https://oreil.ly/cVGVf_](https://oreil.ly/cVGVf))

[![9](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/9.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO4-9)

Optimizer to be used

[![10](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/10.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO4-10)

Additional metrics to be collected

The same holds true for the performance of the model out-of-sample. It is still well above 60%. This can be considered already quite good:

```
In [30]: split = int(len(data) * 0.8)  

In [31]: train = data.iloc[:split].copy()  

In [32]: test = data.iloc[split:].copy()  

In [33]: set_seeds()
         model = create_model(hl=1, hu=128)

In [34]: %%time
         h = model.fit(train[cols], train['d'],
                   epochs=50, verbose=False,
                   validation_split=0.2, shuffle=False,
                   class_weight=cw(train))
         CPU times: user 4.72 s, sys: 686 ms, total: 5.41 s
         Wall time: 3.14 s

Out[34]: <keras.callbacks.callbacks.History at 0x7fbfc3231250>

In [35]: model.evaluate(train[cols], train['d'])  
         1746/1746 [==============================] - 0s 13us/step

Out[35]: [0.612861613500842, 0.5853379368782043]

In [36]: model.evaluate(test[cols], test['d'])  
         437/437 [==============================] - 0s 16us/step

Out[36]: [0.5946959675858714, 0.6247139573097229]

In [37]: test['p'] = np.where(model.predict(test[cols]) > 0.5, 1, 0)

In [38]: test['p'].value_counts()
Out[38]: 1    291
         0    146
         Name: p, dtype: int64
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO5-1)

Splits the whole data set…

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO5-2)

…into the training data set…

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO5-3)

…and the test data set.

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO5-4)

Evaluates the _in-sample_ performance.

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO5-5)

Evaluates the _out-of-sample_ performance.

[Figure 7-2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#figure\_dnn\_02) shows how the accuracy on the training and validation data sub-sets changes over the training epochs:

```
In [39]: res = pd.DataFrame(h.history)

In [40]: res[['accuracy', 'val_accuracy']].plot(figsize=(10, 6), style='--');
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0702.png" alt="aiif 0702" height="1421" width="2471"><figcaption></figcaption></figure>

**Figure 7-2. Training and validation accuracy values**

The analysis in this section sets the stage for the more elaborate use of DNNs with `Keras`. It presents a baseline market prediction approach. The following sections add different elements that are primarily supposed to improve the out-of-sample model performance and to avoid overfitting of the model to the training data.

## Normalization

The baseline prediction in [“Baseline Prediction”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#dnn\_baseline) takes the lagged features as they are. In [Chapter 6](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#ai\_first\_finance), the features data is normalized by subtracting the mean of the training data for every feature and dividing it by the standard deviation of the training data. This normalization technique is called _Gaussian normalization_ and proves often, if not always, to be an important aspect when training a neural network. As the following Python code and its results illustrate, the in-sample performance increases significantly when working with normalized features data. The out-of-sample performance also slightly increases. However, there is no guarantee that the out-of-sample performance increases through features normalization:

```
In [41]: mu, std = train.mean(), train.std()  

In [42]: train_ = (train - mu) / std  

In [43]: set_seeds()
         model = create_model(hl=2, hu=128)

In [44]: %%time
         h = model.fit(train_[cols], train['d'],
                   epochs=50, verbose=False,
                   validation_split=0.2, shuffle=False,
                   class_weight=cw(train))
         CPU times: user 5.81 s, sys: 879 ms, total: 6.69 s
         Wall time: 3.53 s

Out[44]: <keras.callbacks.callbacks.History at 0x7fbfa51353d0>

In [45]: model.evaluate(train_[cols], train['d'])  
         1746/1746 [==============================] - 0s 14us/step

Out[45]: [0.4253406366728084, 0.887170672416687]

In [46]: test_ = (test - mu) / std  

In [47]: model.evaluate(test_[cols], test['d'])  
         437/437 [==============================] - 0s 24us/step

Out[47]: [1.1377735263422917, 0.681922197341919]

In [48]: test['p'] = np.where(model.predict(test_[cols]) > 0.5, 1, 0)

In [49]: test['p'].value_counts()
Out[49]: 0    281
         1    156
         Name: p, dtype: int64
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO6-1)

Calculates the mean and standard deviation for all _training features_

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO6-2)

Normalizes the _training data_ set based on Gaussian normalization

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO6-3)

Evaluates the _in-sample_ performance

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO6-4)

Normalizes the _test data_ set based on Gaussian normalization

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO6-5)

Evaluates the _out-of-sample_ performance

A major problem that often arises is _overfitting_. It is impressively visualized in [Figure 7-3](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#figure\_dnn\_03), which shows a steadily improving training accuracy while the validation accuracy decreases slowly:

```
In [50]: res = pd.DataFrame(h.history)

In [51]: res[['accuracy', 'val_accuracy']].plot(figsize=(10, 6), style='--');
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0703.png" alt="aiif 0703" height="1421" width="2418"><figcaption></figcaption></figure>

**Figure 7-3. Training and validation accuracy values (normalized features data)**

Three candidate methods to avoid overfitting are _dropout_, _regularization_, and _bagging_. The following sections discuss these methods. The impact of the chosen optimizer is also discussed later in this chapter.

## Dropout

The idea of _dropout_ is that neural networks should not use all hidden units during the training stage. The analogy to the human brain is that a human being regularly forgets information that was previously learned. This, so to say, keeps the human brain “open minded.” Ideally, a neural network should behave similarly: the connections in the DNN should not become too strong in order to avoid overfitting to the training data.

Technically, a `Keras` model has additional layers between the hidden layers that manage the dropout. The major parameter is the rate with which the hidden units of a layer get dropped. These drops in general happen in randomized fashion. This can be avoided by fixing the `seed` parameter. While the in-sample performance decreases, the out-of-sample performance slightly decreases as well. However, the difference between the two performance measures is smaller, which is in general a desirable situation:

```
In [52]: from keras.layers import Dropout

In [53]: def create_model(hl=1, hu=128, dropout=True, rate=0.3,
                          optimizer=optimizer):
             model = Sequential()
             model.add(Dense(hu, input_dim=len(cols),
                             activation='relu'))
             if dropout:
                 model.add(Dropout(rate, seed=100))  
             for _ in range(hl):
                 model.add(Dense(hu, activation='relu'))
                 if dropout:
                     model.add(Dropout(rate, seed=100))  
             model.add(Dense(1, activation='sigmoid'))
             model.compile(loss='binary_crossentropy', optimizer=optimizer,
                          metrics=['accuracy'])
             return model

In [54]: set_seeds()
         model = create_model(hl=1, hu=128, rate=0.3)

In [55]: %%time
         h = model.fit(train_[cols], train['d'],
                   epochs=50, verbose=False,
                   validation_split=0.15, shuffle=False,
                   class_weight=cw(train))
         CPU times: user 5.46 s, sys: 758 ms, total: 6.21 s
         Wall time: 3.53 s

Out[55]: <keras.callbacks.callbacks.History at 0x7fbfa6386550>

In [56]: model.evaluate(train_[cols], train['d'])
         1746/1746 [==============================] - 0s 20us/step

Out[56]: [0.4423361133190911, 0.7840778827667236]

In [57]: model.evaluate(test_[cols], test['d'])
         437/437 [==============================] - 0s 34us/step

Out[57]: [0.5875822428434883, 0.6430205702781677]
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO7-1)

Adds dropout after each layer

As [Figure 7-4](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#figure\_dnn\_04) illustrates, the training accuracy and validation accuracy now do not drift apart as fast as before:

```
In [58]: res = pd.DataFrame(h.history)

In [59]: res[['accuracy', 'val_accuracy']].plot(figsize=(10, 6), style='--');
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0704.png" alt="aiif 0704" height="1421" width="2444"><figcaption></figcaption></figure>

**Figure 7-4. Training and validation accuracy values (with dropout)**

## INTENTIONAL FORGETTING

Dropout in the `Sequential` model of `Keras` emulates what all human beings experience: forgetting previously memorized information. This is accomplished by deactivating certain hidden units of a hidden layer during training. In effect, this often avoids, to a larger extent, overfitting a neural network to the training data.

## Regularization

Another means to avoid overfitting is _regularization_. With regularization, large weights in the neural network get penalized in the calculation of the loss (function). This avoids the situation where certain connections in the DNN become too strong and dominant. Regularization can be introduced in a `Keras` DNN through a parameter in the `Dense` layers. Depending on the regularization parameter chosen, training and test accuracy can be kept quite close together. Two regularizers are in general used, one based on the linear norm, `l1`, and one based on the Euclidean norm, `l2`. The following Python code adds regularization to the model creation function:

```
In [60]: from keras.regularizers import l1, l2

In [61]: def create_model(hl=1, hu=128, dropout=False, rate=0.3,
                          regularize=False, reg=l1(0.0005),
                          optimizer=optimizer, input_dim=len(cols)):
             if not regularize:
                 reg = None
             model = Sequential()
             model.add(Dense(hu, input_dim=input_dim,
                             activity_regularizer=reg,  
                             activation='relu'))
             if dropout:
                 model.add(Dropout(rate, seed=100))
             for _ in range(hl):
                 model.add(Dense(hu, activation='relu',
                                 activity_regularizer=reg))  
                 if dropout:
                     model.add(Dropout(rate, seed=100))
             model.add(Dense(1, activation='sigmoid'))
             model.compile(loss='binary_crossentropy', optimizer=optimizer,
                          metrics=['accuracy'])
             return model

In [62]: set_seeds()
         model = create_model(hl=1, hu=128, regularize=True)

In [63]: %%time
         h = model.fit(train_[cols], train['d'],
                   epochs=50, verbose=False,
                   validation_split=0.2, shuffle=False,
                   class_weight=cw(train))
         CPU times: user 5.49 s, sys: 1.05 s, total: 6.54 s
         Wall time: 3.15 s

Out[63]: <keras.callbacks.callbacks.History at 0x7fbfa6b8e110>

In [64]: model.evaluate(train_[cols], train['d'])
         1746/1746 [==============================] - 0s 15us/step

Out[64]: [0.5307255412568205, 0.7691867351531982]

In [65]: model.evaluate(test_[cols], test['d'])
         437/437 [==============================] - 0s 22us/step

Out[65]: [0.8428352184644826, 0.6590389013290405]
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO8-1)

Regularization is added to each layer.

[Figure 7-5](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#figure\_dnn\_05) shows the training and validation accuracy under regularization. The two performance measures are much closer together than previously seen:

```
In [66]: res = pd.DataFrame(h.history)

In [67]: res[['accuracy', 'val_accuracy']].plot(figsize=(10, 6), style='--');
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0705.png" alt="aiif 0705" height="1421" width="2444"><figcaption></figcaption></figure>

**Figure 7-5. Training and validation accuracy values (with regularization)**

Of course, dropout and regularization can be used together. The idea is that the two measures combined even better avoid overfitting and bring the in-sample and out-of-sample accuracy values closer together. And indeed the difference between the two measures is lowest in this case:

```
In [68]: set_seeds()
         model = create_model(hl=2, hu=128,
                              dropout=True, rate=0.3,  
                              regularize=True, reg=l2(0.001),  
                             )

In [69]: %%time
         h = model.fit(train_[cols], train['d'],
                   epochs=50, verbose=False,
                   validation_split=0.2, shuffle=False,
                   class_weight=cw(train))
         CPU times: user 7.06 s, sys: 958 ms, total: 8.01 s
         Wall time: 4.28 s

Out[69]: <keras.callbacks.callbacks.History at 0x7fbfa701cb50>

In [70]: model.evaluate(train_[cols], train['d'])
         1746/1746 [==============================] - 0s 18us/step

Out[70]: [0.5007762827004764, 0.7691867351531982]

In [71]: model.evaluate(test_[cols], test['d'])
         437/437 [==============================] - 0s 23us/step

Out[71]: [0.6191965124699835, 0.6864988803863525]
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO9-1)

Dropout is added to the model creation.

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO9-2)

Regularization is added to the model creation.

[Figure 7-6](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#figure\_dnn\_06) shows the training and validation accuracy when combining dropout with regularization. The difference between training and validation data accuracy over the training epochs is some four percentage points only on average:

```
In [72]: res = pd.DataFrame(h.history)

In [73]: res[['accuracy', 'val_accuracy']].plot(figsize=(10, 6), style='--');
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0706.png" alt="aiif 0706" height="1421" width="2444"><figcaption></figcaption></figure>

**Figure 7-6. Training and validation accuracy values (with dropout and regularization)**

## PENALIZING LARGE WEIGHTS

Regularization avoids overfitting by penalizing large weights in a neural network. Single weights cannot get that large enough to dominate a neural network. The penalties keep weights on a comparable level.

## Bagging

The bagging method to avoid overfitting is already used in [Chapter 6](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#ai\_first\_finance), although only for the `scikit-learn` `MLPRegressor` model. There is also a wrapper for a `Keras` DNN classification model to expose it in `scikit-learn` fashion, namely the `KerasClassifier` class. The following Python code combines the `Keras` DNN modeling based on the wrapper with the `BaggingClassifier` from `scikit-learn`. The in-sample and out-of-sample performance measures are relatively high, around 70%. However, the result is driven by the class imbalance, as addressed previously, and as reflected here in the high frequency of the `0` predictions:

```
In [75]: from sklearn.ensemble import BaggingClassifier
         from keras.wrappers.scikit_learn import KerasClassifier

In [76]: max_features = 0.75

In [77]: set_seeds()
         base_estimator = KerasClassifier(build_fn=create_model,
                                 verbose=False, epochs=20, hl=1, hu=128,
                                 dropout=True, regularize=False,
                                 input_dim=int(len(cols) * max_features))  

In [78]: model_bag = BaggingClassifier(base_estimator=base_estimator,
                                   n_estimators=15,
                                   max_samples=0.75,
                                   max_features=max_features,
                                   bootstrap=True,
                                   bootstrap_features=True,
                                   n_jobs=1,
                                   random_state=100,
                                  )  

In [79]: %time model_bag.fit(train_[cols], train['d'])
         CPU times: user 40 s, sys: 5.23 s, total: 45.3 s
         Wall time: 26.3 s

Out[79]: BaggingClassifier(base_estimator=<keras.wrappers.scikit_learn.KerasClassifier
          object at 0x7fbfa7cc7b90>,
         bootstrap_features=True, max_features=0.75, max_samples=0.75,
                           n_estimators=15, n_jobs=1, random_state=100)

In [80]: model_bag.score(train_[cols], train['d'])
Out[80]: 0.720504009163803

In [81]: model_bag.score(test_[cols], test['d'])
Out[81]: 0.6704805491990846

In [82]: test['p'] = model_bag.predict(test_[cols])

In [83]: test['p'].value_counts()
Out[83]: 0    408
         1     29
         Name: p, dtype: int64
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO10-1)

The base estimator, here a `Keras` `Sequential` model, is instantiated.

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO10-2)

The `BaggingClassifier` model is instantiated for a number of equal base estimators.

## DISTRIBUTING LEARNING

Bagging, in a sense, distributes learning among a number of neural networks (or other models) in that each neural network, for example, only sees certain parts of the training data set and only a selection of the features. This avoids the risk that a single neural network overfits the complete training data set. The prediction is based on all selectively trained neural networks together.

## Optimizers

The `Keras` package offers a selection of optimizers that can be used in combination with the `Sequential` model (see [_https://oreil.ly/atpu8_](https://oreil.ly/atpu8)). Different optimizers might show different performances, with regard to both the time the training takes and the prediction accuracy. The following Python code uses different optimizers and benchmarks their performance. In all cases, the default parametrization of `Keras` is used. The out-of-sample performance does not vary that much. However, the in-sample performance, given the different optimizers, varies by a wide margin:

```
In [84]: import time

In [85]: optimizers = ['sgd', 'rmsprop', 'adagrad', 'adadelta',
                       'adam', 'adamax', 'nadam']

In [86]: %%time
         for optimizer in optimizers:
             set_seeds()
             model = create_model(hl=1, hu=128,
                              dropout=True, rate=0.3,
                              regularize=False, reg=l2(0.001),
                              optimizer=optimizer
                             )  
             t0 = time.time()
             model.fit(train_[cols], train['d'],
                       epochs=50, verbose=False,
                       validation_split=0.2, shuffle=False,
                       class_weight=cw(train))  
             t1 = time.time()
             t = t1 - t0
             acc_tr = model.evaluate(train_[cols], train['d'], verbose=False)[1]  
             acc_te = model.evaluate(test_[cols], test['d'], verbose=False)[1]  
             out = f'{optimizer:10s} | time[s]: {t:.4f} | in-sample={acc_tr:.4f}'
             out += f' | out-of-sample={acc_te:.4f}'
             print(out)
         sgd        | time[s]: 2.8092 | in-sample=0.6363 | out-of-sample=0.6568
         rmsprop    | time[s]: 2.9480 | in-sample=0.7600 | out-of-sample=0.6613
         adagrad    | time[s]: 2.8472 | in-sample=0.6747 | out-of-sample=0.6499
         adadelta   | time[s]: 3.2068 | in-sample=0.7279 | out-of-sample=0.6522
         adam       | time[s]: 3.2364 | in-sample=0.7365 | out-of-sample=0.6545
         adamax     | time[s]: 3.2465 | in-sample=0.6982 | out-of-sample=0.6476
         nadam      | time[s]: 4.1275 | in-sample=0.7944 | out-of-sample=0.6590
         CPU times: user 35.9 s, sys: 4.55 s, total: 40.4 s
         Wall time: 23.1 s
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO11-1)

Instantiates the DNN model for the given optimizer

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO11-2)

Fits the model with the given optimizer

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO11-3)

Evaluates the _in-sample_ performance

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#co\_dense\_neural\_networks\_CO11-4)

Evaluates the _out-of-sample_ performance

## Conclusions

This chapter dives deeper into the world of DNNs and uses `Keras` as the primary package. `Keras` offers a high degree of flexibility in composing DNNs. The results in this chapter are promising in that both in-sample and out-of-sample performance—with regard to the prediction accuracy—are consistently 60% and higher. However, prediction accuracy is just one side of the coin. An appropriate trading strategy must be available and implementable to economically profit from the predictions, or “signals.” This topic of paramount importance in the context of algorithmic trading is discussed in detail in [Part IV](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/part04.html#part\_economic\_inefficiencies). The next two chapters first illustrate the use of different neural networks (recurrent and convolutional neural networks) and learning techniques (reinforcement learning).

## References

`Keras` is a powerful and comprehensive package for deep learning with TensforFlow as its primary backend. The project is also evolving fast. Make sure to stay up to date via the [main project page](http://keras.io/). The major resources about `Keras` in book form are the following:

* Chollet, Francois. 2017. _Deep Learning with Python_. Shelter Island: Manning.
* Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. _Deep Learning_. Cambridge: MIT Press. [_http://deeplearningbook.org_](http://deeplearningbook.org/).

[1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#idm46319975934968-marker) See Chollet (2017) for more details and background information on the `Keras` package. See Goodfellow et al. (2016) for a comprehensive treatment of neural networks and related methods.

[2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#idm46319972990776-marker) See this [blog post](https://oreil.ly/3X1Qk), which discusses solutions to class imbalance with `Keras`.
