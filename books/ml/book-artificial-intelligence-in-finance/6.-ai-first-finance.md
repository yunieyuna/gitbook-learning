# 6. AI-First Finance

## Chapter 6. AI-First Finance

> A computation takes information and transforms it, implementing what mathematicians call a _function_….If you’re in possession of a function that inputs all the world’s financial data and outputs the best stocks to buy, you’ll soon be extremely rich.
>
> Max Tegmark (2017)

This chapter sets out to combine data-driven finance with the machine learning approach from the previous chapter. It only represents the beginning of this endeavor in that, for the first time, neural networks are used to discover statistical inefficiencies. [“Efficient Markets”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#aiff\_efficiency) discusses the efficient market hypothesis and uses OLS regression to illustrate it based on financial time series data. [“Market Prediction Based on Returns Data”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#aiff\_returns) for the first time applies neural networks, alongside OLS regression, to predict the future direction of a financial instrument’s price (“market direction”). The analysis relies on returns data only. [“Market Prediction with More Features”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#aiff\_features) adds more features to the mix, such as typical financial indicators. In this context, first results indicate that statistical inefficiencies might indeed be present. This is confirmed in [“Market Prediction Intraday”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#aiff\_intraday), which works with intraday data as compared to end-of-day data. Finally, [“Conclusions”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#aiff\_effectiveness) discusses the effectiveness of big data in combination with AI in certain domains and argues that AI-first, theory-free finance might represent a way out of the theory fallacies in traditional finance.

## Efficient Markets

One of the hypotheses with the strongest empirical support is the _efficient market hypothesis_ (EMH). It is also called the _random walk hypothesis_ (RWH).[1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#idm46319981816728) Simply speaking, the hypothesis says that the prices of financial instruments at a certain point in time reflect all available information at this point in time. If the EMH holds true, a discussion about whether the price of a stock is too high or too low would be pointless. The price of a stock, given the EMH, is at all times exactly on its appropriate level given the available information.

Lots of effort has been put into refining and formalizing the idea of efficient markets since the formulation and first discussions of the EMH in the 1960s. The definitions as presented in Jensen (1978) are still used today. Jensen defines an efficient market as follows:

> A market is efficient with respect to an information set �� if it is impossible to make economic profits by trading on the basis of information set ��. By economic profits, we mean the risk adjusted returns net of all costs.

In this context, Jensen distinguishes three forms of market efficiency:

Weak form of EMH

In this case, the information set �� only encompasses the past price and return history of the market.

Semi-strong form of EMH

In this case, the information set �� is taken to be all publicly available information, including not only the past price and return history but also financial reports, news articles, weather data, and so on.

Strong form of EMH

This case is given when the information set �� includes all information available to anyone (that is, even private information).

No matter which form is assumed, the implications of the EMH are far reaching. In his pioneering article on the EMH, Fama (1965) concludes the following:

> For many years, economists, statisticians, and teachers of finance have been interested in developing and testing models of stock price behavior. One important model that has evolved from this research is the theory of random walks. This theory casts serious doubt on many other methods for describing and predicting stock price behavior—methods that have considerable popularity outside the academic world. For example, we shall see later that, if the random-walk theory is an accurate description of reality, then the various “technical” or “chartist” procedures for predicting stock prices are completely without value.

In other words, if the EMH holds true, then any kind of research or data analysis for the purposes of achieving above-market returns should be useless in practice. On the other hand, a multitrillion-dollar asset management industry has evolved that promises such above-market returns due to rigorous research and the active management of capital. In particular, the hedge fund industry is based on promises to deliver _alpha_—that is, returns that are above-market and even independent, at least to a large extent, of the market returns. How hard it is to live up to such a promise is shown by the data from a recent study by [Preqin](https://oreil.ly/C38Tl). The study reports a drop in the Preqin All-Strategies Hedge Fund index of –3.42% for the year 2018. Close to 40% of all hedge funds covered by the study experienced losses of 5% or greater for that year.

If a stock price (or the price of any other financial instrument) follows a standard random walk, then the returns are normally distributed with zero mean. The stock price goes up with 50% probability and down with 50% probability. In such a context, the best predictor of tomorrow’s stock price, in a least-squares sense, is today’s stock price. This is due to the Markov property of random walks, namely that the distribution of the future stock prices is independent of the history of the price process; it only depends on the current price level. Therefore, in the context of a random walk, the analysis of the historical prices (or returns) is useless for predicting future prices.

Against this background, a semiformal test for efficient markets can be implemented as follows.[2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#idm46319982734776) Take a financial time series, lag the price data multiple times, and use the lagged price data as features data for an OLS regression that uses the current price level as the labels data. This is similar in spirit to charting techniques that rely on historical price formations to predict future prices.

The following Python code implements such an analysis based on lagged price data for a number of financial instruments—both tradable ones and nontradable ones. First, import the data and its visualization (see [Figure 6-1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#figure\_aiff\_01)):

```
In [1]: import numpy as np
        import pandas as pd
        from pylab import plt, mpl
        plt.style.use('seaborn')
        mpl.rcParams['savefig.dpi'] = 300
        mpl.rcParams['font.family'] = 'serif'
        pd.set_option('precision', 4)
        np.set_printoptions(suppress=True, precision=4)

In [2]: url = 'http://hilpisch.com/aiif_eikon_eod_data.csv'  

In [3]: data = pd.read_csv(url, index_col=0, parse_dates=True).dropna()  

In [4]: (data / data.iloc[0]).plot(figsize=(10, 6), cmap='coolwarm');  
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO1-1)

Reads the data into a `DataFrame` object

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO1-3)

Plots the normalized time series data

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0601.png" alt="aiif 0601" height="1394" width="2402"><figcaption></figcaption></figure>

**Figure 6-1. Normalized time series data (end-of-day)**

Second, the price data for all financial time series is lagged and stored in `DataFrame` objects:

```
In [5]: lags = 7  

In [6]: def add_lags(data, ric, lags):
            cols = []
            df = pd.DataFrame(data[ric])
            for lag in range(1, lags + 1):
                col = 'lag_{}'.format(lag)  
                df[col] = df[ric].shift(lag)  
                cols.append(col)  
            df.dropna(inplace=True)  
            return df, cols

In [7]: dfs = {}
        for sym in data.columns:
            df, cols = add_lags(data, sym, lags)  
            dfs[sym] = df  

In [8]: dfs[sym].head(7)  
Out[8]:                GLD   lag_1   lag_2   lag_3   lag_4   lag_5   lag_6   lag_7
        Date
        2010-01-13  111.54  110.49  112.85  111.37  110.82  111.51  109.70  109.80
        2010-01-14  112.03  111.54  110.49  112.85  111.37  110.82  111.51  109.70
        2010-01-15  110.86  112.03  111.54  110.49  112.85  111.37  110.82  111.51
        2010-01-19  111.52  110.86  112.03  111.54  110.49  112.85  111.37  110.82
        2010-01-20  108.94  111.52  110.86  112.03  111.54  110.49  112.85  111.37
        2010-01-21  107.37  108.94  111.52  110.86  112.03  111.54  110.49  112.85
        2010-01-22  107.17  107.37  108.94  111.52  110.86  112.03  111.54  110.49
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO2-1)

The number of lags (in trading days)

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO2-2)

Creates a column name

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO2-3)

Lags the price data

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO2-4)

Adds the column name to a `list` object

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO2-5)

Deletes all incomplete data rows

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO2-6)

Creates the lagged data for every financial time series

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/7.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO2-7)

Stores the results in a `dict` object

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/8.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO2-8)

Shows a sample of the lagged price data

Third, with the data prepared, the OLS regression analysis is straightforward to conduct. [Figure 6-2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#figure\_aiff\_02) shows the average optimal regression results. Without a doubt, the price data that is lagged by only one day has the highest explanatory power. Its weight is close to 1, supporting the idea that the best predictor for tomorrow’s price of a financial instrument is its price today. This also holds true for the single regression results obtained per financial time series:

```
In [9]: regs = {}
        for sym in data.columns:
            df = dfs[sym]  
            reg = np.linalg.lstsq(df[cols], df[sym], rcond=-1)[0]  
            regs[sym] = reg  

In [10]: rega = np.stack(tuple(regs.values()))  

In [11]: regd = pd.DataFrame(rega, columns=cols, index=data.columns)  

In [12]: regd  
Out[12]:          lag_1   lag_2   lag_3   lag_4   lag_5   lag_6   lag_7
         AAPL.O  1.0106 -0.0592  0.0258  0.0535 -0.0172  0.0060 -0.0184
         MSFT.O  0.8928  0.0112  0.1175 -0.0832 -0.0258  0.0567  0.0323
         INTC.O  0.9519  0.0579  0.0490 -0.0772 -0.0373  0.0449  0.0112
         AMZN.O  0.9799 -0.0134  0.0206  0.0007  0.0525 -0.0452  0.0056
         GS.N    0.9806  0.0342 -0.0172  0.0042 -0.0387  0.0585 -0.0215
         SPY     0.9692  0.0067  0.0228 -0.0244 -0.0237  0.0379  0.0121
         .SPX    0.9672  0.0106  0.0219 -0.0252 -0.0318  0.0515  0.0063
         .VIX    0.8823  0.0591 -0.0289  0.0284 -0.0256  0.0511  0.0306
         EUR=    0.9859  0.0239 -0.0484  0.0508 -0.0217  0.0149 -0.0055
         XAU=    0.9864  0.0069  0.0166 -0.0215  0.0044  0.0198 -0.0125
         GDX     0.9765  0.0096 -0.0039  0.0223 -0.0364  0.0379 -0.0065
         GLD     0.9766  0.0246  0.0060 -0.0142 -0.0047  0.0223 -0.0106

In [13]: regd.mean().plot(kind='bar', figsize=(10, 6));  
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO3-1)

Gets the data for the current time series

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO3-2)

Implements the regression analysis

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO3-3)

Stores the optimal regression parameters in a `dict` object

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO3-4)

Combines the optimal results into a single `ndarray` object

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO3-5)

Puts the results into a `DataFrame` object and shows them

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO3-7)

Visualizes the average optimal regression parameters (weights) for every lag

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0602.png" alt="aiif 0602" height="1502" width="2418"><figcaption></figcaption></figure>

**Figure 6-2. Average optimal regression parameters for the lagged prices**

Given this semiformal analysis, there seems to be strong supporting evidence for the EMH in its weak form, at least. It is noteworthy that the OLS regression analysis as implemented here violates several assumptions. Among those is that the features are assumed to be noncorrelated among each other, whereas they should ideally be highly correlated with the labels data. However, the lagged price data leads to highly correlated features. The following Python code presents the correlation data, which shows a close-to-perfect correlation between all features. This explains why only one feature (“lag 1”) is enough to accomplish the approximation and prediction based on the OLS regression approach. Adding more, highly correlated features does not yield any improvements. Another fundamental assumption violated is the _stationarity_ of the time series data, which the following code also tests for:[3](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#idm46319980947432)

```
In [14]: dfs[sym].corr()  
Out[14]:           GLD   lag_1   lag_2   lag_3   lag_4   lag_5   lag_6   lag_7
         GLD    1.0000  0.9972  0.9946  0.9920  0.9893  0.9867  0.9841  0.9815
         lag_1  0.9972  1.0000  0.9972  0.9946  0.9920  0.9893  0.9867  0.9842
         lag_2  0.9946  0.9972  1.0000  0.9972  0.9946  0.9920  0.9893  0.9867
         lag_3  0.9920  0.9946  0.9972  1.0000  0.9972  0.9946  0.9920  0.9893
         lag_4  0.9893  0.9920  0.9946  0.9972  1.0000  0.9972  0.9946  0.9920
         lag_5  0.9867  0.9893  0.9920  0.9946  0.9972  1.0000  0.9972  0.9946
         lag_6  0.9841  0.9867  0.9893  0.9920  0.9946  0.9972  1.0000  0.9972
         lag_7  0.9815  0.9842  0.9867  0.9893  0.9920  0.9946  0.9972  1.0000

In [15]: from statsmodels.tsa.stattools import adfuller  

In [16]: adfuller(data[sym].dropna())  
Out[16]: (-1.9488969577009954,
          0.3094193074034718,
          0,
          2515,
          {'1%': -3.4329527780962255,
           '5%': -2.8626898965523724,
           '10%': -2.567382133955709},
          8446.683102944744)
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO4-1)

Shows the correlations between the lagged time series

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO4-2)

Tests for stationarity using the [Augmented Dickey-Fuller](https://oreil.ly/rfdaC) test

In summary, if the EMH holds true, active or algorithmic portfolio management or trading would not make economic sense. Simply investing in a stock or an efficient portfolio in the MVP sense, say, and passively holding the investment over a long period would yield without any effort at least the same, if not superior, returns. According to the CAPM and the MVP, the higher the risk the investor is willing to bear, the higher the expected return should be. In fact, as Copeland et al. (2005, ch. 10) point out, the CAPM and the EMH form a joint hypothesis about financial markets: if the EMH is rejected, then the CAPM must be rejected as well, since its derivation assumes the EMH to hold true.

## Market Prediction Based on Returns Data

As [Chapter 2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch02.html#superintelligence) shows, ML and, in particular, DL algorithms have generated breakthroughs in recent years in fields that have proven resistant over pretty long periods of time to standard statistical or mathematical methods. What about the financial markets? Might ML and DL algorithms be capable of discovering inefficiencies where traditional financial econometrics methods, such as OLS regression, fail? Of course, there are no simple and concise answers to these questions yet.

However, some concrete examples might shed light on possible answers. To this end, the same data as in the previous section is used to derive log returns from the price data. The idea is to compare the performance of OLS regression to the performance of neural networks in predicting the next day’s direction of movement for the different time series. The goal at this stage is to discover _statistical inefficiencies_ as compared to _economic inefficiencies_. Statistical inefficiencies are given when a model is able to predict the direction of the future price movement with a certain edge (say, the prediction is correct in 55% or 60% of the cases). Economic inefficiencies would only be given if the statistical inefficiencies can be exploited profitably through a trading strategy that takes into account, for example, transaction costs.

The first step in the analysis is to create data sets with lagged log returns data. The normalized lagged log returns data is also tested for stationarity (given), and the features are tested for correlation (not correlated). Since the following analyses rely on time-series-related data only, they are dealing with _weak form market efficiency_:

```
In [17]: rets = np.log(data / data.shift(1))  

In [18]: rets.dropna(inplace=True)

In [19]: dfs = {}
         for sym in data:
             df, cols = add_lags(rets, sym, lags)  
             mu, std = df[cols].mean(), df[cols].std()  
             df[cols] = (df[cols] - mu) / std  
             dfs[sym] = df

In [20]: dfs[sym].head()  
Out[20]:                GLD   lag_1   lag_2   lag_3   lag_4   lag_5   lag_6   lag_7
         Date
         2010-01-14  0.0044  0.9570 -2.1692  1.3386  0.4959 -0.6434  1.6613 -0.1028
         2010-01-15 -0.0105  0.4379  0.9571 -2.1689  1.3388  0.4966 -0.6436  1.6614
         2010-01-19  0.0059 -1.0842  0.4385  0.9562 -2.1690  1.3395  0.4958 -0.6435
         2010-01-20 -0.0234  0.5967 -1.0823  0.4378  0.9564 -2.1686  1.3383  0.4958
         2010-01-21 -0.0145 -2.4045  0.5971 -1.0825  0.4379  0.9571 -2.1680  1.3384

In [21]: adfuller(dfs[sym]['lag_1'])  
Out[21]: (-51.568251505825536,
          0.0,
          0,
          2507,
          {'1%': -3.4329610922579095,
           '5%': -2.8626935681060375,
           '10%': -2.567384088736619},
          7017.165474260225)

In [22]: dfs[sym].corr()  
Out[22]:           GLD   lag_1   lag_2       lag_3   lag_4       lag_5   lag_6   lag_7
         GLD    1.0000 -0.0297  0.0003  1.2635e-02 -0.0026 -5.9392e-03  0.0099 -0.0013
         lag_1 -0.0297  1.0000 -0.0305  8.1418e-04  0.0128 -2.8765e-03 -0.0053  0.0098
         lag_2  0.0003 -0.0305  1.0000 -3.1617e-02  0.0003  1.3234e-02 -0.0043 -0.0052
         lag_3  0.0126  0.0008 -0.0316  1.0000e+00 -0.0313 -6.8542e-06  0.0141 -0.0044
         lag_4 -0.0026  0.0128  0.0003 -3.1329e-02  1.0000 -3.1761e-02  0.0002  0.0141
         lag_5 -0.0059 -0.0029  0.0132 -6.8542e-06 -0.0318  1.0000e+00 -0.0323  0.0002
         lag_6  0.0099 -0.0053 -0.0043  1.4115e-02  0.0002 -3.2289e-02  1.0000 -0.0324
         lag_7 -0.0013  0.0098 -0.0052 -4.3869e-03  0.0141  2.1707e-04 -0.0324  1.0000
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO5-1)

Derives the log returns from the price data

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO5-2)

Lags the log returns data

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO5-3)

Applies _Gaussian normalization_ to the features data[4](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#idm46319979605496)

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO5-5)

Shows a sample of the lagged returns data

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO5-6)

Tests for stationarity of the time series data

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO5-7)

Shows the correlation data for the features

First, the OLS regression is implemented and the predictions resulting from the regression are generated. The analysis is implemented on the complete data set. It shall show how well the algorithms perform in-sample. The accuracy with which OLS regression predicts the next day’s direction of movement is slightly, or even a few percentage points, above 50% with one exception:

```
In [23]: from sklearn.metrics import accuracy_score

In [24]: %%time
         for sym in data:
             df = dfs[sym]
             reg = np.linalg.lstsq(df[cols], df[sym], rcond=-1)[0]  
             pred = np.dot(df[cols], reg)  
             acc = accuracy_score(np.sign(df[sym]), np.sign(pred))  
             print(f'OLS | {sym:10s} | acc={acc:.4f}')
         OLS | AAPL.O     | acc=0.5056
         OLS | MSFT.O     | acc=0.5088
         OLS | INTC.O     | acc=0.5040
         OLS | AMZN.O     | acc=0.5048
         OLS | GS.N       | acc=0.5080
         OLS | SPY        | acc=0.5080
         OLS | .SPX       | acc=0.5167
         OLS | .VIX       | acc=0.5291
         OLS | EUR=       | acc=0.4984
         OLS | XAU=       | acc=0.5207
         OLS | GDX        | acc=0.5307
         OLS | GLD        | acc=0.5072
         CPU times: user 201 ms, sys: 65.8 ms, total: 267 ms
         Wall time: 60.8 ms
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO6-1)

The regression step

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO6-2)

The prediction step

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO6-3)

The accuracy of the prediction

Second, the same analysis is done again but this time with a neural network from `scikit-learn` as the model for learning and predicting. The prediction accuracy in-sample is significantly above 50% throughout and above 60% in a few cases:

```
In [25]: from sklearn.neural_network import MLPRegressor

In [26]: %%time
         for sym in data.columns:
             df = dfs[sym]
             model = MLPRegressor(hidden_layer_sizes=[512],
                                  random_state=100,
                                  max_iter=1000,
                                  early_stopping=True,
                                  validation_fraction=0.15,
                                  shuffle=False)  
             model.fit(df[cols], df[sym])  
             pred = model.predict(df[cols])  
             acc = accuracy_score(np.sign(df[sym]), np.sign(pred))  
             print(f'MLP | {sym:10s} | acc={acc:.4f}')
         MLP | AAPL.O     | acc=0.6005
         MLP | MSFT.O     | acc=0.5853
         MLP | INTC.O     | acc=0.5766
         MLP | AMZN.O     | acc=0.5510
         MLP | GS.N       | acc=0.6527
         MLP | SPY        | acc=0.5419
         MLP | .SPX       | acc=0.5399
         MLP | .VIX       | acc=0.6579
         MLP | EUR=       | acc=0.5642
         MLP | XAU=       | acc=0.5522
         MLP | GDX        | acc=0.6029
         MLP | GLD        | acc=0.5259
         CPU times: user 1min 37s, sys: 6.74 s, total: 1min 44s
         Wall time: 14 s
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO7-1)

Model instantiation

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO7-2)

Model fitting

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO7-3)

Prediction step

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO7-4)

Accuracy calculation

Third, the same analysis again but with a neural network from the `Keras` package. The accuracy results are similar to those from the `MLPRegressor`, but with a higher average accuracy:

```
In [27]: import tensorflow as tf
         from keras.layers import Dense
         from keras.models import Sequential
         Using TensorFlow backend.

In [28]: np.random.seed(100)
         tf.random.set_seed(100)

In [29]: def create_model(problem='regression'):  
             model = Sequential()
             model.add(Dense(512, input_dim=len(cols),
                             activation='relu'))
             if problem == 'regression':
                 model.add(Dense(1, activation='linear'))
                 model.compile(loss='mse', optimizer='adam')
             else:
                 model.add(Dense(1, activation='sigmoid'))
                 model.compile(loss='binary_crossentropy', optimizer='adam')
             return model

In [30]: %%time
         for sym in data.columns[:]:
             df = dfs[sym]
             model = create_model()  
             model.fit(df[cols], df[sym], epochs=25, verbose=False)  
             pred = model.predict(df[cols])  
             acc = accuracy_score(np.sign(df[sym]), np.sign(pred))  
             print(f'DNN | {sym:10s} | acc={acc:.4f}')
         DNN | AAPL.O     | acc=0.6292
         DNN | MSFT.O     | acc=0.5981
         DNN | INTC.O     | acc=0.6073
         DNN | AMZN.O     | acc=0.5781
         DNN | GS.N       | acc=0.6196
         DNN | SPY        | acc=0.5829
         DNN | .SPX       | acc=0.6077
         DNN | .VIX       | acc=0.6392
         DNN | EUR=       | acc=0.5845
         DNN | XAU=       | acc=0.5881
         DNN | GDX        | acc=0.5829
         DNN | GLD        | acc=0.5666
         CPU times: user 34.3 s, sys: 5.34 s, total: 39.6 s
         Wall time: 23.1 s
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO8-1)

Model creation function

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO8-2)

Model instantiation

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO8-3)

Model fitting

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO8-4)

Prediction step

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO8-5)

Accuracy calculation

This simple example shows that neural networks can outperform OLS regression significantly _in-sample_ in predicting the next day’s direction of price movements. However, how does the picture change when testing for the _out-of-sample_ performance of the two model types?

To this end, the analyses are repeated, but the training (fitting) step is implemented on the first 80% of the data while the performance is tested on the remaining 20%. OLS regression is implemented first. Out-of-sample OLS regression shows similar accuracy levels as in-sample—around 50%:

```
In [31]: split = int(len(dfs[sym]) * 0.8)

In [32]: %%time
         for sym in data.columns:
             df = dfs[sym]
             train = df.iloc[:split]  
             reg = np.linalg.lstsq(train[cols], train[sym], rcond=-1)[0]
             test = df.iloc[split:]  
             pred = np.dot(test[cols], reg)
             acc = accuracy_score(np.sign(test[sym]), np.sign(pred))
             print(f'OLS | {sym:10s} | acc={acc:.4f}')
         OLS | AAPL.O     | acc=0.5219
         OLS | MSFT.O     | acc=0.4960
         OLS | INTC.O     | acc=0.5418
         OLS | AMZN.O     | acc=0.4841
         OLS | GS.N       | acc=0.4980
         OLS | SPY        | acc=0.5020
         OLS | .SPX       | acc=0.5120
         OLS | .VIX       | acc=0.5458
         OLS | EUR=       | acc=0.4482
         OLS | XAU=       | acc=0.5299
         OLS | GDX        | acc=0.5159
         OLS | GLD        | acc=0.5100
         CPU times: user 200 ms, sys: 60.6 ms, total: 261 ms
         Wall time: 61.7 ms
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO9-1)

Creates the _training_ data sub-set

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO9-2)

Creates the _test_ data sub-set

The performance of the `MLPRegressor` model is out-of-sample much worse when compared to the in-sample numbers and similar to the OLS regression results:

```
In [34]: %%time
         for sym in data.columns:
             df = dfs[sym]
             train = df.iloc[:split]
             model = MLPRegressor(hidden_layer_sizes=[512],
                                  random_state=100,
                                  max_iter=1000,
                                  early_stopping=True,
                                  validation_fraction=0.15,
                                  shuffle=False)
             model.fit(train[cols], train[sym])
             test = df.iloc[split:]
             pred = model.predict(test[cols])
             acc = accuracy_score(np.sign(test[sym]), np.sign(pred))
             print(f'MLP | {sym:10s} | acc={acc:.4f}')
         MLP | AAPL.O     | acc=0.4920
         MLP | MSFT.O     | acc=0.5279
         MLP | INTC.O     | acc=0.5279
         MLP | AMZN.O     | acc=0.4641
         MLP | GS.N       | acc=0.5040
         MLP | SPY        | acc=0.5259
         MLP | .SPX       | acc=0.5478
         MLP | .VIX       | acc=0.5279
         MLP | EUR=       | acc=0.4980
         MLP | XAU=       | acc=0.5239
         MLP | GDX        | acc=0.4880
         MLP | GLD        | acc=0.5000
         CPU times: user 1min 39s, sys: 4.98 s, total: 1min 44s
         Wall time: 13.7 s
```

The same holds true for the `Sequential` model from `Keras` for which the out-of-sample numbers also show accuracy values between a few percentage points above and below the 50% threshold:

```
In [35]: %%time
         for sym in data.columns:
             df = dfs[sym]
             train = df.iloc[:split]
             model = create_model()
             model.fit(train[cols], train[sym], epochs=50, verbose=False)
             test = df.iloc[split:]
             pred = model.predict(test[cols])
             acc = accuracy_score(np.sign(test[sym]), np.sign(pred))
             print(f'DNN | {sym:10s} | acc={acc:.4f}')
         DNN | AAPL.O     | acc=0.5179
         DNN | MSFT.O     | acc=0.5598
         DNN | INTC.O     | acc=0.4821
         DNN | AMZN.O     | acc=0.4920
         DNN | GS.N       | acc=0.5179
         DNN | SPY        | acc=0.4861
         DNN | .SPX       | acc=0.5100
         DNN | .VIX       | acc=0.5378
         DNN | EUR=       | acc=0.4661
         DNN | XAU=       | acc=0.4602
         DNN | GDX        | acc=0.4841
         DNN | GLD        | acc=0.5378
         CPU times: user 50.4 s, sys: 7.52 s, total: 57.9 s
         Wall time: 32.9 s
```

## WEAK FORM MARKET EFFICIENCY

Although the labeling as _weak form_ market efficiency might suggest otherwise, it is the hardest form in the sense that only time-series-related data can be used to identify statistical inefficiencies. With the semi-strong form, any other source of publicly available data could be added to improve prediction accuracy.

Based on the approaches chosen in this section, markets seem to be at least efficient in the weak form. Just analyzing historical return patterns based on OLS regression or neural networks might not be enough to discover statistical inefficiencies.

There are two major elements of the approach chosen in this section that can be adjusted in the hope of improving prediction results:

Features

In addition to vanilla price-and-returns data, other features can be added to the data, such as technical indicators (for example, simple moving averages, or SMAs for short). The hope is, in the technical chartist’s tradition, that such indicators improve the prediction accuracy.

Bar length

Instead of working with end-of-day data, intraday data might allow for higher prediction accuracies. Here, the hope is that one is more likely to discover statistical inefficiencies during the day as compared to at end of day, when all market participants in general pay the highest attention to making their final trades—by taking into account all available information.

The following two sections address these elements.

## Market Prediction with More Features

In trading, there is a long tradition of using technical indicators to generate, based on observed patterns, buy or sell signals. Such technical indicators, basically of any kind, can also be used as features for the training of neural networks.

The following Python code uses an SMA, rolling minimum and maximum values, momentum, and rolling volatility as features:

```
In [36]: url = 'http://hilpisch.com/aiif_eikon_eod_data.csv'

In [37]: data = pd.read_csv(url, index_col=0, parse_dates=True).dropna()

In [38]: def add_lags(data, ric, lags, window=50):
             cols = []
             df = pd.DataFrame(data[ric])
             df.dropna(inplace=True)
             df['r'] = np.log(df / df.shift())
             df['sma'] = df[ric].rolling(window).mean()  
             df['min'] = df[ric].rolling(window).min()  
             df['max'] = df[ric].rolling(window).max()  
             df['mom'] = df['r'].rolling(window).mean()  
             df['vol'] = df['r'].rolling(window).std()  
             df.dropna(inplace=True)
             df['d'] = np.where(df['r'] > 0, 1, 0)  
             features = [ric, 'r', 'd', 'sma', 'min', 'max', 'mom', 'vol']
             for f in features:
                 for lag in range(1, lags + 1):
                     col = f'{f}_lag_{lag}'
                     df[col] = df[f].shift(lag)
                     cols.append(col)
             df.dropna(inplace=True)
             return df, cols

In [39]: lags = 5

In [40]: dfs = {}
         for ric in data:
             df, cols = add_lags(data, ric, lags)
             dfs[ric] = df.dropna(), cols
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO10-1)

Simple moving average (SMA)

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO10-2)

Rolling minimum

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO10-3)

Rolling maximum

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO10-4)

Momentum as average of log returns

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO10-5)

Rolling volatility

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO10-6)

Direction as binary feature

## TECHNICAL INDICATORS AS FEATURES

As the preceding examples show, basically any traditional technical indicator used for investing or intraday trading can be used as a feature to train ML algorithms. In that sense, AI and ML do not necessarily render such indicators obsolete, rather they can indeed enrich the ML-driven derivation of trading strategies.

In-sample, the performance of the `MLPClassifier` model is now much better when taking into account the new features and normalizing them for training. The `Sequential` model of `Keras` reaches accuracies of around 70% for the number of epochs trained. From experience, these can be easily increased by increasing the number of epochs and/or the capacity of the neural network:

```
In [41]: from sklearn.neural_network import MLPClassifier

In [42]: %%time
         for ric in data:
             model = MLPClassifier(hidden_layer_sizes=[512],
                                   random_state=100,
                                   max_iter=1000,
                                   early_stopping=True,
                                   validation_fraction=0.15,
                                   shuffle=False)
             df, cols = dfs[ric]
             df[cols] = (df[cols] - df[cols].mean()) / df[cols].std()  
             model.fit(df[cols], df['d'])
             pred = model.predict(df[cols])
             acc = accuracy_score(df['d'], pred)
             print(f'IN-SAMPLE | {ric:7s} | acc={acc:.4f}')
         IN-SAMPLE | AAPL.O  | acc=0.5510
         IN-SAMPLE | MSFT.O  | acc=0.5376
         IN-SAMPLE | INTC.O  | acc=0.5607
         IN-SAMPLE | AMZN.O  | acc=0.5559
         IN-SAMPLE | GS.N    | acc=0.5794
         IN-SAMPLE | SPY     | acc=0.5729
         IN-SAMPLE | .SPX    | acc=0.5941
         IN-SAMPLE | .VIX    | acc=0.6940
         IN-SAMPLE | EUR=    | acc=0.5766
         IN-SAMPLE | XAU=    | acc=0.5672
         IN-SAMPLE | GDX     | acc=0.5847
         IN-SAMPLE | GLD     | acc=0.5567
         CPU times: user 1min 1s, sys: 4.5 s, total: 1min 6s
         Wall time: 9.05 s

In [43]: %%time
         for ric in data:
             model = create_model('classification')
             df, cols = dfs[ric]
             df[cols] = (df[cols] - df[cols].mean()) / df[cols].std()  
             model.fit(df[cols], df['d'], epochs=50, verbose=False)
             pred = np.where(model.predict(df[cols]) > 0.5, 1, 0)
             acc = accuracy_score(df['d'], pred)
             print(f'IN-SAMPLE | {ric:7s} | acc={acc:.4f}')
         IN-SAMPLE | AAPL.O  | acc=0.7156
         IN-SAMPLE | MSFT.O  | acc=0.7156
         IN-SAMPLE | INTC.O  | acc=0.7046
         IN-SAMPLE | AMZN.O  | acc=0.6640
         IN-SAMPLE | GS.N    | acc=0.6855
         IN-SAMPLE | SPY     | acc=0.6696
         IN-SAMPLE | .SPX    | acc=0.6579
         IN-SAMPLE | .VIX    | acc=0.7489
         IN-SAMPLE | EUR=    | acc=0.6737
         IN-SAMPLE | XAU=    | acc=0.7143
         IN-SAMPLE | GDX     | acc=0.6826
         IN-SAMPLE | GLD     | acc=0.7078
         CPU times: user 1min 5s, sys: 7.06 s, total: 1min 12s
         Wall time: 44.3 s
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO11-1)

Normalizes the features data

Are these improvements to be transferred to the out-of-sample prediction accuracies? The following Python code repeats the analysis, this time with the training and test split as used before. Unfortunately, the picture is mixed at best. The numbers do not represent real improvements when compared to the approach, relying only on lagged returns data as features. For selected instruments, there seems to be an edge of a few percentage points in the prediction accuracy compared to the 50% benchmark. For others, however, the accuracy is still below 50%—as illustrated for the `MLPClassifier` model:

```
In [44]: def train_test_model(model):
             for ric in data:
                 df, cols = dfs[ric]
                 split = int(len(df) * 0.85)
                 train = df.iloc[:split].copy()
                 mu, std = train[cols].mean(), train[cols].std()  
                 train[cols] = (train[cols] - mu) / std
                 model.fit(train[cols], train['d'])
                 test = df.iloc[split:].copy()
                 test[cols] = (test[cols] - mu) / std
                 pred = model.predict(test[cols])
                 acc = accuracy_score(test['d'], pred)
                 print(f'OUT-OF-SAMPLE | {ric:7s} | acc={acc:.4f}')

In [45]: model_mlp = MLPClassifier(hidden_layer_sizes=[512],
                                   random_state=100,
                                   max_iter=1000,
                                   early_stopping=True,
                                   validation_fraction=0.15,
                                   shuffle=False)

In [46]: %time train_test_model(model_mlp)
         OUT-OF-SAMPLE | AAPL.O  | acc=0.4432
         OUT-OF-SAMPLE | MSFT.O  | acc=0.4595
         OUT-OF-SAMPLE | INTC.O  | acc=0.5000
         OUT-OF-SAMPLE | AMZN.O  | acc=0.5270
         OUT-OF-SAMPLE | GS.N    | acc=0.4838
         OUT-OF-SAMPLE | SPY     | acc=0.4811
         OUT-OF-SAMPLE | .SPX    | acc=0.5027
         OUT-OF-SAMPLE | .VIX    | acc=0.5676
         OUT-OF-SAMPLE | EUR=    | acc=0.4649
         OUT-OF-SAMPLE | XAU=    | acc=0.5514
         OUT-OF-SAMPLE | GDX     | acc=0.5162
         OUT-OF-SAMPLE | GLD     | acc=0.4946
         CPU times: user 44.9 s, sys: 2.64 s, total: 47.5 s
         Wall time: 6.37 s
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO12-1)

Training data set statistics are used for normalization.

The good in-sample performance and the not-so-good out-of-sample performance suggest that overfitting of the neural network might play a crucial role. One approach to avoid overfitting is to use ensemble methods that combine multiple trained models of the same type to come up with a more robust meta model and better out-of-sample predictions. One such method is called _bagging_. `scikit-learn` has an implementation of this approach in the form of the [`BaggingClassifier` class](https://oreil.ly/gQLFZ). Using multiple estimators allows for training every one of them without exposing them to the complete training data set or all features. This should help in avoiding overfitting.

The following Python code implements a bagging approach based on a number of base estimators of the same type (`MLPClassifier`). The prediction accuracies are now consistently above 50%. Some accuracy values are above 55%, which can be considered pretty high in this context. Overall, bagging seems to avoid, at least to some extent, overfitting and seems to improve the predictions noticeably:

```
In [47]: from sklearn.ensemble import BaggingClassifier

In [48]: base_estimator = MLPClassifier(hidden_layer_sizes=[256],
                                   random_state=100,
                                   max_iter=1000,
                                   early_stopping=True,
                                   validation_fraction=0.15,
                                   shuffle=False)  

In [49]: model_bag = BaggingClassifier(base_estimator=base_estimator,  
                                   n_estimators=35,  
                                   max_samples=0.25,  
                                   max_features=0.5,  
                                   bootstrap=False,  
                                   bootstrap_features=True,  
                                   n_jobs=8,  
                                   random_state=100
                                  )

In [50]: %time train_test_model(model_bag)
         OUT-OF-SAMPLE | AAPL.O  | acc=0.5243
         OUT-OF-SAMPLE | MSFT.O  | acc=0.5703
         OUT-OF-SAMPLE | INTC.O  | acc=0.5027
         OUT-OF-SAMPLE | AMZN.O  | acc=0.5270
         OUT-OF-SAMPLE | GS.N    | acc=0.5243
         OUT-OF-SAMPLE | SPY     | acc=0.5595
         OUT-OF-SAMPLE | .SPX    | acc=0.5514
         OUT-OF-SAMPLE | .VIX    | acc=0.5649
         OUT-OF-SAMPLE | EUR=    | acc=0.5108
         OUT-OF-SAMPLE | XAU=    | acc=0.5378
         OUT-OF-SAMPLE | GDX     | acc=0.5162
         OUT-OF-SAMPLE | GLD     | acc=0.5432
         CPU times: user 2.55 s, sys: 494 ms, total: 3.05 s
         Wall time: 11.1 s
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO13-1)

The base estimator

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO13-3)

The number of estimators used

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO13-4)

Maximum percentage of training data used per estimator

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO13-5)

Maximum number of features used per estimator

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO13-6)

Whether to bootstrap (reuse) data

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO13-7)

Whether to bootstrap (reuse) features

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/7.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#co\_ai\_first\_finance\_CO13-8)

Number of parallel jobs

## END-OF-DAY MARKET EFFICIENCY

The efficient market hypothesis dates back to the 1960s and 1970s, periods during which end-of-day data was basically the only available time series data. Back in those days (and still today), it could be assumed that market players paid particularly close attention to their positions and trades the closer the end of the trading session came. This might be more true for stocks, say, and a bit less so for currencies, which are traded in principle around the clock.

## Market Prediction Intraday

This chapter has not produced conclusive evidence, but the analyses implemented so far point more in the direction that markets are weakly efficient on an end-of-day basis. What about intraday markets? Are there more consistent statistical inefficiencies to be spotted? To work toward an answer of this question, another data set is necessary. The following Python code uses a data set that is composed of the same instruments as in the end-of-day data set, but now contains hourly closing prices. Since trading hours might differ from instrument to instrument, the data set is incomplete. This is no problem, though, since the analyses are implemented time series by time series.

The technical implementation for the hourly data is essentially the same as before, relying on the same code as the end-of-day analysis:

```
In [51]: url = 'http://hilpisch.com/aiif_eikon_id_data.csv'

In [52]: data = pd.read_csv(url, index_col=0, parse_dates=True)

In [53]: data.info()
         <class 'pandas.core.frame.DataFrame'>
         DatetimeIndex: 5529 entries, 2019-03-01 00:00:00 to 2020-01-01 00:00:00
         Data columns (total 12 columns):
          #   Column  Non-Null Count  Dtype
         ---  ------  --------------  -----
          0   AAPL.O  3384 non-null   float64
          1   MSFT.O  3378 non-null   float64
          2   INTC.O  3275 non-null   float64
          3   AMZN.O  3381 non-null   float64
          4   GS.N    1686 non-null   float64
          5   SPY     3388 non-null   float64
          6   .SPX    1802 non-null   float64
          7   .VIX    2959 non-null   float64
          8   EUR=    5429 non-null   float64
          9   XAU=    5149 non-null   float64
          10  GDX     3173 non-null   float64
          11  GLD     3351 non-null   float64
         dtypes: float64(12)
         memory usage: 561.5 KB

In [54]: lags = 5

In [55]: dfs = {}
         for ric in data:
             df, cols = add_lags(data, ric, lags)
             dfs[ric] = df, cols
```

The prediction accuracies intraday are again distributed around 50% with a relatively wide spread for the single neural network. On the positive side, some accuracy values are above 55%. The bagging meta model shows a more consistent out-of-sample performance, though, with many of the observed accuracy values a few percentage points above the 50% benchmark:

```
In [56]: %time train_test_model(model_mlp)
         OUT-OF-SAMPLE | AAPL.O  | acc=0.5420
         OUT-OF-SAMPLE | MSFT.O  | acc=0.4930
         OUT-OF-SAMPLE | INTC.O  | acc=0.5549
         OUT-OF-SAMPLE | AMZN.O  | acc=0.4709
         OUT-OF-SAMPLE | GS.N    | acc=0.5184
         OUT-OF-SAMPLE | SPY     | acc=0.4860
         OUT-OF-SAMPLE | .SPX    | acc=0.5019
         OUT-OF-SAMPLE | .VIX    | acc=0.4885
         OUT-OF-SAMPLE | EUR=    | acc=0.5130
         OUT-OF-SAMPLE | XAU=    | acc=0.4824
         OUT-OF-SAMPLE | GDX     | acc=0.4765
         OUT-OF-SAMPLE | GLD     | acc=0.5455
         CPU times: user 1min 4s, sys: 5.05 s, total: 1min 9s
         Wall time: 9.56 s

In [57]: %time train_test_model(model_bag)
         OUT-OF-SAMPLE | AAPL.O  | acc=0.5660
         OUT-OF-SAMPLE | MSFT.O  | acc=0.5431
         OUT-OF-SAMPLE | INTC.O  | acc=0.5072
         OUT-OF-SAMPLE | AMZN.O  | acc=0.5110
         OUT-OF-SAMPLE | GS.N    | acc=0.5020
         OUT-OF-SAMPLE | SPY     | acc=0.5120
         OUT-OF-SAMPLE | .SPX    | acc=0.4677
         OUT-OF-SAMPLE | .VIX    | acc=0.5092
         OUT-OF-SAMPLE | EUR=    | acc=0.5242
         OUT-OF-SAMPLE | XAU=    | acc=0.5255
         OUT-OF-SAMPLE | GDX     | acc=0.5085
         OUT-OF-SAMPLE | GLD     | acc=0.5374
         CPU times: user 2.64 s, sys: 439 ms, total: 3.08 s
         Wall time: 12.4 s
```

## INTRADAY MARKET EFFICIENCY

Even if markets are _weakly efficient on an end-of-day basis_, they can nevertheless be _weakly inefficient intraday_. Such statistical inefficiencies might result from temporary imbalances, buy or sell pressures, market overreactions, technically driven buy or sell orders, and so on. The central question is whether such statistical inefficiencies, once discovered, can be exploited profitably via specific trading strategies.

## Conclusions

In their widely cited article “The Unreasonable Effectiveness of Data,” Halevy et al. (2009) point out that economists suffer from what they call _physics envy_. By that, they mean the inability to explain human behavior in the same mathematically elegant way that physicists are able to describe even complex real-world phenomena. One such example is Albert Einstein’s probably best-known formula �=��2, which equates energy with the mass of an object times the speed of light squared.

In economics and finance, researchers for decades have tried to emulate the physical approach in deriving and proving simple, elegant equations to explain economic and financial phenomena. But as [Chapter 3](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch03.html#normative\_finance) and [Chapter 4](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch04.html#data\_driven\_finance) together show, many of the most elegant financial theories have hardly any supporting evidence in the real financial world in which the simplifying assumptions, such as normal distributions and linear relationships, do not hold.

As Halevy et al. (2009) explain in their article, there might be domains, such as natural languages and the rules they follow, that defy the derivation and formulation of concise, elegant theories. Researchers might simply need to rely on complex theories and models that are driven by data. For language in particular, the World Wide Web represents a treasure trove of _big data_. And big data seems to be required to train ML and DL algorithms on certain tasks, such as natural language processing or translation on a human level.

After all, finance might be a discipline that has more in common with natural language than with physics. Maybe there are, after all, no simple, elegant formulas that describe important financial phenomena, such as the daily change in a currency rate or the price of a stock.[5](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#idm46319975422024) Maybe the truth might be found only in the big data that nowadays is available in programmatic fashion to financial researchers and academics alike.

This chapter presents the beginning of the quest to uncover the truth, to discover the holy grail of finance: proving that markets are not that efficient after all. The relatively simple neural network approaches of this chapter only rely on time-series-related features for the training. The labels are simple and straightforward: whether the market (financial instrument’s price) goes up or down. The goal is to discover _statistical inefficiencies_ in predicting the future market direction. This in turn represents the first step in exploiting such inefficiencies economically through an implementable trading strategy.

Agrawal et al. (2018) explain in detail, with many examples, that predictions themselves are only one side of the coin. Decision and implementation rules that specify in detail how a certain prediction is dealt with are equally important. The same holds true in an algorithmic trading context: the signal (prediction) is only the beginning. The hard part is to optimally execute an appropriate trade, to monitor active trades, to implement appropriate risk measures—such as stop loss and take profit orders—and so on.

In its quest for statistical inefficiencies, this chapter relies on data and neural networks only. There is no theory involved, and there are no assumptions about how market participants might behave, or similar reasonings. The major modeling effort is done with regard to preparing the features, which of course represent what the modeler considers important. One implicit assumption in the approach taken is that statistical inefficiencies can be discovered based on time-series-related data only. This is to say that markets are not even weakly efficient—the most difficult form of the three to disprove.

Relying on financial data only and applying general ML and DL algorithms and models to it are what this book considers _AI-first finance_. No theories needed, no modeling of human behavior, no assumptions about distributions or the nature of relationships—just data and algorithms. In that sense, AI-first finance could also be labeled _theory-free_ or _model-free finance_.

## References

Books and papers cited in this chapter:

* Agrawal, Ajay, Joshua Gans, and Avi Goldfarb. 2018. _Prediction Machines: The Simple Economics of Artificial Intelligence._ Boston: Harvard Business Review Press.
* Copeland, Thomas, Fred Weston, and Kuldeep Shastri. 2005. _Financial Theory and Corporate Policy_. 4th ed. Boston: Pearson.
* Fama, Eugene. 1965. “Random Walks in Stock Market Prices.” _Financial Analysts Journal_ (September/October): 55-59.
* Halevy, Alon, Peter Norvig, and Fernando Pereira. 2009. “The Unreasonable Effectiveness of Data.” _IEEE Intelligent Systems_, Expert Opinion.
* Hilpisch, Yves. 2018. _Python for Finance: Mastering Data-Driven Finance._ 2nd ed. Sebastopol: O’Reilly.
* Jensen, Michael. 1978. “Some Anomalous Evidence Regarding Market Efficiency.” _Journal of Financial Economics_ 6 (2/3): 95-101.
* Tegmark, Max. 2017. _Life 3.0: Being Human in the Age of Artificial Intelligence_. United Kingdom: Penguin Random House.
* Tsay, Ruey S. 2005. _Analysis of Financial Time Series._ Hoboken: Wiley.

[1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#idm46319981816728-marker) For the purposes of this chapter and the book, the two hypotheses are treated as equal, although the RWH is somewhat stronger than the EMH. See, for instance, Copeland et al. (2005, ch. 10).

[2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#idm46319982734776-marker) See also Hilpisch (2018, ch. 15).

[3](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#idm46319980947432-marker) For details on _stationarity_ in financial time series, see Tsay (2005, sec. 2.1). Tsay points out: “The foundation of time series analysis is stationarity.”

[4](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#idm46319979605496-marker) Another term for the approach is _z-score normalization_.

[5](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#idm46319975422024-marker) There are, of course, more simple financial aspects that allow the modeling by a simple formula. An example might be the derivation of a continuous discount factor  for a period of two years �=2 if the relevant log return is  0.01. It is given by �(�,�)=exp(-��)=exp(-0.01 ·2)= 0.9802. AI or ML cannot offer any benefits here.
