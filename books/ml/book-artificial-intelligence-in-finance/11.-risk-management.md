# 11. Risk Management

## Chapter 11. Risk Management

> A significant barrier to deploying autonomous vehicles (AVs) on a massive scale is safety assurance.
>
> Majid Khonji et al. (2019)

> Having better prediction raises the value of judgment. After all, it doesn’t help to know the likelihood of rain if you don’t know how much you like staying dry or how much you hate carrying an umbrella.
>
> Ajay Agrawal et al. (2018)

Vectorized backtesting in general enables one to judge the economic potential of a prediction-based algorithmic trading strategy on an as-is basis (that is, in its pure form). Most AI agents applied in practice have more components than just the prediction model. For example, the AI of autonomous vehicles (AVs) comes not standalone but rather with a large number of rules and heuristics that restrict what actions the AI takes or can take. In the context of AVs, this primarily relates to managing risks, such as those resulting from collisions or crashes.

In a financial context, AI agents or trading bots are also not deployed as-is in general. Rather, there are a number of standard risk measures that are typically used, such as _(trailing) stop loss orders_ or _take profit orders_. The reasoning is clear. When placing directional bets in financial markets, too-large losses are to be avoided. Similarly, when a certain profit level is reached, the success is to be protected by early close outs. How such risk measures are handled is a matter, more often than not, of human judgment, supported probably by a formal analysis of relevant data and statistics. Conceptually, this is a major point discussed in the book by Agrawal et al. (2018): AI provides improved predictions, but human judgment still plays a role in setting decision rules and action boundaries.

This chapter has a threefold purpose. First, it backtests in both _vectorized_ and _event-based_ fashion algorithmic trading strategies that result from a trained deep Q-learning agent. Henceforth, such agents are called _trading bots_. Second, it assesses risks related to the financial instrument on which the strategies are implemented. And third, it backtests typical risk measures, such as stop loss orders, using the event-based approach introduced in this chapter. The major benefit of event-based backtesting when compared to vectorized backtesting is a higher degree of flexibility in modeling and analyzing decision rules and risk management measures. In other words, it allows one to zoom in on details that are pushed toward the background when working with vectorized programming approaches.

[“Trading Bot”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#rm\_trading\_bot) introduces and trains the trading bot based on the financial Q-learning agent from [Chapter 9](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#reinforcement\_learning). [“Vectorized Backtesting”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#rm\_vec\_back) uses vectorized backtesting from [Chapter 10](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch10.html#vectorized\_backtesting) to judge the (pure) economic performance of the trading bot. Event-based backtesting is introduced in [“Event-Based Backtesting”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#rm\_event\_back). First, a base class is discussed. Second, based on the base class, the backtesting of the trading bot is implemented and conducted. In this context, also see Hilpisch (2020, ch. 6). [“Assessing Risk”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#rm\_assessing\_risk) analyzes selected statistical measures important for setting risk management rules, such as _maximum drawdown_ and _average true range_ (ATR). [“Backtesting Risk Measures”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#rm\_risk\_measures) then backtests the impact of major risk measures on the performance of the trading bot.

## Trading Bot

This section presents a trading bot based on the financial Q-learning agent, `FQLAgent`, from [Chapter 9](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#reinforcement\_learning). This is the trading bot that is analyzed in subsequent sections. As usual, our imports come first:

```
In [1]: import os
        import numpy as np
        import pandas as pd
        from pylab import plt, mpl
        plt.style.use('seaborn')
        mpl.rcParams['savefig.dpi'] = 300
        mpl.rcParams['font.family'] = 'serif'
        pd.set_option('mode.chained_assignment', None)
        pd.set_option('display.float_format', '{:.4f}'.format)
        np.set_printoptions(suppress=True, precision=4)
        os.environ['PYTHONHASHSEED'] = '0'
```

[“Finance Environment”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#rm\_finance) presents a Python module with the `Finance` class used in the following. [“Trading Bot”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#rm\_trading\_bot) provides the Python module with the `TradingBot` class and some helper functions for plotting training and validation results. Both classes are pretty close to the ones introduced in [Chapter 9](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#reinforcement\_learning), which is why they are used here without further explanations.

The following code trains the trading bot on historical end-of-day (EOD) data, including a sub-set of the data used for validation. [Figure 11-1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#figure\_rm\_01) shows average total rewards as achieved for the different training episodes:

```
In [2]: import finance
        import tradingbot
        Using TensorFlow backend.

In [3]: symbol = 'EUR='
        features = [symbol, 'r', 's', 'm', 'v']

In [4]: a = 0
        b = 1750
        c = 250

In [5]: learn_env = finance.Finance(symbol, features, window=20, lags=3,
                         leverage=1, min_performance=0.9, min_accuracy=0.475,
                         start=a, end=a + b, mu=None, std=None)

In [6]: learn_env.data.info()
        <class 'pandas.core.frame.DataFrame'>
        DatetimeIndex: 1750 entries, 2010-02-02 to 2017-01-12
        Data columns (total 6 columns):
         #   Column  Non-Null Count  Dtype
        ---  ------  --------------  -----
         0   EUR=    1750 non-null   float64
         1   r       1750 non-null   float64
         2   s       1750 non-null   float64
         3   m       1750 non-null   float64
         4   v       1750 non-null   float64
         5   d       1750 non-null   int64
        dtypes: float64(5), int64(1)
        memory usage: 95.7 KB

In [7]: valid_env = finance.Finance(symbol, features=learn_env.features,
                                    window=learn_env.window,
                                    lags=learn_env.lags,
                                    leverage=learn_env.leverage,
                                    min_performance=0.0, min_accuracy=0.0,
                                    start=a + b, end=a + b + c,
                                    mu=learn_env.mu, std=learn_env.std)

In [8]: valid_env.data.info()
        <class 'pandas.core.frame.DataFrame'>
        DatetimeIndex: 250 entries, 2017-01-13 to 2018-01-10
        Data columns (total 6 columns):
         #   Column  Non-Null Count  Dtype
        ---  ------  --------------  -----
         0   EUR=    250 non-null    float64
         1   r       250 non-null    float64
         2   s       250 non-null    float64
         3   m       250 non-null    float64
         4   v       250 non-null    float64
         5   d       250 non-null    int64
        dtypes: float64(5), int64(1)
        memory usage: 13.7 KB

In [9]: tradingbot.set_seeds(100)
        agent = tradingbot.TradingBot(24, 0.001, learn_env, valid_env)

In [10]: episodes = 61

In [11]: %time agent.learn(episodes)
         =======================================================================
         episode: 10/61 | VALIDATION | treward:  247 | perf: 0.936 | eps: 0.95
         =======================================================================
         =======================================================================
         episode: 20/61 | VALIDATION | treward:  247 | perf: 0.897 | eps: 0.86
         =======================================================================
         =======================================================================
         episode: 30/61 | VALIDATION | treward:  247 | perf: 1.035 | eps: 0.78
         =======================================================================
         =======================================================================
         episode: 40/61 | VALIDATION | treward:  247 | perf: 0.935 | eps: 0.70
         =======================================================================
         =======================================================================
         episode: 50/61 | VALIDATION | treward:  247 | perf: 0.890 | eps: 0.64
         =======================================================================
         =======================================================================
         episode: 60/61 | VALIDATION | treward:  247 | perf: 0.998 | eps: 0.58
         =======================================================================
         episode: 61/61 | treward:   17 | perf: 0.979 | av: 475.1 | max: 1747
         CPU times: user 51.4 s, sys: 2.53 s, total: 53.9 s
         Wall time: 47 s

In [12]: tradingbot.plot_treward(agent)
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_1101.png" alt="aiif 1101" height="1491" width="2497"><figcaption></figcaption></figure>

**Figure 11-1. Average total reward per training episode**

[Figure 11-2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#figure\_rm\_02) compares the gross performance of the trading bot on the training data—exhibiting quite some variance due to alternating between exploitation and exploration—with the one on the validation data set making use of exploitation only:

```
In [13]: tradingbot.plot_performance(agent)
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_1102.png" alt="aiif 1102" height="1491" width="2484"><figcaption></figcaption></figure>

**Figure 11-2. Gross performance on training and validation data set**

This trained trading bot is used for backtesting in the following sections.

## Vectorized Backtesting

Vectorized backtesting cannot directly be applied to the trading bot. [Chapter 10](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch10.html#vectorized\_backtesting) uses dense neural networks (DNNs) to illustrate the approach. In this context, the data with the features and labels sub-sets is prepared first and then fed to the DNN to generate all predictions at once. In a reinforcement learning (RL) context, data is generated and collected by interacting with the environment action by action and step by step.

To this end, the following Python code defines the `backtest` function, which takes as input a `TradingBot` instance and a `Finance` instance. It generates in the original `DataFrame` objects of the `Finance` environment columns with the positions the trading bot takes and the resulting strategy performance:

```
In [14]: def reshape(s):
             return np.reshape(s, [1, learn_env.lags,
                                   learn_env.n_features])  

In [15]: def backtest(agent, env):
             env.min_accuracy = 0.0
             env.min_performance = 0.0
             done = False
             env.data['p'] = 0  
             state = env.reset()
             while not done:
                 action = np.argmax(
                     agent.model.predict(reshape(state))[0, 0])  
                 position = 1 if action == 1 else -1  
                 env.data.loc[:, 'p'].iloc[env.bar] = position  
                 state, reward, done, info = env.step(action)
             env.data['s'] = env.data['p'] * env.data['r'] * learn_env.leverage  
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO1-1)

Reshapes a single feature-label combination

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO1-2)

Generates a column for the position values

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO1-3)

Derives the optimal action (prediction) given the trained DNN

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO1-4)

Derives the resulting position (`+1` for long/upwards, `–1` for short/downwards)…

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO1-5)

…and stores in the corresponding column at the appropriate index position

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO1-6)

Calculates the strategy log returns given the position values

Equipped with the `backtest` function, vectorized backtesting boils down to a few lines of Python code as in [Chapter 10](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch10.html#vectorized\_backtesting).

[Figure 11-3](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#figure\_rm\_03) compares the passive benchmark investment’s gross performance with the strategy gross performance:

```
In [16]: env = agent.learn_env  

In [17]: backtest(agent, env)  

In [18]: env.data['p'].iloc[env.lags:].value_counts()  
Out[18]:  1    961
         -1    786
         Name: p, dtype: int64

In [19]: env.data[['r', 's']].iloc[env.lags:].sum().apply(np.exp)  
Out[19]: r   0.7725
         s   1.5155
         dtype: float64

In [20]: env.data[['r', 's']].iloc[env.lags:].sum().apply(np.exp) - 1  
Out[20]: r   -0.2275
         s    0.5155
         dtype: float64

In [21]: env.data[['r', 's']].iloc[env.lags:].cumsum(
                 ).apply(np.exp).plot(figsize=(10, 6));
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO2-1)

Specifies the relevant environment

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO2-2)

Generates the additional data required

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO2-3)

Counts the number of long and short positions

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO2-4)

Calculates the gross performances for the passive benchmark investment (`r`) and the strategy (`s`)…

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO2-5)

…as well as the corresponding net performances

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_1103.png" alt="aiif 1103" height="1394" width="2418"><figcaption></figcaption></figure>

**Figure 11-3. Gross performance of the passive benchmark investment and the trading bot (in-sample)**

To get a more realistic picture of the performance of the trading bot, the following Python code creates a test environment with data that the trading bot has not yet seen. [Figure 11-4](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#figure\_rm\_04) shows how the trading bot fares compared to the passive benchmark investment:

```
In [22]: test_env = finance.Finance(symbol, features=learn_env.features,
                                    window=learn_env.window,
                                    lags=learn_env.lags,
                                    leverage=learn_env.leverage,
                                    min_performance=0.0, min_accuracy=0.0,
                                    start=a + b + c, end=None,
                                    mu=learn_env.mu, std=learn_env.std)

In [23]: env = test_env

In [24]: backtest(agent, env)

In [25]: env.data['p'].iloc[env.lags:].value_counts()
Out[25]: -1    437
          1     56
         Name: p, dtype: int64

In [26]: env.data[['r', 's']].iloc[env.lags:].sum().apply(np.exp)
Out[26]: r   0.9144
         s   1.0992
         dtype: float64

In [27]: env.data[['r', 's']].iloc[env.lags:].sum().apply(np.exp) - 1
Out[27]: r   -0.0856
         s    0.0992
         dtype: float64

In [28]: env.data[['r', 's']].iloc[env.lags:].cumsum(
                     ).apply(np.exp).plot(figsize=(10, 6));
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_1104.png" alt="aiif 1104" height="1428" width="2444"><figcaption></figcaption></figure>

**Figure 11-4. Gross performance of the passive benchmark investment and the trading bot (out-of-sample)**

The out-of-sample performance without any risk measures implemented seems already promising. However, to be able to properly judge the real performance of a trading strategy, risk measures should be included. This is where event-based backtesting comes into play.

## Event-Based Backtesting

Given the results of the previous section, the out-of-sample performance without any risk measures seems already promising. However, to be able to properly analyze risk measures, such as trailing stop loss orders, _event-based backtesting_ is required. This section introduces this alternative approach to judging the performance of algorithmic trading strategies.

[“Backtesting Base Class”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#rm\_base) presents the `BacktestingBase` class that can be flexibly used to test different types of directional trading strategies. The code has detailed comments on the important lines. This base class provides the following methods:

`get_date_price()`

For a given `bar` (index value for the `DataFrame` object containing the financial data), it returns the relevant `date` and `price`.

`print_balance()`

For a given `bar`, it prints the current (cash) balance of the trading bot.

`calculate_net_wealth()`

For a given `price`, it returns the net wealth composed of the current (cash) balance and the instrument position.

`print_net_wealth()`

For a given `bar`, it prints the net wealth of the trading bot.

`place_buy_order()`, `place_sell_order()`

For a given `bar` and a given number of `units` or a given `amount`, these methods place buy or sell orders and adjust relevant quantities accordingly (for example, accounting for transaction costs).

`close_out()`

At a given `bar`, this method closes open positions and calculates and reports performance statistics.

The following Python code illustrates how an instance of the `BacktestingBase` class functions based on some simple steps:

```
In [29]: import backtesting as bt

In [30]: bb = bt.BacktestingBase(env=agent.learn_env, model=agent.model,
                                 amount=10000, ptc=0.0001, ftc=1.0,
                                 verbose=True)  

In [31]: bb.initial_amount  
Out[31]: 10000

In [32]: bar = 100  

In [33]: bb.get_date_price(bar)  
Out[33]: ('2010-06-25', 1.2374)

In [34]: bb.env.get_state(bar)  
Out[34]:               EUR=       r       s       m      v
         Date
         2010-06-22 -0.0242 -0.5622 -0.0916 -0.2022 1.5316
         2010-06-23  0.0176  0.6940 -0.0939 -0.0915 1.5563
         2010-06-24  0.0354  0.3034 -0.0865  0.6391 1.0890

In [35]: bb.place_buy_order(bar, amount=5000)  
         2010-06-25 | buy 4040 units for 1.2374
         2010-06-25 | current balance = 4999.40

In [36]: bb.print_net_wealth(2 * bar)  
         2010-11-16 | net wealth = 10450.17

In [37]: bb.place_sell_order(2 * bar, units=1000)  
         2010-11-16 | sell 1000 units for 1.3492
         2010-11-16 | current balance = 6347.47

In [38]: bb.close_out(3 * bar)  
         ==================================================
         2011-04-11 | *** CLOSING OUT ***
         2011-04-11 | sell 3040 units for 1.4434
         2011-04-11 | current balance = 10733.97
         2011-04-11 | net performance [%] = 7.3397
         2011-04-11 | number of trades [#] = 3
         ==================================================
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO3-1)

Instantiates a `BacktestingBase` object

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO3-2)

Looks up the `initial_amount` attribute value

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO3-3)

Fixes a `bar` value

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO3-4)

Retrieves the `date` and `price` values for the `bar`

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO3-5)

Retrieves the state of the `Finance` environment for the `bar`

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO3-6)

Places a buy order using the `amount` parameter

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/7.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO3-7)

Prints the net wealth at a later point (`2 * bar`)

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/8.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO3-8)

Places a sell order at that later point using the `units` parameter

[![9](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/9.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO3-9)

Closes out the remaining long position even later (`3 * bar`)

Inheriting from the `BacktestingBase` class, the `TBBacktester` class implements the event-based backtesting for the trading bot:

```
In [39]: class TBBacktester(bt.BacktestingBase):
             def _reshape(self, state):
                 ''' Helper method to reshape state objects.
                 '''
                 return np.reshape(state, [1, self.env.lags, self.env.n_features])
             def backtest_strategy(self):
                 ''' Event-based backtesting of the trading bot's performance.
                 '''
                 self.units = 0
                 self.position = 0
                 self.trades = 0
                 self.current_balance = self.initial_amount
                 self.net_wealths = list()
                 for bar in range(self.env.lags, len(self.env.data)):
                     date, price = self.get_date_price(bar)
                     if self.trades == 0:
                         print(50 * '=')
                         print(f'{date} | *** START BACKTEST ***')
                         self.print_balance(bar)
                         print(50 * '=')
                     state = self.env.get_state(bar)  
                     action = np.argmax(self.model.predict(
                                 self._reshape(state.values))[0, 0])  
                     position = 1 if action == 1 else -1  
                     if self.position in [0, -1] and position == 1:  
                         if self.verbose:
                             print(50 * '-')
                             print(f'{date} | *** GOING LONG ***')
                         if self.position == -1:
                             self.place_buy_order(bar - 1, units=-self.units)
                         self.place_buy_order(bar - 1,
                                              amount=self.current_balance)
                         if self.verbose:
                             self.print_net_wealth(bar)
                         self.position = 1
                     elif self.position in [0, 1] and position == -1:  
                         if self.verbose:
                             print(50 * '-')
                             print(f'{date} | *** GOING SHORT ***')
                         if self.position == 1:
                             self.place_sell_order(bar - 1, units=self.units)
                         self.place_sell_order(bar - 1,
                                               amount=self.current_balance)
                         if self.verbose:
                             self.print_net_wealth(bar)
                         self.position = -1
                     self.net_wealths.append((date,
                                              self.calculate_net_wealth(price)))  
                 self.net_wealths = pd.DataFrame(self.net_wealths,
                                                 columns=['date', 'net_wealth'])  
                 self.net_wealths.set_index('date', inplace=True)  
                 self.net_wealths.index = pd.DatetimeIndex(
                                                 self.net_wealths.index)  
                 self.close_out(bar)
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO4-1)

Retrieves the state of the `Finance` environment

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO4-2)

Generates the optimal action (prediction) given the state and the `model` object

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO4-3)

Derives the optimal position (long/short) given the optimal action (prediction)

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO4-4)

Enters a _long_ position if the conditions are met

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO4-5)

Enters a _short_ position if the conditions are met

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO4-6)

Collects the net wealth values over time and transforms them into a `DataFrame` object

The application of the `TBBacktester` class is straightforward, given that the `Finance` and `TradingBot` instances are already available. The following code backtests the trading bot first on the _learning environment_ data—without and with transaction costs. [Figure 11-5](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#figure\_rm\_05) compares the two cases visually over time:

```
In [40]: env = learn_env

In [41]: tb = TBBacktester(env, agent.model, 10000,
                           0.0, 0, verbose=False)  

In [42]: tb.backtest_strategy()  
         ==================================================
         2010-02-05 | *** START BACKTEST ***
         2010-02-05 | current balance = 10000.00
         ==================================================
         ==================================================
         2017-01-12 | *** CLOSING OUT ***
         2017-01-12 | current balance = 14601.85
         2017-01-12 | net performance [%] = 46.0185
         2017-01-12 | number of trades [#] = 828
         ==================================================

In [43]: tb_ = TBBacktester(env, agent.model, 10000,
                            0.00012, 0.0, verbose=False)

In [44]: tb_.backtest_strategy()  
         ==================================================
         2010-02-05 | *** START BACKTEST ***
         2010-02-05 | current balance = 10000.00
         ==================================================
         ==================================================
         2017-01-12 | *** CLOSING OUT ***
         2017-01-12 | current balance = 13222.08
         2017-01-12 | net performance [%] = 32.2208
         2017-01-12 | number of trades [#] = 828
         ==================================================

In [45]: ax = tb.net_wealths.plot(figsize=(10, 6))
         tb_.net_wealths.columns = ['net_wealth (after tc)']
         tb_.net_wealths.plot(ax=ax);
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO5-1)

Event-based backtest in-sample _without_ transaction costs

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO5-3)

Event-based backtest in-sample _with_ transaction costs

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_1105.png" alt="aiif 1105" height="1394" width="2482"><figcaption></figcaption></figure>

**Figure 11-5. Gross performance of the trading bot before and after transaction costs (in-sample)**

[Figure 11-6](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#figure\_rm\_06) compares the gross performances of the trading bot for the _test environment_ data over time—again, before and after transaction costs:

```
In [46]: env = test_env

In [47]: tb = TBBacktester(env, agent.model, 10000,
                           0.0, 0, verbose=False)  

In [48]: tb.backtest_strategy()  
         ==================================================
         2018-01-17 | *** START BACKTEST ***
         2018-01-17 | current balance = 10000.00
         ==================================================
         ==================================================
         2019-12-31 | *** CLOSING OUT ***
         2019-12-31 | current balance = 10936.79
         2019-12-31 | net performance [%] = 9.3679
         2019-12-31 | number of trades [#] = 186
         ==================================================

In [49]: tb_ = TBBacktester(env, agent.model, 10000,
                            0.00012, 0.0, verbose=False)

In [50]: tb_.backtest_strategy()  
         ==================================================
         2018-01-17 | *** START BACKTEST ***
         2018-01-17 | current balance = 10000.00
         ==================================================
         ==================================================
         2019-12-31 | *** CLOSING OUT ***
         2019-12-31 | current balance = 10695.72
         2019-12-31 | net performance [%] = 6.9572
         2019-12-31 | number of trades [#] = 186
         ==================================================

In [51]: ax = tb.net_wealths.plot(figsize=(10, 6))
         tb_.net_wealths.columns = ['net_wealth (after tc)']
         tb_.net_wealths.plot(ax=ax);
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO6-1)

Event-based backtest out-of-sample _without_ transaction costs

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO6-3)

Event-based backtest out-of-sample _with_ transaction costs

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_1106.png" alt="aiif 1106" height="1428" width="2482"><figcaption></figcaption></figure>

**Figure 11-6. Gross performance of the trading bot before and after transaction costs (out-of-sample)**

How does the performance before transaction costs from the event-based backtesting compare to the performance from the vectorized backtesting? [Figure 11-7](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#figure\_rm\_07) shows the normalized net wealth compared to the gross performance over time. Due to the different technical approaches, the two time series are not exactly the same but are pretty similar. The performance difference can be mainly explained by the fact that the event-based backtesting assumes the same amount for every position taken. Vectorized backtesting takes compound effects into account, leading to a slightly higher reported performance:

```
In [52]: ax = (tb.net_wealths / tb.net_wealths.iloc[0]).plot(figsize=(10, 6))
         tp = env.data[['r', 's']].iloc[env.lags:].cumsum().apply(np.exp)
         (tp / tp.iloc[0]).plot(ax=ax);
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_1107.png" alt="aiif 1107" height="1428" width="2444"><figcaption></figcaption></figure>

**Figure 11-7. Gross performance of the passive benchmark investment and the trading bot (vectorized and event-based backtesting)**

## PERFORMANCE DIFFERENCES

The performance numbers from the vectorized and the event-based backtesting are close but not exactly the same. In the first case, it is assumed that financial instruments are perfectly divisible. Compounding is also done continuously. In the latter case, only full units of the financial instrument are accepted for trading, which is closer to reality. The net wealth calculations are based on price differences. The event-based code as it is used does not, for example, check whether the current balance is large enough to cover a certain trade by cash. This is for sure a simplifying assumption, and buying on margin, for instance, may not always be possible. Code adjustments in this regard are easily added to the `BacktestingBase` class.

## Assessing Risk

The implementation of risk measures requires the understanding of the risks involved in trading the chosen financial instrument. Therefore, to properly set parameters for risk measures, such as stop loss orders, an assessment of the risk of the underlying instrument is important. There are many approaches available to measure the risk of a financial instrument. There are, for example, _nondirected risk measures_, such as volatility or average true range (ATR). There are also _directed measures_, such as maximum drawdown or value-at-risk (VaR).

A common practice when setting target levels for stop loss (SL), trailing stop loss (TSL), or take profit orders (TP) is to relate such levels to ATR values.[1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#idm46319941838744) The following Python code calculates the ATR in absolute and relative terms for the financial instrument on which the trading bot is trained and backtested (that is, the EUR/USD exchange rate). The calculations rely on the data from the learning environment and use a typical window length of 14 days (bars). [Figure 11-8](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#figure\_rm\_08) shows the calculated values, which vary significantly over time:

```
In [53]: data = pd.DataFrame(learn_env.data[symbol])  

In [54]: data.head()  
Out[54]:              EUR=
         Date
         2010-02-02 1.3961
         2010-02-03 1.3898
         2010-02-04 1.3734
         2010-02-05 1.3662
         2010-02-08 1.3652

In [55]: window = 14  

In [56]: data['min'] = data[symbol].rolling(window).min()  

In [57]: data['max'] = data[symbol].rolling(window).max()  

In [58]: data['mami'] = data['max'] - data['min']  

In [59]: data['mac'] = abs(data['max'] - data[symbol].shift(1))  

In [60]: data['mic'] = abs(data['min'] - data[symbol].shift(1))  

In [61]: data['atr'] = np.maximum(data['mami'], data['mac'])  

In [62]: data['atr'] = np.maximum(data['atr'], data['mic'])  

In [63]: data['atr%'] = data['atr'] / data[symbol]  

In [64]: data[['atr', 'atr%']].plot(subplots=True, figsize=(10, 6));
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO7-1)

The instrument price column from the original `DataFrame` object

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO7-3)

The window length to be used for the calculations

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO7-4)

The rolling minimum

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO7-5)

The rolling maximum

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO7-6)

The difference between rolling maximum and minimum

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO7-7)

The absolute difference between rolling maximum and previous day’s price

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/7.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO7-8)

The absolute difference between rolling minimum and previous day’s price

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/8.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO7-9)

The maximum of the max-min difference and the max-price difference

[![9](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/9.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO7-10)

The maximum between the previous maximum and the min-price difference (= ATR)

[![10](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/10.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO7-11)

The ATR value in percent from the absolute ATR value and the price

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_1108.png" alt="aiif 1108" height="1394" width="2444"><figcaption></figcaption></figure>

**Figure 11-8. Average true range (ATR) in absolute (price) and relative (%) terms**

The code that follows displays the final values for ATR in absolute and relative terms. A typical rule would be to set, for example, the SL level at the entry price minus _x_ times ATR. Depending on the risk appetite of the trader or investor, _x_ might be smaller than 1 or larger. This is where human judgment or formal risk policies come into play. If _x = 1_, then the SL level is set at about 2% below the entry level:

```
In [65]: data[['atr', 'atr%']].tail()
Out[65]:               atr   atr%
         Date
         2017-01-06 0.0218 0.0207
         2017-01-09 0.0218 0.0206
         2017-01-10 0.0218 0.0207
         2017-01-11 0.0199 0.0188
         2017-01-12 0.0206 0.0194
```

However, _leverage_ plays an important role in this context. If a leverage of, say, 10 is used, which is actually quite low for foreign exchange trading, then the ATR numbers need to be multiplied by the leverage. As a consequence, for an assumed ATR factor of 1, the same SL level from before now is to be set at about 20% instead of just 2%. Or, when taking the median value of the ATR from the whole data set, it is set to be at about 25%:

```
In [66]: leverage = 10

In [67]: data[['atr', 'atr%']].tail() * leverage
Out[67]:               atr   atr%
         Date
         2017-01-06 0.2180 0.2070
         2017-01-09 0.2180 0.2062
         2017-01-10 0.2180 0.2066
         2017-01-11 0.1990 0.1881
         2017-01-12 0.2060 0.1942

In [68]: data[['atr', 'atr%']].median() * leverage
Out[68]: atr    0.3180
         atr%   0.2481
         dtype: float64
```

The basic idea behind relating SL or TP levels to ATR is that one should avoid setting them either too low or too high. Consider a 10 times leveraged position for which the ATR is 20%. Setting an SL level of only 3% or 5% might reduce the financial risk for the position, but it introduces the risk of a stop out that happens too early and that is due to typical movements in the financial instrument. Such “typical movements” within certain ranges are often called _noise_. The SL order should protect, in general, from unfavorable market movements that are larger than typical price movements (noise).

The same holds true for a take profit level. If it is set too high, say at three times the ATR level, decent profits might not be secured and positions might remain open for too long until they give up previous profits. Even if formal analyses and mathematical formulas can be used in this context, the setting of such target levels involves, as they say, more art than science. In a financial context, there is quite a degree of freedom for setting such target levels, and human judgment can come to the rescue. In other contexts, such as for AVs, this is different, as no human judgment is needed to instruct the AI to avoid any collisions with human beings.

## NONNORMALITY AND NONLINEARITY

A _margin stop out_ closes a trading position in cases when the margin, or the invested equity, is used up. Assume a leveraged trading position with a margin stop out in place. For a leverage of 10, for example, the margin is 10% equity. An _unfavorable_ move of 10% or larger in the traded instrument eats up all the equity and triggers the close out of the position—a loss of 100% of the equity. A _favorable_ move of the underlying of, say, 25% leads to a return on equity of 150%. Even if returns of the traded instrument are normally distributed, leverage and margin stop outs lead to nonnormally distributed returns and asymmetric, nonlinear relationships between the traded instrument and the trading position.

## Backtesting Risk Measures

Having an idea of the ATR of a financial instrument is often a good start for the implementation of risk measures. To be able to properly backtest the effect of the typical risk management orders, some adjustments to the `BacktestingBase` class are helpful. The following Python code presents a new base class—`BacktestBaseRM`, which inherits from `BacktestingBase`—that helps in tracking the entry price of the previous trade as well as the maximum and minimum prices since that trade. These values are used to calculate the relevant performance measures during the event-based backtesting to which SL, TSL, and TP orders relate:

```
#
# Event-Based Backtesting
# --Base Class (2)
#
# (c) Dr. Yves J. Hilpisch
#
from backtesting import *


class BacktestingBaseRM(BacktestingBase):

    def set_prices(self, price):
        ''' Sets prices for tracking of performance.
            To test for e.g. trailing stop loss hit.
        '''
        self.entry_price = price  
        self.min_price = price  
        self.max_price = price  

    def place_buy_order(self, bar, amount=None, units=None, gprice=None):
        ''' Places a buy order for a given bar and for
            a given amount or number of units.
        '''
        date, price = self.get_date_price(bar)
        if gprice is not None:
            price = gprice
        if units is None:
            units = int(amount / price)
        self.current_balance -= (1 + self.ptc) * units * price + self.ftc
        self.units += units
        self.trades += 1
        self.set_prices(price)  
        if self.verbose:
            print(f'{date} | buy {units} units for {price:.4f}')
            self.print_balance(bar)

    def place_sell_order(self, bar, amount=None, units=None, gprice=None):
        ''' Places a sell order for a given bar and for
            a given amount or number of units.
        '''
        date, price = self.get_date_price(bar)
        if gprice is not None:
            price = gprice
        if units is None:
            units = int(amount / price)
        self.current_balance += (1 - self.ptc) * units * price - self.ftc
        self.units -= units
        self.trades += 1
        self.set_prices(price)  
        if self.verbose:
            print(f'{date} | sell {units} units for {price:.4f}')
            self.print_balance(bar)
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO8-1)

Sets the _entry_ price for the most recent trade

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO8-2)

Sets the initial _minimum_ price since the most ecent trade

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO8-3)

Sets the initial _maximum_ price since the most recent trade

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO8-4)

Sets the relevant prices after a trade is executed

Based on this new base class, [“Backtesting Class”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#rm\_backtest) presents a new backtesting class, `TBBacktesterRM`, that allows the inclusion of SL, TSL, and TP orders. The relevant code parts are discussed in the following sub-sections. The parametrization of the backtesting examples orients itself roughly on an ATR level of about 2%, as calculated in the previous section.

## EUT AND RISK MEASURES

EUT, MVP, and the CAPM (see Chapters [3](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch03.html#normative\_finance) and [4](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch04.html#data\_driven\_finance)) assume that financial agents know about the future distribution of the returns of a financial instrument. MPT and the CAPM assume furthermore that returns are normally distributed and that there is, for example, a linear relationship between the market portfolio’s returns and the returns of a traded financial instrument. The use of SL, TSL, and TP orders leads—similar and in addition to leverage in combination with margin stop out—to a “guaranteed nonnormal” distribution and to highly asymmetric, nonlinear payoffs of a trading position in relation to the traded instrument.

### Stop Loss

The first risk measure is the SL order. It fixes a certain price level or, more often, a fixed percent value that triggers the closing of a position. For example, if the entry price for an unleveraged position is 100 and the SL level is set to 5%, then a long position is closed out at 95 while a short position is closed out at 105.

The following Python code is the relevant part of the `TBBacktesterRM` class that handles an SL order. For the SL order, the class allows one to specify whether the price level for the order is guaranteed or not.[2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#idm46319941449128) Working with guaranteed SL price levels might lead to too-optimistic performance results:

```
# stop loss order
if sl is not None and self.position != 0:  
    rc = (price - self.entry_price) / self.entry_price  
    if self.position == 1 and rc < -self.sl:  
        print(50 * '-')
        if guarantee:
            price = self.entry_price * (1 - self.sl)
            print(f'*** STOP LOSS (LONG  | {-self.sl:.4f}) ***')
        else:
            print(f'*** STOP LOSS (LONG  | {rc:.4f}) ***')
        self.place_sell_order(bar, units=self.units, gprice=price)  
        self.wait = wait  
        self.position = 0   
    elif self.position == -1 and rc > self.sl:  
        print(50 * '-')
        if guarantee:
            price = self.entry_price * (1 + self.sl)
            print(f'*** STOP LOSS (SHORT | -{self.sl:.4f}) ***')
        else:
            print(f'*** STOP LOSS (SHORT | -{rc:.4f}) ***')
        self.place_buy_order(bar, units=-self.units, gprice=price)  
        self.wait = wait  
        self.position = 0  
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO9-1)

Checks whether an SL is defined and whether the position is not neutral

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO9-2)

Calculates the performance based on the entry price for the last trade

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO9-3)

Checks whether an SL event is given for a _long_ position

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO9-4)

Closes the _long_ position, at either the current price or the guaranteed price level

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO9-5)

Sets the number of bars to wait before the next trade happens to `wait`

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO9-6)

Sets the position to neutral

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/7.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO9-7)

Checks whether an SL event is given for a _short_ position

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/8.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO9-8)

Closes the _short_ position, at either the current price or the guaranteed price level

The following Python code backtests the trading strategy of the trading bot without and with an SL order. For the given parametrization, the SL order has a negative impact on the strategy performance:

```
In [69]: import tbbacktesterrm as tbbrm

In [70]: env = test_env

In [71]: tb = tbbrm.TBBacktesterRM(env, agent.model, 10000,
                                   0.0, 0, verbose=False)  

In [72]: tb.backtest_strategy(sl=None, tsl=None, tp=None, wait=5)  
         ==================================================
         2018-01-17 | *** START BACKTEST ***
         2018-01-17 | current balance = 10000.00
         ==================================================
         ==================================================
         2019-12-31 | *** CLOSING OUT ***
         2019-12-31 | current balance = 10936.79
         2019-12-31 | net performance [%] = 9.3679
         2019-12-31 | number of trades [#] = 186
         ==================================================

In [73]: tb.backtest_strategy(sl=0.0175, tsl=None, tp=None,
                              wait=5, guarantee=False)  
         ==================================================
         2018-01-17 | *** START BACKTEST ***
         2018-01-17 | current balance = 10000.00
         ==================================================
         --------------------------------------------------
         *** STOP LOSS (SHORT | -0.0203) ***
         ==================================================
         2019-12-31 | *** CLOSING OUT ***
         2019-12-31 | current balance = 10717.32
         2019-12-31 | net performance [%] = 7.1732
         2019-12-31 | number of trades [#] = 188
         ==================================================

In [74]: tb.backtest_strategy(sl=0.017, tsl=None, tp=None,
                              wait=5, guarantee=True)  
         ==================================================
         2018-01-17 | *** START BACKTEST ***
         2018-01-17 | current balance = 10000.00
         ==================================================
         --------------------------------------------------
         *** STOP LOSS (SHORT | -0.0170) ***
         ==================================================
         2019-12-31 | *** CLOSING OUT ***
         2019-12-31 | current balance = 10753.52
         2019-12-31 | net performance [%] = 7.5352
         2019-12-31 | number of trades [#] = 188
         ==================================================
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO10-1)

Instantiates the backtesting class for risk management

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO10-2)

Backtests the trading bot performance without any risk measure

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO10-3)

Backtests the trading bot performance with an SL order (_no_ guarantee)

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO10-4)

Backtests the trading bot performance with an SL order (_with_ guarantee)

### Trailing Stop Loss

In contrast to a regular SL order, a TSL order is adjusted whenever a new high is observed after the base order has been placed. Assume the base order for an unleveraged long position has an entry price of 95 and the TSL is set to 5%. If the instrument price reaches 100 and falls back to 95, this implies a TSL event, and the position is closed at the entry price level. If the price reaches 110 and falls back to 104.5, this would imply another TSL event.

The following Python code is the relevant part of the `TBBacktesterRM` class that handles a TSL order. To handle such an order correctly, the maximum prices (highs) and the minimum prices (lows) need to be tracked. The maximum price is relevant for a long position, whereas the minimum price is relevant for a short position:

```
# trailing stop loss order
if tsl is not None and self.position != 0:
    self.max_price = max(self.max_price, price)  
    self.min_price = min(self.min_price, price)  
    rc_1 = (price - self.max_price) / self.entry_price  
    rc_2 = (self.min_price - price) / self.entry_price  
    if self.position == 1 and rc_1 < -self.tsl:  
        print(50 * '-')
        print(f'*** TRAILING SL (LONG  | {rc_1:.4f}) ***')
        self.place_sell_order(bar, units=self.units)
        self.wait = wait
        self.position = 0
    elif self.position == -1 and rc_2 < -self.tsl:  
        print(50 * '-')
        print(f'*** TRAILING SL (SHORT | {rc_2:.4f}) ***')
        self.place_buy_order(bar, units=-self.units)
        self.wait = wait
        self.position = 0
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO11-1)

Updates the _maximum_ price if necessary

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO11-2)

Updates the _minimum_ price if necessary

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO11-3)

Calculates the relevant performance for a _long_ position

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO11-4)

Calculates the relevant performance for a _short_ position

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO11-5)

Checks whether a TSL event is given for a _long_ position

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO11-6)

Checks whether a TSL event is given for a _short_ position

As the backtesting results that follow show, using a TSL order with the given parametrization reduces the gross performance compared to a strategy without a TSL order in place:

```
In [75]: tb.backtest_strategy(sl=None, tsl=0.015,
                              tp=None, wait=5)  
         ==================================================
         2018-01-17 | *** START BACKTEST ***
         2018-01-17 | current balance = 10000.00
         ==================================================
         --------------------------------------------------
         *** TRAILING SL (SHORT | -0.0152) ***
         --------------------------------------------------
         *** TRAILING SL (SHORT | -0.0169) ***
         --------------------------------------------------
         *** TRAILING SL (SHORT | -0.0164) ***
         --------------------------------------------------
         *** TRAILING SL (SHORT | -0.0191) ***
         --------------------------------------------------
         *** TRAILING SL (SHORT | -0.0166) ***
         --------------------------------------------------
         *** TRAILING SL (SHORT | -0.0194) ***
         --------------------------------------------------
         *** TRAILING SL (SHORT | -0.0172) ***
         --------------------------------------------------
         *** TRAILING SL (SHORT | -0.0181) ***
         --------------------------------------------------
         *** TRAILING SL (SHORT | -0.0153) ***
         --------------------------------------------------
         *** TRAILING SL (SHORT | -0.0160) ***
         ==================================================
         2019-12-31 | *** CLOSING OUT ***
         2019-12-31 | current balance = 10577.93
         2019-12-31 | net performance [%] = 5.7793
         2019-12-31 | number of trades [#] = 201
         ==================================================
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO12-1)

Backtests the trading bot performance with a TSL order

### Take Profit

Finally, there are TP orders. A TP order closes out a position that has reached a certain profit level. Say an unleveraged long position is opened at a price of 100 and the TP order is set to a level of 5%. If the price reaches 105, the position is closed.

The following code from the `TBBacktesterRM` class finally shows the part that handles a TP order. The TP implementation is straightforward, given the references of the SL and TSL order codes. For the TP order, there is also the option to backtest with a guaranteed price level as compared to the relevant high/low price levels, which would most probably lead to performance values that are too optimistic:[3](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#idm46319940378104)

```
# take profit order
if tp is not None and self.position != 0:
    rc = (price - self.entry_price) / self.entry_price
    if self.position == 1 and rc > self.tp:
        print(50 * '-')
        if guarantee:
            price = self.entry_price * (1 + self.tp)
            print(f'*** TAKE PROFIT (LONG  | {self.tp:.4f}) ***')
        else:
            print(f'*** TAKE PROFIT (LONG  | {rc:.4f}) ***')
        self.place_sell_order(bar, units=self.units, gprice=price)
        self.wait = wait
        self.position = 0
    elif self.position == -1 and rc < -self.tp:
        print(50 * '-')
        if guarantee:
            price = self.entry_price * (1 - self.tp)
            print(f'*** TAKE PROFIT (SHORT | {self.tp:.4f}) ***')
        else:
            print(f'*** TAKE PROFIT (SHORT | {-rc:.4f}) ***')
        self.place_buy_order(bar, units=-self.units, gprice=price)
        self.wait = wait
        self.position = 0
```

For the given parametrization, adding a TP order—without guarantee—improves the trading bot performance noticeably compared to the passive benchmark investment. This result might be too optimistic given the considerations from before. Therefore, the TP order with guarantee leads to a more realistic performance value in this case:

```
In [76]: tb.backtest_strategy(sl=None, tsl=None, tp=0.015,
                              wait=5, guarantee=False)  
         ==================================================
         2018-01-17 | *** START BACKTEST ***
         2018-01-17 | current balance = 10000.00
         ==================================================
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0155) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0155) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0204) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0240) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0168) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0156) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0183) ***
         ==================================================
         2019-12-31 | *** CLOSING OUT ***
         2019-12-31 | current balance = 11210.33
         2019-12-31 | net performance [%] = 12.1033
         2019-12-31 | number of trades [#] = 198
         ==================================================

In [77]: tb.backtest_strategy(sl=None, tsl=None, tp=0.015,
                              wait=5, guarantee=True)  
         ==================================================
         2018-01-17 | *** START BACKTEST ***
         2018-01-17 | current balance = 10000.00
         ==================================================
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0150) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0150) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0150) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0150) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0150) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0150) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0150) ***
         ==================================================
         2019-12-31 | *** CLOSING OUT ***
         2019-12-31 | current balance = 10980.86
         2019-12-31 | net performance [%] = 9.8086
         2019-12-31 | number of trades [#] = 198
         ==================================================
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO13-1)

Backtests the trading bot performance with a TP order (_no_ guarantee)

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO13-2)

Backtests the trading bot performance with a TP order (_with_ guarantee)

Of course, SL/TSL orders can also be combined with TP orders. The backtest results of the Python code that follows are in both cases worse than those for the strategy without the risk measures in place. In managing risk, there is hardly any free lunch:

```
In [78]: tb.backtest_strategy(sl=0.015, tsl=None,
                              tp=0.0185, wait=5)  
         ==================================================
         2018-01-17 | *** START BACKTEST ***
         2018-01-17 | current balance = 10000.00
         ==================================================
         --------------------------------------------------
         *** STOP LOSS (SHORT | -0.0203) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0202) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0213) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0240) ***
         --------------------------------------------------
         *** STOP LOSS (SHORT | -0.0171) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0188) ***
         --------------------------------------------------
         *** STOP LOSS (SHORT | -0.0153) ***
         --------------------------------------------------
         *** STOP LOSS (SHORT | -0.0154) ***
         ==================================================
         2019-12-31 | *** CLOSING OUT ***
         2019-12-31 | current balance = 10552.00
         2019-12-31 | net performance [%] = 5.5200
         2019-12-31 | number of trades [#] = 201
         ==================================================

In [79]: tb.backtest_strategy(sl=None, tsl=0.02,
                              tp=0.02, wait=5)  
         ==================================================
         2018-01-17 | *** START BACKTEST ***
         2018-01-17 | current balance = 10000.00
         ==================================================
         --------------------------------------------------
         *** TRAILING SL (SHORT | -0.0235) ***
         --------------------------------------------------
         *** TRAILING SL (SHORT | -0.0202) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0250) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0227) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0240) ***
         --------------------------------------------------
         *** TRAILING SL (SHORT | -0.0216) ***
         --------------------------------------------------
         *** TAKE PROFIT (SHORT | 0.0241) ***
         --------------------------------------------------
         *** TRAILING SL (SHORT | -0.0206) ***
         ==================================================
         2019-12-31 | *** CLOSING OUT ***
         2019-12-31 | current balance = 10346.38
         2019-12-31 | net performance [%] = 3.4638
         2019-12-31 | number of trades [#] = 198
         ==================================================
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO14-1)

Backtests the trading bot performance with an SL and TP order

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO14-2)

Backtests the trading bot performance with a TSL and TP order

## PERFORMANCE IMPACT

Risk measures have their reasoning and benefits. However, reducing risk may come at the price of lower overall performance. On the other hand, the backtesting example with the TP order shows performance improvements that can be explained by the fact that, given the ATR of a financial instrument, a certain profit level can be considered good enough to realize the profit. Any hope to see even higher profits typically is smashed by the market turning around again.

## Conclusions

This chapter has three main topics. It backtests the performance of a trading bot (that is, a trained deep Q-learning agent) out-of-sample in both vectorized and event-based fashion. It also assesses risks in the form of the average true range (ATR) indicator that measures the _typical_ variation in the price of the financial instrument of interest. Finally, the chapter discusses and backtests event-based typical risk measures in the form of stop loss (SL), trailing stop loss (TSL), and take profit (TP) orders.

Similar to autonomous vehicles (AVs), trading bots are hardly ever deployed based on the predictions of their AI only. To avoid large downside risks and to improve the (risk-adjusted) performance, risk measures usually come into play. Standard risk measures, as discussed in this chapter, are available on almost every trading platform, as well as for retail traders. The next chapter illustrates this in the context of the [Oanda](http://oanda.com/) trading platform. The event-based backtesting approach provides the algorithmic flexibility to properly backtest the effects of such risk measures. While “reducing risk” may sound appealing, the backtest results indicate that the reduction in risk often comes at a cost: the performance might be lower when compared to the pure strategy without any risk measures. However, when finely tuned, the results also show that TP orders, for example, can also have a positive effect on the performance.

## References

Books and papers cited in this chapter:

* Agrawal, Ajay, Joshua Gans, and Avi Goldfarb. 2018. _Prediction Machines: The Simple Economics of Artificial Intelligence._ Boston: Harvard Business Review Press.
* Hilpisch, Yves. 2020. _Python for Algorithmic Trading: From Idea to Cloud Deployment._ Sebastopol: O’Reilly.
* Khonji, Majid, Jorge Dias, and Lakmal Seneviratne. 2019. “Risk-Aware Reasoning for Autonomous Vehicles.” arXiv. October 6, 2019. [_https://oreil.ly/2Z6WR_](https://oreil.ly/2Z6WR).

## Python Code

### Finance Environment

The following is the Python module with the `Finance` environment class:

```
#
# Finance Environment
#
# (c) Dr. Yves J. Hilpisch
# Artificial Intelligence in Finance
#
import math
import random
import numpy as np
import pandas as pd


class observation_space:
    def __init__(self, n):
        self.shape = (n,)


class action_space:
    def __init__(self, n):
        self.n = n

    def sample(self):
        return random.randint(0, self.n - 1)


class Finance:
    intraday = False
    if intraday:
        url = 'http://hilpisch.com/aiif_eikon_id_eur_usd.csv'
    else:
        url = 'http://hilpisch.com/aiif_eikon_eod_data.csv'

    def __init__(self, symbol, features, window, lags,
                 leverage=1, min_performance=0.85, min_accuracy=0.5,
                 start=0, end=None, mu=None, std=None):
        self.symbol = symbol
        self.features = features
        self.n_features = len(features)
        self.window = window
        self.lags = lags
        self.leverage = leverage
        self.min_performance = min_performance
        self.min_accuracy = min_accuracy
        self.start = start
        self.end = end
        self.mu = mu
        self.std = std
        self.observation_space = observation_space(self.lags)
        self.action_space = action_space(2)
        self._get_data()
        self._prepare_data()

    def _get_data(self):
        self.raw = pd.read_csv(self.url, index_col=0,
                               parse_dates=True).dropna()
        if self.intraday:
            self.raw = self.raw.resample('30min', label='right').last()
            self.raw = pd.DataFrame(self.raw['CLOSE'])
            self.raw.columns = [self.symbol]

    def _prepare_data(self):
        self.data = pd.DataFrame(self.raw[self.symbol])
        self.data = self.data.iloc[self.start:]
        self.data['r'] = np.log(self.data / self.data.shift(1))
        self.data.dropna(inplace=True)
        self.data['s'] = self.data[self.symbol].rolling(self.window).mean()
        self.data['m'] = self.data['r'].rolling(self.window).mean()
        self.data['v'] = self.data['r'].rolling(self.window).std()
        self.data.dropna(inplace=True)
        if self.mu is None:
            self.mu = self.data.mean()
            self.std = self.data.std()
        self.data_ = (self.data - self.mu) / self.std
        self.data['d'] = np.where(self.data['r'] > 0, 1, 0)
        self.data['d'] = self.data['d'].astype(int)
        if self.end is not None:
            self.data = self.data.iloc[:self.end - self.start]
            self.data_ = self.data_.iloc[:self.end - self.start]

    def _get_state(self):
        return self.data_[self.features].iloc[self.bar -
                                              self.lags:self.bar]

    def get_state(self, bar):
        return self.data_[self.features].iloc[bar - self.lags:bar]

    def seed(self, seed):
        random.seed(seed)
        np.random.seed(seed)

    def reset(self):
        self.treward = 0
        self.accuracy = 0
        self.performance = 1
        self.bar = self.lags
        state = self.data_[self.features].iloc[self.bar -
                                               self.lags:self.bar]
        return state.values

    def step(self, action):
        correct = action == self.data['d'].iloc[self.bar]
        ret = self.data['r'].iloc[self.bar] * self.leverage
        reward_1 = 1 if correct else 0
        reward_2 = abs(ret) if correct else -abs(ret)
        self.treward += reward_1
        self.bar += 1
        self.accuracy = self.treward / (self.bar - self.lags)
        self.performance *= math.exp(reward_2)
        if self.bar >= len(self.data):
            done = True
        elif reward_1 == 1:
            done = False
        elif (self.performance < self.min_performance and
              self.bar > self.lags + 15):
            done = True
        elif (self.accuracy < self.min_accuracy and
              self.bar > self.lags + 15):
            done = True
        else:
            done = False
        state = self._get_state()
        info = {}
        return state.values, reward_1 + reward_2 * 5, done, info
```

### Trading Bot

The following is the Python module with the `TradingBot` class, based on a financial Q-learning agent:

```
#
# Financial Q-Learning Agent
#
# (c) Dr. Yves J. Hilpisch
# Artificial Intelligence in Finance
#
import os
import random
import numpy as np
from pylab import plt, mpl
from collections import deque
import tensorflow as tf
from keras.layers import Dense, Dropout
from keras.models import Sequential
from keras.optimizers import Adam, RMSprop

os.environ['PYTHONHASHSEED'] = '0'
plt.style.use('seaborn')
mpl.rcParams['savefig.dpi'] = 300
mpl.rcParams['font.family'] = 'serif'


def set_seeds(seed=100):
    ''' Function to set seeds for all
        random number generators.
    '''
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)


class TradingBot:
    def __init__(self, hidden_units, learning_rate, learn_env,
                 valid_env=None, val=True, dropout=False):
        self.learn_env = learn_env
        self.valid_env = valid_env
        self.val = val
        self.epsilon = 1.0
        self.epsilon_min = 0.1
        self.epsilon_decay = 0.99
        self.learning_rate = learning_rate
        self.gamma = 0.5
        self.batch_size = 128
        self.max_treward = 0
        self.averages = list()
        self.trewards = []
        self.performances = list()
        self.aperformances = list()
        self.vperformances = list()
        self.memory = deque(maxlen=2000)
        self.model = self._build_model(hidden_units,
                             learning_rate, dropout)

    def _build_model(self, hu, lr, dropout):
        ''' Method to create the DNN model.
        '''
        model = Sequential()
        model.add(Dense(hu, input_shape=(
            self.learn_env.lags, self.learn_env.n_features),
            activation='relu'))
        if dropout:
            model.add(Dropout(0.3, seed=100))
        model.add(Dense(hu, activation='relu'))
        if dropout:
            model.add(Dropout(0.3, seed=100))
        model.add(Dense(2, activation='linear'))
        model.compile(
            loss='mse',
            optimizer=RMSprop(lr=lr)
        )
        return model

    def act(self, state):
        ''' Method for taking action based on
            a) exploration
            b) exploitation
        '''
        if random.random() <= self.epsilon:
            return self.learn_env.action_space.sample()
        action = self.model.predict(state)[0, 0]
        return np.argmax(action)

    def replay(self):
        ''' Method to retrain the DNN model based on
            batches of memorized experiences.
        '''
        batch = random.sample(self.memory, self.batch_size)
        for state, action, reward, next_state, done in batch:
            if not done:
                reward += self.gamma * np.amax(
                    self.model.predict(next_state)[0, 0])
            target = self.model.predict(state)
            target[0, 0, action] = reward
            self.model.fit(state, target, epochs=1,
                           verbose=False)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def learn(self, episodes):
        ''' Method to train the DQL agent.
        '''
        for e in range(1, episodes + 1):
            state = self.learn_env.reset()
            state = np.reshape(state, [1, self.learn_env.lags,
                                       self.learn_env.n_features])
            for _ in range(10000):
                action = self.act(state)
                next_state, reward, done, info = self.learn_env.step(action)
                next_state = np.reshape(next_state,
                                        [1, self.learn_env.lags,
                                         self.learn_env.n_features])
                self.memory.append([state, action, reward,
                                    next_state, done])
                state = next_state
                if done:
                    treward = _ + 1
                    self.trewards.append(treward)
                    av = sum(self.trewards[-25:]) / 25
                    perf = self.learn_env.performance
                    self.averages.append(av)
                    self.performances.append(perf)
                    self.aperformances.append(
                        sum(self.performances[-25:]) / 25)
                    self.max_treward = max(self.max_treward, treward)
                    templ = 'episode: {:2d}/{} | treward: {:4d} | '
                    templ += 'perf: {:5.3f} | av: {:5.1f} | max: {:4d}'
                    print(templ.format(e, episodes, treward, perf,
                                       av, self.max_treward), end='\r')
                    break
            if self.val:
                self.validate(e, episodes)
            if len(self.memory) > self.batch_size:
                self.replay()
        print()

    def validate(self, e, episodes):
        ''' Method to validate the performance of the
            DQL agent.
        '''
        state = self.valid_env.reset()
        state = np.reshape(state, [1, self.valid_env.lags,
                                   self.valid_env.n_features])
        for _ in range(10000):
            action = np.argmax(self.model.predict(state)[0, 0])
            next_state, reward, done, info = self.valid_env.step(action)
            state = np.reshape(next_state, [1, self.valid_env.lags,
                                            self.valid_env.n_features])
            if done:
                treward = _ + 1
                perf = self.valid_env.performance
                self.vperformances.append(perf)
                if e % int(episodes / 6) == 0:
                    templ = 71 * '='
                    templ += '\nepisode: {:2d}/{} | VALIDATION | '
                    templ += 'treward: {:4d} | perf: {:5.3f} | eps: {:.2f}\n'
                    templ += 71 * '='
                    print(templ.format(e, episodes, treward,
                                       perf, self.epsilon))
                break


def plot_treward(agent):
    ''' Function to plot the total reward
        per training episode.
    '''
    plt.figure(figsize=(10, 6))
    x = range(1, len(agent.averages) + 1)
    y = np.polyval(np.polyfit(x, agent.averages, deg=3), x)
    plt.plot(x, agent.averages, label='moving average')
    plt.plot(x, y, 'r--', label='regression')
    plt.xlabel('episodes')
    plt.ylabel('total reward')
    plt.legend()


def plot_performance(agent):
    ''' Function to plot the financial gross
        performance per training episode.
    '''
    plt.figure(figsize=(10, 6))
    x = range(1, len(agent.performances) + 1)
    y = np.polyval(np.polyfit(x, agent.performances, deg=3), x)
    plt.plot(x, agent.performances[:], label='training')
    plt.plot(x, y, 'r--', label='regression (train)')
    if agent.val:
        y_ = np.polyval(np.polyfit(x, agent.vperformances, deg=3), x)
        plt.plot(x, agent.vperformances[:], label='validation')
        plt.plot(x, y_, 'r-.', label='regression (valid)')
    plt.xlabel('episodes')
    plt.ylabel('gross performance')
    plt.legend()
```

### Backtesting Base Class

The following is the Python module with the `BacktestingBase` class for event-based backtesting:

```
#
# Event-Based Backtesting
# --Base Class (1)
#
# (c) Dr. Yves J. Hilpisch
# Artificial Intelligence in Finance
#


class BacktestingBase:
    def __init__(self, env, model, amount, ptc, ftc, verbose=False):
        self.env = env  
        self.model = model  
        self.initial_amount = amount  
        self.current_balance = amount  
        self.ptc = ptc   
        self.ftc = ftc   
        self.verbose = verbose  
        self.units = 0  
        self.trades = 0  

    def get_date_price(self, bar):
        ''' Returns date and price for a given bar.
        '''
        date = str(self.env.data.index[bar])[:10]  
        price = self.env.data[self.env.symbol].iloc[bar]  
        return date, price

    def print_balance(self, bar):
        ''' Prints the current cash balance for a given bar.
        '''
        date, price = self.get_date_price(bar)
        print(f'{date} | current balance = {self.current_balance:.2f}')  

    def calculate_net_wealth(self, price):
        return self.current_balance + self.units * price  

    def print_net_wealth(self, bar):
        ''' Prints the net wealth for a given bar
            (cash + position).
        '''
        date, price = self.get_date_price(bar)
        net_wealth = self.calculate_net_wealth(price)
        print(f'{date} | net wealth = {net_wealth:.2f}')  

    def place_buy_order(self, bar, amount=None, units=None):
        ''' Places a buy order for a given bar and for
            a given amount or number of units.
        '''
        date, price = self.get_date_price(bar)
        if units is None:
            units = int(amount / price)  
            # units = amount / price  
        self.current_balance -= (1 + self.ptc) * \
            units * price + self.ftc  
        self.units += units  
        self.trades += 1  
        if self.verbose:
            print(f'{date} | buy {units} units for {price:.4f}')
            self.print_balance(bar)

    def place_sell_order(self, bar, amount=None, units=None):
        ''' Places a sell order for a given bar and for
            a given amount or number of units.
        '''
        date, price = self.get_date_price(bar)
        if units is None:
            units = int(amount / price)  
            # units = amount / price  
        self.current_balance += (1 - self.ptc) * \
            units * price - self.ftc  
        self.units -= units  
        self.trades += 1  
        if self.verbose:
            print(f'{date} | sell {units} units for {price:.4f}')
            self.print_balance(bar)

    def close_out(self, bar):
        ''' Closes out any open position at a given bar.
        '''
        date, price = self.get_date_price(bar)
        print(50 * '=')
        print(f'{date} | *** CLOSING OUT ***')
        if self.units < 0:
            self.place_buy_order(bar, units=-self.units)  
        else:
            self.place_sell_order(bar, units=self.units)  
        if not self.verbose:
            print(f'{date} | current balance = {self.current_balance:.2f}')
        perf = (self.current_balance / self.initial_amount - 1) * 100  
        print(f'{date} | net performance [%] = {perf:.4f}')
        print(f'{date} | number of trades [#] = {self.trades}')
        print(50 * '=')
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-1)

The relevant `Finance` environment

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-2)

The relevant DNN model (from the trading bot)

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-3)

The initial/current balance

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-5)

Proportional transaction costs

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-6)

Fixed transaction costs

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-7)

Whether the prints are verbose or not

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/7.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-8)

The initial number of units of the financial instrument traded

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/8.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-9)

The initial number of trades implemented

[![9](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/9.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-10)

The relevant _date_ given a certain bar

[![10](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/10.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-11)

The relevant _instrument price_ at a certain bar

[![11](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/11.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-12)

The output of the _date_ and _current balance_ for a certain bar

[![12](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/12.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-13)

The calculation of the _net wealth_ from the current balance and the instrument position

[![13](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/13.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-14)

The output of the _date_ and the _net wealth_ at a certain bar

[![14](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/14.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-15)

The number of units to be traded given the trade amount

[![15](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/15.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-17)

The impact of the trade and the associated costs on the current balance

[![16](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/16.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-18)

The adjustment of the number of units held

[![17](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/17.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-19)

The adjustment of the number of trades implemented

[![18](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/18.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-25)

The closing of a _short_ position…

[![19](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/19.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-26)

…or of a _long_ position

[![20](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/20.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#co\_risk\_management\_CO15-27)

The net performance given the initial amount and the final current balance

### Backtesting Class

The following is the Python module with the `TBBacktesterRM` class for event-based backtesting including risk measures (stop loss, trailing stop loss, take profit orders):

```
#
# Event-Based Backtesting
# --Trading Bot Backtester
# (incl. Risk Management)
#
# (c) Dr. Yves J. Hilpisch
#
import numpy as np
import pandas as pd
import backtestingrm as btr


class TBBacktesterRM(btr.BacktestingBaseRM):
    def _reshape(self, state):
        ''' Helper method to reshape state objects.
        '''
        return np.reshape(state, [1, self.env.lags, self.env.n_features])

    def backtest_strategy(self, sl=None, tsl=None, tp=None,
                          wait=5, guarantee=False):
        ''' Event-based backtesting of the trading bot's performance.
            Incl. stop loss, trailing stop loss and take profit.
        '''
        self.units = 0
        self.position = 0
        self.trades = 0
        self.sl = sl
        self.tsl = tsl
        self.tp = tp
        self.wait = 0
        self.current_balance = self.initial_amount
        self.net_wealths = list()
        for bar in range(self.env.lags, len(self.env.data)):
            self.wait = max(0, self.wait - 1)
            date, price = self.get_date_price(bar)
            if self.trades == 0:
                print(50 * '=')
                print(f'{date} | *** START BACKTEST ***')
                self.print_balance(bar)
                print(50 * '=')

            # stop loss order
            if sl is not None and self.position != 0:
                rc = (price - self.entry_price) / self.entry_price
                if self.position == 1 and rc < -self.sl:
                    print(50 * '-')
                    if guarantee:
                        price = self.entry_price * (1 - self.sl)
                        print(f'*** STOP LOSS (LONG  | {-self.sl:.4f}) ***')
                    else:
                        print(f'*** STOP LOSS (LONG  | {rc:.4f}) ***')
                    self.place_sell_order(bar, units=self.units, gprice=price)
                    self.wait = wait
                    self.position = 0
                elif self.position == -1 and rc > self.sl:
                    print(50 * '-')
                    if guarantee:
                        price = self.entry_price * (1 + self.sl)
                        print(f'*** STOP LOSS (SHORT | -{self.sl:.4f}) ***')
                    else:
                        print(f'*** STOP LOSS (SHORT | -{rc:.4f}) ***')
                    self.place_buy_order(bar, units=-self.units, gprice=price)
                    self.wait = wait
                    self.position = 0

            # trailing stop loss order
            if tsl is not None and self.position != 0:
                self.max_price = max(self.max_price, price)
                self.min_price = min(self.min_price, price)
                rc_1 = (price - self.max_price) / self.entry_price
                rc_2 = (self.min_price - price) / self.entry_price
                if self.position == 1 and rc_1 < -self.tsl:
                    print(50 * '-')
                    print(f'*** TRAILING SL (LONG  | {rc_1:.4f}) ***')
                    self.place_sell_order(bar, units=self.units)
                    self.wait = wait
                    self.position = 0
                elif self.position == -1 and rc_2 < -self.tsl:
                    print(50 * '-')
                    print(f'*** TRAILING SL (SHORT | {rc_2:.4f}) ***')
                    self.place_buy_order(bar, units=-self.units)
                    self.wait = wait
                    self.position = 0

            # take profit order
            if tp is not None and self.position != 0:
                rc = (price - self.entry_price) / self.entry_price
                if self.position == 1 and rc > self.tp:
                    print(50 * '-')
                    if guarantee:
                        price = self.entry_price * (1 + self.tp)
                        print(f'*** TAKE PROFIT (LONG  | {self.tp:.4f}) ***')
                    else:
                        print(f'*** TAKE PROFIT (LONG  | {rc:.4f}) ***')
                    self.place_sell_order(bar, units=self.units, gprice=price)
                    self.wait = wait
                    self.position = 0
                elif self.position == -1 and rc < -self.tp:
                    print(50 * '-')
                    if guarantee:
                        price = self.entry_price * (1 - self.tp)
                        print(f'*** TAKE PROFIT (SHORT | {self.tp:.4f}) ***')
                    else:
                        print(f'*** TAKE PROFIT (SHORT | {-rc:.4f}) ***')
                    self.place_buy_order(bar, units=-self.units, gprice=price)
                    self.wait = wait
                    self.position = 0

            state = self.env.get_state(bar)
            action = np.argmax(self.model.predict(
                self._reshape(state.values))[0, 0])
            position = 1 if action == 1 else -1
            if self.position in [0, -1] and position == 1 and self.wait == 0:
                if self.verbose:
                    print(50 * '-')
                    print(f'{date} | *** GOING LONG ***')
                if self.position == -1:
                    self.place_buy_order(bar - 1, units=-self.units)
                self.place_buy_order(bar - 1, amount=self.current_balance)
                if self.verbose:
                    self.print_net_wealth(bar)
                self.position = 1
            elif self.position in [0, 1] and position == -1 and self.wait == 0:
                if self.verbose:
                    print(50 * '-')
                    print(f'{date} | *** GOING SHORT ***')
                if self.position == 1:
                    self.place_sell_order(bar - 1, units=self.units)
                self.place_sell_order(bar - 1, amount=self.current_balance)
                if self.verbose:
                    self.print_net_wealth(bar)
                self.position = -1
            self.net_wealths.append((date, self.calculate_net_wealth(price)))
        self.net_wealths = pd.DataFrame(self.net_wealths,
                                        columns=['date', 'net_wealth'])
        self.net_wealths.set_index('date', inplace=True)
        self.net_wealths.index = pd.DatetimeIndex(self.net_wealths.index)
        self.close_out(bar)
```

[1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#idm46319941838744-marker) For more details on the ATR measure, see [ATR (1) Investopedia](https://oreil.ly/2sUsg) or [ATR (2) Investopedia](https://oreil.ly/zwrnO). The definition used in the Python code differs slightly from the one found in these references.

[2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#idm46319941449128-marker) A _guaranteed_ stop loss order might only be available in certain jurisdictions for certain groups of broker clients, such as retail investors/traders.

[3](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch11.html#idm46319940378104-marker) A take profit order has a fixed target price level. Therefore, it is unrealistic to use the high price of a time interval for a long position or the low price of the interval for a short position to calculate the realized profit.
