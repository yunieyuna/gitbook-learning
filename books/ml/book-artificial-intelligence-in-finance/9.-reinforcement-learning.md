# 9. Reinforcement Learning

## Chapter 9. Reinforcement Learning

> Like a human, our agents learn for themselves to achieve successful strategies that lead to the greatest long-term rewards. This paradigm of learning by trial-and-error, solely from rewards or punishments, is known as reinforcement learning.[1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#idm46319962547144)
>
> DeepMind (2016)

The learning algorithms applied in Chapters [7](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#dense\_networks) and [8](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#recurrent\_networks) fall into the category of _supervised learning_. These methods require that there is a data set available with features and labels that allows the algorithms to learn relationships between the features and labels to succeed at estimation or classification tasks. As the simple example in [Chapter 1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch01.html#artificial\_intelligence) illustrates, _reinforcement learning_ (RL) works differently. To begin with, there is no need for a comprehensive data set of features and labels to be given up front. The data is rather generated by the learning agent while interacting with the environment of interest. This chapter covers RL in some detail and introduces fundamental notions, as well as one of the most popular algorithms used in the field: _Q-learning_ (QL). Neural networks are not replaced by RL algorithms; they generally play an important role in this context as well.

[“Fundamental Notions”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#rl\_notions) explains fundamental notions in RL, such as environments, states, and agents. [“OpenAI Gym”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#rl\_oai\_gym) introduces the OpenAI Gym suite of RL environments of which the `CartPole` environment is used as an example. In this environment, which [Chapter 2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch02.html#superintelligence) introduces and discusses briefly, agents must learn how to balance a pole on a cart by moving the cart to the left or to the right. [“Monte Carlo Agent”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#rl\_mc\_agent) shows how to solve the `CartPole` problem by the use of dimensionality reduction and Monte Carlo simulation. Standard supervised learning algorithms such as DNNs are in general not suited to solve problems such as the `CartPole` one since they lack a notion of delayed reward. This problem is illustrated in [“Neural Network Agent”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#rl\_nn\_agent). [“DQL Agent”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#rl\_dql\_agent) discusses a QL agent that explicitly takes into account delayed rewards and is able to solve the `CartPole` problem. The same agent is applied in [“Simple Finance Gym”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#rl\_sf\_gym) to a simple financial market environment. Although the agent does not perform too well in this setting, the example shows that QL agents can also learn to trade and to become what is often called a _trading bot_. To improve the learning of QL agents, [“Better Finance Gym”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#rl\_bf\_gym) presents an improved financial market environment that, among other benefits, allows the use of more than one type of feature to describe the state of the environment. Based on this improved environment, [“FQL Agent”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#rl\_fql\_agent) introduces and applies an improved financial QL agent that performs better as a trading bot.

## Fundamental Notions

This section gives a brief overview of the fundamental notions in RL. Among them are the following:

Environment

The _environment_ defines the problem at hand. This can be a computer game to be played or a financial market to be traded in.

State

A _state_ subsumes all relevant parameters that describe the current state of the environment. In a computer game, this might be the whole screen with all its pixels. In a financial market, this might include current and historical price levels or financial indicators such as moving averages, macroeconomic variables, and so on.

Agent

The term _agent_ subsumes all elements of the RL algorithm that interacts with the environment and that learns from these interactions. In a gaming context, the agent might represent a player playing the game. In a financial context, the agent could represent a trader placing bets on rising or falling markets.

Action

An agent can choose one _action_ from a (limited) set of allowed actions. In a computer game, movements to the left or right might be allowed actions, whereas in a financial market, going long or short could be admissible actions.

Step

Given an action of an agent, the state of the environment is updated. One such update is generally called a _step_. The concept of a step is general enough to encompass both heterogeneous and homogeneous time intervals between two steps. While in computer games, real-time interaction with the game environment is simulated by rather short, homogeneous time intervals (“game clock”), a trading bot interacting with a financial market environment could take actions at longer, heterogeneous time intervals, for instance.

Reward

Depending on the action an agent chooses, a _reward_ (or penalty) is awarded. For a computer game, points are a typical reward. In a financial context, profit (or loss) is a standard reward (or penalty).

Target

The _target_ specifies what the agent tries to maximize. In a computer game, this in general is the score reached by the agent. For a financial trading bot, this might be the accumulated trading profit.

Policy

The _policy_ defines which action an agent takes given a certain state of the environment. Given a certain state of a computer game, represented by all the pixels that make up the current scene, the policy might specify that the agent chooses “move right” as the action. A trading bot that observes three price increases in a row might decide, according to its policy, to short the market.

Episode

An _episode_ is a set of steps from the initial state of the environment until success is achieved or failure is observed. In a game, this is from the start of the game until a win or loss. In the financial world, for example, this is from the beginning of the year to the end of the year or to bankruptcy.

Sutton and Barto (2018) provide a detailed introduction to the RL field. The book discusses the preceding notions in detail and illustrates them on the basis of a multitude of concrete examples. The following sections again choose a practical, implementation-oriented approach to RL. The examples discussed illustrate all of the preceding notions on the basis of Python code.

## OpenAI Gym

In most of the success stories as presented in [Chapter 2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch02.html#superintelligence), RL plays a dominant role. This has spurred widespread interest in RL as an algorithm. OpenAI is an organization that strives to facilitate research in AI in general and in RL in particular. OpenAI has developed and open sourced a suite of environments, called [`OpenAI Gym`](https://gym.openai.com/), that allows the training of RL agents via a standardized API.

Among the many environments, there is the [`CartPole`](https://oreil.ly/f6tAK) environment (or game) that simulates a classical RL problem. A pole is standing upright on a cart, and the goal is to learn a policy to balance the pole on the cart by moving the cart either to the right or to the left. The state of the environment is given by four parameters, describing the following physical measurements: cart position, cart velocity, pole angle, and pole velocity (at tip). [Figure 9-1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#figure\_rl\_cp) shows a visualization of the environment.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0901.png" alt="aiif 0901" width="640"><figcaption></figcaption></figure>

**Figure 9-1. CartPole environment of OpenAI Gym**

Consider the following Python code that instantiates an environment object for `CartPole` and inspects the _observation space_. The observation space is a model for the state of the environment:

```
In [1]: import os
        import math
        import random
        import numpy as np
        import pandas as pd
        from pylab import plt, mpl
        plt.style.use('seaborn')
        mpl.rcParams['savefig.dpi'] = 300
        mpl.rcParams['font.family'] = 'serif'
        np.set_printoptions(precision=4, suppress=True)
        os.environ['PYTHONHASHSEED'] = '0'
In [2]: import gym

In [3]: env = gym.make('CartPole-v0')  

In [4]: env.seed(100)  
        env.action_space.seed(100)  
Out[4]: [100]

In [5]: env.observation_space  
Out[5]: Box(4,)

In [6]: env.observation_space.low.astype(np.float16)  
Out[6]: array([-4.8  ,   -inf, -0.419,   -inf], dtype=float16)

In [7]: env.observation_space.high.astype(np.float16)  
Out[7]: array([4.8  ,   inf, 0.419,   inf], dtype=float16)

In [8]: state = env.reset()  

In [9]: state  
Out[9]: array([-0.0163,  0.0238, -0.0392, -0.0148])
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO1-1)

The environment object, with fixed seed values

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO1-4)

The observation space with minimal and maximal values

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO1-7)

Reset of the environment

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO1-8)

Initial state: cart position, cart velocity, pole angle, pole angular velocity

In the following environment, the allowed actions are described by the _action space_. In this case there are two, and they are represented by `0` (push cart to the left) and `1` (push cart to the right):

```
In [10]: env.action_space  
Out[10]: Discrete(2)

In [11]: env.action_space.n  
Out[11]: 2

In [12]: env.action_space.sample()  
Out[12]: 1

In [13]: env.action_space.sample()   
Out[13]: 0

In [14]: a = env.action_space.sample()  
         a  
Out[14]: 1

In [15]: state, reward, done, info = env.step(a)  
         state, reward, done, info  
Out[15]: (array([-0.0158,  0.2195, -0.0395, -0.3196]), 1.0, False, {})
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO2-1)

The action space

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO2-3)

Random actions sampled from the action space

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO2-7)

Step forward based on random action

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO2-8)

New state of the environment, reward, success/failure, additional information

As long as `done=False`, the agent is still in the game and can choose another action. Success is achieved when the agent reaches a total of 200 steps in a row or a total reward of 200 (reward of 1.0 per step). A failure is observed when the pole on the cart reaches a certain angle that would lead to the pole falling from the cart. In that case, `done=True` is returned.

A simple agent is one that follows a completely random policy: no matter what state is observed, the agent chooses a random action. This is what the following code implements. The number of steps the agent can go only depends in such a case on how lucky it is. No learning in the form of updating the policy is taking place:

```
In [16]: env.reset()
         for e in range(1, 200):
             a = env.action_space.sample()  
             state, reward, done, info = env.step(a) 
             print(f'step={e:2d} | state={state} | action={a} | reward={reward}')
             if done and (e + 1) < 200:  
                 print('*** FAILED ***')  
                 break
         step= 1 | state=[-0.0423  0.1982  0.0256 -0.2476] | action=1 | reward=1.0
         step= 2 | state=[-0.0383  0.0028  0.0206  0.0531] | action=0 | reward=1.0
         step= 3 | state=[-0.0383  0.1976  0.0217 -0.2331] | action=1 | reward=1.0
         step= 4 | state=[-0.0343  0.0022  0.017   0.0664] | action=0 | reward=1.0
         step= 5 | state=[-0.0343  0.197   0.0184 -0.2209] | action=1 | reward=1.0
         step= 6 | state=[-0.0304  0.0016  0.0139  0.0775] | action=0 | reward=1.0
         step= 7 | state=[-0.0303  0.1966  0.0155 -0.2107] | action=1 | reward=1.0
         step= 8 | state=[-0.0264  0.0012  0.0113  0.0868] | action=0 | reward=1.0
         step= 9 | state=[-0.0264  0.1962  0.013  -0.2023] | action=1 | reward=1.0
         step=10 | state=[-0.0224  0.3911  0.009  -0.4908] | action=1 | reward=1.0
         step=11 | state=[-0.0146  0.5861 -0.0009 -0.7807] | action=1 | reward=1.0
         step=12 | state=[-0.0029  0.7812 -0.0165 -1.0736] | action=1 | reward=1.0
         step=13 | state=[ 0.0127  0.9766 -0.0379 -1.3714] | action=1 | reward=1.0
         step=14 | state=[ 0.0323  1.1722 -0.0654 -1.6758] | action=1 | reward=1.0
         step=15 | state=[ 0.0557  0.9779 -0.0989 -1.4041] | action=0 | reward=1.0
         step=16 | state=[ 0.0753  0.7841 -0.127  -1.1439] | action=0 | reward=1.0
         step=17 | state=[ 0.0909  0.5908 -0.1498 -0.8936] | action=0 | reward=1.0
         step=18 | state=[ 0.1028  0.7876 -0.1677 -1.2294] | action=1 | reward=1.0
         step=19 | state=[ 0.1185  0.9845 -0.1923 -1.5696] | action=1 | reward=1.0
         step=20 | state=[ 0.1382  0.7921 -0.2237 -1.3425] | action=0 | reward=1.0
         *** FAILED ***

In [17]: done
Out[17]: True
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO3-1)

Random action policy

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO3-2)

Stepping forward one step

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO3-3)

Failure if less than 200 steps

## DATA THROUGH INTERACTION

Whereas in supervised learning the training, validation, and test data sets are assumed to exist before the training begins, in RL the agent generates its data itself by interacting with the environment. In many contexts, such as in games, this is a huge simplification. Consider the game of chess: instead of loading thousands of historical human-played chess games into a computer, an RL agent can generate thousands or millions of games itself by playing against another chess engine or another version of itself, for instance.

## Monte Carlo Agent

The `CartPole` problem does not necessarily require a full-fledged RL approach nor some neural network to be solved. This section presents a simple solution to the problem based on Monte Carlo simulation. To this end, a specific policy is defined that makes use of _dimensionality reduction_. In that case, the four parameters defining a state of the environment are collapsed, via a linear combination, into a single real-valued parameter.[2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#idm46319961483496) The following Python code implements this idea:

```
In [18]: np.random.seed(100)  

In [19]: weights = np.random.random(4) * 2 - 1  

In [20]: weights  
Out[20]: array([ 0.0868, -0.4433, -0.151 ,  0.6896])

In [21]: state = env.reset()  

In [22]: state  
Out[22]: array([-0.0347, -0.0103,  0.047 , -0.0315])

In [23]: s = np.dot(state, weights)  
         s  
Out[23]: -0.02725361929630797
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO4-1)

Random weights for fixed seed value

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO4-4)

Initial state of the environment

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO4-6)

Dot product of state and weights

The policy is then defined based on the sign of the single state parameter `s`:

```
In [24]: if s < 0:
             a = 0
         else:
             a = 1

In [25]: a
Out[25]: 0
```

This policy can then be used to play an episode of the `CartPole` game. Given the random nature of the weights applied, the results are in general not better than those of the random action policy of the previous section:

```
In [26]: def run_episode(env, weights):
             state = env.reset()
             treward = 0
             for _ in range(200):
                 s = np.dot(state, weights)
                 a = 0 if s < 0 else 1
                 state, reward, done, info = env.step(a)
                 treward += reward
                 if done:
                     break
             return treward

In [27]: run_episode(env, weights)
Out[27]: 41.0
```

Therefore, Monte Carlo simulation is applied to test a large number of different weights. The following code simulates a large number of weights, checks them for success or failure, and then chooses the weights that yield success:

```
In [28]: def set_seeds(seed=100):
             random.seed(seed)
             np.random.seed(seed)
             env.seed(seed)

In [29]: set_seeds()
         num_episodes = 1000

In [30]: besttreward = 0
         for e in range(1, num_episodes + 1):
             weights = np.random.rand(4) * 2 - 1  
             treward = run_episode(env, weights)  
             if treward > besttreward:  
                 besttreward = treward  
                 bestweights = weights  
                 if treward == 200:
                     print(f'SUCCESS | episode={e}')
                     break
                 print(f'UPDATE  | episode={e}')
         UPDATE  | episode=1
         UPDATE  | episode=2
         SUCCESS | episode=13

In [31]: weights
Out[31]: array([-0.4282,  0.7048,  0.95  ,  0.7697])
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO5-1)

Random weights.

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO5-2)

Total reward for these weights.

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO5-3)

Improvement observed?

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO5-4)

Replace best total reward.

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO5-5)

Replace best weights.

The `CartPole` problem is considered solved by an agent if the average total reward over 100 consecutive episodes is 195 or higher. As the following code demonstrates, this is indeed the case for the Monte Carlo agent:

```
In [32]: res = []
         for _ in range(100):
             treward = run_episode(env, weights)
             res.append(treward)
         res[:10]
Out[32]: [200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]

In [33]: sum(res) / len(res)
Out[33]: 200.0
```

This is, of course, a strong benchmark that other, more sophisticated approaches are up against.

## Neural Network Agent

The `CartPole` game can be cast into a classification setting as well. The state of an environment consists of four feature values. The correct action given the feature values is the label. By interacting with the environment, a neural network agent can collect a data set consisting of combinations of feature values and labels. Given this incrementally growing data set, a neural network can be trained to learn the correct action given a state of the environment. The neural network represents the policy in this case. The agent updates the policy based on new experiences.

First, some imports:

```
In [34]: import tensorflow as tf
         from keras.layers import Dense, Dropout
         from keras.models import Sequential
         from keras.optimizers import Adam, RMSprop
         from sklearn.metrics import accuracy_score
         Using TensorFlow backend.

In [35]: def set_seeds(seed=100):
             random.seed(seed)
             np.random.seed(seed)
             tf.random.set_seed(seed)
             env.seed(seed)
             env.action_space.seed(100)
```

Second is the `NNAgent` class that combines the major elements of the agent: the neural network model for the policy, choosing an action given the policy, updating the policy (training the neural network), and the learning process itself over a number of episodes. The agent uses both _exploration_ and _exploitation_ to choose an action. Exploration refers to a random action, independent of the current policy. Exploitation refers to an action as derived from the current policy. The idea is that some degree of exploration ensures a richer experience and thereby improved learning for the agent:

```
In [36]: class NNAgent:
             def __init__(self):
                 self.max = 0  
                 self.scores = list()
                 self.memory = list()
                 self.model = self._build_model()

             def _build_model(self):  
                 model = Sequential()
                 model.add(Dense(24, input_dim=4,
                                 activation='relu'))
                 model.add(Dense(1, activation='sigmoid'))
                 model.compile(loss='binary_crossentropy',
                               optimizer=RMSprop(lr=0.001))
                 return model

             def act(self, state):  
                 if random.random() <= 0.5:
                     return env.action_space.sample()
                 action = np.where(self.model.predict(
                     state, batch_size=None)[0, 0] > 0.5, 1, 0)
                 return action

             def train_model(self, state, action):  
                 self.model.fit(state, np.array([action,]),
                                epochs=1, verbose=False)

             def learn(self, episodes):  
                 for e in range(1, episodes + 1):
                     state = env.reset()
                     for _ in range(201):
                         state = np.reshape(state, [1, 4])
                         action = self.act(state)
                         next_state, reward, done, info = env.step(action)
                         if done:
                             score = _ + 1
                             self.scores.append(score)
                             self.max = max(score, self.max)  
                             print('episode: {:4d}/{} | score: {:3d} | max: {:3d}'
                                   .format(e, episodes, score, self.max), end='\r')
                             break
                         self.memory.append((state, action))
                         self.train_model(state, action)  
                         state = next_state
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO6-1)

The maximum total reward

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO6-2)

The DNN classification model for the policy

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO6-3)

The method to choose an action (exploration and exploitation)

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO6-4)

The method to update the policy (train the neural network)

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO6-5)

The method to learn from interacting with the environment

The neural network agent does not solve the problem for the configuration shown. The maximum total reward of 200 is not achieved even once:

```
In [37]: set_seeds(100)
         agent = NNAgent()

In [38]: episodes = 500

In [39]: agent.learn(episodes)
         episode:  500/500 | score:  11 | max:  44
In [40]: sum(agent.scores) / len(agent.scores)  
Out[40]: 13.682
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO7-1)

Average total reward over all episodes

Something seems to be missing with this approach. One major missing element is the idea of looking beyond the current state and action to be chosen. The approach implemented does not, by any means, take into account that success is only achieved when the agent survives 200 consecutive steps. Simply speaking, the agent avoids taking the wrong action but does not learn to win the game.

Analyzing the collected history of states (features) and actions (labels) reveals that the neural network reaches an accuracy of around 75%.

However, this does not translate into a winning policy as seen before:

```
In [41]: f = np.array([m[0][0] for m in agent.memory])  
         f  
Out[41]: array([[-0.0163,  0.0238, -0.0392, -0.0148],
                [-0.0158,  0.2195, -0.0395, -0.3196],
                [-0.0114,  0.0249, -0.0459, -0.0396],
                ...,
                [ 0.0603,  0.9682, -0.0852, -1.4595],
                [ 0.0797,  1.1642, -0.1144, -1.7776],
                [ 0.103 ,  1.3604, -0.15  , -2.1035]])

In [42]: l = np.array([m[1] for m in agent.memory])  
         l  
Out[42]: array([1, 0, 1, ..., 1, 1, 1])

In [43]: accuracy_score(np.where(agent.model.predict(f) > 0.5, 1, 0), l)
Out[43]: 0.7525626872733008
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO8-1)

Features (states) from all episodes

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO8-3)

Labels (actions) from all episodes

## DQL Agent

Q-learning (QL) is an algorithm that takes into account delayed rewards in addition to immediate rewards from an action. The algorithm is due to Watkins (1989) and Watkins and Dayan (1992) and is explained in detail in Sutton and Barto (2018, ch. 6). QL addresses the problem of looking beyond the immediate next reward as encountered with the neural network agent.

The algorithm works roughly as follows. There is an _action-value_ policy �, which assigns a value to every combination of a state and an action. The higher the value is, the better the action from the point of view of the agent will be. If the agent uses the policy � to choose an action, it selects the action with the highest value.

How is the value of an action derived? The value of an action is composed of its _direct reward_ and the _discounted value_ of the optimal action in the next state. The following is the formal expression:

�(��,��)=��+1+�max��(��+1,�)

Here, �� is the state at step (time) �, �� is the action taken at state ��, ��+1 is the direct reward of action ��, 0<�<1 is a discount factor, and max��(��+1,�) is the maximum delayed reward given the optimal action from the current policy �.

In a simple environment, with only a limited number of possible states, � can, for example, be represented by a _table_, listing for every state-action combination the corresponding value. However, in more interesting or complex settings, such as the `CartPole` environment, the number of states is too large for � to be written out comprehensively. Therefore, � is in general understood to be a _function_.

This is where neural networks come into play. In realistic settings and environments, a closed-form solution for the function � might not exist or might be too hard to derive, say, based on dynamic programming. Therefore, QL algorithms generally target _approximations_ only. Neural networks, with their universal approximation capabilities, are a natural choice to accomplish the approximation of �.

Another critical element of QL is _replay_. The QL agent replays a number of experiences (state-action combinations) to update the policy function � regularly. This can improve the learning considerably. Furthermore, the QL agent presented in the following—`DQLAgent`—also alternates between exploration and exploitation during the learning. The alternation is done in a systematic way in that the agent starts with exploration only—in the beginning it could not have learned anything—and slowly but steadily decreases the exploration rate � until it reaches a minimum level:[3](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#idm46319961119528)

```
In [44]: from collections import deque
         from keras.optimizers import Adam, RMSprop

In [45]: class DQLAgent:
             def __init__(self, gamma=0.95, hu=24, opt=Adam,
                    lr=0.001, finish=False):
                 self.finish = finish
                 self.epsilon = 1.0  
                 self.epsilon_min = 0.01  
                 self.epsilon_decay = 0.995  
                 self.gamma = gamma  
                 self.batch_size = 32  
                 self.max_treward = 0
                 self.averages = list()
                 self.memory = deque(maxlen=2000)  
                 self.osn = env.observation_space.shape[0]
                 self.model = self._build_model(hu, opt, lr)

             def _build_model(self, hu, opt, lr):
                 model = Sequential()
                 model.add(Dense(hu, input_dim=self.osn,
                                 activation='relu'))
                 model.add(Dense(hu, activation='relu'))
                 model.add(Dense(env.action_space.n, activation='linear'))
                 model.compile(loss='mse', optimizer=opt(lr=lr))
                 return model

             def act(self, state):
                 if random.random() <= self.epsilon:
                     return env.action_space.sample()
                 action = self.model.predict(state)[0]
                 return np.argmax(action)

             def replay(self):
                 batch = random.sample(self.memory, self.batch_size)  
                 for state, action, reward, next_state, done in batch:
                     if not done:
                         reward += self.gamma * np.amax(
                             self.model.predict(next_state)[0])  
                     target = self.model.predict(state)
                     target[0, action] = reward
                     self.model.fit(state, target, epochs=1,
                                    verbose=False)  
                 if self.epsilon > self.epsilon_min:
                     self.epsilon *= self.epsilon_decay  

             def learn(self, episodes):
                 trewards = []
                 for e in range(1, episodes + 1):
                     state = env.reset()
                     state = np.reshape(state, [1, self.osn])
                     for _ in range(5000):
                         action = self.act(state)
                         next_state, reward, done, info = env.step(action)
                         next_state = np.reshape(next_state,
                                                 [1, self.osn])
                         self.memory.append([state, action, reward,
                                              next_state, done])  
                         state = next_state
                         if done:
                             treward = _ + 1
                             trewards.append(treward)
                             av = sum(trewards[-25:]) / 25
                             self.averages.append(av)
                             self.max_treward = max(self.max_treward, treward)
                             templ = 'episode: {:4d}/{} | treward: {:4d} | '
                             templ += 'av: {:6.1f} | max: {:4d}'
                             print(templ.format(e, episodes, treward, av,
                                                self.max_treward), end='\r')
                             break
                     if av > 195 and self.finish:
                         break
                     if len(self.memory) > self.batch_size:
                         self.replay()  
             def test(self, episodes):
                 trewards = []
                 for e in range(1, episodes + 1):
                     state = env.reset()
                     for _ in range(5001):
                         state = np.reshape(state, [1, self.osn])
                         action = np.argmax(self.model.predict(state)[0])
                         next_state, reward, done, info = env.step(action)
                         state = next_state
                         if done:
                             treward = _ + 1
                             trewards.append(treward)
                             print('episode: {:4d}/{} | treward: {:4d}'
                                   .format(e, episodes, treward), end='\r')
                             break
                 return trewards
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO9-1)

Initial exploration rate

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO9-2)

Minimum exploration rate

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO9-3)

Decay rate for exploration rate

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO9-4)

Discount factor for delayed reward

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO9-5)

Batch size for replay

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO9-6)

`deque` collection for limited history

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/7.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO9-7)

Random selection of history batch for replay

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/8.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO9-8)

� value for state-action pair

[![9](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/9.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO9-9)

Update of the neural network for the new action-value pairs

[![10](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/10.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO9-10)

Update of the exploration rate

[![11](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/11.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO9-11)

Storing the new data

[![12](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/12.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO9-12)

Replay to update the policy based on past experiences

How does the QL agent perform? As the code that follows shows, it reaches a winning state for `CartPole` of a total reward of 200. [Figure 9-2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#figure\_rl\_01) shows the moving average of scores and how it increases over time, although not monotonically. To the contrary, the performance of the agent can significantly decrease at times, as [Figure 9-2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#figure\_rl\_01) shows. Among other things, the exploration that is taking place throughout leads to random actions that might not necessarily lead to good results in terms of total rewards but may lead to beneficial experiences for updating the policy network:

```
In [46]: episodes = 1000

In [47]: set_seeds(100)
         agent = DQLAgent(finish=True)

In [48]: agent.learn(episodes)
         episode:  400/1000 | treward:  200 | av:  195.4 | max:  200
In [49]: plt.figure(figsize=(10, 6))
         x = range(len(agent.averages))
         y = np.polyval(np.polyfit(x, agent.averages, deg=3), x)
         plt.plot(agent.averages, label='moving average')
         plt.plot(x, y, 'r--', label='trend')
         plt.xlabel('episodes')
         plt.ylabel('total reward')
         plt.legend();
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0902.png" alt="aiif 0902" height="1491" width="2497"><figcaption></figcaption></figure>

**Figure 9-2. Average total rewards of `DQLAgent` for `CartPole`**

Does the QL agent solve the `CartPole` problem? In this particular case, it does, given the definition of success by OpenAI Gym:

```
In [50]: trewards = agent.test(100)
         episode:  100/100 | treward:  200
In [51]: sum(trewards) / len(trewards)
Out[51]: 200.0
```

## Simple Finance Gym

To transfer the QL approach to the financial domain, this section provides a class that mimics an OpenAI Gym environment, but for a financial market as represented by financial time series data. The idea is that, similar to the `CartPole` environment, four historical prices represent the state of the financial market. An agent can decide, when presented with the state, whether to go long or to go short. In that case, the two environments are comparable since a state is given by four parameters and an agent can take two different actions.

To mimic the OpenAI Gym API, two helper classes are needed—one for the observation space, and one for the action space:

```
In [52]: class observation_space:
             def __init__(self, n):
                 self.shape = (n,)

In [53]: class action_space:
             def __init__(self, n):
                 self.n = n
             def seed(self, seed):
                 pass
             def sample(self):
                 return random.randint(0, self.n - 1)
```

The following Python code defines the `Finance` class. It retrieves end-of-day historical prices for a number of symbols. The major methods of the class are `.reset()` and `.step()`. The `.step()` method checks whether the right action has been taken, defines the reward accordingly, and checks for success or failure. A success is achieved when the agent is able to correctly trade through the whole data set. This can, of course, be defined differently (say, a success is achieved when the agent trades successfully for 1,000 steps). A failure is defined as an accuracy ratio of less than 50% (total rewards divided by total number of steps). However, this is only checked for after a certain number of steps to avoid the high initial variance of this metric:

```
In [54]: class Finance:
             url = 'http://hilpisch.com/aiif_eikon_eod_data.csv'
             def __init__(self, symbol, features):
                 self.symbol = symbol
                 self.features = features
                 self.observation_space = observation_space(4)
                 self.osn = self.observation_space.shape[0]
                 self.action_space = action_space(2)
                 self.min_accuracy = 0.475  
                 self._get_data()
                 self._prepare_data()
             def _get_data(self):
                 self.raw = pd.read_csv(self.url, index_col=0,
                                        parse_dates=True).dropna()
             def _prepare_data(self):
                 self.data = pd.DataFrame(self.raw[self.symbol])
                 self.data['r'] = np.log(self.data / self.data.shift(1))
                 self.data.dropna(inplace=True)
                 self.data = (self.data - self.data.mean()) / self.data.std()
                 self.data['d'] = np.where(self.data['r'] > 0, 1, 0)
             def _get_state(self):
                 return self.data[self.features].iloc[
                     self.bar - self.osn:self.bar].values  
             def seed(self, seed=None):
                 pass
             def reset(self):  
                 self.treward = 0
                 self.accuracy = 0
                 self.bar = self.osn
                 state = self.data[self.features].iloc[
                     self.bar - self.osn:self.bar]
                 return state.values
             def step(self, action):
                 correct = action == self.data['d'].iloc[self.bar]  
                 reward = 1 if correct else 0  
                 self.treward += reward  
                 self.bar += 1  
                 self.accuracy = self.treward / (self.bar - self.osn)  
                 if self.bar >= len(self.data):  
                     done = True
                 elif reward == 1:  
                     done = False
                 elif (self.accuracy < self.min_accuracy and
                       self.bar > self.osn + 10):  
                     done = True
                 else:  
                     done = False
                 state = self._get_state()
                 info = {}
                 return state, reward, done, info
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO10-1)

Defines the minimum accuracy required.

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO10-2)

Selects the data defining the state of the financial market.

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO10-3)

Resets the environment to its initial values.

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO10-4)

Checks whether the agent has chosen the right action (successful trade).

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO10-5)

Defines the reward the agent receives.

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO10-6)

Adds the reward to the total reward.

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/7.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO10-7)

Moves the environment one step forward.

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/8.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO10-8)

Calculates the accuracy of successful actions (trades) given all steps (trades).

[![9](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/9.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO10-9)

If the agent reaches the end of the data set, success is achieved.

[![10](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/10.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO10-10)

If the agent takes the right action, it can move on.

[![11](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/11.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO10-11)

If, after some initial steps, the accuracy drops under the minimum level, the episode ends (failure).

[![12](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/12.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO10-12)

For the remaining cases, the agent can move on.

Instances of the `Finance` class behave like an environment of the OpenAI Gym. In particular, in this base case, the instance behaves exactly like the `CartPole` environment:

```
In [55]: env = Finance('EUR=', 'EUR=')  

In [56]: env.reset()
Out[56]: array([1.819 , 1.8579, 1.7749, 1.8579])

In [57]: a = env.action_space.sample()
         a
Out[57]: 0

In [58]: env.step(a)
Out[58]: (array([1.8579, 1.7749, 1.8579, 1.947 ]), 0, False, {})
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO11-1)

Specifies which symbol and which type of feature (symbol or log return) to be used to define the data representing the state

Can the `DQLAgent`, as developed for the `CartPole` game, learn to trade in a financial market? Yes, it can, as the following code illustrates. However, although the agent improves its trading skill (on average) over the training episodes, the results are not too impressive (see [Figure 9-3](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#figure\_rl\_02)):

```
In [59]: set_seeds(100)
         agent = DQLAgent(gamma=0.5, opt=RMSprop)

In [60]: episodes = 1000

In [61]: agent.learn(episodes)
         episode: 1000/1000 | treward: 2511 | av: 1012.7 | max: 2511
In [62]: agent.test(3)
         episode:    3/3 | treward: 2511
Out[62]: [2511, 2511, 2511]

In [63]: plt.figure(figsize=(10, 6))
         x = range(len(agent.averages))
         y = np.polyval(np.polyfit(x, agent.averages, deg=3), x)
         plt.plot(agent.averages, label='moving average')
         plt.plot(x, y, 'r--', label='regression')
         plt.xlabel('episodes')
         plt.ylabel('total reward')
         plt.legend();
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0903.png" alt="aiif 0903" height="1491" width="2523"><figcaption></figcaption></figure>

**Figure 9-3. Average total rewards of `DQLAgent` for `Finance`**

## GENERAL RL AGENTS

This section provides a class for a financial market environment that mimics the API of an OpenAI Gym environment. It also applies, without any changes to the agent itself, the QL agent to the new financial market environment. Although the performance of the agent in this new environment might not be impressive, it illustrates that the approach of RL, as introduced in this chapter, is rather general. RL agents can in general learn from different environments they interact with. This explains to some extent why AlphaZero from DeepMind is able to master not only the game of Go but also chess and shogi, as discussed in [Chapter 2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch02.html#superintelligence).

## Better Finance Gym

The idea in the previous section is to develop a simple class that allows RL within a financial market setting. The major goal in that section is to replicate the API of an OpenAI Gym environment. However, there is no need to restrict such an environment to a single type of feature to describe the state of the financial market nor to use only four lags. This section introduces an improved `Finance` class that allows for multiple features, a flexible number of lags, and specific start and end points for the base data set used. This, among other things, allows the use of one part of the data set for learning and another one for validation or testing. The Python code presented in the following also allows the use of leverage. This might be helpful when intraday data is considered with relatively small absolute returns:

```
In [64]: class Finance:
             url = 'http://hilpisch.com/aiif_eikon_eod_data.csv'
             def __init__(self, symbol, features, window, lags,
                          leverage=1, min_performance=0.85,
                          start=0, end=None, mu=None, std=None):
                 self.symbol = symbol
                 self.features = features  
                 self.n_features = len(features)
                 self.window = window
                 self.lags = lags  
                 self.leverage = leverage  
                 self.min_performance = min_performance  
                 self.start = start
                 self.end = end
                 self.mu = mu
                 self.std = std
                 self.observation_space = observation_space(self.lags)
                 self.action_space = action_space(2)
                 self._get_data()
                 self._prepare_data()
             def _get_data(self):
                 self.raw = pd.read_csv(self.url, index_col=0,
                                        parse_dates=True).dropna()
             def _prepare_data(self):
                 self.data = pd.DataFrame(self.raw[self.symbol])
                 self.data = self.data.iloc[self.start:]
                 self.data['r'] = np.log(self.data / self.data.shift(1))
                 self.data.dropna(inplace=True)
                 self.data['s'] = self.data[self.symbol].rolling(
                                                       self.window).mean()   
                 self.data['m'] = self.data['r'].rolling(self.window).mean()  
                 self.data['v'] = self.data['r'].rolling(self.window).std()  
                 self.data.dropna(inplace=True)
                 if self.mu is None:
                     self.mu = self.data.mean()  
                     self.std = self.data.std()  
                 self.data_ = (self.data - self.mu) / self.std  
                 self.data_['d'] = np.where(self.data['r'] > 0, 1, 0)
                 self.data_['d'] = self.data_['d'].astype(int)
                 if self.end is not None:
                     self.data = self.data.iloc[:self.end - self.start]
                     self.data_ = self.data_.iloc[:self.end - self.start]
             def _get_state(self):
                 return self.data_[self.features].iloc[self.bar -
                                         self.lags:self.bar]
             def seed(self, seed):
                 random.seed(seed)
                 np.random.seed(seed)
             def reset(self):
                 self.treward = 0
                 self.accuracy = 0
                 self.performance = 1
                 self.bar = self.lags
                 state = self.data_[self.features].iloc[self.bar-
                                 self.lags:self.bar]
                 return state.values
             def step(self, action):
                 correct = action == self.data_['d'].iloc[self.bar]
                 ret = self.data['r'].iloc[self.bar] * self.leverage  
                 reward_1 = 1 if correct else 0
                 reward_2 = abs(ret) if correct else -abs(ret)  
                 self.treward += reward_1
                 self.bar += 1
                 self.accuracy = self.treward / (self.bar - self.lags)
                 self.performance *= math.exp(reward_2)  
                 if self.bar >= len(self.data):
                     done = True
                 elif reward_1 == 1:
                     done = False
                 elif (self.performance < self.min_performance and
                       self.bar > self.lags + 5):
                     done = True
                 else:
                     done = False
                 state = self._get_state()
                 info = {}
                 return state.values, reward_1 + reward_2 * 5, done, info
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO12-1)

The features to define the state

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO12-3)

The number of lags to be used

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO12-4)

The minimum gross performance required

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO12-5)

Additional financial features (simple moving average, momentum, rolling volatility)

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO12-8)

Gaussian normalization of the data

[![6](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/6.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO12-11)

The leveraged return for the step

[![7](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/7.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO12-12)

The return-based reward for the step

[![8](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/8.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#co\_reinforcement\_learning\_CO12-13)

The gross performance after the step

The new `Finance` class gives more flexibility for the modeling of the financial market environment. The following code shows an example for two features and five lags:

```
In [65]: env = Finance('EUR=', ['EUR=', 'r'], 10, 5)

In [66]: a = env.action_space.sample()
         a
Out[66]: 0

In [67]: env.reset()
Out[67]: array([[ 1.7721, -1.0214],
                [ 1.5973, -2.4432],
                [ 1.5876, -0.1208],
                [ 1.6292,  0.6083],
                [ 1.6408,  0.1807]])

In [68]: env.step(a)
Out[68]: (array([[ 1.5973, -2.4432],
                 [ 1.5876, -0.1208],
                 [ 1.6292,  0.6083],
                 [ 1.6408,  0.1807],
                 [ 1.5725, -0.9502]]),
          1.0272827803740798,
          False,
          {})
```

**DIFFERENT TYPES OF ENVIRONMENTS AND DATA**

It is important to notice that there is a fundamental difference between the `CartPole` environment and the two versions of the `Finance` environment. In the `CartPole` environment, no data is available up front. Only an initial state is chosen with some degree of randomness. Given this state and the action taken by an agent, deterministic transformations are applied to generate new states (data). This is possible since a physical system is simulated that follows physical laws.

The `Finance` environment, on the other hand, starts with real, historical market data and only presents the available data to the agent in similar fashion as the `CartPole` environment (that is, step by step and state by state). In this case, the action of the agent does not really influence the environment; the environment instead evolves deterministically, and the agent learns how to behave optimally—trade profitably—in that environment.

In that sense, the `Finance` environment is more comparable, say, to the problem of finding the fastest way through a labyrinth. In such a case, the data representing the labyrinth is given up front and the agent is only presented with the relevant sub-set of the data (the current state) as it moves around the labyrinth.

## FQL Agent

Relying on the new `Finance` environment, this section improves on the simple DQL agent to improve the performance in the financial market context. The `FQLAgent` class is able to handle multiple features and a flexible number of lags. It also distinguishes the learning environment (`learn_env`) from the validation environment (`valid_env`). This allows one to gain a more realistic picture of the out-of-sample performance of the agent during training. The basic structure of the class and the RL/QL learning approach is the same for both the `DQLAgent` class and the `FQLAgent` class:

```
In [69]: class FQLAgent:
             def __init__(self, hidden_units, learning_rate, learn_env, valid_env):
                 self.learn_env = learn_env
                 self.valid_env = valid_env
                 self.epsilon = 1.0
                 self.epsilon_min = 0.1
                 self.epsilon_decay = 0.98
                 self.learning_rate = learning_rate
                 self.gamma = 0.95
                 self.batch_size = 128
                 self.max_treward = 0
                 self.trewards = list()
                 self.averages = list()
                 self.performances = list()
                 self.aperformances = list()
                 self.vperformances = list()
                 self.memory = deque(maxlen=2000)
                 self.model = self._build_model(hidden_units, learning_rate)

             def _build_model(self, hu, lr):
                 model = Sequential()
                 model.add(Dense(hu, input_shape=(
                     self.learn_env.lags, self.learn_env.n_features),
                                 activation='relu'))
                 model.add(Dropout(0.3, seed=100))
                 model.add(Dense(hu, activation='relu'))
                 model.add(Dropout(0.3, seed=100))
                 model.add(Dense(2, activation='linear'))
                 model.compile(
                     loss='mse',
                     optimizer=RMSprop(lr=lr)
                 )
                 return model

             def act(self, state):
                 if random.random() <= self.epsilon:
                     return self.learn_env.action_space.sample()
                 action = self.model.predict(state)[0, 0]
                 return np.argmax(action)

             def replay(self):
                 batch = random.sample(self.memory, self.batch_size)
                 for state, action, reward, next_state, done in batch:
                     if not done:
                         reward += self.gamma * np.amax(
                             self.model.predict(next_state)[0, 0])
                     target = self.model.predict(state)
                     target[0, 0, action] = reward
                     self.model.fit(state, target, epochs=1,
                                    verbose=False)
                 if self.epsilon > self.epsilon_min:
                     self.epsilon *= self.epsilon_decay

             def learn(self, episodes):
                 for e in range(1, episodes + 1):
                     state = self.learn_env.reset()
                     state = np.reshape(state, [1, self.learn_env.lags,
                                                self.learn_env.n_features])
                     for _ in range(10000):
                         action = self.act(state)
                         next_state, reward, done, info = \
                                         self.learn_env.step(action)
                         next_state = np.reshape(next_state,
                                         [1, self.learn_env.lags,
                                          self.learn_env.n_features])
                         self.memory.append([state, action, reward,
                                              next_state, done])
                         state = next_state
                         if done:
                             treward = _ + 1
                             self.trewards.append(treward)
                             av = sum(self.trewards[-25:]) / 25
                             perf = self.learn_env.performance
                             self.averages.append(av)
                             self.performances.append(perf)
                             self.aperformances.append(
                                 sum(self.performances[-25:]) / 25)
                             self.max_treward = max(self.max_treward, treward)
                             templ = 'episode: {:2d}/{} | treward: {:4d} | '
                             templ += 'perf: {:5.3f} | av: {:5.1f} | max: {:4d}'
                             print(templ.format(e, episodes, treward, perf,
                                           av, self.max_treward), end='\r')
                             break
                     self.validate(e, episodes)
                     if len(self.memory) > self.batch_size:
                         self.replay()
             def validate(self, e, episodes):
                 state = self.valid_env.reset()
                 state = np.reshape(state, [1, self.valid_env.lags,
                                            self.valid_env.n_features])
                 for _ in range(10000):
                     action = np.argmax(self.model.predict(state)[0, 0])
                     next_state, reward, done, info = self.valid_env.step(action)
                     state = np.reshape(next_state, [1, self.valid_env.lags,
                                            self.valid_env.n_features])
                     if done:
                         treward = _ + 1
                         perf = self.valid_env.performance
                         self.vperformances.append(perf)
                         if e % 20 == 0:
                             templ = 71 * '='
                             templ += '\nepisode: {:2d}/{} | VALIDATION | '
                             templ += 'treward: {:4d} | perf: {:5.3f} | '
                             templ += 'eps: {:.2f}\n'
                             templ += 71 * '='
                             print(templ.format(e, episodes, treward,
                                                perf, self.epsilon))
                         break
```

The following Python code shows that the performance of the `FQLAgent` is substantially better than that of the simple `DQLAgent` that solves the `CartPole` problem. This trading bot seems to learn about trading rather consistently through interacting with the financial market environment (see [Figure 9-4](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#figure\_rl\_03)):

```
In [70]: symbol = 'EUR='
         features = [symbol, 'r', 's', 'm', 'v']

In [71]: a = 0
         b = 2000
         c = 500

In [72]: learn_env = Finance(symbol, features, window=10, lags=6,
                          leverage=1, min_performance=0.85,
                          start=a, end=a + b, mu=None, std=None)

In [73]: learn_env.data.info()
         <class 'pandas.core.frame.DataFrame'>
         DatetimeIndex: 2000 entries, 2010-01-19 to 2017-12-26
         Data columns (total 5 columns):
          #   Column  Non-Null Count  Dtype
         ---  ------  --------------  -----
          0   EUR=    2000 non-null   float64
          1   r       2000 non-null   float64
          2   s       2000 non-null   float64
          3   m       2000 non-null   float64
          4   v       2000 non-null   float64
         dtypes: float64(5)
         memory usage: 93.8 KB

In [74]: valid_env = Finance(symbol, features, window=learn_env.window,
                          lags=learn_env.lags, leverage=learn_env.leverage,
                          min_performance=learn_env.min_performance,
                          start=a + b, end=a + b + c,
                          mu=learn_env.mu, std=learn_env.std)

In [75]: valid_env.data.info()
         <class 'pandas.core.frame.DataFrame'>
         DatetimeIndex: 500 entries, 2017-12-27 to 2019-12-20
         Data columns (total 5 columns):
          #   Column  Non-Null Count  Dtype
         ---  ------  --------------  -----
          0   EUR=    500 non-null    float64
          1   r       500 non-null    float64
          2   s       500 non-null    float64
          3   m       500 non-null    float64
          4   v       500 non-null    float64
         dtypes: float64(5)
         memory usage: 23.4 KB

In [76]: set_seeds(100)
         agent = FQLAgent(24, 0.0001, learn_env, valid_env)

In [77]: episodes = 61

In [78]: agent.learn(episodes)
         =======================================================================
         episode: 20/61 | VALIDATION | treward:  494 | perf: 1.169 | eps: 0.68
         =======================================================================
         =======================================================================
         episode: 40/61 | VALIDATION | treward:  494 | perf: 1.111 | eps: 0.45
         =======================================================================
         =======================================================================
         episode: 60/61 | VALIDATION | treward:  494 | perf: 1.089 | eps: 0.30
         =======================================================================
         episode: 61/61 | treward: 1994 | perf: 1.268 | av: 1615.1 | max: 1994
In [79]: agent.epsilon
Out[79]: 0.291602079838278

In [80]: plt.figure(figsize=(10, 6))
         x = range(1, len(agent.averages) + 1)
         y = np.polyval(np.polyfit(x, agent.averages, deg=3), x)
         plt.plot(agent.averages, label='moving average')
         plt.plot(x, y, 'r--', label='regression')
         plt.xlabel('episodes')
         plt.ylabel('total reward')
         plt.legend();
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0904.png" alt="aiif 0904" height="1491" width="2523"><figcaption></figcaption></figure>

**Figure 9-4. Average total rewards of `FQLAgent` for `Finance`**

An interesting picture also arises for the training and validation performances, as shown in [Figure 9-5](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#figure\_rl\_04). The training performance shows a high variance, which is due, for example, to the exploration that is going on in addition to the exploitation of the currently optimal policy. In comparison, the validation performance has a much lower variance because it only relies on the exploitation of the currently optimal policy:

```
In [81]: plt.figure(figsize=(10, 6))
         x = range(1, len(agent.performances) + 1)
         y = np.polyval(np.polyfit(x, agent.performances, deg=3), x)
         y_ = np.polyval(np.polyfit(x, agent.vperformances, deg=3), x)
         plt.plot(agent.performances[:], label='training')
         plt.plot(agent.vperformances[:], label='validation')
         plt.plot(x, y, 'r--', label='regression (train)')
         plt.plot(x, y_, 'r-.', label='regression (valid)')
         plt.xlabel('episodes')
         plt.ylabel('gross performance')
         plt.legend();
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0905.png" alt="aiif 0905" height="1491" width="2484"><figcaption></figcaption></figure>

**Figure 9-5. Training and validation performance of the `FQLAgent` per episode**

## Conclusions

This chapter discusses reinforcement learning as one of the most successful algorithm classes that AI has to offer. Most of the advances and success stories discussed in [Chapter 2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch02.html#superintelligence) have their origin in improvements in the field of RL. In this context, neural networks are not rendered useless. To the contrary, they play an important role in approximating the optimal action policy, usually in the form of a policy � that, given a certain state, assigns each action a value. The higher the value is, the better the action will be, taking into account both immediate and delayed rewards.

The inclusion of delayed rewards, of course, is relevant in many important contexts. In a gaming context, with multiple actions available in general, it is optimal to choose the one that promises the highest total reward—and probably not just the highest immediate reward. The final total score is what is to be maximized. The same holds true in a financial context. The long-term performance is in general the appropriate goal for trading and investing, not a quick short-term profit that might come at an increased risk of going bankrupt.

The examples in this chapter also demonstrate that the RL approach is rather flexible and general in that it can be applied to different settings equally well. The DQL agent that solves the `CartPole` problem can also learn how to trade in a financial market, although not too well. Based on improvements of the `Finance` environment and the FQL agent, the FQL trading bot shows a respectable performance both in-sample (on the training data) and out-of-sample (on the validation data).

## References

Books and papers cited in this chapter:

* Sutton, Richard S. and Andrew G. Barto. 2018. _Reinforcement Learning: An Introduction_. Cambridge and London: MIT Press.
* Watkins, Christopher. 1989. _Learning from Delayed Rewards_. Ph.D. thesis, University of Cambridge.
* Watkins, Christopher and Peter Dayan. 1992. “Q-Learning.” _Machine Learning_ 8 (May): 279-282.

[1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#idm46319962547144-marker) See [Deep Reinforcement Learning](https://oreil.ly/h-EFL).

[2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#idm46319961483496-marker) See, for example, this [blog post](https://oreil.ly/84RwE).

[3](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch09.html#idm46319961119528-marker) The implementation is similar to the one found in this [blog post](https://oreil.ly/8mI4m).
