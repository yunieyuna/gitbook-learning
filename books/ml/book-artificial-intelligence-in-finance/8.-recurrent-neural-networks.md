# 8. Recurrent Neural Networks

## Chapter 8. Recurrent Neural Networks

> History never repeats itself, but it rhymes.
>
> Mark Twain (probably)

> My life seemed to be a series of events and accidents. Yet when I look back, I see a pattern.
>
> Bernoît Mandelbrot

This chapter is about _recurrent neural networks_ (RNNs). This type of network is specifically designed to learn about sequential data, such as text or time series data. The discussion in this chapter takes, as before, a practical approach and relies mainly on worked-out Python examples, making use of `Keras`.[1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#idm46319968305752)

[“First Example”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#rnn\_first) and [“Second Example”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#rnn\_second) introduce RNNs on the basis of two simple examples with sample numerical data. The application of RNNs to predict sequential data is illustrated. [“Financial Price Series”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#rnn\_fin\_price\_series) then works with financial price series data and applies the RNN approach to predict such a series directly via estimation. [“Financial Return Series”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#rnn\_fin\_ret\_series) then works with returns data to predict the future direction of the price of a financial instrument also via an estimation approach. [“Financial Features”](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#rnn\_fin\_features) adds financial features to the mix—in addition to price and return data—to predict the market direction. Three different approaches are illustrated in this section: prediction via a shallow RNN for both estimation and classification, as well as prediction via a deep RNN for classification.

The chapter shows that the application of RNNs to financial time series data can achieve a prediction accuracy of well above 60% out-of-sample in the context of directional market predictions. However, the results obtained cannot fully keep up with those seen in [Chapter 7](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch07.html#dense\_networks). This might come as a surprise, since RNNs are meant to work well with financial time series data, which is the primary focus of this book.

## First Example

To illustrate the training and usage of RNNs, consider a simple example based on a sequence of integers. First, some imports and configurations:

```
In [1]: import os
        import random
        import numpy as np
        import pandas as pd
        import tensorflow as tf
        from pprint import pprint
        from pylab import plt, mpl
        plt.style.use('seaborn')
        mpl.rcParams['savefig.dpi'] = 300
        mpl.rcParams['font.family'] = 'serif'
        pd.set_option('precision', 4)
        np.set_printoptions(suppress=True, precision=4)
        os.environ['PYTHONHASHSEED'] = '0'

In [2]: def set_seeds(seed=100):  
            random.seed(seed)
            np.random.seed(seed)
            tf.random.set_seed(seed)
        set_seeds()  
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO1-1)

Function to set all seed values

Second is the simple data set that is transformed into an appropriate shape:

```
In [3]: a = np.arange(100)  
        a
Out[3]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
               17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,
               34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,
               51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,
               68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,
               85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])

In [4]: a = a.reshape((len(a), -1))  

In [5]: a.shape  
Out[5]: (100, 1)

In [6]: a[:5]  
Out[6]: array([[0],
               [1],
               [2],
               [3],
               [4]])
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO2-1)

Sample data

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO2-2)

Reshaping to two dimensions

Using the `TimeseriesGenerator`, the raw data can be transformed into an object suited for the training of an RNN. The idea is to use a certain number of lags of the original data to train the model to predict the next value in the sequence. For example, `0, 1, 2` are the three lagged values (features) used to predict the value `3` (label). In the same way, `1, 2, 3` are used to predict `4`:

```
In [7]: from keras.preprocessing.sequence import TimeseriesGenerator
        Using TensorFlow backend.

In [8]: lags = 3

In [9]: g = TimeseriesGenerator(a, a, length=lags, batch_size=5)  

In [10]: pprint(list(g)[0])  
         (array([[[0],
                 [1],
                 [2]],

                [[1],
                 [2],
                 [3]],

                [[2],
                 [3],
                 [4]],

                [[3],
                 [4],
                 [5]],

                [[4],
                 [5],
                 [6]]]),
          array([[3],
                [4],
                [5],
                [6],
                [7]]))
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO3-1)

`TimeseriesGenerator` creates batches of lagged sequential data.

The creation of the RNN model is similar to DNNs. The following Python code uses a single hidden layer of type `SimpleRNN` (Chollet 2017, ch. 6; also see [Keras recurrent layers](https://oreil.ly/kpuqA)). Even with relatively few hidden units, the number of trainable parameters is quite large. The `.fit()` method takes as input generator objects such as those created with `TimeseriesGenerator`:

```
In [11]: from keras.models import Sequential
         from keras.layers import SimpleRNN, LSTM, Dense

In [12]: model = Sequential()
         model.add(SimpleRNN(100, activation='relu',
                             input_shape=(lags, 1)))  
         model.add(Dense(1, activation='linear'))
         model.compile(optimizer='adagrad', loss='mse',
                       metrics=['mae'])

In [13]: model.summary()  
         Model: "sequential_1"
         _________________________________________________________________
         Layer (type)                 Output Shape              Param #
         =================================================================
         simple_rnn_1 (SimpleRNN)     (None, 100)               10200
         _________________________________________________________________
         dense_1 (Dense)              (None, 1)                 101
         =================================================================
         Total params: 10,301
         Trainable params: 10,301
         Non-trainable params: 0
         _________________________________________________________________

In [14]: %%time
         h = model.fit(g, epochs=1000, steps_per_epoch=5,
                             verbose=False)  
         CPU times: user 17.4 s, sys: 3.9 s, total: 21.3 s
         Wall time: 30.8 s

Out[14]: <keras.callbacks.callbacks.History at 0x7f7f079058d0>
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO4-1)

The single hidden layer is of type `SimpleRNN`.

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO4-2)

The summary of the shallow RNN.

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO4-3)

The fitting of the RNN based on the generator object.

The performance metrics might show relatively erratic behavior when training RNNs (see [Figure 8-1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#figure\_rnn\_01)):

```
In [15]: res = pd.DataFrame(h.history)

In [16]: res.tail(3)
Out[16]:        loss     mae
         997  0.0001  0.0109
         998  0.0007  0.0211
         999  0.0001  0.0101

In [17]: res.iloc[10:].plot(figsize=(10, 6), style=['--', '--']);
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0801.png" alt="aiif 0801" height="1421" width="2379"><figcaption></figcaption></figure>

**Figure 8-1. Performance metrics during RNN training**

Having a trained RNN available, the following Python code generates in-sample and out-of-sample predictions:

```
In [18]: x = np.array([21, 22, 23]).reshape((1, lags, 1))
         y = model.predict(x, verbose=False)  
         int(round(y[0, 0]))
Out[18]: 24

In [19]: x = np.array([87, 88, 89]).reshape((1, lags, 1))
         y = model.predict(x, verbose=False)  
         int(round(y[0, 0]))
Out[19]: 90

In [20]: x = np.array([187, 188, 189]).reshape((1, lags, 1))
         y = model.predict(x, verbose=False)  
         int(round(y[0, 0]))
Out[20]: 190

In [21]: x = np.array([1187, 1188, 1189]).reshape((1, lags, 1))
         y = model.predict(x, verbose=False)  
         int(round(y[0, 0]))
Out[21]: 1194
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO5-1)

In-sample prediction

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO5-3)

Out-of-sample prediction

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO5-4)

Far-out-of-sample prediction

Even for far-out-of-sample predictions, the results are good in general in this simple case. However, the problem at hand could, for example, be perfectly solved by the application of OLS regression. Therefore, the effort involved for the training of an RNN for such a problem is quite high given the performance of the RNN.

## Second Example

The first example illustrates the training of an RNN for a simple problem that is easy to solve not only by OLS regression but also by a human being inspecting the data. The second example is a bit more challenging. The input data is transformed by a quadratic term and a trigonometric term, as well as by adding white noise to it. [Figure 8-2](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#figure\_rnn\_02) shows the resulting sequence for the interval \[-2�,2�]:

```
In [22]: def transform(x):
             y = 0.05 * x ** 2 + 0.2 * x + np.sin(x) + 5  
             y += np.random.standard_normal(len(x)) * 0.2  
             return y

In [23]: x = np.linspace(-2 * np.pi, 2 * np.pi, 500)
         a = transform(x)

In [24]: plt.figure(figsize=(10, 6))
         plt.plot(x, a);
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO6-1)

Deterministic transformation

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO6-2)

Stochastic transformation

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0802.png" alt="aiif 0802" height="1420" width="2379"><figcaption></figcaption></figure>

**Figure 8-2. Sample sequence data**

As before, the raw data is reshaped, `TimeseriesGenerator` is applied, and the RNN with a single hidden layer is trained:

```
In [25]: a = a.reshape((len(a), -1))

In [26]: a[:5]
Out[26]: array([[5.6736],
                [5.68  ],
                [5.3127],
                [5.645 ],
                [5.7118]])

In [27]: lags = 5

In [28]: g = TimeseriesGenerator(a, a, length=lags, batch_size=5)

In [29]: model = Sequential()
         model.add(SimpleRNN(500, activation='relu', input_shape=(lags, 1)))
         model.add(Dense(1, activation='linear'))
         model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])

In [30]: model.summary()
         Model: "sequential_2"
         _________________________________________________________________
         Layer (type)                 Output Shape              Param #
         =================================================================
         simple_rnn_2 (SimpleRNN)     (None, 500)               251000
         _________________________________________________________________
         dense_2 (Dense)              (None, 1)                 501
         =================================================================
         Total params: 251,501
         Trainable params: 251,501
         Non-trainable params: 0
         _________________________________________________________________

In [31]: %%time
         model.fit(g, epochs=500,
                             steps_per_epoch=10,
                             verbose=False)
         CPU times: user 1min 6s, sys: 14.6 s, total: 1min 20s
         Wall time: 23.1 s

Out[31]: <keras.callbacks.callbacks.History at 0x7f7f09c11810>
```

The following Python code predicts sequence values for the interval \[-6�,6�]. This interval is three times the size of the training interval and contains out-of-sample predictions both on the left-hand side and on the right-hand side of the training interval. [Figure 8-3](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#figure\_rnn\_03) shows that the model performs quite well, even out-of-sample:

```
In [32]: x = np.linspace(-6 * np.pi, 6 * np.pi, 1000)  
         d = transform(x)

In [33]: g_ = TimeseriesGenerator(d, d, length=lags, batch_size=len(d))  

In [34]: f = list(g_)[0][0].reshape((len(d) - lags, lags, 1))  

In [35]: y = model.predict(f, verbose=False)  

In [36]: plt.figure(figsize=(10, 6))
         plt.plot(x[lags:], d[lags:], label='data', alpha=0.75)
         plt.plot(x[lags:], y, 'r.', label='pred', ms=3)
         plt.axvline(-2 * np.pi, c='g', ls='--')
         plt.axvline(2 * np.pi, c='g', ls='--')
         plt.text(-15, 22, 'out-of-sample')
         plt.text(-2, 22, 'in-sample')
         plt.text(10, 22, 'out-of-sample')
         plt.legend();
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO7-1)

Enlarges the sample data set

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO7-4)

In-sample _and_ out-of-sample prediction

## SIMPLICITY OF EXAMPLES

The first two examples are deliberately chosen to be simple. Both problems posed in the examples can be solved more efficiently with OLS regression, for example, by allowing for trigonometric basis functions in the second example. However, the training of RNNs for nontrivial sequence data, such as financial time series data, is basically the same. In such a context, OLS regression, for instance, can in general not keep up with the capabilities of RNNs.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0803.png" alt="aiif 0803" height="1421" width="2404"><figcaption></figcaption></figure>

**Figure 8-3. In-sample and out-of-sample predictions of the RNN**

## Financial Price Series

As a first application of RNNs to financial time series data, consider intraday EUR/USD quotes. With the approach introduced in the previous two sections, the training of the RNN on the financial time series is straightforward. First, the data is imported and resampled. The data is also normalized and transformed into the appropriate `ndarray` object:

```
In [37]: url = 'http://hilpisch.com/aiif_eikon_id_eur_usd.csv'

In [38]: symbol = 'EUR_USD'

In [39]: raw = pd.read_csv(url, index_col=0, parse_dates=True)

In [40]: def generate_data():
             data = pd.DataFrame(raw['CLOSE'])  
             data.columns = [symbol]  
             data = data.resample('30min', label='right').last().ffill()  
             return data

In [41]: data = generate_data()

In [42]: data = (data - data.mean()) / data.std()  

In [43]: p = data[symbol].values  

In [44]: p = p.reshape((len(p), -1))  
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO8-1)

Selects a single column

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO8-2)

Renames the column

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO8-3)

Resamples the data

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO8-4)

Applies Gaussian normalization

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO8-5)

Reshapes the data set to two dimensions

Second, the RNN is trained based on the generator object. The function `create_rnn_model()` allows the creation of an RNN with a `SimpleRNN` or an `LSTM` (_long short-term memory_) layer (Chollet 2017, ch. 6; also see [Keras recurrent layers](https://oreil.ly/kpuqA)).

```
In [45]: lags = 5

In [46]: g = TimeseriesGenerator(p, p, length=lags, batch_size=5)

In [47]: def create_rnn_model(hu=100, lags=lags, layer='SimpleRNN',
                                    features=1, algorithm='estimation'):
             model = Sequential()
             if layer == 'SimpleRNN':
                 model.add(SimpleRNN(hu, activation='relu',
                                     input_shape=(lags, features)))  
             else:
                 model.add(LSTM(hu, activation='relu',
                                input_shape=(lags, features)))  
             if algorithm == 'estimation':
                 model.add(Dense(1, activation='linear'))  
                 model.compile(optimizer='adam', loss='mse', metrics=['mae'])
             else:
                 model.add(Dense(1, activation='sigmoid'))  
                 model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])
             return model

In [48]: model = create_rnn_model()

In [49]: %%time
         model.fit(g, epochs=500, steps_per_epoch=10,
                             verbose=False)
         CPU times: user 20.8 s, sys: 4.66 s, total: 25.5 s
         Wall time: 11.2 s

Out[49]: <keras.callbacks.callbacks.History at 0x7f7ef6716590>
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO9-1)

Adds a `SimpleRNN` layer or `LSTM` layer

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO9-3)

Adds an output layer for _estimation_ or _classification_

Third, the in-sample prediction is generated. As [Figure 8-4](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#figure\_rnn\_04) illustrates, the RNN is capable of capturing the structure of the normalized financial time series data. Based on this visualization, the prediction accuracy seems quite good:

```
In [50]: y = model.predict(g, verbose=False)

In [51]: data['pred'] = np.nan
         data['pred'].iloc[lags:] = y.flatten()

In [52]: data[[symbol, 'pred']].plot(
                     figsize=(10, 6), style=['b', 'r-.'],
                     alpha=0.75);
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0804.png" alt="aiif 0804" height="1528" width="2411"><figcaption></figcaption></figure>

**Figure 8-4. In-sample prediction for financial price series by the RNN (whole data set)**

However, the visualization suggests a result that does not hold up upon closer inspection. [Figure 8-5](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#figure\_rnn\_05) zooms in and only shows 50 data points from the original data set and of the prediction. It becomes clear that the prediction values from the RNN are basically just the most previous lag, shifted by one time interval. Visually speaking, the prediction line is the financial time series itself, moved one time interval to the right:

```
In [53]: data[[symbol, 'pred']].iloc[50:100].plot(
                     figsize=(10, 6), style=['b', 'r-.'],
                     alpha=0.75);
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0805.png" alt="aiif 0805" height="1528" width="2451"><figcaption></figcaption></figure>

**Figure 8-5. In-sample prediction for financial price series by the RNN (data sub-set)**

## RNNS AND EFFICIENT MARKETS

The results for the prediction of a financial price series based on an RNN are in line with the OLS regression approach used in [Chapter 6](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch06.html#ai\_first\_finance) to illustrate the EMH. There, it is illustrated that, in a least-squares sense, today’s price is the best predictor for tomorrow’s price. The application of an RNN to price data does not yield any other insight.

## Financial Return Series

As previous analyses have shown, it might be easier to predict returns instead of prices. Therefore, the following Python code repeats the preceding analysis based on log returns:

```
In [54]: data = generate_data()

In [55]: data['r'] = np.log(data / data.shift(1))

In [56]: data.dropna(inplace=True)

In [57]: data = (data - data.mean()) / data.std()

In [58]: r = data['r'].values

In [59]: r = r.reshape((len(r), -1))

In [60]: g = TimeseriesGenerator(r, r, length=lags, batch_size=5)

In [61]: model = create_rnn_model()

In [62]: %%time
         model.fit(g, epochs=500, steps_per_epoch=10,
                             verbose=False)
         CPU times: user 20.4 s, sys: 4.2 s, total: 24.6 s
         Wall time: 11.3 s

Out[62]: <keras.callbacks.callbacks.History at 0x7f7ef47a8dd0>
```

As [Figure 8-6](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#figure\_rnn\_06) shows, the RNN’s predictions are not too good in absolute terms. However, they seem to get the market direction (sign of the return) somehow right:

```
In [63]: y = model.predict(g, verbose=False)

In [64]: data['pred'] = np.nan
         data['pred'].iloc[lags:] = y.flatten()
         data.dropna(inplace=True)

In [65]: data[['r', 'pred']].iloc[50:100].plot(
                     figsize=(10, 6), style=['b', 'r-.'],
                     alpha=0.75);
         plt.axhline(0, c='grey', ls='--')
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/aiif_0806.png" alt="aiif 0806" height="1528" width="2411"><figcaption></figcaption></figure>

**Figure 8-6. In-sample prediction for financial return series by the RNN (data sub-set)**

While [Figure 8-6](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#figure\_rnn\_06) only provides an indication, the relatively high accuracy score supports the assumption that the RNN might perform better on a return than on a price series:

```
In [66]: from sklearn.metrics import accuracy_score

In [67]: accuracy_score(np.sign(data['r']), np.sign(data['pred']))
Out[67]: 0.6806532093445226
```

However, to get a realistic picture, a train-test split is in order. The accuracy score out-of-sample is not as high as the one seen for the whole data set in-sample, but it is still high for the problem at hand:

```
In [68]: split = int(len(r) * 0.8)  

In [69]: train = r[:split]  

In [70]: test = r[split:]  

In [71]: g = TimeseriesGenerator(train, train, length=lags, batch_size=5)  

In [72]: set_seeds()
         model = create_rnn_model(hu=100)

In [73]: %%time
         model.fit(g, epochs=100, steps_per_epoch=10, verbose=False)  
         CPU times: user 5.67 s, sys: 1.09 s, total: 6.75 s
         Wall time: 2.95 s

Out[73]: <keras.callbacks.callbacks.History at 0x7f7ef5482dd0>

In [74]: g_ = TimeseriesGenerator(test, test, length=lags, batch_size=5)  

In [75]: y = model.predict(g_)  

In [76]: accuracy_score(np.sign(test[lags:]), np.sign(y))  
Out[76]: 0.6708428246013668
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO10-1)

Splits the data into train and test data sub-sets

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO10-4)

Fits the model on the training data

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO10-6)

Tests the model on the testing data

## Financial Features

The application of RNNs is not restricted to the raw price or return data. Additional features can also be included to improve the prediction of the RNN. The following Python code adds typical financial features to the data set:

```
In [77]: data = generate_data()

In [78]: data['r'] = np.log(data / data.shift(1))

In [79]: window = 20
         data['mom'] = data['r'].rolling(window).mean()  
         data['vol'] = data['r'].rolling(window).std()  

In [80]: data.dropna(inplace=True)
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO11-1)

Adds a time series _momentum_ feature

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO11-2)

Adds a rolling _volatility_ feature

### Estimation

The out-of-sample accuracy, maybe somewhat surprisingly, drops significantly in the estimation case. In other words, there is no improvement observed from adding financial features in this particular case:

```
In [81]: split = int(len(data) * 0.8)

In [82]: train = data.iloc[:split].copy()

In [83]: mu, std = train.mean(), train.std()  

In [84]: train = (train - mu) / std  

In [85]: test = data.iloc[split:].copy()

In [86]: test = (test - mu) / std  

In [87]: g = TimeseriesGenerator(train.values, train['r'].values,
                                 length=lags, batch_size=5)  

In [88]: set_seeds()
         model = create_rnn_model(hu=100, features=len(data.columns),
                                  layer='SimpleRNN')

In [89]: %%time
         model.fit(g, epochs=100, steps_per_epoch=10,
                             verbose=False)  
         CPU times: user 5.24 s, sys: 1.08 s, total: 6.32 s
         Wall time: 2.73 s

Out[89]: <keras.callbacks.callbacks.History at 0x7f7ef313c950>

In [90]: g_ = TimeseriesGenerator(test.values, test['r'].values,
                                  length=lags, batch_size=5)  

In [91]: y = model.predict(g_).flatten()  

In [92]: accuracy_score(np.sign(test['r'].iloc[lags:]), np.sign(y))  
Out[92]: 0.37299771167048057
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO12-1)

Calculates the first and second moment of the training data

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO12-2)

Applies Gaussian normalization to the training data

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO12-3)

Applies Gaussian normalization to the testing data—based on the statistics from the training data

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO12-4)

Fits the model on the training data

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO12-6)

Tests the model on the testing data

### Classification

The analyses so far use a `Keras` RNN model for _estimation_ to predict the future direction of the price of the financial instrument. The problem at hand is probably better cast directly into a _classification_ setting. The following Python code works with binary labels data and predicts the direction of the price movement directly. It also works this time with an LSTM layer. The out-of-sample accuracy is quite high even for a relatively small number of hidden units and only a few training epochs. The approach again takes class imbalance into account by adjusting the class weights appropriately. The prediction accuracy is quite high in this case with around 65%:

```
In [93]: set_seeds()
         model = create_rnn_model(hu=50,
                     features=len(data.columns),
                     layer='LSTM',
                     algorithm='classification')  

In [94]: train_y = np.where(train['r'] > 0, 1, 0)  

In [95]: np.bincount(train_y)  
Out[95]: array([2374, 1142])

In [96]: def cw(a):
             c0, c1 = np.bincount(a)
             w0 = (1 / c0) * (len(a)) / 2
             w1 = (1 / c1) * (len(a)) / 2
             return {0: w0, 1: w1}

In [97]: g = TimeseriesGenerator(train.values, train_y,
                                 length=lags, batch_size=5)

In [98]: %%time
         model.fit(g, epochs=5, steps_per_epoch=10,
                             verbose=False, class_weight=cw(train_y))
         CPU times: user 1.25 s, sys: 159 ms, total: 1.41 s
         Wall time: 947 ms

Out[98]: <keras.callbacks.callbacks.History at 0x7f7ef43baf90>

In [99]: test_y = np.where(test['r'] > 0, 1, 0)  

In [100]: g_ = TimeseriesGenerator(test.values, test_y,
                                   length=lags, batch_size=5)

In [101]: y = np.where(model.predict(g_, batch_size=None) > 0.5, 1, 0).flatten()

In [102]: np.bincount(y)
Out[102]: array([492, 382])

In [103]: accuracy_score(test_y[lags:], y)
Out[103]: 0.6498855835240275
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO13-1)

RNN model for classification

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO13-2)

Binary training labels

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO13-3)

Class frequency for training labels

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO13-4)

Binary testing labels

### Deep RNNs

Finally, consider deep RNNs, which are RNNs with multiple hidden layers. They are as easily created as deep DNNs. The only requirement is that for the nonfinal hidden layers, the parameter `return_sequences` is set to `True`. The following Python function to create a deep RNN also allows for the addition of `Dropout` layers to potentially avoid overfitting. The prediction accuracy is comparable to the one seen in the previous sub-section:

```
In [104]: from keras.layers import Dropout

In [105]: def create_deep_rnn_model(hl=2, hu=100, layer='SimpleRNN',
                                    optimizer='rmsprop', features=1,
                                    dropout=False, rate=0.3, seed=100):
              if hl <= 2: hl = 2  
              if layer == 'SimpleRNN':
                  layer = SimpleRNN
              else:
                  layer = LSTM
              model = Sequential()
              model.add(layer(hu, input_shape=(lags, features),
                               return_sequences=True,
                              ))  
              if dropout:
                  model.add(Dropout(rate, seed=seed))  
              for _ in range(2, hl):
                  model.add(layer(hu, return_sequences=True))
                  if dropout:
                      model.add(Dropout(rate, seed=seed))  
              model.add(layer(hu))  
              model.add(Dense(1, activation='sigmoid'))  
              model.compile(optimizer=optimizer,
                            loss='binary_crossentropy',
                            metrics=['accuracy'])
              return model

In [106]: set_seeds()
          model = create_deep_rnn_model(
                      hl=2, hu=50, layer='SimpleRNN',
                      features=len(data.columns),
                      dropout=True, rate=0.3)  

In [107]: %%time
          model.fit(g, epochs=200, steps_per_epoch=10,
                              verbose=False, class_weight=cw(train_y))
          CPU times: user 14.2 s, sys: 2.85 s, total: 17.1 s
          Wall time: 7.09 s

Out[107]: <keras.callbacks.callbacks.History at 0x7f7ef6428790>

In [108]: y = np.where(model.predict(g_, batch_size=None) > 0.5, 1, 0).flatten()

In [109]: np.bincount(y)
Out[109]: array([550, 324])

In [110]: accuracy_score(test_y[lags:], y)
Out[110]: 0.6430205949656751
```

[![1](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/1.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO14-1)

A minimum of two hidden layers is ensured.

[![2](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/2.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO14-2)

The first hidden layer.

[![3](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/3.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO14-3)

The `Dropout` layers.

[![4](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/4.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO14-5)

The final hidden layer.

[![5](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492055426/files/assets/5.png)](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#co\_recurrent\_neural\_networks\_CO14-6)

The model is built for classification.

## Conclusions

This chapter introduces RNNs with `Keras` and illustrates the application of such neural networks to financial time series data. On the Python level, working with RNNs is not too different from working with DNNs. One major difference is that the training and test data must necessarily be presented in a sequential form to the respective methods. However, this is made easy by the application of the `TimeseriesGenerator` function, which transforms sequential data into a generator object that `Keras` RNNs can work with.

The examples in this chapter work with both financial price series and financial return series. In addition, financial features, such as time series momentum, can also be added easily. The functions presented for model creation allow, among other things, for one to use `SimpleRNN` or `LSTM` layers as well as different optimizers. They also allow one to model estimation and classification problems in the context of shallow and deep neural networks.

The out-of-sample prediction accuracy, when predicting market direction, is relatively high for the classification examples—but it’s not that high and can even be quite low for the estimation examples.

## References

Books and papers cited in this chapter:

* Chollet, François. 2017. _Deep Learning with Python_. Shelter Island: Manning.
* Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. _Deep Learning_. Cambridge: MIT Press. [_http://deeplearningbook.org_](http://deeplearningbook.org/).

[1](https://learning.oreilly.com/library/view/artificial-intelligence-in/9781492055426/ch08.html#idm46319968305752-marker) For technical details of RNNs, refer to Goodfellow et al. (2016, ch. 10). For the practical implementation, refer to Chollet (2017, ch. 6).
