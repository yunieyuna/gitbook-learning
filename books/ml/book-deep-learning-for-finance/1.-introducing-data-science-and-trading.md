# 1. Introducing Data Science and Trading

## Chapter 1. Introducing Data Science and Trading

The best way to begin learning about complex topics is to slowly build up momentum until you start completing the puzzle. Understanding deep learning for finance requires a certain knowledge in data science and financial markets.

This chapter lays the building blocks needed to have a thorough understanding of data science and its uses, but also of financial markets and how trading and forecasting can benefit from data science.

By the end of the chapter, you should know what data science is, what its applications are, and how you can use it in finance to extract value.

## Understanding Data

It is impossible to understand the field of data science without understanding the types and structures of data first. After all, the first word for the name of this immense field is _data_. So what is data? And more importantly, what can you do with it?

_Data_ in its simplest and purest form is a collection of raw information that can be of any type (numerical, text, boolean, etc.).

The final aim of collecting data is decision-making. This is done through a complex process which ranges from the act of gathering and processing data to interpreting it and using the results to make a decision.

Let’s take an example of using data to make a decision. Suppose you have a portfolio composed of five different equal-weighted dividend-paying stocks as detailed in [Table 1-1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch01.html#table-1-1).

| Stock | Dividend yield |
| ----- | -------------- |
| A     | 5.20%          |
| B     | 3.99%          |
| C     | 4.12%          |
| D     | 6.94%          |
| E     | 5.55%          |

**NOTE**

A _dividend_ is the payment made to shareholders from a company’s profits. The _dividend yield_ is the amount distributed in monetary units over the current share price of the company.

Analyzing this data can help you understand the average dividend yield you are receiving from your portfolio. The average is basically the sum divided by the quantity, and it gives a quick snapshot of the overall dividend yield of the portfolio:

��������������������=5.20%+3.99%+4.12%+6.94%+5.55%5=5.16%

Therefore, the average dividend yield of your portfolio is 5.16%. This information can help you compare your average dividend yield to other portfolios so that you know if you have to make any adjustments.

Another metric you can calculate is the number of stocks held in the portfolio. This may provide a first information brick in constructing a wall of diversification. Even though these two pieces of information (average dividend yield and the number of stocks in the portfolio) are very simple, complex data analysis begins with simple metrics and may sometimes not require sophisticated models to properly interpret the situation.

The two metrics you have calculated in the previous example are called the _average_ (or mean) and the _count_ (or number of elements). They are part of a field called _descriptive statistics_ discussed in [Chapter 3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#ch03), which is also itself part of data science.

Let’s take another example of data analysis for inferential purposes. Suppose you have calculated a yearly correlation measure between two commodities and you want to predict whether the next yearly correlation will be positive or negative. [Table 1-2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch01.html#table-1-2) has the details of the calculations.

| Year | Correlation |
| ---- | ----------- |
| 2015 | Positive    |
| 2016 | Positive    |
| 2017 | Positive    |
| 2018 | Negative    |
| 2019 | Positive    |
| 2020 | Positive    |
| 2021 | Positive    |
| 2022 | Positive    |
| 2023 | Positive    |

**NOTE**

_Correlation_ is a measure of the linear reliance between two time series. A _positive correlation_ generally means that the two time series move on average in the same direction, while a _negative correlation_ generally means that the two time series move on average in opposite directions. Correlation is discussed in [Chapter 3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#ch03).

From [Table 1-2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch01.html#table-1-2), the historical correlation between the two commodities was mostly positive with around 88% of the time. Taking into account historical observations, you can say that there is an 88% probability that the next correlation measure will be positive. This also means that there is a 12% probability that the next correlation measure will be negative:

�(�������������������)=89=88.88%

This is another basic example of how to use data to infer observations and make decisions. Of course, the assumption here is that historical results reflect exactly the future results, which is unlikely in real life, but occasionally, to predict the future, all you have is the past.

Now, before discussing data science, let’s review what types of data there can be and segment them into different groups:

Numerical dataThis type of data is composed of numbers that reflect a certain type of information that is collected at regular or irregular intervals. Examples can include market data (OHLC[1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch01.html#id237), volume, spreads, etc.) and financial statements data (assets, revenue, costs, etc.).Categorical dataData that can be organized into groups or categories using names or labels. It is qualitative rather than quantitative. For example, the blood type of patients is a type of categorical data. Another example is eye color of different samples from a population._Text data_Text data is on the rise during the recent years with the development of _natural language processing_ (NLP). Machine learning models use text data to translate, interpret, and analyze the sentiment of the text.Visual dataImages and videos are also considered data, and you can process and transform them into valuable information. For example, a _convolutional neural network_ (CNN) is a type of algorithm (discussed in [Chapter 8](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#ch08)) that can recognize and categorize photos by labels (for example, labeling cat photos as cats).Audio dataAudio data is very valuable and can help save time on transcriptions. For example, you can use algorithms on audio to create captions and automatic subtitles. You can also create models that interpret the sentiment of the speaker using the tone and the volume.

_Data science_ is a transdisciplinary field that tries to extract intelligence and conclusions from data using different techniques and models, be they simple or complex. The process of data science is composed of many phases besides to just analyzing data. The following summarizes the different stages of data science:

1. _Data gathering_: This process involves the acquisition of data from reliable and accurate sources. A widely known quote in computer science generally credited to George Fueschel goes as follows "_Garbage in, garbage out_“, and it sums up the need to have quality data that you can rely on for proper analysis. Basically, if you have inaccurate or faulty data, then all your process would be invalid.&#x20;
2. _Data preprocessing_: Occasionally, when you acquire data, it can be in a raw form that needs preparation for the data science models. For example, dropping some unnecessary data, handling missing values, or eliminating invalid and duplicate data are part of the preprocessing phase. Other more complex examples can include _normalization_ and _denoising_ of data. The aim of this step is to get the data ready for analysis.
3. _Data exploration_: This is a basic statistical research tool used to find trends and other characteristics in data. An example of data exploration is to calculate the mean of the data.
4. _Data visualization_: This is an important step that is an add-on to the previous step. It includes creating visualizations such as histograms and heatmaps to help identify patterns and trends and make the interpretation easier.
5. _Data analysis_: This is the long-awaited step which is basically the main focus of the data science process. This is where you _fit_ (train) the data using different learning models so that they interpret and predict the future outcome based on the given parameters.
6. _Data interpretation_: This phase deals with the feedback and conclusions after the models have performed their jobs. _Optimization_ may also be a part of this phase which then loops back to phase 5 in order to run the models again with the updated parameters before interpreting them again and evaluating the performance.

**NOTE**

To sum up the previous list, data science comprises many steps that start with acquiring the data through interpreting and optimizing the models that predict the future values of the data.

Let’s take a simple example in Python that applies the data science process discussed in the previous six steps. Suppose you want to analyze and predict the VIX, a volatility time series indicator.

**NOTE**

There is a hidden step that I refer to as _step zero_ which is the idea and the intuition for the whole process. You wouldn’t be applying the process if you didn’t have a motive first. For example, believing that inflation numbers may drive the returns of certain commodities is an idea and a motive to start exploring the data in search for real numbers that prove this hypothesis.

The _VIX_ stands for the _volatility index_ and it represents the implied volatility of the S\&P 500 index. It has been available since 1993 and is issued by the Chicago board options exchange (CBOE).

Because it is meant to measure the level of fear or uncertainty in the stock market, the VIX is frequently referred to as the _fear index_. The index is a percentage and is computed using the pricing of options on the S\&P 500 index. A higher VIX value correlates with a greater market turbulence and uncertainty, whereas a lower value correlates with greater stability on average.

The first step is data gathering, which in this case can be automated using Python. The next code block connects to the website of the Federal Reserve of Saint Louis and downloads the historical data of the VIX between January 1990 and January 2023.

**NOTE**

Note that [Chapter 6](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch06.html#ch06) is entirely dedicated into introducing Python and how to write code. For the moment, you do not have to understand the code as it is not yet the learning outcome.

<pre><code><strong># Importing the required library
</strong>import pandas_datareader as pdr
<strong># Setting the beginning and end of the historical data
</strong>start_date = '1990-01-01'
end_date   = '2023-01-23'
<strong># Creating a dataframe and downloading the VIX data
</strong>vix = pdr.DataReader('VIXCLS', 'fred', start_date, end_date)
<strong># Printing the latest five observations of the dataframe
</strong>print(vix.tail())
</code></pre>

The code uses the `pandas` library to import the `DataReader` function, which fetches the historical data online from a variety of sources. The `DataReader` function takes the name of the data as a first argument, followed by the source, and the dates. The output of `print(vix.tail())` is shown in [Table 1-3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch01.html#table-1-3):

| DATE       | VIXCLS |
| ---------- | ------ |
| 2023-01-17 | 19.36  |
| 2023-01-1  | 20.34  |
| 2023-01-19 | 20.52  |
| 2023-01-20 | 19.85  |
| 2023-01-23 | 19.81  |

Let’s move on to the second step: data preprocessing. I divide this part into checking for invalid data and transforming the data so that it is ready for use. When dealing with time series, especially downloaded time series, you may sometimes encounter `nan` values which are not numbers as there has not been a proper input in their respective cells.

**NOTE**

The `nan` observation stands for _Not a Number_ and it occurs due to missing, invalid, or corrupt data.

You can deal with `nan` values in many ways. For the sake of this example, let’s see the simplest way of dealing with these invalid values, which is to eliminate them. But first, let’s write a simple code that outputs the number of `nan` values in the dataframe so that you have an idea on how many values you will delete:

<pre><code><strong># Calculating the number of nan values
</strong>count_nan = vix['VIXCLS'].isnull().sum()
<strong># Printing the result
</strong>print('Number of nan values in the VIX dataframe: ' + str(count_nan))
</code></pre>

The code uses the `isnull()` function and sums the number it gets which gives out the number of `nan` values. The output of the previous code snippet is as follows:

```
Number of nan values in the VIX dataframe: 292
```

Now that you have an idea of how many rows you will delete, you can use the following code to drop the invalid rows:

<pre><code><strong># Dropping the nan values from the rows
</strong>vix = vix.dropna()
</code></pre>

The second part of the second step is to transform the data. Data science models typically require _stationary_ data, which is data with stable statistical properties such as the mean.

**NOTE**

The concept of _stationarity_ and the required statistics metrics are discussed in detail in [Chapter 3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#ch03). For now, all you need to know is that it is likely that you will have to transform your raw data into stationary data when using data science models.

To transform the VIX data into stationary data, you can simply take the differences from one value relative to the previous value. The following code snippet takes the VIX dataframe and transforms it into a theoretically implied[2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch01.html#id238) stationary data:

<pre><code><strong># Taking the differences in an attempt to make the data stationary
</strong>vix = vix.diff(periods = 1, axis = 0)
<strong># Dropping the first value of the data frame
</strong>vix = vix.iloc[1: , :]
</code></pre>

The third step is data exploration, which is all about understanding the data you have in front you, statistically speaking. As you will see statistical metrics in detail in [Chapter 3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#ch03), I’ll limit the discussion to just calculating the mean of the dataset.

The _mean_ is simply the value that can represent the other values in the dataset if they were to elect a leader. It is the sum of the values divided by their quantity. The mean is the simplest stat in the descriptive statistics world and it is definitely the most used one. The following formula shows the mathematical representation of the mean of a set of values:

�¯=1�∑�=1���

You can easily calculate the mean of the dataset as follows:

<pre><code><strong># Calculating the mean of the dataset
</strong>mean = vix["VIXCLS"].mean()
<strong># Printing the result
</strong>print('The mean of the dataset = ' + str(mean))
</code></pre>

The output of the previous code snippet is as follows:

```
The mean of the dataset = 0.0003
```

The next step is data visualization, which is mostly considered as the fun step. Let’s chart the VIX’s differenced values through time. The following code snippet plots the VIX data shown in [Figure 1-1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch01.html#figure-1-1):

<pre><code><strong># Importing the required library
</strong>import matplotlib.pyplot as plt
<strong># Plotting the latest 250 observations in black with a label
</strong>plt.plot(vix[-250:], color = 'black', linewidth = 1.5, 
         label = 'Change in VIX')
<strong># Plotting a red dashed horizontal line that is equal to mean
</strong>plt.axhline(y = mean, color = 'red', linestyle = 'dashed')
<strong># Calling a grid to facilitate the visual component
</strong>plt.grid()
<strong># Calling the legend function so it appears with the chart
</strong>plt.legend()
<strong># Calling the plot
</strong>plt.show()
</code></pre>

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0101.png" alt="" height="448" width="600"><figcaption></figcaption></figure>

**Figure 1-1. Change in VIX since early 2022**

Steps 5 and 6, data analysis and data interpretation, are what you are going to study thoroughly in this book, so let’s skip them for now and concentrate on the introductory part of data science.

Let’s go back to the invalid or missing data problem before moving on. Sometimes, data is incomplete and has missing cells. Even though this has the potential to hinder the predictive ability of the algorithm, it should not stop you from continuing the analysis as there are quick fixes that help lessen the negative impact of the empty cells. For instance, consider [Table 1-4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch01.html#table-1-4):

| Quarter | GDP      |
| ------- | -------- |
| Q1 2020 | 0.9%     |
| Q2 2020 | 1.2%     |
| Q3 2020 | 0.5%     |
| Q4 2020 | 0.4%     |
| Q1 2021 | **#N/A** |
| Q2 2021 | 1.0%     |
| Q3 2021 | 1.1%     |
| Q4 2021 | 0.6%     |

The table contains the quartely gross domestic product (GDP[3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch01.html#id239)) of a hypothetical country. Notice how the table is missing the Q1 value of 2021. There are three basic ways to solve this issue:

* _Delete the cell that contains the missing value:_ This is the technique used in the VIX example. It simply considers that the time stamp does not exist. It is the simplest fix.
* _Assume that the missing cell is equal to the previous cell:_ This technique assumes that the current missing value equals the value previous to it. It is also a simple fix that has the aim of smoothing the data instead of completely ignoring the issue.
* _Calculate a mean or a median of the cells around the empty value:_ This technique takes smoothing one step further and assumes that the missing value is equal to the mean between the previous and the next value. Additionally, it can also be the mean of a few past observations.

Data science englobes a range of mathematical and statistical concepts. It entails a deep understanding of machine learning algorithms. These concepts are discussed in detail but also in an easy-to-grasp manner so that technical and non-technical readers can benefit from their intuition. Many models are assumed to be black boxes and there is a hint of truth in this, but the job of a data scientist is to understand the models before interpreting their results. This helps in understanding the limitations of the models.

This book uses Python as the go-to programming language to create the algorithms. As mentioned, [Chapter 6](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch06.html#ch06) introduces Python and the required knowledge to know how to manipulate and analyze the data, but also provides the foundations to create the different models, which as you will see, are simpler than you might expect.

Before moving on to the next section, let’s have a look at the concept of data storage. After all, data is valuable, but you need to store it somewhere where it is easily fetched and analyzed.

_Data storage_ refers to the techniques and areas used to store and organize data for future analysis. Data is stored in many formats such as `csv` and `xlsx`. Other types of formats may include `xml`, `json`, and even `jpg` for images. The format is chosen according to the structure and organization of the data.

Data can also be stored in clouds or on-premise depending on the capacity of storage and the costs. For example, you may want to choose to keep your historical 1-minute Apple stock data in a cloud so that you save space on your local computer as opposed to keeping them in a `csv` file.​

When dealing with time series in Python, you are mostly going to deal with two types of data storages: arrays and data frames. Let’s take a look at what they are:

_Arrays_An _array_ is used to store elements of the same kind. Typically, a homogeneous data set (such as numbers) is best kept in an array.Data framesA _data frame_ is a 2-dimensional structure that can hold data of various types. It can be compared to a table with columns and rows.

In general, arrays should be used whenever a homogeneous data collection needs to be efficiently stored. When dealing with heterogeneous data or needing to edit and analyze data in a tabular manner, you should use data frames.

**NOTE**

Data science is continuously evolving. New storage methods are being developed by time in an attempt to make them more efficient and increase their capacity and speed.

## Understanding Data Science

Data science has rapidly become an essential part in technology and progress. Algorithms rely on information provided from data science tools to perform their tasks. But what are algorithms?

An _algorithm_ is a set of ordered procedures, that have the aim of completing a certain activity or address a particular issue. Algorithms can be as simple as flipping a coin or as sophisticated as the Risch algorithm [4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch01.html#id240).

Let’s take a very simple algorithm that updates a charting platform with the necessary financial data. This algorithm would follow these steps:

1. Connect the server and the online data provider.
2. Copy the financial data with the most recent time stamp.
3. Paste the data into the charting platform.
4. Loop back to the first step and redo the whole process.

That is the nature of algorithms: performing a certain set of instructions with a finite or an infinite goal.

**NOTE**

The six data science stages that you saw in the previous section can also be considered an algorithm.

Trading strategies are also algorithms as they have clear rules for the initiation and liquidation of positions. An example of a trading strategy is market arbitrage.

_Arbitrage_ is a type of trading strategy that aims to profit from price differences of the same asset quoted on different exchanges. These price differences are anomalies that are erased by arbitrageurs through their buying and selling activities. Consider a stock that is traded on exchange A and exchange B in different countries (for simplicity reasons, the two countries use the same currency). Naturally, the stock must trade at the same price on both exchanges. When this condition does not hold, arbitrageurs come out of their lairs to hunt.

They buy the stock on the cheaper exchange and immediately sell it on the more expensive exchange, thus ensuring a virtually risk-free profit. These operations are performed at lightning speed as differences do not last long due to the sheer power and speed of arbitrageurs. Here’s a clear example:

* The stock’s price at exchange A = $10.00
* The stock’s price at exchange B = $10.50

The algorithm of the arbitrageur in this case will perform the following:

1. Buy the stock on exchange A for $10.00.
2. Sell the stock immediately on exchange B for $10.50.
3. Pocket the difference ($0.50) and repeat until the gap is closed.

**NOTE**

Trading and execution algorithms can be highly complex and require specialized knowledge and a certain market edge.

Up until now, you should be aware of the main uses of data science: data interpretation and prediction:

Data interpretationAlso commonly referred to as _business intelligence_ or simply _data intelligence_. The aim of deploying the algorithms is to understand the whats and hows of data.Data predictionAlso commonly referred to as _predictive analytics_ or simply _forecasting_. The aim of deploying the algorithms is to understand the whats next of data.

The main aim of using learning algorithms in financial markets is mainly to predict data so that you take an informed trading decision with the aim of capital appreciation at a success rate higher than random. This is done through many simple and complex algorithm that I discuss in this book. These learning algorithms or models can be categorized as follows:

Supervised learning_Supervised learning algorithms_ are models that require labeled data to function. This means that you must provide data so that the model trains itself on these past values and understands the hidden patterns with the aim of being able to deliver future outputs when encountering new data. Examples of supervised learning include _linear regression algorithms_ and _random forest_ models.Unsupervised learning_Unsupervised learning algorithms_ are models that do not require labeled data to function. This means that they can do the job with unlabelled data since they are built to find hidden patterns on their own. Examples include _clustering_ algorithms and _principal component analysis_ (PCA).Reinforcement learning_Reinforcement learning_ algorithms are models that do not require data at all as they discover their environment and learn from it on their own. As opposed to supervised and unsupervised learning models, reinforcement learning models gain knowledge through feedback obtained from the environment via a reward system. Since this is generally applied to situations in which an agent interacts with the environment and learns to adopt behaviors that maximize the reward over time, it may not be the go-to algorithm for time series regression. On the other hand, it can be used to develop a policy that can apply to time series data to create predictions.

As you may have noticed, the book’s title is _Deep Learning for Finance_. This means that in addition to other learning models, I will be spending a sizable portion of the book discussing deep learning models for time series prediction. Deep learning mostly revolves around the use of neural networks, an algorithm discussed in depth in [Chapter 8](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#ch08).

Deep supervised learning models (such as deep neural networks) can learn hierarchical representations of the data because they include many layers, with each layer extracting features at a different level of abstraction. As a result, hidden and complex patterns are learned by deep models that may be difficult for shallow (not deep) models to learn.

On the other hand, shallow supervised learning models (like linear regression) have a limited ability to learn complex non-linear relationships. But, they require less computational effort and are therefore faster.

Data science algorithms are deployed pretty much everywhere nowadays and not just in finance. Examples include the following:

* _Business analytics_: Optimizing pricing, predicting customer turnover, or improving marketing initiatives using data analysis.
* _Healthcare_: Improving patient outcomes, finding innovative therapies, or lowering healthcare costs through in-depth analysis of patient data.
* _Sports_: Sports data analysis to enhance team performance, player scouting, or bets.
* _Research_: Analyzing data to support scientific investigation, prove theories, or gain new knowledge.

When someone talks about data science applications, it helps to know what a data scientist does.

A _data scientist_ must evaluate and understand complex data in order to get insights and provide guidance for decision-making. Common tasks involved in this include developing statistical models, applying machine learning techniques, and visualizing data. They support the implementation of data-driven solutions and also inform stakeholders of their results. It is worth mentioning that building and maintaining the infrastructure is handled by _data engineers_.

In other words, a data scientist is more concerned with the interpretation and analysis of data, whereas a data engineer is more concerned with the tools and infrastructure needed to gather, store, and analyze data.

By now you should understand everything you need to get you started with data science. Let’s introduce the second main topic of the book: financial markets. After all, this book aims to show how to create data science models and algorithms and apply them on financial data in order to extract predictive value from them.

## Introduction to Financial Markets and Trading

The aim of this book is to present a hands-on approach onto applying different learning models to forecast financial time series data. It is therefore imperative to gain a solid knowledge on how trading and financial markets work.

_Financial markets_ are places where people can trade financial instruments, such as stocks, bonds, and currencies. The act of buying and selling is referred to as _trading_. The main, but not only, aim of buying a financial instrument is capital appreciation. The buyer believes that the value of the instrument is greater than its price, therefore the buyer buys the stock (_goes long_) and sells whenever they believe that the current price equals the current value. In contrast, traders can also make money if the price of the instrument goes down. This process is referred to as _short selling_ and is common in certain markets such as futures and foreign exchange (FX).

The process of short selling entails borrowing the financial instrument from a third party, selling it on the market, and buying it back, before returning it to the third party. Ideally, as you expect the price of the instrument to drop, you would buy it back cheaper (after the price decrease) and give it back to the third party at the market price thus pocketing the difference.

Long (buy) position exampleA trader expects the share price of Microsoft to increase over the next couple of years due to improved technological regulations, which would increase the earnings. They therefore buy a number of shares at $250 and aim to sell them at $500. The trader is therefore long Microsoft stock (also referred to as being _bullish_).Short (sell) position exampleA trader expects the share price of Lockheed Martin to decrease over the next couple of days due to signals from a technical strategy. They therefore sell short a number of shares at $450 and aim to buy them back at $410. The trader is therefore short Lockheed Martin stock (called being _bearish_).

**NOTE**

Markets that are trending upwards are referred to as bullish markets. Derived from the word _bull_ and its aggressive nature, being bullish is related to optimism, euphoria, and greed.

On the other hand, markets that are trending downwards are referred to as bearish markets. Derived from the word _bear_ and its defensive nature, being bearish is related to pessimism, panic, and fear.

Financial instruments may come in their raw form (spot) and also in derivatives form. _Derivatives_ are products that traders use to trade markets in certain ways. For example, a _forward_ or a _futures_ contract is a derivative contract where a buyer locks in a price for an asset to buy it at a later time.

Another type of derivatives is an option. An _option_ is the right but not the obligation to buy a certain asset at a specific price in the future by paying a premium now (the option’s price). When a buyer wants to buy the underlying stock, they exercise their option to do so; otherwise, they may let the option expire.

Trading activity may also occur for hedging purposes as it is not limited to just speculation. An example of this would be AirFrance (the main French airline company) hedging its business operations by buying oil futures. Buying oil futures protects AirFrance from rising oil prices which may hurt its main operations (aviation). The rising costs from using fuel to power the planes are offset by the gains from the futures. This allows the airline to focus on its main business. This whole process is called _hedging_.

Let’s take an example to make things clearer, let’s say an airline company expects to consume a certain amount of fuel in the next six months, but they are worried about the potential increase in oil prices over that period. To protect against this price risk, the airline can enter into a futures contract to purchase oil at a fixed price on a future date.

If the price of oil increases during that time, the airline would still be able to purchase the oil at the lower, fixed price agreed upon in the futures contract. On the other hand, if the price of oil decreases, the airline would be obligated to pay the higher, fixed price, but the lower market price for the oil would offset that cost.

In this way, the airline can mitigate the risk of price fluctuations in the oil market and stabilize their fuel costs. This can help the airline to better manage its budget and forecast its future earnings. As you can see, the aim is not to make financial gains from the trading operations -- it aims to simply stabilize its costs by locking in a known price for oil.

Typically, financial instruments are grouped in asset classes based on their type:

Stock marketsA _stock market_ is an exchange place (electronic or physical) where companies issue shares of stock to raise money for business. When people buy shares of a company’s stock, they become part owners of that company and may become entitled to dividends according the company’s policy. Depending on the type of stocks, they can also gain the right to vote in board meetings. Fixed incomeGovernments and businesses can borrow money in the fixed income market. When a person purchases a bond, they are effectively lending money to the borrower, who has agreed to repay the loan along with interest. Depending on the borrower’s creditworthiness and the prevailing interest rates, the bond’s value may increase or decrease.CurrenciesThe FX market, also referred to as the currencies market, is a place where people may purchase and sell various currencies. A currency’s value can increase or decrease based on a variety of variables, including the economy, interest rates, and political stability of the nation.CommoditiesAgricultural products, gold, oil, and other physical assets with industrial or other uses are referred to as _commodities_. They typically offer a means to profit from global economic trends as well as being a form of hedge against inflation.Alternative investmentsIn the world of finance, non-traditional investments such as real estate, private equity, and hedge funds are referred to as _alternative asset classes_. These alternative asset classes have the potential to offer better returns than traditional assets and offer the benefit of diversity, but they also tend to be less liquid and may be more difficult to evaluate. It’s crucial to remember that each of these asset classes has unique qualities and various levels of risk, so investors should do their homework before investing in any of these assets.

Financial markets allow businesses and governments to raise money they need to operate. They also provide opportunities for investors to make money speculating and investing in interesting opportunities. Trading activities provide liquidity to the markets, which makes the price more efficient and less costly. In other words, the more the market is liquid, the less are the costs to trade in it as the number of orders makes it less likely to heavily impact the market when trading. But how do markets really work? What causes the price to go up and down?

_Market microstructure_ is the research that deals with the trading of securities in financial markets. It looks at how trading works as well as how traders, investors, and market makers behave. Understanding price formation and the variables that affect trading costs is the aim of market microstructure research.

Order flow, liquidity, market effectiveness, and price discovery are just a few of the many subjects covered by market microstructure research. Additionally, it looks at how various trading techniques, including limit orders, market orders, and algorithmic trading, affect market dynamics. _Liquidity_ is possibly the most important market microstructure concept. It describes how easily an asset may be bought or sold without materially changing its price. Liquidity can vary between financial instruments and over time. It can be impacted by a number of variables, including trading volume and volatility.

Finally, I want to discuss another important area of market microstructure: _price discovery_. This refers to the method used to set prices in a market. Prices can be affected by elements like order flow, market maker activity, and the presence of various trading methods.

Imagine you want to buy a sizable number of shares in two stocks: stock A and stock B. Stock A is very liquid while stock B is very illiquid. If you want to execute the buy order on stock A, you are likely to get filled at the desired market price with minimal impact if any. However, with stock B, you are likely to get a worse price as there is not enough sellers willing to sell at your desired buy price, and therefore, as you create more demand from your orders, the price rises to match the sellers’ prices and thus, you will buy at a higher (worse) price. This is the impact liquidity can have on your trading.

## Applications of Data Science in Finance

Let’s begin peeking into the main areas of data science for finance. Every field has its challenges and problems that need simple and complex solutions. Finance is no different. Recent years have seen a gigantic leap in using data science to improve the world of finance, from the corporate world to the markets world. Let’s discuss some of these areas:

Forecasting the market’s directionThe aim of using data science on financial time series is to uncover patterns, trends, and relationships in historical market data that can be used to make predictions about future market movements.Financial fraud detectionFinancial transactions can be examined for patterns and anomalies using data science models, which attempt to spot possible fraud. By examining transaction data from credit cards to spot odd or suspect spending patterns, one way to use data science to stop financial fraud is to find unusual or suspicious patterns of expenditures. This can involve making numerous minor purchases quickly after one another or making significant or frequent purchases from the same store.Risk managementTo examine financial data and spot potential risks to portfolios, data science approaches might be applied. This can involve analyzing vast amounts of historical data using methods like statistical modeling, machine learning, and artificial intelligence in order to spot patterns and trends that can be used to forecast risk factors. Credit scoringData science can be used to examine financial data and credit history, forecast a person’s or a company’s creditworthiness, and make loan decisions. Utilizing financial data, such as income and credit history, to forecast a person’s creditworthiness is one example of applying data science for credit score research. This can involve developing a prediction model that can use a number of indicators, such as prior credit performance, income, and job history, in order to evaluate a person’s likelihood of repaying a loan using techniques like statistical modeling and machine learning. Natural Language Processing (NLP)

In order to make better judgments, NLP analyzes and extracts insights from unstructured financial data, such as news articles, reports, and social media posts. A famous example of NLP is using the sentiment of the text to extract possible trading opportunities stemming from the intentions and feelings of the market participants and experts. NLP falls into the field of sentiment analysis (with help from machine learning).

## Summary

Data science keeps growing every day with new techniques and models appearing regularly that aim to improve the interpretation of data. This chapter has provided a simple introduction to what you need to know about data science and how you can use it in finance.

The next three chapters present the required knowledge in statistics, probability, and math that you may need when trying to understand data science models. Even though the aim of the book is to present a hands-on approach of creating and applying the different models using Python, it helps for you to understand what you’re dealing with instead of blindly applying them onto data.

If you need a Python refresher, see [Chapter 6](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch06.html#ch06), which is a basic introduction. It sets the foundations to what’s to come next in the book. You do not need to become a Python master to perform data science but you must understand code and what it refers to, and especially how to debug and detect errors in the code.

[1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch01.html#id237-marker) OHLC refers to the four essential pieces of market data: open price, high price, low price, and close price.

[2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch01.html#id238-marker) The reason I am saying implied is because stationarity must be verified through statistical checks that you will see in [Chapter 3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#ch03). At the moment, the assumption is that differencing the data gives stationary time series.

[3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch01.html#id239-marker) The GDP measure is discussed in more detail in [Chapter 12](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch12.html#ch12).

[4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch01.html#id240-marker) The Rish algorithm is an indefinite integration technique used to find antiderivatives, a concept you will see in [Chapter 4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch04.html#ch04).
