# 3. Descriptive Statistics and Data Analysis

## Chapter 3. Descriptive Statistics and Data Analysis

_Descriptive statistics_ is a field that describes data and extracts as much information as possible from it. Basically, descriptive statistics can act like the representative of the data since it briefs up its tendencies, behavior, and trends.

Trading and analysis borrows a lot from the metrics of this field. You will see in this chapter the main concepts that you need in order to have a solid grasp on data analysis. I always found that the best educational tools are practical examples, therefore, I will present this chapter using one example of an economic time series, the consumer price index (CPI).

The CPI measures the prices paid by urban consumers for a selection of products and services on a monthly basis (meaning that every month, a new observation is released to the public, thus forming a continuous time series). The inflation rate between any two time periods is measured by percentage changes in the price index. For example, if the price of bread last year was $1.00 and the price today is $1.01, then the inflation is 1.00%. The CPI is typically released on a year-on-year basis, which means that it is reported as the difference between the current monthly observation and the observation 12 months ago.

Import the CPI data as follows:

<pre><code><strong># Importing the required library
</strong>import pandas_datareader as pdr
<strong># Setting the beginning and end of the historical data
</strong>start_date = '1950-01-01'
end_date   = '2023-01-23'
<strong># Creating a dataframe and downloading the CPI data
</strong>cpi = pdr.DataReader('CPIAUCSL', 'fred', start_date, end_date)
<strong># Printing the latest five observations of the dataframe
</strong>print(cpi.tail())
<strong># Checking if there are nan values in the CPI dataframe
</strong>count_nan = cpi['CPIAUCSL'].isnull().sum()
<strong># Printing the result
</strong>print('Number of nan values in the CPI dataframe: ' + str(count_nan))
<strong># Transforming the CPI into a year-on-year measure
</strong>cpi = cpi.pct_change(periods = 12, axis = 0) * 100
<strong># Dropping the nan values from the rows
</strong>cpi = cpi.dropna()
</code></pre>

**NOTE**

A dedicated GitHub is provided with the code seen in this chapter. To replicate the charts and the results, you can refer to the link[1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#id241).

The year-on-year change is the most observed transformation on the CPI as it gives a clear and simple measurement of the change in the overall price level over a sufficiently enough period of time to account for short-term swings and seasonal impacts.

Hence, the yearly change of the CPI serves as a gauge of the general trend in inflation. It is also simple to comprehend and compare across other nations and historical times, making it a popular measure among policymakers and economists (albeit the flaw of element weightings in the baskets between countries). The next sections show how to make statistical sense of time series data using the CPI example.

## Measures of Central Tendency

_Central tendency_ refers to the metrics that summarize the dataset into a value that can represent them. The first and most known central tendency measure is the mean (average). The _mean_ is simply the sum of the values divided by their quantity. It is the value that represents the data the best. The mathematical formula of the mean is as follows:

�¯=1�∑��=1��=1�(�1+...+��)

Let’s take a simple example of two datasets. Suppose you want to calculate the mean on dataset A and dataset B. How would you do it?

* Dataset A = \[1, 2, 3, 4, 5]
* Dataset B = \[1, 1, 1, 1]

Dataset A contains 5 values (quantity) with a total sum of 15. This means that the mean is equal to 3. Dataset B contains 4 values with a total sum of 4. This means that the mean is equal to 1.

**NOTE**

When all the values in a dataset are the same, the mean is the same as the values.

[Figure 3-1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-1) shows the US CPI year-on-year values since 2003. The higher dashed line is the monthly mean calculated since 2003. The lower dashed line symbolizes zero where below it are deflationary periods.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0301.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 3-1. US CPI year-on-year changes since 2003**

You can create [Figure 3-1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-1) by using the following code:

<pre><code><strong># Calculating the mean of the CPI over the last 20 years
</strong>cpi_latest = cpi.iloc[-240:]
mean = cpi_latest["CPIAUCSL"].mean()
<strong># Printing the result
</strong>print('The mean of the dataset: ' + str(mean), '%')
<strong># Importing the required library
</strong>import matplotlib.pyplot as plt
<strong># Plotting the latest observations in black with a label
</strong>plt.plot(cpi_latest[:], color = 'black', linewidth = 1.5, 
         label = 'Change in CPI Year-on-Year')
<strong># Plotting horizontal lines that represent the mean and the zero threshold
</strong>plt.axhline(y = mean, color = 'red', linestyle = 'dashed', 
         label = 'Mean')
plt.axhline(y = 0, color = 'blue', linestyle = 'dashed', linewidth = 1)
plt.grid()
plt.legend()
</code></pre>

The output of the mean should be as follows.

```
The mean of the dataset: 2.49 %
```

This means that the average yearly inflation is around 2.50%. Even though the Federal Reserve does not have a clearly defined inflation target, it is generally believed that there is a consensus to maintain the annual change in inflation around 2.00%, hence not far from the historical observations. With the high inflation numbers recorded since 2021 as a result of political and economic turmoil, it becomes necessary to revert back to the mean to stabilize the current situation. This example gives a numerical value to what is referred to as normality (\~2.50%) since 2003.

Clearly, with the high inflation numbers (\~6.00%) around the beginning of 2023, the situation is a bit far from normality, but how far? This question is answered in the next section that discusses measures of variability. For now, let’s continue the discussion on central tendency.

The next measure is the _median_ which in simple terms is the value that splits the data set into two equal sides. In other words, if you arrange the dataset in an ascending order, the middle value is the median. The median is used whenever there are many outliers or skew in the distribution (which may bias the mean and make it less representative).

There are generally two topics associated with calculating the median, the first one relates to a dataset that contains an even number of values (for example, 24 observations) and the second one relates to a dataset that contains an uneven number of values (for example, 47 observations):

Calculating the median of an even data setIf the arranged dataset has an even number of values, the median is the average of the two middle values. Calculating the median of an uneven data setIf the arranged dataset has an uneven (odd) number of values, the median is simply the middle value.&#x20;

Let’s take a simple example of two datasets. Suppose you want to calculate the median on dataset A and dataset B. How would you do it?

* Dataset A = \[1, 2, 3, 4, 5]
* Dataset B = \[1, 2, 3, 4]

Dataset A contains five values which is an uneven number. This means that the middle value is the median. In this case, it is 3 (notice how it is also the mean of the dataset). Dataset B contains four values which is an even number. This means that the average between the two middle values is the median. In this case, it is 2.5 which is the average between 2 and 3.

[Figure 3-2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-2) shows the US CPI year-on-year values since 2003. The higher dashed line is the monthly median calculated since 2003. The lower dashed line symbolizes zero. Basically, this is like [Figure 3-1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-1) but instead of the mean, the median is charted.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0302.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 3-2. US CPI year-on-year changes since 2003**

You can create [Figure 3-2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-2) by using the following code:

<pre><code><strong># Calculating the median
</strong>median = cpi_latest["CPIAUCSL"].median() 
<strong># Printing the result
</strong>print('The median of the dataset: ' + str(median), '%')
<strong># Plotting the latest observations in black with a label
</strong>plt.plot(cpi_latest[:], color = 'black', linewidth = 1.5, 
         label = 'Change in CPI Year-on-Year')
<strong># Plotting horizontal lines that represent the mean and the zero threshold
</strong>plt.axhline(y = median, color = 'red', linestyle = 'dashed', 
            label = 'Median')
plt.axhline(y = 0, color = 'blue', linestyle = 'dashed', linewidth = 1)
plt.grid()
plt.legend()
</code></pre>

The output of the median should be as follows:

```
The median of the dataset: 2.12 %
```

Clearly, the median is less impacted by the recent outliers that are coming from unusual environments. The median is around 2.12% which more in line with the implied target of 2.00%.

**NOTE**

Remember that [Chapter 6](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch06.html#ch06) will give you all you need to know about the Python snippets you are seeing in this chapter, so you don’t need to worry if you are missing out on the coding concepts.

The last central tendency measure in this section is the _mode_, which is the value that is the most frequently observed (but also the least used in data analysis).

Let’s take a simple example of two datasets. Suppose you want to calculate the mode on the following datasets. How would you do it?

* Dataset A = \[1, 2, 2, 4, 5]
* Dataset B = \[1, 2, 3, 4]
* Dataset C = \[1, 1, 2, 2, 3]

Dataset A contains two times the value 2 which makes it the mode. Dataset B doesn’t have a mode as every value is observed once. Dataset C is _multimodal_ since it contains more than one mode (which are 1 and 2).

**NOTE**

The mode is useful with categorical variables (like credit rankings) as opposed to continuous variables (like price and returns time series)

You are unlikely to use the mode in analyzing time series as the mean and the median are more useful. To list a few examples that use the mean and the median in financial analysis:

* Calculating a moving mean (average) on the price data to detect the underlying trend. You will see more about moving averages in [Chapter 5](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch05.html#ch05).
* Calculating a rolling median on a price-derived indicator to detect its neutrality zone.
* Calculating the expected return of a security using the historical mean.

The discussion on central tendency metrics is very important especially that the mean and the median are heavily used not only as standalone indicators but also as components in more complex measures.

**NOTE**

The key takeaways from this section are as follows:

* There are mainly three central tendency measures: the mean, the median, and the mode.
* The mean is the sum divided by the quantity while the median is the value that splits the data in half. The mode is the most frequent value in the dataset.

### Measures of Variability

_Measures of variability_ describe how spread out the values in a dataset are relative to the central tendency measures (mostly the mean). The first and most known measure of variability is the variance. The _variance_ describes a set of numbers’ variability from their mean. The idea behind the variance’s formula is to determine how far away from the mean each data point is, then square those deviations to make sure that all numbers are positive (this is because distance cannot be negative). And finally, divide the deviations by the number of the observations.

The formula to find the variance is as follows:

�2=1�∑��=1(��-�¯)2

The intuition behind this formula is to calculate the sum of the squared deviations of each data point from the mean thus giving different distance observations, and then calculating the mean of these distance observations.

Let’s take a simple example of two datasets. Suppose you want to calculate the variance of dataset A and dataset B. How would you do it?

* Dataset A = \[1, 2, 3, 4, 5]
* Dataset B = \[5, 5, 5, 5]

The first step is to calculate the mean of the dataset as that is the benchmark from where you will calculate the variability of the data. Dataset A has a mean of 3. The next step calculates the variance:

(�1-�¯)2=(1-3)2=4

(�2-�¯)2=(2-3)2=1

(�3-�¯)2=(3-3)2=0

(�4-�¯)2=(4-3)2=1

(�5-�¯)2=(5-3)2=4

The previous results are summed up as follows:

4+1+0+1+4=10

And finally, the result is divided by the quantity of the observations to find the variance:

�2=105=2

As for dataset B, you should think about it intuitively. If the observations are all equal, they all represent the dataset which also means that they are their own mean. What would you say about the variance of the data in this case, considering that all the values are equal to the mean?

If your response is that the variance is zero, then you are correct. Mathematically, you can calculate it as follows:

(�1-�¯)2=(5-5)2=0

(�2-�¯)2=(5-5)2=0

(�3-�¯)2=(5-5)2=0

(�4-�¯)2=(5-5)2=0

The previous results sum up to zero and if you divide zero by 4 (the quantity of the dataset), you will get zero. Intuitively, there is no variance because all the values are constant and they do not deviate from their mean.

�2=04=0

Remaining in the inflation example, you can calculate the variance using the following code:

<pre><code><strong># Calculating the variance
</strong>variance = cpi_latest["CPIAUCSL"].var() 
<strong># Printing the result
</strong>print('The variance of the dataset: ' + str(variance), '%')
</code></pre>

The output of the variance should be as follows:

```
The variance of the dataset: 3.62 %
```

There is a flaw nonetheless, and it is that the variance represents squared values that are not comparable to the mean since they use different units. This is easily fixable by taking the square root of the variance. Doing so brings the next measure of variability, the _standard deviation._ It is the square root of the variance and is the average deviation of the values from the mean.

A low standard deviation indicates that the values tend to be close to the mean (low volatility), while a high standard deviation indicates that the values are spread out over a wider range relative to their mean (high volatility).

**NOTE**

The words standard deviation and volatility are used interchangeably. They refer to the same thing.

The formula to find the standard deviation is as follows:

�=1�∑��=1(��-�¯)2

If you consider the previous examples with the variance, then the standard deviation can be found as follows:

���������=2=1.41

���������=0=0

Standard deviation is commonly used with the mean since they both use the same units. You will soon understand the importance of this stat when I discuss the normal distribution function in the next section.

You can calculate the standard deviation in Python using the following code:

<pre><code><strong># Calculating the standard deviation
</strong>standard_deviation = cpi_latest["CPIAUCSL"].std() 
<strong># Printing the result
</strong>print('The standard deviation of the dataset: ' + 
      str(standard_deviation), '%')
</code></pre>

The output of the standard deviation should be as follows:

```
The standard deviation of the dataset: 1.90 %
```

How are you supposed to interpret the standard deviation? On average, the CPI year-on-year values tend to be ±1.90% from the mean of the same period which is at 2.49%.

In the coming section, you will see how to make better use of standard deviation figures. The last measure of variability in this section is the range. The _range_ is a very simple stat that shows the distance between the greatest value and the lowest value in the dataset. This gives you a quick glance about the two historical extreme values. The formula to find the range is as follows:

�����=���(�)-���(�)

In Python, you can easily do this as there are built-in functions that show the maximum and the minimum value given a set of data:

<pre><code><strong># Calculating the range
</strong>range_metric = max(cpi["CPIAUCSL"]) - min(cpi["CPIAUCSL"])
<strong># Printing the result
</strong>print('The range of the dataset: ' + str(range_metric), '%')
</code></pre>

The output of the following code should be as follows:

```
The range of the dataset: 16.5510 %
```

[Figure 3-3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-3) shows the CPI values since 1951. The diagonal dashed line represents the range and the horizontal dashed line represents the zero threshold.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0303.png" alt="" height="448" width="600"><figcaption></figcaption></figure>

**Figure 3-3. US CPI year-on-year change since 1951 with a diagonal dashed line that represents the range**

The range of the CPI shows the size of the variations in inflation measures from one period to another. Yearly changes in inflation numbers vary from one country to another. Generally, developed world countries such as France and the United States have stable variations (in times of stability) while emerging and frontier world countries such as Turkey and Argentina have more volatile and more extreme inflation numbers.

**NOTE**

The key takeaways from this section are as follows:

* The three key variability metrics that you should know are the variance, the standard deviation, and the range.
* The standard deviation is the square root of the variance. This is done so that it becomes comparable to the mean.
* The range is the difference between the highest and the lowest value in a dataset. It is a quick snapshot of the overall stretch of the observations.

### Measures of Shape

_Measures of shape_ describe the distribution of the values around the central tendency measures in a dataset. The mean and the standard deviation are the two factors that describe the normal distribution. The standard deviation depicts the spread or dispersion of the data and the mean reflects the distribution’s center.

A _probability distribution_ is a mathematical function that describes the likelihood of different outcomes or events in a random experiment. In other words, it gives the probabilities of all possible values of a random variable.

There are many types of probability distributions, including discrete and continuous distributions. _Discrete distributions_ take on a finite number of values. The most known discrete distributions are the Bernoulli distribution, Binomial distribution, and Poisson distribution.

_Continuous distributions_ are used for random variables that can take on any value within a given range (such as stock prices and returns). The most known distribution is the normal distribution.

The _normal distribution_ (also known as the _Gaussian distribution_) is a type of continuous probability distribution that is symmetrical around the mean and has a bell shape. It is one of the most widely used distributions in statistical analysis and is often used to describe natural phenomena such as age, weight, and test scores. [Figure 3-4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-4) shows the shape of a normal distribution.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0304.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 3-4. A normal distribution plot with mean = 0 and standard deviation = 1**

You can generate [Figure 3-4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-4) using the following code block:

<pre><code><strong># Importing libraries
</strong>import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats
<strong># Generate data for the plot
</strong>data = np.linspace(-3, 3, num = 1000)
<strong># Define the mean and standard deviation of the normal distribution
</strong>mean = 0
std = 1
<strong># Generate the function of the normal distribution
</strong>pdf = stats.norm.pdf(data, mean, std)
<strong># Plot the normal distribution plot
</strong>plt.plot(data, pdf, '-', color = 'black', lw = 2)
plt.axvline(mean, color = 'black', linestyle = '--')
plt.grid()
plt.show()
</code></pre>

**NOTE**

Since normally distributed variables are common, most statistical tests and models assume that the analyzed data is normal. With financial returns, they are assumed normal even though they experience a form of skew and kurtosis, two measures of shape discussed in this section.

In a normal distribution, the data is distributed symmetrically around the mean which also means that the mean is equal to the median and to the mode. Furthermore, around 68% of the data fall within one standard deviation from the mean, around 95% fall within two standard deviations, and around 99.7% fall within three standard deviations. This property makes the normal distribution a useful tool for making inferences.

To sum up, what you should retain from the normal distribution is the following:

* The mean and the standard deviation describe the distribution.
* The mean splits the distribution halfway making it equal to the median. Due to the symmetrical property, the mode is also equal to the mean and the median.

Now, let’s discuss the measures of shape. The first measure of shape is skewness. _Skewness_ describes a distribution’s asymmetry. It analyzes how far from being symmetrical the distribution deviates.

The skewness of a normal distribution is equal to zero. This means that the distribution is perfectly symmetrical around its mean, with an equal number of data points on either side of the mean.

A _positive skew_ indicates that the distribution has a long tail to the right which means that the mean is greater than the median due to the fact that the mean is sensible to outliers which will push it upwards (therefore, to the right of the _x_-axis). Similarly, the mode will be the lowest value between the three central tendency measures. [Figure 3-5](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-5) shows a positive skew.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0305.png" alt="" height="448" width="600"><figcaption></figcaption></figure>

**Figure 3-5. An example of a positively skewed distribution**

A _negative skew_ indicates that the distribution has a long tail to the left which means that the mean is lower than the median. Similarly, the mode will be the greatest value between the three central tendency measures. [Figure 3-6](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-6) shows a negative skew.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0306.png" alt="" height="448" width="600"><figcaption></figcaption></figure>

**Figure 3-6. An example of a negatively skewed distribution**

**NOTE**

How can skewness be interpreted in the world of financial markets? If the distribution is positively skewed, it means that there are more returns above the mean than below it (the tail of the distribution is longer on the positive side).

On the other hand, if the distribution is negatively skewed, it means that there are more returns below the mean than above it (the tail of the distribution is longer on the negative side).

The skew of a returns series can provide information about the risk and return of an investment. For example, a positively skewed returns series may indicate that the investment has a potential for a few large gains with a risk of frequent small losses.

The formula to find skewness is as follows:

�˜3=∑�=1�(��-�¯)3��3

Let’s check the skewness of the US CPI year-on-year data since 2003:

<pre><code><strong># Calculating the skew
</strong>skew = cpi_latest["CPIAUCSL"].skew() 
<strong># Printing the result
</strong>print('The skew of the dataset: ' + str(skew))
</code></pre>

The output of the following code should be as follows:

```
The skew of the dataset: 1.17
```

The skew of the data is 1.46 but what does that mean? Let’s chart the distribution of the data so that the interpretation becomes easier. You can do this using the following code snippet:

<pre><code><strong># Plotting the histogram of the data
</strong>fig, ax = plt.subplots()
ax.hist(cpi['CPIAUCSL'], bins = 30, edgecolor = 'black', color = 'white')
<strong># Add vertical lines for better interpretation
</strong>ax.axvline(mean, color='black', linestyle='--', label = 'Mean', 
           linewidth = 2)
ax.axvline(median, color='grey', linestyle='-.', label = 'Median', 
           linewidth = 2)
plt.grid()
plt.legend()
plt.show()
</code></pre>

[Figure 3-7](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-7) shows the result of the previous code snippet. The data is clearly positively skewed since the mean is greater than the median and the skewness is positive (above zero).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0307.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 3-7. Data distribution of the US CPI year-on-year**

Remember, skewness is a measure of the asymmetry of a probability distribution. It therefore, measures the degree to which the distribution deviates from normality. The rules of thumb to interpret skewness are as follows:

* If skewness is between -0.5 and 0.5, the data is considered symmetrical.
* If skewness is between -1.0 and – 0.5 or between 0.5 and 1.0, the data is considered mildly skewed.
* If skewness is less than -1.0 or greater than 1.0, the data is considered highly skewed

What does a positive skew mean? 1.17 is a highly skewed data (in the positive side) which is in line with a monetary policy that favors inflation as the economy grows (with a few inflationary spikes that cause the skew).

**NOTE**

It may be interesting to know that with a skewed distribution, the median may be the preferred metric since the mean tends to be pulled by outliers, thus distorting its value.

The next measure of shape is _kurtosis,_ which is a description of the peakedness or flatness of a distribution relative to a normal distribution. Kurtosis describes the tails of a distribution, in particular, whether the tails are thicker or thinner than those of a normal distribution.

A normal distribution has a kurtosis of 3, which means it is a mesokurtic distribution. If a distribution has a kurtosis greater than 3, it is referred to as leptokurtic, meaning it has a higher peak and fatter tails than a normal distribution. If a distribution has a kurtosis less than 3, it is referred to as platykurtic, meaning it has a flatter peak and thinner tails than a normal distribution.

The formula to find kurtosis is as follows:

�=∑�=1�(��-�¯)4��4

Sometimes, kurtosis is measured as _excess kurtosis_ to give it a starting value of zero (for a normal distribution). This means that the kurtosis measure is subtracted from 3 so as to calculate the excess kurtosis. Let’s calculate excess kurtosis for the US CPI year-on-year data:

<pre><code><strong># Calculating the excess kurtosis
</strong>excess_kurtosis = cpi_latest["CPIAUCSL"].kurtosis() 
<strong># Printing the result
</strong>print('The excess kurtosis of the dataset: ' + str(excess_kurtosis))
</code></pre>

The output of the following code should be as follows:

```
The excess kurtosis of the dataset: 2.15
```

In the case of the US CPI year-on-year values of the past twenty years, excess kurtosis is 2.17 which is more in line with a leptokurtic (peakier with fatter tails) distribution. A positive value indicates a distribution more peaked than normal and a negative kurtosis indicates a shape flatter than normal.

**NOTE**

Independent from statistics, it is interesting to know the terminology of what you are analyzing. _Inflation_ is the decrease in the purchasing power of the economic agents (such as households). The decrease in the purchasing power means that agents can buy less over time with the same amount of money. This is also referred to as a general price increase. Inflation in the economic sense has the following forms:

* _Inflation_: Controlled inflation is associated with a steady economic growth and expansion. It is a desired attribute for a growing economy. Regulators monitor inflation and try to stabilize it in order to prevent social and economic distress.
* _Deflation_: Whenever inflation is in the negative territory, it is referred to as deflation. Deflation is very dangerous for the economy, and as tempting as it may be for consumers who see a price decrease, deflation is a growth killer and may cause extended economic gluts which lead to unemployment and bearish stock markets.
* _Stagflation_: This occurs when inflation is either high or rising while the economic growth is slowing down. Simultaneously, unemployment remains high. It is one of the worst possible case scenarios.
* _Disinflation_: This is a decrease in inflation but in the positive territory. For example, if this year’s inflation is 2% while last year’s inflation is 3%, you can say that there was disinflation on a yearly basis.
* _Hyper inflation_: This is the nightmarish scenario that occurs when inflation goes out of control and experiences astronomical percent changes such as millions of percentage change from year to year (famous cases include Zimbabwe, Yugoslavia, and Greece).

Finally, one last metric to see in the descriptive statistics department, the quantiles. _Quantiles_ are measures of both shape and variability since they provide information about the distribution of values (shape) and provide information about the dispersion of such values (variability). The most used type of quantiles are called quartiles.

_Quartiles_ divide the dataset into four equal parts. This is done by arranging the data in order and then performing the split. Consider [Table 3-1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#table-3-1) as an example:

| Value |
| ----- |
| 1     |
| 2     |
| 4     |
| 5     |
| 7     |
| 8     |
| 9     |

The quartiles are as follows:

* The lower quartile (Q1) is the first quarter which in this case is 2.
* The midde quartile (Q2) which is also the median which in this case is 5.
* The upper quartile (Q3) in this case is 8.

Mathematically, you can calculate Q1 and Q3 using the following formulas:

�1=(�+14)

�3=3(�+14)

Keep in mind that the result of the formulae gives you the ranking of the values but not the values themselves:

�1=(7+14)=2������=2

�3=3(7+14)=6�ℎ����=8

The _interquartile range_ (IQR), is the difference between Q3 and Q1 and provides a measure of the spread of the middle 50% of the values in a data set. The IQR is robust to outliers (since it relies on middle values) and provides a brief summary of the spread of the bulk of the values. The IQR of the data in [Table 3-1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#table-3-1) is 6 as per the following formula:

���=�3-�1

���=8-2=6

The IQR is a valuable indicator and can be used as an input or a risk metric in many different models. It can also be used to detect outliers in the data since it is immune to them. Also, the IQR can help evaluate the current volatility of the analyzed asset which in turn can be used with other methods to create more powerful models. As understood, the IQR outperforms the range metric in terms of usefulness and interpretation as the former is prone to outliers.

Be careful with calculating quartiles as there are many methods that use different calculations for the same dataset. The most important thing is to use a consistent way all throughout your analyses. The method used to calculate the quartiles in [Table 3-1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#table-3-1) is called the _Tukey’s hinges_ method.

**NOTE**

The key takeways from this section are the following:

* The normal distribution is a continuous probability distribution that has a bell-shaped curve. The majority of the data cluster around the mean. The mean, median, and the mode of a normal distribution curve are all equal.
* Skewness measures the asymmetry of a probability distribution.&#x20;
* Kurtosis measures the peakedness of a probability distribution. Excess kurtosis is commonly used to describe the current probability distribution.
* Quantiles divide the arranged dataset into equal parts. The most famous quantiles are quartiles which divide the data into four equal parts.
* The IQR is the difference between the third quartile and the first quartile. It is immune to outliers and thus, very helpful in data analysis.

### Visualizing Data

If you remember from [Chapter 1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch01.html#ch01), I have presented a six-step process in data science. Step four dealt with data visualization. This section will show you a few ways to present the data in a clear visual manner that allows you to interpret it.

There are many types of statistical plots that are commonly used to visualize data such as scatter plots and line plots. Let’s discuss some of them.

The first data visualization method is _scatter plots_, which are used to graph the relationship between two variables through points that correspond to the intersection between the variables. Use the following code on the CPI data:

<pre><code><strong># Importing the required library
</strong>import matplotlib.pyplot as plt
<strong># Resetting the index
</strong>cpi = cpi.reset_index()
<strong># Creating the chart
</strong>fig, ax = plt.subplots()
ax.scatter(cpi['DATE'], cpi['CPIAUCSL'], color = 'black', 
           s = 8,  label = 'Change in CPI Year-on-Year')
plt.grid()
plt.legend()
plt.show()
</code></pre>

[Figure 3-8](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-8) shows the result of a scatter plot in time. This means that you have the CPI data as the first variable (_y_-axis) and time as the second variable (_x_-axis). However, scatter plots are more commonly used to compare variables, thus removing the time variable can give more insights.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0308.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 3-8. Scatter plot of US CPI versus the time axis**

Take the UK CPI year-on-year change and compare it with the US CPI year-on-year. Notice the positive association between the two in [Figure 3-9](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-9), as higher values of one are correlated with higher values of the other. Correlation is a key measure that you will see in detail in the next section. The code to plot [Figure 3-9](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-9) is as follows:

<pre><code><strong># Setting the beginning and end of the historical data
</strong>start_date = '1995-01-01'
end_date   = '2022-12-01'
<strong># Creating a dataframe and downloading the CPI data
</strong>cpi_us = pdr.DataReader('CPIAUCSL', 'fred', start_date, end_date)
cpi_uk = pdr.DataReader('GBRCPIALLMINMEI', 'fred', start_date, end_date)
<strong># Dropping the NaN values from the rows
</strong>cpi_us = cpi_us.dropna()
cpi_uk = cpi_uk.dropna()
<strong># Transforming the CPI into a year-on-year measure
</strong>cpi_us = cpi_us.pct_change(periods = 12, axis = 0) * 100
cpi_us = cpi_us.dropna()
cpi_uk = cpi_uk.pct_change(periods = 12, axis = 0) * 100
cpi_uk = cpi_uk.dropna()
<strong># Creating the chart
</strong>fig, ax = plt.subplots()
ax.scatter(cpi_us['CPIAUCSL'], cpi_uk['GBRCPIALLMINMEI'], 
           color = 'black', s = 8, label = 'Change in CPI Year-on-Year')
<strong># Adding a few aesthetic elements to the chart
</strong>ax.set_xlabel('US CPI')
ax.set_ylabel('UK CPI')
ax.axvline(x = 0, color='black', linestyle = 'dashed', linewidth = 1)
ax.axhline(y = 0, color='black', linestyle = 'dashed', linewidth = 1)
ax.set_ylim(-2,)
plt.grid()
plt.legend()
plt.show()
</code></pre>

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0309.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 3-9. Scatter plot of UK CPI versus US CPI**

Scatter plots are good when visualizing the correlation between data. They are also easy to draw and interpret. Generally, when the points are scattered in such a way that when a diagonal upwards sloping line can be drawn to represent them, the correlation is assumed to be positive since whenever variables on the _x_-axis increase, variables on the _y_-axis also increase. On the other hand, when a diagonal downwards sloping line can be drawn to represent the different variables, a negative correlation may exist. A negative correlation implies that whenever variables on the _x_-axis move, it is likely that variables on the _y_-axis move the other way.

[Figure 3-10](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-10) draw a best fit line between the two inflation data. Notice how it is upwards sloping:

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0310.png" alt="" height="448" width="600"><figcaption></figcaption></figure>

**Figure 3-10. Scatter plot of UK CPI versus US CPI with a best fit line**

Let’s now move to another charting method. _Line plots_ are basically scatter plots that are joined and are mostly charted against the time axis (_x_-axis). You have already seen line plots in previous charts such as Figures [3-1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-1) and [3-2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-2) as it is the most basic form of plotting.

​The advantage of line plots is their simplicity and ease of implementation. They also show the evolution of the series through time which helps detecting trends and patterns. In [Chapter 5](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch05.html#ch05), you will see a more elaborate version of plotting financial time series called _candlestick plots_. [Figure 3-11](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-11) shows a basic line plot on the US CPI since 1951:

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0311.png" alt="" height="448" width="600"><figcaption></figcaption></figure>

**Figure 3-11. Line plot of US CPI versus the time axis**

To create [Figure 3-11](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-11), you can use the following code snippet:

<pre><code><strong># Creating the chart
</strong>plt.plot(cpi['DATE'], cpi['CPIAUCSL'], color = 'black', 
         label = 'Change in CPI Year-on-Year')
plt.grid()
plt.legend()
plt.show()
</code></pre>

Next up are _bar plots_ which display the distribution of variables (generally, categorical). [Figure 3-12](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-12) shows a bar plot on the US CPI since the beginning of 2022:

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0312.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 3-12. Bar plot of US CPI versus the time axis**

To create [Figure 3-12](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-12), you can use the following code snippet:

<pre><code><strong># Taking the values of the previous twelve months
</strong>cpi_one_year = cpi.iloc[-12:]
<strong># Creating the chart
</strong>plt.bar(cpi_one_year['DATE'], cpi_one_year['CPIAUCSL'], 
        color = 'black', label = 'Change in CPI Year-on-Year', width = 7)
plt.grid()
plt.legend()
plt.show()
</code></pre>

Bar plots can be limited for plotting continuous data such as the US CPI or stock prices. They can also be misleading when the scale is off. Bar plots are also not recommended for large datasets since they clutter up the space. For the latter reason, histograms are a better fit.

A _histogram_ is a specific sort of bar chart that is used to display the frequency distribution of continuous data by using bars to represent statistical information. It indicates the number of observations that fall into the class or bin of values. An example of a histogram is [Figure 3-13](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-13) (and [Figure 3-7](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-7) from the last section):​

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0313.png" alt="" height="448" width="600"><figcaption></figcaption></figure>

**Figure 3-13. Histogram plot of US CPI**

<pre><code><strong># Creating the chart
</strong>fig, ax = plt.subplots()
ax.hist(cpi['CPIAUCSL'], bins = 30, edgecolor = 'black', 
        color = 'white', label = 'Change in CPI Year-on-Year',)
<strong># Add vertical lines for better interpretation
</strong>ax.axvline(0, color = 'black')
plt.grid()
plt.legend()
plt.show()
</code></pre>

Notice how the bar plot is charted against the time axis while the histogram does not have a time horizon because it is a group of values with the aim of showing the overall distribution points. Visually, you can see the positive skewness of the distribution.

**NOTE**

An example of a _categorical variable_ is gender.

Another classic plotting technique in statistics is the _box and whisker plot_. It is used to visualize the distribution of continuous variables while including the median and the quartiles, as well as the outliers. The way to understand the box and whisker plot is as follows:

* The box represents the IQR. The box is drawn between the first quartile and the third quartile. The height of the box indicates the spread of the data in this range.
* The line inside the box represents the median.
* The whiskers extend from the top and bottom of the box to the highest and lowest data points that are still within 1.5 times the IQR. These data points are called _outliers_ and are represented as individual points on the plot.

[Figure 3-14](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-14) shows a box and whisker plot on the US CPI since 1950:

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0314.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 3-14. Box and whisker plot of US CPI**

You can also plot it without the outliers (any value that lies more than one and a half times the length of the box from either end of the box). To create [Figure 3-14](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-14), you can use the following code snippet:

<pre><code><strong># Creating the chart
</strong>cpi_latest = cpi.iloc[-240:]
fig, ax = plt.subplots()
ax.boxplot(cpi_latest['CPIAUCSL'])
plt.grid()
plt.legend()
plt.show()
</code></pre>

To remove the outliers from the plot, you simply use the following tweak:

<pre><code><strong># Replace the corresponding code line with the following
</strong>fig, ax = plt.subplots()
ax.boxplot(cpi_latest['CPIAUCSL'], showfliers = False)
</code></pre>

Which will give you [Figure 3-15](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-15):

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0315.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 3-15. Box and whisker plot of US CPI with no outliers**

​Many more data visualization techniques exist such as _heatmaps_ (commonly used with correlation data and temperature mapping) and _pie charts_ (commonly used for budgeting and segmentation). It all depends on what you need to understand and what fits better with your needs. For example, a line plot is better suited for time series that only have one feature (for example, only the close price of a certain security is available). A histogram plot is better suited with probability distribution data.

**NOTE**

The key takeways from this section are the following:

* Data visualization depends on the type of analysis and interpretation you want to do. Some plots are better suited with certain types of data.&#x20;
* Data visualization helps with the initial interpretation before confirming it numerically.
* You are more likely to use line plots and candlestick plots when dealing with financial time series.

### Correlation

_Correlation_ is a measure used to calculate the degree of the linear relationship between two variables. It is a number between -1.0 and 1.0 with -1.0 designating a strong negative relationship between the variables and 1.0 designating a strong positive relationship.

A value of zero indicates that there is no linear association between the variables. However, correlation does not imply causation. Two variables are said to be correlated if they move in the same direction, but this does not imply that one causes the other to move or that they move as a result of the same events.

Most people agree that some assets have natural correlations. For instance, because they are both part of the same industry and are affected by the same trends and events, the stocks of Apple and Microsoft are positively connected (which means their general trend is in the same direction). [Figure 3-16](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-16) shows the chart between the two stocks. Notice how they move together.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0316.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 3-16. Apple and Microsoft stock prices since 2021**

The tops and bottoms of both stocks occur at almost the exact same time. Similarly, as the United States and the UK have similar economic drivers and impacts, they are also likely to have positively correlated inflation numbers as you have seen previously in the chapter.

Checking for correlation is done through visual interpretation and mathematical formulae. Before seeing an example, let’s deeply understand the roots of calculating correlation so that you know where it comes from and what are its limitations.

**NOTE**

Simply put, to calculate correlation, you need to measure how close the points in a scatter plot of the two variables are to a straight line. The more they look like a straight line, the more they are positively correlated, hence the term, _linear correlation_.

There are two main[2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#id242) ways to calculate correlation, it is either by using the Spearman method or the Pearson method.

The _Pearson_ correlation coefficient is a measure of the linear association between two variables calculated from the standard deviation and the covariance between them.

_Covariance_ calculates the average of the difference between the means of the two variables. If the two variables have a tendency to move together, the covariance is positive and if the two variables typically move in opposite directions, the covariance is negative. It ranges between positive infinity and negative infinity.

The formula for calculating the covariance between variables _x_ and _y_ is as follows:

�����=∑�=1�(��-�¯)(��-�¯)�

Therefore, covariance is the sum of the products of the average deviations between the variables and their respective means (which measures the degree of their association). An average is taken to normalize this calculation. The Pearson correlation coefficient is calculated as follows:

���=∑�=1�(��-�¯)(��-�¯)∑�=1�(��-�¯)2∑�=1�(��-�¯)2

Simplifying the previous correlation formula gives you the following:

���=���������

Therefore, Pearson correlation coefficient is simply the covariance between two variables divided by the product of their standard deviation. Let’s calculate the correlation between the US CPI year-on-year changes and the UK CPI year-on-year changes. The intuition is that the correlation is above zero as the UK and the US are economically related. The following code block calculates the Pearson correlation coefficient for the two time series:

<pre><code><strong># Importing the required libraries
</strong>import pandas_datareader as pdr
import pandas as pd
<strong># Setting the beginning and end of the historical data
</strong>start_date = '1995-01-01'
end_date   = '2022-12-01'
<strong># Creating a dataframe and downloading the CPI data
</strong>cpi_us = pdr.DataReader('CPIAUCSL', 'fred', start_date, end_date)
cpi_uk = pdr.DataReader('GBRCPIALLMINMEI', 'fred', start_date, end_date)
<strong># Dropping the nan values from the rows
</strong>cpi_us = cpi_us.dropna()
cpi_uk = cpi_uk.dropna()
<strong># Transforming the US CPI into a year-on-year measure
</strong>cpi_us = cpi_us.pct_change(periods = 12, axis = 0) * 100
cpi_us = cpi_us.dropna()
<strong># Transforming the UK CPI into a year-on-year measure
</strong>cpi_uk = cpi_uk.pct_change(periods = 12, axis = 0) * 100
cpi_uk = cpi_uk.dropna()
<strong># Joining both CPI data into one dataframe
</strong>combined_cpi_data = pd.concat([cpi_us['CPIAUCSL'], 
                               cpi_uk['GBRCPIALLMINMEI']], axis = 1)
<strong># Calculating Pearson correlation
</strong>combined_cpi_data.corr(method = 'pearson')
</code></pre>

The output is as follows:

```
                 CPIAUCSL  GBRCPIALLMINMEI
CPIAUCSL         1.000000         0.732164
GBRCPIALLMINMEI  0.732164         1.000000
```

The correlation between the two is a whopping 0.73. This is in line with the expectations. Pearson correlation is usually used with variables that have proportional changes and are normally distributed.

_Spearman correlation_ is a non-parametric rank correlation that measures the strength of the relationship between the variables. It is suitable for variables that do not follow a normal distribution.

**NOTE**

Remember, financial returns are not normally distributed but are sometimes treated that way for simplicity.

Unlike Pearson correlation, the Spearman rank correlation takes into account the order of the values, rather than the actual values. To calculate Spearman correlation, follow these steps:

1. Rank the values of each variable. This is done by inputing 1 instead of the smallest variable and inputing the length of the dataset instead of the largest number.
2. Calculate the difference in ranks. Mathematically, the difference in ranks is represented by the letter _d_ in the mathematical formula to come. Then, calculate their squared differences.
3. Sum the squared differences you have calculated from step 2.
4. Use the following formula to calculate Spearman correlation.

�=1-6∑�=1���2�3-�

As with Pearson correlation, Spearman correlation also ranges from -1.00 to 1.00 with the same interpretation.

**NOTE**

Strong positive correlations are generally upwards of 0.70, while strong negative correlations are generally downwards of -0.70.

The following code block calculates the Spearman rank correlation coefficient for the two time series:

<pre><code><strong># Calculating Spearman rank correlation
</strong>combined_cpi_data.corr(method = 'spearman')
</code></pre>

The output is as follows:

```
                 CPIAUCSL  GBRCPIALLMINMEI
CPIAUCSL         1.000000         0.472526
GBRCPIALLMINMEI  0.472526         1.000000
```

Let’s answer a very important question after getting this difference in results. Why are the two measures so different?

The first thing to keep in mind is what they measure. Pearson correlation measures the linear relationship (trend) between the variables while Spearman rank correlation measures the monotonic trend. The word _monotonic_ refers to moving in the same direction but not exactly at the same rate or magnitude. Also, Spearman correlation transforms the data to an ordinal type (through the ranks) as opposed to Pearson correlation which uses the actual values.

_Autocorrelation_ (also referred to as serial correlation) is a statistical method used to look at the relationship between a given time series and a lagged version of it. It is generally used to predict future values through patterns in data, such as seasonality or trends. Autocorrelation is therefore the values’ relationship with the previous values. For example, comparing each day’s Microsoft stock price to the preceding day and see if there is a discernible correlation there. Algorithmically speaking, this can be represented in [Table 3-2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#table-3-2):

| t             | t-1           |
| ------------- | ------------- |
|  $       1.25 |  $       1.65 |
|  $       1.77 |  $       1.25 |
|  $       1.78 |  $       1.77 |
|  $       1.25 |  $       1.78 |
|  $       1.90 |  $       1.25 |

Each row represents a time period. Column _t_ is the current price and column _t-1_ is the previous price put on the row that represents the present. This is done when creating machine learning models so as to understand the relationship between the current values and the previous ones at every time step (row).

Positive auto correlation frequently occurs in trending assets and is associated with the idea of persistence (trend following). On the other hand, ranging markets exhibit negative auto correlation, which is associated with the idea of anti-persistence (mean reversion).

**NOTE**

Measures of short-term correlation between different time series (for example, Nvidia and Oracle stocks) are typically computed using returns (such as differences) on prices rather than real prices. However, it is possible to utilize the prices directly to identify long-term trends.

The following code block calculates the autocorrelation of the US CPI year-on-year:

<pre><code><strong># Creating a dataframe and downloading the CPI data
</strong>cpi = pdr.DataReader('CPIAUCSL', 'fred', start_date, end_date)
<strong># Transforming the US CPI into a year-on-year measure
</strong>cpi = cpi.pct_change(periods = 12, axis = 0) * 100
cpi = cpi.dropna()
<strong># Transforming the data frame to a series structure
</strong>cpi = cpi.iloc[:,0]
<strong># Calculating autocorrelation with a lag of 1
</strong>print('Correlation with a lag of 1 = ', round(cpi.autocorr(lag = 1), 2))
<strong># Calculating autocorrelation with a lag of 6
</strong>print('Correlation with a lag of 6 = ', round(cpi.autocorr(lag = 6), 2))
<strong># Calculating autocorrelation with a lag of 12
</strong>print('Correlation with a lag of 12 = ', round(cpi.autocorr(lag = 12), 2))
</code></pre>

A lag of 12 means that every data is compared to the data from twelve periods ago and then a correlation measure is calculated. The output of the code is as follows:

```
Correlation with a lag of 1 =  0.97
Correlation with a lag of 6 =  0.65
Correlation with a lag of 12 =  0.17
```

Now, before proceeding to the next section, let’s revert back to information theory and discuss one interesting correlation coefficient that is able to pick-up on non-linear relationships. One of these ways is the maximal information coefficient (MIC).

The _maximal information coefficient_ (MIC) is a non-parametric measure of association between two variables designed to handle large and complex data. It is generally seen as a more robust alternative to traditional measures of correlation, such as Pearson correlation and Spearman rank correlation. Introduced by _Reshef et al._, the MIC uses concepts from information theory that you have seen in [Chapter 2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch02.html#ch02).

The MIC measures the strength of the association between two variables by counting the number of cells in a contingency table that are maximally informative about the relationship between the variables. The MIC value ranges from 0 to 1, with higher values indicating stronger associations. It can handle high-dimensional data and can identify non-linear relationships between variables. It is however non-directional which means that values close to 1 only suggest a strong correlation between the two variables but it does not say whether the correlation is positive or negative.

**NOTE**

In other words, the mutual information between the two variables within each bin is calculated after the range of each variable has been divided into a set of bins.

The strength of the association between the two variables is then estimated using the maximum mutual information value across all bins.

Let’s check out a practical example that showcases the strength of the MIC in detecting non-linear relationships. The following example simulates two wave series (sine and cosine). Intuitively, looking at [Figure 3-17](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-17), it seems that there is a lag-lead relationship between the two.&#x20;

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0317.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 3-17. The two wave series showing a form of non-linear relationship**

The following Python code snippet creates the two time series and plots [Figure 3-17](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-17):

<pre><code><strong># Importing the required libraries
</strong>import numpy as np
import matplotlib.pyplot as plt
<strong># Setting the range of the data
</strong>data_range = np.arange(0, 30, 0.1)
<strong># Creating the sine and the cosine waves
</strong>sine = np.sin(data_range)
cosine = np.cos(data_range)
<strong># Plotting
</strong>plt.plot(sine, color = 'black', label = 'Sine Function')
plt.plot(cosine, color = 'grey', linestyle = 'dashed', 
         label = 'Cosine Function')
plt.grid()
plt.legend()
</code></pre>

Now, the job is to calculate the three correlation measures and analyze their results. For simplicity, the code will be ommited from this section and will be shared in [Chapter 6](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch06.html#ch06) as there are a few things to understand in Python first. The following is the result:

```
Correlation | Pearson:  0.035
Correlation | Spearman:  0.027
Correlation | MIC: 0.602
```

Let’s interpret the results:

* _Pearson correlation_: Notice the absence of any type of correlation here due to it missing out on the non-linear association.
* _Spearman correlation_: The same situation applies here with an extremely weak correlation.
* _MIC_: The measure returned a strong relationship of 0.60 between the two which is closer to reality. It seems that the MIC states that both waves have a strong relationship albeit non-linear.

The MIC is useful in economic analysis, financial analysis, and even finding trading signals if used properly. Non-linear relationships are abundant in such complex fields, and being able to detect them may provide a sizable edge.

**NOTE**

The key takeways from this section are the following:

* Correlation is a measure used to calculate the degree of the linear relationship between variables. It is a number between -1.0 and 1.0 with -1.0 designating a strong negative relationship between the variables and 1.0 designating a strong positive relationship.
* There are two main types of correlation measures, Spearman and Pearson. Both have their advantages and limitations.
* Autocorrelation is the correlation of the variable with its own lagged values. For example, if the autocorrelation of Walmart’s stock returns is positive, it denotes a trending configuration.
* Correlation measures can also refer to non-linear relationships when you use the right tool, for example, the MIC.

### The Concept of Stationarity

Stationarity is one of the key concepts in statistical analysis and machine learning. _Stationarity_ is when the statistical characteristics of the time series (mean, variance, etc.) are constant over time. In other words, no discernable trend is detectable when plotting the data across time.

The different learning models rely on data stationarity as it is one of the basics of statistical modelling and this is mainly for simplicity. In finance, price time series are not stationary as they show trends with varying variance (volatility). Take a look at [Figure 3-18](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-18) and see if you can detect a trend. Would you say that this time series is stationary?

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0318.png" alt="" height="448" width="600"><figcaption></figcaption></figure>

**Figure 3-18. Simulated data with a varying mean across time**

Naturally, the answer is no as a rising trend is clearly in progress. States like this are undesirable for statistical analyses and machine learning. Luckily, there are transformations that you can apply to the time series to make them stationary. But first, let’s see how to check for stationarity the mathematical way as the visual way does not prove anything. The right process to deal with data stationarity problems is to follow these steps:

1. Check for stationarity using the different statistical tests that you will see in this section.
2. If the tests show data stationarity, you are ready to use the data for the algorithms. If the tests show that the data is not stationary, you have to proceed to the next step.
3. Subtract every value from the value before it (difference the values).
4. Re-check for stationarity using the same tests on the new transformed data.
5. If the test shows data stationarity, then you have successfully transformed your data. Otherwise, re-do the transformation and check again until you have stationary data.

**NOTE**

Ascending or descending time series have varying mean and variances through time and are therefore most likely non-stationary. There are exceptions to this and you will see why later.

Remember, the aim of stationarity is stable and constant mean and variance over time. Therefore, when you look at [Figure 3-19](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-19), what can you infer?

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0319.png" alt="" height="448" width="600"><figcaption></figcaption></figure>

**Figure 3-19. Simulated data with a mean around zero across time**

Visually, it looks like the data does not have a trend and it also looks like it fluctuates around a stable mean with stable variance around it. The first impression is that the data is stationary. Of course, this must be proved by statistical tests.

The first and most basic test is the _Augmented-Dickey-Fuller_ (ADF) test. The ADF tests for stationarity using hypothesis testing.

The ADF test searches for a unit root in the data. A _unit root_ is a property of non-stationary data and in the context of time series analysis, refers to a characteristic of a stochastic process where the series has a root equal to 1. In simpler terms, it means that its statistical properties, such as the mean and variance, change over time. Here’s what you need to know:

* The null hypothesis assumes the presence of a unit root. This means that if you are trying to prove that the data is stationary, you are looking to reject the null hypothesis (as seen in the hypothesis testing section from [Chapter 2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch02.html#ch02)).
* The alternative hypothesis is therefore the absence of a unit root and the stationarity of the data.
* The p-value obtained from the test must be less than the significance level chosen (in most cases, it is 5%).

Let’s take the US CPI year-on-year data and test it for stationarity. The following code snippet checks for stationarity using the ADF test:

<pre><code><strong># Importing the required library
</strong>from statsmodels.tsa.stattools import adfuller
<strong># Applying the ADF test on the CPI data
</strong>print('p-value: %f' % adfuller(cpi)[1])
</code></pre>

The output of the code is as follows:

```
p-value: 0.0152
```

Assuming a 5% significance level, it seems that it is possible to accept that the year-on-year data is stationary (however, if you want to be stricter and use a 1% significance level, then the p-value suggests that the data is non-stationary). In any way, even looking at the chart can make you scratch your head. Remember that in [Figure 3-11](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-11), the yearly changes in the US CPI seem to be stable but does not resemble stationary data. This is why numerical and statistical tests are used.

Now, let’s the do the same thing but omit taking the yearly changes. In other words, applying the code on the raw US CPI data and not taking year-on-year changes. Here’s the code:

<pre><code><strong># Creating a dataframe and downloading the CPI data
</strong>cpi = pdr.DataReader('CPIAUCSL', 'fred', start_date, end_date)
<strong># Applying the ADF test on the CPI data
</strong>print('p-value: %f' % adfuller(cpi)[1])
</code></pre>

The output of the code is as follows:

```
p-value: 0.999
```

Clearly, the p-value is greater than all significance levels which means that the time series is non-stationary. Let’s sum up these results:

* It seems that you can reject the null hypothesis using a 5% significance level when it comes to the year-on-year changes on the US CPI. The dataset is assumed to be stationary.
* It seems that you cannot reject the null hypothesis using a 5% significance level when it comes to the raw values of the US CPI. The dataset is assumed to be non-stationary.

This becomes obvious when you plot the raw values of the US CPI, as shown in [Figure 3-20](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-20).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0320.png" alt="" height="448" width="600"><figcaption></figcaption></figure>

**Figure 3-20. Absolute values of the US CPI showing a clearly trending nature**

The other test that you must be aware of is called the The _Kwiatkowski-Phillips-Schmidt-Shin_ (KPSS) which is also a statistical test with the aim of determining whether the time series is stationary or non-stationary. However, the KPSS test can detect stationarity in trending time series which makes it a powerful tool.

Trending time series can actually be stationary on the condition they have a stable mean.

**WARNING**

The ADF test has a a null hypothesis that argues for non-stationarity and an alternative hypothesis that argues for stationarity. The KPSS test on the other hand, has a null hypothesis that argues for stationarity and an alternative hypothesis that argues for non-stationarity.

Before analyzing the inflation data, let’s see how can a trending time series be stationary. Remember that stationarity refers to a stable mean and standard deviation, so if somehow you have a gradually ascending or descending time series with stable statistical properties, it may be stationary. The next code snippet simulates a sine wave and then adds a touch of a trending nature to it:

<pre><code><strong># Importing the required libraries
</strong>import numpy as np
import matplotlib.pyplot as plt
<strong># Creating the first time series using sine waves
</strong>length = np.pi * 2 * 5
sinewave = np.sin(np.arange(0, length, length / 1000))
<strong># Creating the second time series using trending sine waves
</strong>sinewave_ascending = np.sin(np.arange(0, length, length / 1000))
<strong># Defining the trend variable
</strong>a = 0.01
<strong># Looping to add a trend factor
</strong>for i in range(len(sinewave_ascending)): 
    sinewave_ascending[i] = a + sinewave_ascending[i]
    a = 0.01 + a
</code></pre>

Plotting the two series as shown in [Figure 3-21](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-21) shows how the trending sinewave seems to be stable. But let’s prove this through statistical tests.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0321.png" alt="" height="448" width="600"><figcaption></figcaption></figure>

**Figure 3-21. A normal sine wave simulated series with a trending sine wave**

[Figure 3-21](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#figure-3-21) is generated using the following code (make sure you have defined the series using the previous code block):

<pre><code><strong># Plotting the series
</strong>plt.plot(sinewave, label = 'Sine Wave', color = 'black')
plt.plot(sinewave_ascending, label = 'Ascending Sine Wave', 
         color = 'grey')
plt.grid()
plt.legend()
plt.show()
</code></pre>

Let’s try the ADF test on both series and see what the results are:

<pre><code><strong># ADF testing | Normal sine wave
</strong>print('p-value: %f' % adfuller(sinewave)[1])
<strong># ADF testing | Ascending sine wave
</strong>print('p-value: %f' % adfuller(sinewave_ascending)[1])
</code></pre>

The output is as follows:

<pre><code><strong>p-value: 0.000000 # For the sine wave
</strong><strong>p-value: 0.989341 # For the ascending sine wave
</strong></code></pre>

Clearly, the ADF test is consistent with the idea that trending markets cannot be stationary. But what about the KPSS test? The following code uses the KPSS on the same data to check for stationarity:

<pre><code><strong># Importing the KPSS library
</strong>from statsmodels.tsa.stattools import kpss
<strong># KPSS testing | Normal sine wave
</strong>print('p-value: %f' % kpss(sinewave)[1])
<strong># KPSS testing | Ascending sine wave
</strong>print('p-value: %f' % kpss(sinewave_ascending)[1])
<strong># KPSS testing while taking into account the trend | Ascending sine wave
</strong>print('p-value: %f' % kpss(sinewave_ascending, regression = 'ct')[1])
<strong>'''
</strong><strong>The 'ct' argument is used to check if the dataset is stationary 
</strong><strong>around a trend. By default, the argument is 'c' which is used
</strong><strong>to check if the data is stationary around a constant.
</strong><strong>'''
</strong></code></pre>

The output is as follows:

<pre><code><strong>p-value: 0.10 # For the sine wave
</strong><strong>p-value: 0.01 # For the ascending sine wave without trend consideration
</strong><strong>p-value: 0.10 # For the ascending sine wave with trend consideration
</strong></code></pre>

Remember that the null hypothesis of the KPSS test is that the data is stationary, therefore if the p-value is greater than the significance level, the data is considered stationary since it is not possible to reject the null hypothesis.

The KPSS statistic when taking into account the trend, states that the ascending sine wave is a stationary time series. This is a basic example on how you can find stationary data in trending time series.

Let’s take the US CPI year-on-year data and test it for stationarity. The following code snippet checks for stationarity using the KPSS test:

<pre><code><strong># Applying the KPSS (no trend consideration) test on the CPI data
</strong>print('p-value: %f' % kpss(cpi)[1])
<strong># Applying the KPSS (with trend consideration) test on the CPI data
</strong>print('p-value: %f' % kpss(cpi, regression = 'ct')[1])
</code></pre>

The output of the code is as follows:

<pre><code><strong>p-value: 0.010000 # without trend consideration
</strong><strong>p-value: 0.010000 # with trend consideration
</strong></code></pre>

It seems that the results from the KPSS test are in contradiction with the results from the ADF test. This may happen from time to time and differencing may solve the issue (bear in mind, that the year-on-year data is already a differenced time series from the absolute CPI values but some time series may need more than one differencing to become stationary and it also depends on the period of differencing). The safest solution in contradiction is to transform once more the data.

Before finishing this section on stationarity, let’s discuss a complex topic that you will later see in action in [Chapter 9](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch09.html#ch09). Transforming the data may cause an unusual problem that is, _memory loss_. In his book, _Marco Lopez de Prado_ proposed a technique called fractional differentiation with the aim of making data stationary while preserving some memory.

When a non-stationary time series is differenced in the aim of making it stationary, memory loss occurs, which is another way of saying that the autocorrelation between the values is greatly reduced, thus removing the trend component and the DNA of the underlying asset. The degree of differencing and the persistence of the autocorrelation structure in the original series determines how much memory loss occurs.

**NOTE**

The key takeways from this section are the following:

* Stationarity refers to the concept of stable mean and variance through time. It is a desired characteristic as most machine learning models rely on it.
* Financial price time series are most likely non-stationary and require a first order differencing to become stationary and ready for statistical modelling. Some may even require a second order transformation to become stationary.
* The ADF and KPSS tests check for stationarity in the data with the latter being able to check for stationarity in trending data.
* Trending data may be stationary. Although this characteristic is rare, the KPSS is able to detect the stationarity as opposed to the ADF test.

### Regression Analysis and Statistical Inference

_Inference,_ as Oxford Languages defines it, is a conclusion reached on the basis of evidence and reasoning. Therefore, as opposed to descriptive statistics, inferential statistics use the data or a sample of the data to make inferences (forecasts). The main tool in statistical inference is linear regression.

_Linear regression_ is a basic machine learning algorithm you will see in this book as of [Chapter 7](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#ch07) with the other machine learning algorithms. Hence, let’s just present the intuition of regression analysis in this section. The most basic form of a linear regression equation is as follows:

�=�+��+�

����ℎ������������������,�����ℎ�������������������

����ℎ��������������������,�����ℎ����������������������������

����ℎ�����������������ℎ�������������������ℎ���ℎ�����������������������������������

������������ℎ��ℎ�������ℎ��������������������������ℎ�������ℎ��������������������

����ℎ������������ℎ���������������������

The basic linear regression equation states that a dependent variable (what you want to forecast) is explained by a constant, a sensitivity-adjusted variable, and a residual (error term to account for unexplained variations). Consider [Table 3-3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#table-3-3):

| y   | x   |
| --- | --- |
| 100 | 49  |
| 200 | 99  |
| 300 | 149 |
| 400 | 199 |
| ?   | 249 |

The linear equation to predict _y_ given _x_, is as follows:

��=2+2��

Therefore, the latest _y_ given _x_ = 249 should be 500:

��=2+2��=2+(2×249)=500

Notice how linear regression perfectly captures the linear relationship between the two variables since there is no residual (unexplained variations). When a linear regression perfectly captures the relationship between two variables, it means that their coordinate points are perfectly aligned on a linear line across the _x_-axis.

Multiple linear regression can take the following form:

��=�+�1�1+...+����+��

This basically means that the dependent variable _y_ may be impacted by more than one variable. For instance, if you want to estimate housing prices, you may want to take into account the number of rooms, the surface area, the neighborhood, and any other variable that is likely to impact the price.  Similarly, if you want to predict commodity prices, you may want to take into account the different macroeconomic factors, exchange rates, and any other alternative data.

It is important to understand what every variable refers to. Linear regression has a few assumptions:

Linear relationshipThe relationship between the dependent variable and the independent variable(s) should be linear, meaning that a straight line across the plane can describe the relationship. This is rare in real life when dealing with complex variables.Independence of variablesThe observations should be independent of each other, meaning that the value of one observation does not influence the value of another observation.HomoscedasticityThe variance of the residuals (the difference between the predicted and actual values of the dependent variable) should be constant across all levels of the independent variable(s).Normality of the residualsThe residuals should be normally distributed, meaning that the majority of the residuals are close to zero and the distribution is symmetrical.

​In case of a multiple linear regression, you can add a new assumption, that is the absence of _multicollinearity_. ​The independent variables should not be highly correlated with each other, otherwise it can make it difficult to determine the unique effect of each independent variable on the dependent variable. In other words, this prevents redundancy.

**NOTE**

The key takeways from this section are the following:

* Linear regression is part of the inferential statistics field and it is a linear equation that describes the relationship between variables.
* Linear regression interprets and predicts data following an equation that you obtain when you train past data and expect the relationship to hold in the future.&#x20;

### Summary

Being able to perform data analysis is key towards deploying the right algorithms in order to predict the future values of the time series. Understanding data is done through a wide selection of tools coming from the statistics world. Make sure you understand what stationarity and what correlation are as they offer extremely valuable insights in modeling.

[1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#id241-marker) https://github.com/sofienkaabar/deep-learning-for-finance

[2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#id242-marker) Among others but these two ways are the most popular representations.
