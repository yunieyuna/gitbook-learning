# 8. Deep Learning for Time Series Prediction I

## Chapter 8. Deep Learning for Time Series Prediction I

_Deep learning_ is that slightly more complex and more detailed field than machine learning. Machine learning and deep learning both fall under the umbrella of data science. As you will see, deep learning is mostly about neural networks, a highly sophisticated and powerful algorithm that has enjoyed a lot of coverage and hype and for good reason: it is very powerful and able to catch highly complex non-linear relationships between different variables.

The aim of this chapter is to understand the functioning of neural networks before using them to predict financial time series in Python just like you saw in [Chapter 7](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#ch07).

## A Walk Through Neural Networks

_Artificial neural networks_ (ANNs) have their roots in the study of neurology, where researchers sought to comprehend how the human brain and its intricate network of interconnected neurons functioned. ANNs are designed to produce computational representations of biological neural network behavior.

ANNs have been around since the 1940s and 1950s, when academics first started looking into ways to build computational models based on the human brain. Logician Walter Pitts and neurophysiologist Warren McCulloch were among the early pioneers in this subject. They developed the idea of a computational model based on simplified artificial neurons in a paper[1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#id258).

The development of artificial neural networks gained further momentum in the 1950s and 1960s when researchers like Frank Rosenblatt worked on the _perceptron_, a type of artificial neuron that could learn from its inputs. Rosenblatt’s work paved the way for the development of single-layer neural networks capable of pattern recognition tasks.

With the creation of multi-layer neural networks, also known as _deep neural networks_, and the introduction of more potent algorithms, artificial neural networks made significant strides in the 1980s and 1990s. This innovation made it possible for neural networks to learn hierarchical data representations, which enhanced their performance on challenging tasks. Although multiple researchers contributed to the development and advancement of artificial neural networks, one influential figure is Geoffrey Hinton. Hinton, along with his collaborators, made significant contributions to the field by developing new learning algorithms and architectures for neural networks. His work on deep learning has been instrumental in the recent resurgence and success of artificial neural networks.

An ANN consists of interconnected nodes, called artificial neurons, organized into layers. The layers are typically divided into three types:

* _Input layer_: The input layer receives input data, which could be numerical, categorical, or even raw sensory data. Input layers are explanatory variables that are supposed to be predictive in nature.
* _Hidden layers_: The hidden layers (one or more) process the input data through their interconnected neurons. Each neuron in a layer receives inputs, performs a computation (discussed later), and passes the output to the next layer.
* _Output layer_: The output layer produces the final result or prediction based on the processed information from the hidden layers. The number of neurons in the output layer depends on the type of problem the network is designed to solve.

[Figure 8-1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-1) shows an illustration of an artificial neural network where the information flows from left to right. It begins with the two inputs being connected to the four hidden layers where calculation is done before outputting a weighted prediction in the output layer.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0801.png" alt="" height="292" width="571"><figcaption></figcaption></figure>

**Figure 8-1. A simple illustration of an artificial neural network**

Each neuron in the ANN performs two main operations:

1. The neuron receives inputs from the previous layer or directly from the input data. Each input is multiplied by a weight value, which represents the strength or importance of that connection. The weighted inputs are then summed together.
2. After the weighted sum, an activation function (discussed in the next section) is applied to introduce non-linearity into the output of the neuron. The activation function determines the neuron’s output value based on the summed inputs.

During the training process, the ANN adjusts the weights of its connections to improve its performance. This is typically done through an iterative optimization algorithm, such as gradient descent, where the network’s performance is evaluated using a defined loss function. The algorithm computes the gradient of the loss function with respect to the network’s weights, allowing the weights to be updated in a way that minimizes the error.

ANNs have the ability to learn and generalize from data, making them suitable for tasks like pattern recognition and regression. With the advancements in deep learning, ANNs with multiple hidden layers have shown exceptional performance on complex tasks, leveraging their ability to learn hierarchical representations and capture intricate patterns in the data.

**NOTE**

It is worth noting that the process from inputs to outputs is referred to as _forward propagation_.

### Activation Functions

_Activation functions_ in neural networks introduce non-linearity to the output of a neuron, allowing neural networks to model complex relationships and learn from non-linear data. They determine the output of a neuron based on the weighted sum of its inputs. Let’s discuss these activation functions in detail.

The _sigmoid activation function_ maps the input to a range between 0 and 1 making it suitable for binary classification problems or as a smooth approximation of a step function. The mathematical representation of the function is as follows:

�(�)=11+�-�

Among the advantages of the sigmoid activation function:

* It is a smooth and differentiable function that facilitates gradient-based optimization algorithms.
* It squashes the input to a bounded range, which can be interpreted as a probability or confidence level.

However, it has its limitations as well:

* It suffers from the _vanishing gradient problem_, where gradients become very small for extreme input values. This can hinder the learning process.
* Outputs are not zero-centered, making it less suitable for certain situations, such as optimizing weights using symmetric update rules like the gradient descent.

[Figure 8-2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-2) shows the sigmoid function chart.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0802.png" alt="" height="600" width="600"><figcaption></figcaption></figure>

**Figure 8-2. Graph of the sigmoid function**

The next activation function is the _hyperbolic tangent function_ _(tanh)_ which you have seen in [Chapter 4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch04.html#ch04). The mathematical representation of the function is as follows:

���ℎ(�)=��-�-���+�-�

Among the advantages of the hyperbolic tangent function:

* It is similar to the sigmoid function but zero-centered, which helps alleviate the issue of asymmetric updates in weight optimization.
* Its non-linearity can capture a wider range of data variations compared to the sigmoid function.

Among its limitations:

* It also suffers from the vanishing gradient problem, particularly in deep networks.
* Outputs are still susceptible to saturation at the extremes, resulting in gradients close to zero.

[Figure 8-3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-3) shows the hyperbolic tangent function chart.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0803.png" alt="" height="600" width="600"><figcaption></figcaption></figure>

**Figure 8-3. Graph of the hyperbolic tangent function**

The next function is called _ReLu activation function_. It stands for _rectified linear unit_. This function sets negative values to zero and keeps the positive values unchanged. It is efficient and helps avoid the vanishing gradient problem. The mathematical representation of the function is as follows:

�(�)=���(0,�)

Among the advantages of the ReLu function:

* It is simple to implement, as it only involves taking the maximum of 0 and the input value. The simplicity of ReLU leads to faster computation and training compared to more complex activation functions.
* It helps mitigate the vanishing gradient problem that can occur during deep neural network training. The derivative of ReLU is either 0 or 1, which means that the gradients can flow more freely and avoid becoming exponentially small as the network gets deeper.

Among the limitations of the function:

* It outputs 0 for negative input values, which can lead to information loss. In some cases, it may be beneficial to have activation functions that can produce negative outputs as well.
* It is not a smooth function because its derivative is discontinuous at 0. This can cause optimization difficulties in certain scenarios.

[Figure 8-4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-4) shows the ReLu function chart.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0804.png" alt="" height="600" width="600"><figcaption></figcaption></figure>

**Figure 8-4. Graph of the ReLu function**

One more stop to go: the _leaky ReLu activation function_. This activation function is an extension of the ReLU function that introduces a small slope for negative inputs. The mathematical representation of the function is as follows:

�(�)=���(0.01�,�)

Leaky ReLU addresses the dead neuron problem in ReLU and allows some activation for negative inputs, which can help with the flow of gradients during training.

Among the advantages of the leaky ReLu function:

* It overcomes the issue of dead neurons that can occur with ReLU. By introducing a small slope for negative inputs, Leaky ReLU ensures that even if a neuron is not activated, it can still contribute to the gradient flow during training.
* It is a continuous function, even at negative input values. The non-zero slope for negative inputs allows the activation function to have a defined derivative throughout its input range.

Among the limitations of the function:

* The slope of the leaky part is a hyperparameter that needs to be set manually. It requires careful tuning to strike a balance between avoiding dead neurons and preventing too much leakage that may hinder the non-linearity of the activation function.
* Although leaky ReLU provides a non-zero response for negative inputs, it does not provide the same level of negative activation as some other activation functions, such as the hyperbolic tangent (tanh) or sigmoid. In scenarios where a strong negative activation response is desired, other activation functions might be more suitable.

[Figure 8-5](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-5) shows the leaky ReLu function chart.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0805.png" alt="" height="600" width="600"><figcaption></figcaption></figure>

**Figure 8-5. Graph of the leaky ReLu function**

The choice of the activation function depends on the nature of the problem, the architecture of the network, and the desired behavior of the neurons in the network.

Activation functions typically take the weighted sum of inputs to a neuron and apply a non-linear transformation to it. The transformed value is then passed on as the output of the neuron to the next layer of the network. The specific form and behavior of activation functions can vary, but their overall purpose is to introduce non-linearities that allow the network to learn complex patterns and relationships in the data.

To sum up, activation functions play a crucial role in ANNs by introducing non-linearity into the network’s computations. They are applied to the outputs of individual neurons or intermediate layers and help determine whether a neuron should be activated or not based on the input it receives. Without activation functions, the network would only be able to learn linear relationships between the input and output. However, most real-world problems (especially financial time series) involve complex, non-linear relationships, so activation functions are essential for enabling neural networks to learn and represent such relationships effectively.

### Backpropagation

_Backpropagation_ is a fundamental algorithm used to train neural networks. It allows the network to update its weights in a way that minimizes the difference between the predicted output and the desired output.

**NOTE**

Backpropagation is a shortened term for _backward propagation of errors._

The following list tells the chronological story of the steps involved in the training of neural networks:

1. Randomly initialize the weights and biases of the neural network. This allows you to have a first step when you do not have initial information.
2. Perform _forward propagation_, a technique to calculate the predicted outputs of the network for a given input. As a reminder, this step involves calculating the weighted sum of inputs for each neuron, applying the activation function to the weighted sum, passing the value to the next layer (if it’s not the last), and continuing the process until reaching the output layer (prediction).
3. Compare the predicted output with the actual output (test data) and calculate the loss, which represents the difference between them. The choice of the loss function (for example, MAE or MSE) depends on the specific problem being solved.
4. Perform backpropagation to calculate the gradients of the loss with respect to the weights and biases. In this step, the algorithm will start from the output layer (the last layer) and goes backwards. It will compute the gradient of the loss with respect to the output of each neuron in the current layer. Then, it will calculate the gradient of the loss with respect to the weighted sum of inputs for each neuron in the current layer by applying the chain rule. After that, it will compute the gradient of the loss with respect to the weights and biases of each neuron in the current layer using the gradients from the previous steps. Repeat the above steps until the gradients are calculated for all layers.
5. Update the weights and biases of the network using the calculated gradients and a chosen optimization algorithm after a specific number of batches controlled by the hyperparameter referred to as the batch size. Updating the weights is done by subtracting the product of the learning rate and the gradient of the weights. Adjusting the biases is done by subtracting the product of the learning rate and the gradient of the biases. Repeat the preceding steps until the weights and biases are updated for all layers.
6. The algorithm then repeats steps 2-5 for a specified number of epochs or until a convergence criterion is met. An _epoch_ represents one complete pass through the entire training dataset (the whole process entails passing through the training dataset multiple times ideally).
7. Once the training is completed, evaluate the performance of the trained neural network on a separate validation or test dataset.

**NOTE**

The _learning rate_ is a hyperparameter that determines the step size at which a neural network’s weights are updated during the training process. It controls how quickly or slowly the model learns from the data it’s being trained on.

The _batch size_ is a hyperparameter that determines the number of samples processed before updating the model’s weights during each iteration of the training process. In other words, it specifies how many training examples are used at a time to calculate the gradients and update the weights.

Choosing an appropriate batch size is essential for efficient training and can impact the convergence speed and memory requirements. There is no one-size-fits-all answer to the ideal batch size, as it depends on various factors, such as the dataset size, available computational resources, and the complexity of the model.

Commonly used batch sizes for training MLPs range from small values (such as 16, 32, or 64) to larger ones (such as 128, 256, or even larger). Smaller batch sizes can offer more frequent weight updates and may help the model converge faster, especially when the dataset is large or has a lot of variations. However, smaller batch sizes may also introduce more noise and slower convergence due to frequent updates with less accurate gradients. On the other hand, larger batch sizes can provide more stable gradients and better utilization of parallel processing capabilities, leading to faster training on modern hardware. However, they might require more memory, and the updates are less frequent, which could slow down convergence or make the training process less robust.

As a general rule of thumb, you can start with a moderate batch size like 32 and experiment with different values to find the best trade-off between convergence speed and computational efficiency for your specific MLP model and dataset.

The backpropagation algorithm leverages the chain rule (refer to [Chapter 4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch04.html#ch04) for more information on calculus) to calculate the gradients by propagating the errors backward through the network.

By iteratively adjusting the weights based on the error propagated backward through the network, backpropagation enables the network to learn and improve its predictions over time. Backpropagation is a key algorithm in training neural networks and has contributed to significant advancements in various fields.

### Optimization Algorithms

In neural networks, optimization algorithms, also known as _optimizers_, are used to update the parameters (weights and biases) of the network during the training process. These algorithms aim to minimize the loss function and find the optimal values for the parameters that result in the best performance of the network. There are several types of optimizers:

1. _Gradient descent (GD)_: Gradient descent is the most fundamental optimization algorithm. It updates the network’s weights and biases in the direction opposite to the gradient of the loss function with respect to the parameters. It adjusts the parameters by taking steps proportional to the negative of the gradient, multiplied by a learning rate.
2. _Stochastic dradient descent (SGD)_: SGD is a variant of gradient descent that randomly selects a single training example or a mini-batch of examples to compute the gradient and update the parameters. It provides a computationally efficient approach and introduces noise in the training process, which can help escape local optima.
3. _Adaptive moment estimation (Adam)_: Adam is an adaptive optimization algorithm that computes adaptive learning rates for each parameter based on estimates of the first and second moments of the gradients. Adam is widely used due to its effectiveness and efficiency in various applications.
4. _Root mean square propagation (RMSprop)_: The purpose of RMSprop is to address some of the limitations of the standard gradient descent algorithm, such as slow convergence and oscillations in different directions. RMSprop adjusts the learning rate for each parameter based on the average of the recent squared gradients. It calculates an exponentially weighted moving average of the squared gradients over time.

Each optimizer has its own characteristics, advantages, and limitations, and their performance can vary depending on the dataset and the network architecture. Experimentation and tuning are often necessary to determine the best optimizer for a specific task.

### Regularization Techniques

_Regularization techniques_ in neural networks are methods used to prevent overfitting which can lead to poor performance and reduced ability of the model to make accurate predictions on new examples. Regularization techniques help to control the complexity of a neural network and improve its ability to generalize to unseen data.

_Dropout_ is a regularization technique commonly used in neural networks to prevent overfitting (refer to [Chapter 7](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#ch07) for detailed information on overfitting). It involves randomly omitting (dropping) a fraction of the neurons during training by setting their outputs to zero. This temporarily removes the neurons and their corresponding connections from the network, forcing the remaining neurons to learn more robust and independent representations.

The key idea behind dropout is that it acts as a form of model averaging or ensemble learning. By randomly dropping out neurons, the network becomes less reliant on specific neurons or connections and learns more robust features. Dropout also helps prevent co-adaptation, where certain neurons rely heavily on others, reducing their individual learning capability. As a result, dropout can improve the network’s generalization ability and reduce overfitting.

_Early stopping_ is a technique that also prevents overfitting by monitoring the model’s performance on a validation set during training. It works by stopping the training process when the model’s performance on the validation set starts to deteriorate. The idea behind early stopping is that as the model continues to train, it may start to overfit the training data, causing a decrease in performance on unseen data.

The training process is typically divided into epochs, where each epoch represents a complete pass over the training data. During training, the model’s performance on the validation set is evaluated after each epoch. If the validation loss or a chosen metric starts to worsen consistently for a certain number of epochs, training is stopped, and the model’s parameters from the epoch with the best performance are used as the final model.

Early stopping helps prevent overfitting by finding the optimal point at which the model has learned the most useful patterns without memorizing noise or irrelevant details from the training data. Both dropout and early stopping are key regularization techniques that help prevent overfitting and help stabilize the model.

### Multi-Layer Perceptrons

A _multi-layer perceptron_ (MLP) is a type of ANN that consists of multiple layers of artificial neurons, or nodes, arranged in a sequential manner. It is a _feedforward neural network_, meaning that information flows through the network in one direction, from the input layer to the output layer, without any loops or feedback connections (you will learn more about this later in the next section on recurrent neural networks).

The basic building block of an MLP is a _perceptron_, an artificial neuron that takes multiple inputs, applies weights to those inputs, performs a weighted sum, and passes the result through an activation function to produce an output (basically, the neuron that you have seen already). An MLP contains multiple perceptrons organized in layers. It typically consists of an input layer, one or more hidden layers (the more layers, the deeper the learning process up to a certain point), and an output layer.

**NOTE**

The term _perceptron_ is sometimes used more broadly to refer to a single-layer neural network based on a perceptron-like architecture. In this context, the term _perceptron_ can be used interchangeably with _neural network_ or _single-layer perceptron_.

As a reminder, the input layer receives the raw input data, such as features from a dataset (for example, the stationary values of a moving average). The hidden layers, which are intermediate layers between the input and output layers, perform complex transformations on the input data. Each neuron in a hidden layer takes inputs from all neurons in the previous layer, applies weights, performs the weighted sum, and passes the result through an activation function. The output layer produces the final output of the network.

MLPs are trained using backpropagation, which adjusts the weights of the neurons in the network to minimize the difference between the predicted output and the desired output. They are known for their ability to learn complex nonlinear relationships in data, making them suitable for a wide range of tasks, including pattern recognition. [Figure 8-6](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-6) shows an example of a deep MLP architecture.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0806.png" alt="" height="325" width="600"><figcaption></figcaption></figure>

**Figure 8-6. A simple illustration of an MLP with two hidden layers**

At this stage, it is understood that deep learning is basically neural networks with many hidden layers that add to the complexity of the learning process.

**NOTE**

It is important to download `master_function.py` from the GitHub in order to access the functions seen throughout this book. After downloading it, you must set your Python’s interpreter directory as the path where `master_function.py` is stored.

The aim of this section is to create an MLP to forecast daily S\&P 500 returns. Import the required libraries:

```
from keras.models import Sequential
from keras.layers import Dense
import keras
import numpy as np
import matplotlib.pyplot as plt
import pandas_datareader as pdr
from master_function import data_preprocessing, plot_train_test_values
from master_function import calculate_accuracy, model_bias
from sklearn.metrics import mean_squared_error
```

Now, import the historical data and transform it:

<pre><code><strong># Set the start and end dates for the data
</strong>start_date = '1990-01-01'
end_date   = '2023-06-01'
<strong># Fetch S&#x26;P 500 price data
</strong>data = np.array((pdr.get_data_fred('SP500', start = start_date, 
                                   end = end_date)).dropna())
<strong># Difference the data and make it stationary
</strong>data = np.diff(data[:, 0])
</code></pre>

Set the hyperparameters for the model:

```
num_lags = 100
train_test_split = 0.80
num_neurons_in_hidden_layers = 20
num_epochs = 500
batch_size = 16
```

Use the data preprocessing function to create the four required arrays:

<pre><code><strong># Creating the training and test sets
</strong>x_train, y_train, x_test, y_test = data_preprocessing(data, num_lags, 
                                                      train_test_split)
</code></pre>

The following code block shows how to build the MLP architecture in `keras`. Make sure to understand the notes in the code:

<pre><code><strong># Designing the architecture of the model
</strong>model = Sequential()
<strong># First hidden layer with ReLu as actication function
</strong>model.add(Dense(num_neurons_in_hidden_layers, input_dim = num_lags, 
                activation = 'relu'))  
<strong># Second hidden layer with ReLu as actication function
</strong>model.add(Dense(num_neurons_in_hidden_layers, activation = 'relu'))  
<strong># Output layer
</strong>model.add(Dense(1))
<strong># Compiling
</strong>model.compile(loss = 'mean_squared_error', optimizer = 'adam')
<strong># Fitting the model
</strong>model.fit(x_train, np.reshape(y_train, (-1, 1)), epochs = num_epochs, 
          batch_size = batch_size)
<strong># Predicting in-sample
</strong>y_predicted_train = np.reshape(model.predict(x_train), (-1, 1))
<strong># Predicting out-of-sample
</strong>y_predicted = np.reshape(model.predict(x_test), (-1, 1))
</code></pre>

**NOTE**

When creating a `Dense` layer, you need to specify the `input_dim` parameter in the first layer of your neural network. For subsequent `Dense` layers, the `input_dim` is automatically inferred from the previous layer’s output.

Let’s plot the results and analyze the performance:

```
Accuracy Train =  92.4 %
Accuracy Test =  54.85 %
RMSE Train =  4.3602984254
RMSE Test =  75.7542774467
Correlation In-Sample Predicted/Train =  0.989
Correlation Out-of-Sample Predicted/Test =  0.044
Model Bias =  1.03
```

[Figure 8-7](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-7) shows the evolution of the forecasting task from the last values of `y_train` to the first values of `y_test` and `y_predicted`.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0807.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 8-7. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

The results are extremely volatile when changing the hyperparameters. This is why using sophisticated models on complex data requires a lot of tweaks and optimizations. Consider the following improvements to enhance the results of the model:

* Select relevant features (inputs) that capture the underlying patterns and characteristics of the financial time series. This can involve calculating technical indicators (for example; moving averages and the RSI) or deriving other meaningful variables from the data.
* Review the architecture of the model. Consider increasing the number of layers or neurons to provide the model with more capacity to learn complex patterns. Experiment with different activation functions and regularization techniques such as dropout and early stopping (see [Chapter 9](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch09.html#ch09) for an application of regularization techniques).
* Fine-tune the hyperparameters of your MLP model. Parameters like the batch size and the number of epochs can significantly impact the model’s ability to converge and generalize.
* Combine multiple MLP models into an ensemble. This can involve training several models with different initializations or using different subsets of the data. Aggregating their predictions can lead to better results than using a single model.

As the model trains, the loss function should decrease due to the learning process. This can be seen using the following code (to be run after compiling the model):

```
import tensorflow as tf
losses = []
epochs = []
class LossCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs = None):
        losses.append(logs['loss'])
        epochs.append(epoch + 1)
        plt.clf()
        plt.plot(epochs, losses, marker = 'o')
        plt.title('Loss Curve')
        plt.xlabel('Epoch')
        plt.ylabel('Loss Value')
        plt.grid(True)
        plt.pause(0.01)
model.fit(x_train, np.reshape(y_train, (-1, 1)), epochs = 100, 
          verbose = 0, callbacks = [LossCallback()])
plt.show()
```

The previous code block plots the loss at the end of every epoch, thus creating a dynamic loss curve visualized in real time. Notice how it falls until reaching a plateau where it struggles to decrease. [Figure 8-8](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-8) shows the decreasing loss function across epochs.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0808.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 8-8. Loss value across epochs**

## Recurrent Neural Networks

A _recurrent neural network_ (RNN) is a type of artificial neural network that is designed to process sequential data or data with temporal dependencies. Unlike feedforward neural networks, which process data in a single pass from input to output, RNNs maintain internal memory or hidden states to capture information from previous inputs and utilize it in the processing of subsequent inputs.

The key feature of an RNN is the presence of _recurrent connections_, which create a loop in the network. This loop allows the network to persist information across time steps, making it well-suited for tasks that involve sequential or time-dependent data.

At each time step, an RNN takes an input vector and combines it with the previous hidden state. It then applies activation functions to compute the new hidden state and produces an output. This process is repeated for each time step, with the hidden state being updated and passed along as information flows through the network.

The recurrent connections enable RNNs to capture dependencies and patterns in sequential data. They can model the context and temporal dynamics of the data, making them useful in time series prediction.

However, traditional RNNs suffer from the vanishing gradient problem, where the gradients that are backpropagated through the recurrent connections can become very small or very large, leading to difficulties in training the network. The vanishing gradient problem is resolved in the next section with an enhanced type of neural network. For now, let’s focus on RNNs and their specificities.

[Figure 8-9](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-9) shows an example of an RNN architecture.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0809.png" alt="" height="209" width="600"><figcaption></figcaption></figure>

**Figure 8-9. A simple illustration of an RNN with two hidden layers**

Let’s deploy an RNN algorithm to forecast S\&P 500 daily returns. As usual, import the required libraries:

```
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN
import keras
import numpy as np
import matplotlib.pyplot as plt
import pandas_datareader as pdr
from master_function import data_preprocessing, plot_train_test_values
from master_function import calculate_accuracy, model_bias
from sklearn.metrics import mean_squared_error
```

Now, set the hyperparameters of the model:

```
num_lags = 100
train_test_split = 0.80
num_neurons_in_hidden_layers = 20
num_epochs = 500
batch_size = 16
```

The following code block shows how to build the RNN architecture in `keras`:

<pre><code><strong># Designing the architecture of the model
</strong>model = Sequential()
<strong># First hidden layer
</strong>model.add(Dense(num_neurons_in_hidden_layers, input_dim = num_lags, 
                activation = 'relu'))  
<strong># Second hidden layer
</strong>model.add(Dense(num_neurons_in_hidden_layers, activation = 'relu'))  
<strong># Output layer
</strong>model.add(Dense(1))
<strong># Compiling
</strong>model.compile(loss = 'mean_squared_error', optimizer = 'adam')
<strong># Fitting the model
</strong>model.fit(x_train, np.reshape(y_train, (-1, 1)), epochs = num_epochs, 
          batch_size = batch_size)
<strong># Predicting in-sample
</strong>y_predicted_train = np.reshape(model.predict(x_train), (-1, 1))
<strong># Predicting out-of-sample
</strong>y_predicted = np.reshape(model.predict(x_test), (-1, 1))
</code></pre>

Let’s plot the results and analyze the performance:

```
Accuracy Train =  67.16 %
Accuracy Test =  52.11 %
RMSE Train =  22.7704952044
RMSE Test =  60.3443059267
Correlation In-Sample Predicted/Train =  0.642
Correlation Out-of-Sample Predicted/Test =  -0.022
Model Bias =  2.18
```

[Figure 8-10](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-10) shows the evolution of the forecasting task from the last values of `y_train` to the first values of `y_test` and `y_predicted`.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0810.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 8-10. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

**NOTE**

A good task for you to do is to create an optimization function that loops around different hyperparameters and selects the best ones or averages the best ones. This way, you may be able to obtain a robust model based on the ensembling technique. You can also back-test different markets and different time horizons. Note that these techniques are not only valid for financial time series, but all types of time series.

In summary, RNNs are neural networks that can process sequential data by maintaining internal memory and capturing temporal dependencies. They are powerful models for tasks involving time series or sequential data. As a reminder, stationarity is an essential property for successful time series forecasting. A stationary time series exhibits constant mean, variance, and autocovariance over time. RNNs (among other deep learning models) assume that the underlying time series is stationary, which means the statistical properties of the data do not change over time. If the time series is non-stationary, it may contain trends, seasonality, or other patterns that can affect the performance of RNNs. The optimization and enhancement recommendations on MLPs are also valid on RNNs.

## Long Short-Term Memory

_Long short-term memory_ (LSTM) is a type of recurrent neural network (RNN) that addresses the vanishing gradient problem and allows the network to capture long-term dependencies in sequential data. LSTMs were introduced by Hochreiter and Schmidhuber in 1997.

LSTMs are designed to overcome the limitations of traditional RNNs when dealing with long sequences of data. They achieve this by incorporating specialized memory cells that can retain information over extended time periods. The key idea behind LSTMs is the use of a gating mechanism that controls the flow of information through the memory cells.

The LSTM architecture consists of memory cells, input gates, forget gates, and output gates. The memory cells store and update information at each time step, while the gates regulate the flow of information. Here’s how LSTM’s work:

1. _Input gate_: The input gate determines which information from the current time step should be stored in the memory cell. It takes the current input and the previous hidden state as inputs, and applies a sigmoid activation function to generate a value between 0 and 1 for each component of the memory cell.
2. _Forget gate_: The forget gate determines which information from the previous memory cell should be forgotten. It takes the current input and the previous hidden state as inputs, and applies a sigmoid activation function to produce a forget vector. This vector is then multiplied element-wise with the previous memory cell values, allowing the LSTM to forget irrelevant information.
3. _Update_: The update step combines the information from the input gate and the forget gate. It takes the current input and the previous hidden state as inputs and applies a tanh activation function. The resulting vector is then element-wise multiplied with the input gate output, and the product is added to the product of the forget gate and the previous memory cell values. This update operation determines which new information to store in the memory cell.
4. _Output gate_: The output gate determines the output of the LSTM at the current time step. It takes the current input and the previous hidden state as inputs, and applies a sigmoid activation function. The updated memory cell values are passed through a hyperbolic tangent (tanh) activation function and then multiplied element-wise with the output gate. The resulting vector becomes the current hidden state and is also the output of the LSTM at that time step.

The gating mechanisms in LSTMs allow them to selectively remember or forget information over long sequences, making them well-suited for tasks involving long-term dependencies. By addressing the vanishing gradient problem and capturing long-term dependencies, LSTMs have become a popular choice for sequential data processing and have been instrumental in advancing the field of deep learning.

**NOTE**

Theoretically, RNNs are capable of learning long-term dependencies, but in practice, they do not, hence the need for LSTMs.

As usual, let’s apply LSTMs to the same time series problem. Note however, that the results do not mean anything since the explanatory variables are arbitrary and the hyperparameters are not tuned. The aim of doing such exercises is to understand the code and the logic behind the algorithm. Afterwards, it will be up to you to select the inputs and the variables that you deem worthy to be tested out.

Import the required libraries as follows:

```
from keras.models import Sequential
from keras.layers import Dense, LSTM
import keras
import numpy as np
import matplotlib.pyplot as plt
import pandas_datareader as pdr
from master_function import data_preprocessing, plot_train_test_values
from master_function import calculate_accuracy, model_bias
from sklearn.metrics import mean_squared_error
```

Now, set the hyperparameters of the model:

```
num_lags = 100
train_test_split = 0.80
num_neurons_in_hidden_layers = 20
num_epochs = 100
batch_size = 32
```

The LSTM model requires three-dimensional arrays of features. This can be done using the following code:

```
x_train = x_train.reshape((-1, num_lags, 1))
x_test = x_test.reshape((-1, num_lags, 1))
```

The following code block shows how to build the LSTM architecture in `keras`:

<pre><code><strong># Create the LSTM model
</strong>model = Sequential()
<strong># First LSTM layer
</strong>model.add(LSTM(units = num_neurons_in_hidden_layers, 
               input_shape = (num_lags, 1)))
<strong># Second hidden layer
</strong>model.add(Dense(num_neurons_in_hidden_layers, activation = 'relu'))  
<strong># Output layer
</strong>model.add(Dense(units = 1))
<strong># Compile the model
</strong>model.compile(loss = 'mean_squared_error', optimizer = 'adam')
<strong># Train the model
</strong>model.fit(x_train, y_train, epochs = num_epochs , batch_size = batch_size)
<strong># Predicting in-sample
</strong>y_predicted_train = np.reshape(model.predict(x_train), (-1, 1))
<strong># Predicting out-of-sample
</strong>y_predicted = np.reshape(model.predict(x_test), (-1, 1))
</code></pre>

Let’s plot the results and analyze the performance:

```
Accuracy Train =  65.63 %
Accuracy Test =  50.42 %
RMSE Train =  25.5619843783
RMSE Test =  55.1133475721
Correlation In-Sample Predicted/Train =  0.515
Correlation Out-of-Sample Predicted/Test =  0.057
Model Bias =  2.56
```

[Figure 8-11](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-11) shows the evolution of the forecasting task from the last values of `y_train` to the first values of `y_test` and `y_predicted`. Note that the hyperparameters are the same as the ones used in the RNN model.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0811.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 8-11. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

It is worth seeing how well the algorithm is fitting to the training data. [Figure 8-12](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-12) shows the values from `y_predicted_train` and `y_train`.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0812.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 8-12. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

In the context of LSTMs, a 3D array represents the shape of the input data that is fed into the models. It is typically used to accommodate sequential or time series data in the form of input sequences. The dimensions of a 3D array have specific meanings:

1. _Dimension 1 (samples)_: This dimension represents the number of samples or examples in the dataset. Each sample corresponds to a specific sequence or time series instance. For example, if you have 1,000 time series sequences in your dataset, dimension 1 would be 1,000.
2. _Dimension 2 (time steps)_: This dimension represents the number of time steps or data points in each sequence. It defines the length of the input sequence that the LSTM or RNN model processes at each time step. For instance, if your input sequences have a length of 10 time steps, dimension 2 would be 10.
3. _Dimension 3 (features)_: This dimension represents the number of features or variables associated with each time step in the sequence. It defines the dimensionality of each time step’s data. In the case of univariate time series data, where only a single value is considered at each time step, dimension 3 would typically be 1. For multivariate time series, where multiple variables are observed at each time step, dimension 3 would be greater than 1.

Let’s take a quick break and discuss an interesting topic. Using simple linear algorithms to model complex non-linear relationships is most likely to give bad results. At the same time, using extremely complex methods such as LSTMs on simple and predictable data may not be necessary even though it may provide positive results. [Figure 8-13](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-13) shows an ascending time series that looks like it’s oscillating in regular intervals.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0813.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 8-13. A generated ascending time series with oscillating properties**

Believe it or not, linear regression can actually model this raw time series quite well. By assuming an autoregressive model with 100 features (which means that to predict the next value, the model looks at the last 100 values), the linear regression algorithm can be trained on in-sample data and output the following out-of-sample results as seen in [Figure 8-14](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-14).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0814.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 8-14. Prediction over the ascending time series using linear regression**

But let’s take its first order difference and make it stationary. Take a look at [Figure 8-15](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-15) which shows a stationary time series created from differencing the previous time series shown in [Figure 8-13](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-13).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0815.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 8-15. A generated ascending time series with oscillating properties (differenced)**

The linear regression algorithm can be trained on in-sample data and output the following out-of-sample results as seen in [Figure 8-16](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-16) with extreme accuracy.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0816.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 8-16. Prediction over the differenced time series using linear regression**

Another way of assessing the goodness-of-fit of a linear regression model is to use R². Also known as the _coefficient of determination_, _R²_ is a statistical measure that indicates the proportion of the variance in the dependent variable that can be explained by the independent variable(s) in a regression model.

R² ranges from 0 to 1 and is often expressed as a percentage. A value of 0 indicates that the independent variable(s) cannot explain any of the variability in the dependent variable, while a value of 1 indicates that the independent variable(s) can completely explain the variability in the dependent variable.

In simple terms, R² represents the proportion of the dependent variable’s variability that can be attributed to the independent variable(s) included in the model. It provides a measure of how well the regression model fits the observed data. However, it does not indicate the causal relationship between variables or the overall quality of the model. It is also worth noting that R² is the squared correlation between the two variables. The R² metric for the  differenced time series is 0.935, indicating extremely good fit.

In parallel, using an MLP with a some optimization also yields good results. [Figure 8-17](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-17) shows the results of the differenced values when using a simple MLP model (with two hidden layers containing each 24 neurons and a batch size of 128 ran through 50 epochs).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0817.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 8-17. Prediction over the differenced time series using MLP**

However, the added complexity of using a deep learning method to predict such a simple time series may be not worth it.

## Temporal Convolutional Neural Networks

_Convolutional neural networks_ (CNNs) are a class of deep learning models designed to process structured grid-like data, with a particular emphasis on images and other grid-like data such as time series (less commonly used) and audio spectrograms. CNNs are good at learning and extracting hierarchical patterns and features from input data, making them powerful tools for tasks like image recognition, object detection, image segmentation, and more.

The core building blocks of CNNs are the _convolutional layers_. These layers perform convolution operations by applying a set of learnable filters to input data, resulting in feature maps that capture relevant spatial patterns and local dependencies. Another important concept with CNNs is _pooling layers,_ which downsample the feature maps produced by convolutional layers. Common pooling operations include _max pooling_ (selecting the maximum value in a neighborhood) and _average pooling_ (computing the average value). Pooling helps reduce spatial dimensions, extract dominant features, and improve computational efficiency.

**NOTE**

A CNN that is specifically used for time series forecasting is often referred to as a 1D-CNN or a _temporal convolutional network_.

The term _1D-CNN_ indicates that the convolutional operations are applied along the temporal dimension of the input data, which is characteristic of time series data. It distinguishes it from traditional CNNs that operate on spatial dimensions in tasks such as image recognition.

A typical CNN architecture consists of three main components: an input layer, several alternating convolutional and pooling layers, and fully connected layers at the end. Convolutional layers are responsible for feature extraction, while pooling layers downsample the data. The fully connected layers provide the final predictions.

CNN architectures can vary greatly depending on the specific task. These architectures often employ additional techniques such as dropout regularization to improve performance and address challenges like overfitting.

CNNs can be used for time series prediction by leveraging their ability to capture local patterns and extract relevant features from the input data. The framework of the process is as follows:

1. CNNs use convolutional layers to perform localized feature extraction. The convolutional layers consist of a set of learnable filters that are convolved with the input data. Each filter extracts different features from the input data by applying element-wise multiplications and summations in a sliding window manner. The result is a feature map that highlights important patterns or features at different locations in the input data.
2. Pooling layers are often employed after convolutional layers to reduce the spatial dimensionality of the feature maps. Max pooling is a common technique, where the maximum value within a local neighborhood is selected, effectively downsampling the feature map. Pooling helps in capturing the most salient features while reducing the computational complexity and enhancing the network’s ability to generalize.
3. After the convolutional and pooling layers, the resulting feature maps are typically flattened into a one-dimensional vector. This flattening operation transforms the spatially distributed features into a linear sequence, which can then be passed to fully connected layers.
4. Fully connected layers receive the flattened feature vector as input and learn to map it to the desired output. These layers enable the network to learn complex combinations of features and model the non-linear relationships between input features and target predictions. The last fully connected layer typically represents the output layer, which predicts the target values for the time series.

Before moving to the algorithm creation steps, let’s review some key concepts seen with CNNs. In time series forecasting with CNNs, _filters_ are applied along the temporal dimension of the input data. Instead of considering spatial features as in image data, the filters are designed to capture temporal patterns or dependencies within the time series. Each filter slides across the time series, processing a subset of consecutive time steps at a time. The filter learns to detect specific temporal patterns or features in the input data. For example, it might capture short-term trends, seasonality, or recurring patterns that are relevant for the forecasting task. Multiple filters can be used in each convolutional layer, allowing the network to learn a diverse set of temporal features. Each filter captures different aspects of the time series, enabling the model to capture complex temporal relationships.

Another concept is the _kernel size_, which refers to the length or the number of consecutive time steps that the filter considers during the convolution operation. It defines the receptive field of the filter and influences the size of the extracted temporal patterns. The choice of kernel size depends on the characteristics of the time series data and the patterns to be captured. Smaller kernel sizes, such as 3 or 5, focus on capturing short-term patterns, while larger kernel sizes, such as 7 or 10, are suitable for capturing longer-term dependencies. Experimentation with different kernel sizes can help identify the optimal receptive field that captures the relevant temporal patterns for accurate forecasting. It’s common to have multiple convolutional layers with different kernel sizes to capture patterns at various temporal scales.

Now let’s see how to create a temporal CNN to forecast S\&P 500 returns using its lagged values. Import the required libraries as follows:

```
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
import keras
import numpy as np
import matplotlib.pyplot as plt
import pandas_datareader as pdr
from master_function import data_preprocessing, plot_train_test_values
from master_function import calculate_accuracy, model_bias
from sklearn.metrics import mean_squared_error
```

Next, set the hyperparameters of the model:

```
num_lags = 100 
train_test_split = 0.80 
filters = 64 
kernel_size = 4
pool_size = 2
num_epochs = 100 
batch_size = 8
```

Reshape the features arrays into three-dimensional data structures:

```
x_train = x_train.reshape((-1, num_lags, 1))
x_test = x_test.reshape((-1, num_lags, 1))
```

Afterwards, create the architecture of the TCN and run the algorithm:

<pre><code><strong># Create the temporal CNN model
</strong>model = Sequential()
model.add(Conv1D(filters = filters, kernel_size = kernel_size, 
                 activation = 'relu', input_shape = (num_lags, 1)))
model.add(MaxPooling1D(pool_size = pool_size))
model.add(Flatten())
model.add(Dense(units = 1))
<strong># Compile the model
</strong>model.compile(loss = 'mean_squared_error', optimizer = 'adam')
<strong># Train the model
</strong>model.fit(x_train, y_train, epochs = num_epochs , batch_size = batch_size)
<strong># Predicting in-sample
</strong>y_predicted_train = np.reshape(model.predict(x_train), (-1, 1))
<strong># Predicting out-of-sample
</strong>y_predicted = np.reshape(model.predict(x_test), (-1, 1))
</code></pre>

Let’s plot the results and analyze the performance:

```
Accuracy Train =  68.9 %
Accuracy Test =  49.16 %
RMSE Train =  18.3047790152
RMSE Test =  63.4069105299
Correlation In-Sample Predicted/Train =  0.786
Correlation Out-of-Sample Predicted/Test =  0.041
Model Bias =  0.98
```

[Figure 8-18](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#figure-8-18) shows the evolution of the forecasting task from the last values of `y_train` to the first values of `y_test` and `y_predicted`.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0818.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 8-18. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

It is important to use performance metrics that reflect your choice and search for a better algorithm. Accuracy may be one of the base metrics to give you a quick glance at the predictive abilities of your model, but on its own, it is not enough. The results seen in this chapter reflect only the training using the selected hyperparameters. Optimization will allow you to achieve very good results on certain models.

**NOTE**

There is no strict rule defining the number of hidden layers required to consider a neural network as deep. However, a common convention is that a neural network with two or more hidden layers is typically considered a deep neural network.

## Summary

Applying deep learning algorithms to time series data can offer several benefits and challenges. Deep learning algorithms have shown great utility in time series analysis by effectively capturing complex patterns, extracting meaningful features, and making accurate predictions. However, their success relies heavily on the quality of the data and the chosen features.

The utility of applying deep learning algorithms on time series data stems from their ability to automatically learn hierarchical representations and model intricate temporal dependencies. They can handle non-linear relationships and capture both local and global patterns, making them suitable for a wide range of time series tasks like forecasting, anomaly detection, classification, and signal processing.

However, applying deep learning algorithms to time series can present challenges:

* _Data quality_: Deep learning models heavily rely on large amounts of high-quality labeled data for training. Insufficient or noisy data can hinder the performance of the models, leading to inaccurate predictions or unreliable insights. Data preprocessing, cleaning, and addressing missing values become crucial steps to ensure the quality of the data.
* _Feature engineering_: Deep learning models can automatically learn relevant features from the data. However, the choice and extraction of informative features can significantly impact the model’s performance. Domain knowledge, data exploration, and feature engineering techniques are important in selecting or transforming features that enhance the model’s ability to capture relevant patterns.
* _Model complexity_: Deep learning models are typically complex with a large number of parameters. Training such models requires substantial computational resources, longer training times, and careful hyperparameter tuning. Overfitting, where the model memorizes the training data without generalizing well to unseen data, is also a common challenge.
* _Interpretability_: Deep learning models are often considered black boxes, making it challenging to interpret the learned representations and understand the reasoning behind predictions. This can be a concern in domains where interpretability and explainability are crucial, such as finance.

To overcome these challenges and harness the power of deep learning algorithms for time series analysis, careful consideration of data quality, appropriate feature engineering, model architecture selection, regularization techniques, and interpretability approaches are essential. It is crucial to understand the specific characteristics and requirements of the time series data and task at hand to choose and tailor the deep learning approach accordingly.

[1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#id258-marker) Pitts et. McCulloch, 1943, ‘A Logical Calculus of Ideas Immanent in Nervous Activity’.
