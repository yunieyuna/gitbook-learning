# 10. Deep Reinforcement Learning for Time Series Prediction

## Chapter 10. Deep Reinforcement Learning for Time Series Prediction

_Reinforcement learning_ is a branch of machine learning that deals with sequential decision-making problems. Algorithms in this branch learn to make optimal decisions by interacting with an environment and receiving feedback in the form of rewards. In the context of time series forecasting, it can be used to develop models that make sequential predictions based on historical data. Traditional forecasting approaches often rely on statistical methods or supervised learning techniques, which assume independence between data points. However, time series data exhibits temporal dependencies and patterns, which may be effectively captured using reinforcement learning.

Reinforcement learning models for time series forecasting typically involve an agent that takes actions based on observed states and receives rewards based on the accuracy of its predictions. The agent learns through trial and error to maximize cumulative rewards over time. The key challenge is finding an optimal balance between _exploration_ (trying out new actions) and _exploitation_ (using learned knowledge).

This chapter gives a basic overview of reinforcement learning and deep reinforcement learning with regards to predicting time series data.

## Intuition of Reinforcement Learning

Simplifying things is always the right path into understanding the more advanced details. So, let’s look at reinforcement learning from a simple point of view before digging deeper.

Reinforcement learning deals primarily with rewards and penalties. Imagine a child that gets a reward for doing good things and a punishment for doing bad things. Over time, that child will grow and will develop their experience so that they perform good things and try to avoid as much as possible the bad things (no one is perfect). Therefore, the learning is done through experience.

From a time series perspective, the main idea is still the same. Imagine training a model on past data and letting it then learn by experience while rewarding it for good predictions and letting it learn from its mistakes (where it calibrates its parameters to achieve better accuracy next time). The algorithm is greedy in nature and wants to maximize its rewards, and therefore it becomes better over time in predicting the next likely value, which is of course dependent on the quality and the signal-to-noise ratio of the analyzed time series.

The term _reinforcement learning_ comes from the fact that _positive reinforcement_ is given to the algorithm when it makes right decisions and _negative reinforcement_ is given when it makes bad decisions. The first three concepts you must know are states, actions, and rewards:

* _States_ are the features at every time step. For example, at a certain time step, the current state of the market is its OHLC data and its volume data. In more familiar words, states are the explanatory variables.
* _Actions_ are the decisions a trader may take at every time step. They generally involve buying, selling, or holding. In more familiar words, actions are the algorithms’ decisions when faced with certain states (a simple discretionary example of this would be a trader noticing an overvalued market and deciding to initiate a buy order).
* _Rewards_ are the result of right actions. The simplest reward is the positive return. Note that a poorly designed reward function can lead to model issues such as a buy-and-hold strategy[1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#id259).

[Table 10-1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#table-10-1) shows a simple illustration of the three main elements of reinforcement learning:

|           | Time | Open | High | Low | Close | \| | **Action** | **Reward** |
| --------- | ---- | ---- | ---- | --- | ----- | -- | ---------- | ---------- |
| **State** |   1  | 0    | 0    | 0   | 0     | \| | BUY        |     2      |
| **State** |   2  | 0    | 0    | 0   | 0     | \| | BUY        |     1      |
| **State** |   3  | 0    | 0    | 0   | 0     | \| | SELL       |    -5      |
| **State** |   4  | 0    | 0    | 0   | 0     | \| | HOLD       |     6      |

States are the rows that go from the _Time_ column to the _Close_ column. Actions can be categorical as can be seen from the _Action_ column, and _Rewards_ can either be numerical (for example, a positive or negative profit) or categorical (for example, profit or loss label).

From the list above, it seems complicated to just design a system that looks for rewards. A _reward function_ quantifies the desirability or utility of being in a particular state or taking a specific action. The reward function therefore provides feedback to the agent, indicating the immediate quality of its actions and guiding its learning process. But first, let’s look at what a state-action table is (also known as a Q-table).

A _Q-table_, short for _quality table_, is a data structure to store and update the expected value (called the _Q-value_) of taking a particular action in a given state. The Q-value of a state-action pair (_s_, _a_) at time _t_ represents the expected cumulative reward that an agent can achieve by taking action _a_ in state _s_ following a specific policy. The Q-table is therefore a table-like structure that maps each state-action pair to its corresponding Q-value.

Initially, the Q-table is usually initialized with arbitrary values or set to zero. As the algorithm explores the environment (market) and receives rewards, it updates the Q-values in the table based on the observed rewards and the estimated future rewards. This process is typically done using an algorithm such as _Q-learning_.

**NOTE**

Over time, through repeated exploration and exploitation, the Q-table gradually converges to more accurate estimates of the optimal Q-values, representing the best actions to take in each state. By using the Q-table, the agent can make informed decisions and learn to maximize its cumulative rewards in the given environment. Remember, a reward can be profit, Sharpe ratio, and any other performance metric.

_Q-learning_ is a popular reinforcement learning algorithm that enables an agent to learn optimal actions by iteratively updating its action-value function, known as the the _Bellman equation_, defined as follows:

�(��,��)=�(��,��)+����\[�(��+1,��+1)]

�(��,��)���ℎ����������������(��,��)���ℎ����������������ℎ�������������(������������)

The larger the learning rate (γ), the more the algorithm takes into account the previous experiences. Notice that if γ is equal to zero, it would be synonymous to learning nothing as the second term will cancel itself out. Let’s take a simple example in order to clear things up so far. Consider [Table 10-2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#table-10-2).

| Time (State) | Act (Action)   | Wait (Action)  |
| ------------ | -------------- | -------------- |
|        1     | 2 reward units | 0 reward units |
|        2     | 2 reward units | 0 reward units |
|        3     | 2 reward units | 0 reward units |
|        4     | 2 reward units | 0 reward units |
|        5     | 2 reward units | 4 reward units |

The table describes the results of actions through time. At every time step, acting (doing something), will give a reward of 2, while waiting to act on the fifth time step will give a reward of 4. This means that the choice the agent can make is one of the following:

* Act now and get 2 reward units.
* Wait before acting and get 4 reward units.

Let’st assume γ = 0.80. Using the Bellman equation and working backwards will get you the following results:

�(�1,�1)=0+0.8(2.04)=1.63�(�2,�2)=0+0.8(2.56)=2.04�(�3,�3)=0+0.8(3.20)=2.56�(�4,�4)=0+0.8(4.00)=3.20

[Table 10-2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#table-10-2) may be updated to become [Table 10-3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#table-10-3)(Q-table) as follows:

| Time (State) | Act (Action)   | Wait (Action)     |
| ------------ | -------------- | ----------------- |
|       1      | 2 reward units | 1.63 reward units |
|       2      | 2 reward units | 2.04 reward units |
|       3      | 2 reward units | 2.56 reward units |
|       4      | 2 reward units | 3.20 reward units |
|       5      | 2 reward units | 4.00 reward units |

Therefore, the Q-table is continuously updated with the implied rewards to help maximize the final reward. In order to understand why there is the term _max_ in the Bellman equation, consider the example in [Table 10-4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#table-10-4):

| Time (State) | Buy | Sell | Hold |
| ------------ | --- | ---- | ---- |
|       1      |  5  |  8   |  8   |
|       2      |  3  |  2   |  1   |
|       3      |  2  |  5   |  6   |

Calculate the would-be value of _x_ in a Q-table ([Table 10-5](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#table-10-5)) assuming a learning rate of 0.4.

| Time (State) | Buy      | Sell   | Hold   |
| ------------ | -------- | ------ | ------ |
|      1       |  **?**   |  **?** |  **?** |
|      2       |  **?**   |  x     |  **?** |
|      3       |  2       |  5     |  6     |

Following the formula, you should have the following result:

�=2+0.4(���(2,5,6))=4.4

States (features) must be predictive in nature so that the reinforcement learning algorithm predicts the next value with an accuracy better than random. An example of features can be the values of the RSI, moving averages, and lagged close prices.

It is crucial to keep in mind that the inputs’ statistical preference remains that same, that is, stationarity. This begs the question: How are moving averages used as inputs if they are not stationary? The simple answer is the usual transformation which is to take the percentage difference.

**NOTE**

It is possible to use fractional differencing to transform a non-stationary time series into a stationary one while retaining its memory.

A _policy_ defines the behavior of an agent in an environment. It is a mapping from states to actions, indicating what action the agent should take in a given state. The policy essentially guides the decision-making process of the agent by specifying the action to be executed based on the observed state.

The goal of reinforcement learning is to find an optimal policy that maximizes the agent’s long-term cumulative reward. This is typically achieved through a trial-and-error process, where the agent interacts with the environment, takes actions, receives rewards, and adjusts its policy based on the observed outcomes.

The exploitation policy is generally faster than the exploration policy but may be more limited as it seeks a greater and immediate reward, while there may be a path afterwards that leads to a greater reward. Ideally, the best policy to take is a combination of both. But how to determine this optimal mix? That question is answered by Epsilon (ε).

_Epsilon_ is a parameter used in exploration-exploitation trade-offs. It determines the probability with which an agent selects a random action (exploration) versus selecting the action with the highest estimated value (exploitation).

Commonly used exploration strategies include _epsilon-greedy_ and _softmax_. In epsilon-greedy, the agent selects the action with the highest estimated value with a probability of (1 - ε), and selects a random action with a probability of ε. This allows the agent to explore different actions and potentially discover better policies. As the agent learns over time, the epsilon value is often decayed[2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#id260) gradually to reduce exploration and focus more on exploitation. On the other hand, softmax action selection considers the estimated action values but introduces stochasticity in the decision-making process. The temperature parameter associated with softmax determines the randomness in action selection, where a higher temperature leads to more exploration.

**NOTE**

Do not mix up epsilon and gamma:

* _Gamma_ is a parameter that determines the importance of future rewards. It controls the extent to which the agent values immediate rewards compared to delayed rewards (hence, it is related to a delayed gratification issue). The value of gamma is typically a number between 0 and 1, where a value closer to 1 means the agent considers future rewards more heavily, while a value closer to 0 gives less importance to future rewards. To understand this more, consider having another look at the Bellman equation.
* _Epsilon_ is a parameter used in exploration-exploitation trade-offs. It determines the probability with which an agent selects a random action (exploration) versus selecting the action with the highest estimated value (exploitation).

At this point, you may feel overwhelmed with the amount of new information presented, especially because it differs from what you have seen so far in the book. Before moving to the more complex deep reinforcement learning discussion, a quick summary of what you have seen in this chapter until now may be beneficial. Reinforcement learning is essentially giving the machine a task which it will then learn how to do on its own.

With time series analysis, states represents the current situation or condition of the environment at a particular time. An example of state is a technical indicator’s value. They are represented by Q-tables. Actions are self-explanatory and can be buy, sell, or hold (or even a more complex combination such as decrease weight and increase weight). Rewards are what the algorithm is trying to maximize and can be profit per trade, Sharpe ratio, or any sort of performance evaluation metric. It can also be a penalty such as the number of trades or maximum drawdown (in such a case, you are aiming to minimize it). The reinforcement learning algorithm will go through many iterations and variables through different policies to try to detect hidden patterns and optimize trading decision so that profitability is maximized. This is easier said than done (or coded).

One question is begging an answer: Is using a Q-table to represent the different states of financial time series efficient? This question is answered in the next section.

## Deep Reinforcement Learning

_Deep reinforcement learnin_g combines reinforcement learning techniques with deep learning architectures, particularly deep neural networks. It involves training agents to learn optimal behavior and make decisions by interacting with an environment, using deep neural networks to approximate value functions or policies.

The main difference between a reinforcement learning algorithm and a deep reinforcement learning algorithm is that the former estimates Q-values using the Q-table while the latter estimates Q-values using ANNs (see [Chapter 8](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch08.html#ch08) for detailed knowledge on artificial neural networks).

**NOTE**

As a reminder, _artificial neural networks_ (ANNs) are a type of computational model inspired by the structure and functioning of the human brain. A neural network consists of interconnected nodes organized into layers. The three main types of layers are the input layer, hidden layers, and the output layer. The input layer receives the initial data, which is then processed through the hidden layers, and finally, the output layer produces the network’s prediction.

The main objective of this section is to understand and design a deep reinforcement learning algorithm with the aim of data prediction. Keep in mind that reinforcement learning is still not heavily applied since it suffers from a few issues (discussed in the end of this section) which need resolving before making it one of the main trading algorithms in quantitative finance.

Therefore, deep reinforcement learning will have two main elements with important tasks:

* A deep neural network architecture to recognize patterns and approximate the best function that relates dependent an independent variables.
* A reinforcement learning architecture that allows the algorithm to learn by trial and error how to maximize a certain profit function.

Let’s continue defining a few key concepts before putting things together. _Replay memory_, also known as experience replay, is a technique used in deep reinforcement learning algorithms. It involves storing and reusing past experiences to enhance the learning process and improve the stability and efficiency of the training.

In deep reinforcement learning, an agent interacts with an environment, observes states, takes actions, and receives rewards. Each observation, action, reward, and resulting next state is considered an experience. Replay memory serves as a buffer that stores a collection of these experiences.

The replay memory has the following key features:

1. _Storage_: The replay memory is a data structure that can store a fixed number of experiences. Each experience typically consists of the current state, the action taken, the resulting reward, the next state, and a flag indicating whether the episode terminated.
2. _Sampling_: During the training process, instead of using experiences immediately as they occur, the agent samples a batch of experiences from the replay memory. Randomly sampling experiences from a large pool of stored transitions helps in decorrelating the data and breaking the temporal dependencies that exist in consecutive experiences.
3. _Batch learning_: The sampled batch of experiences is then used to update the agent’s neural network. By learning from a batch of experiences rather than individual experiences, the agent can make more efficient use of computation and improve the learning stability. Batch learning also allows for the application of optimization techniques, such as stochastic gradient descent, to update the network weights.

The replay memory provides several benefits to deep reinforcement learning algorithms. Among those benefits is the experience reuse, as the agent can learn from a more diverse set of data, reducing the bias that can arise from sequential updates. Breaking correlations is another benefit since the sequential nature of experience collection in reinforcement learning can introduce correlations between consecutive experiences. Randomly sampling experiences from the replay memory helps break these correlations, making the learning process more stable.

Up until now, the process has been clear:

1. Defining and initializing the environment.
2. Designing the neural network architecture.
3. Designing the reinforcement learning architecture with experience replay to stabilize the learning process.
4. Interacting with the environment and storing experiences until the learning process is done and predictions on new data are done.

There is one thing missing from the list and that is doubling down on the neural network architecture.

A _Double Deep Q-Network_ (DDQN) model is an extension of the original DQN architecture introduced by DeepMindin 2015. The primary motivation behind DDQN is to address a known issue in the original DQN algorithm called _overestimation bias_, which can lead to suboptimal action selection.

In the original DQN, the action values (Q-values) for each state-action pair are estimated using a single neural network. However, during the learning process, the Q-values are estimated using the maximum Q-value among all possible actions in the next state (take a look at [Table 10-5](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#table-10-5)). This maximum Q-value can sometimes result in an overestimation of the true action values, leading to a suboptimal policy.

The DDQN addresses this overestimation bias by utilizing two separate neural networks, referred to as the _Q-network_ and the _target-network_. The Q-network is a deep neural network that approximates the action-value function (Q-function). In other words, it estimates the value of each possible action in a given state. The Q-network’s parameters (weights and biases) are learned through training to minimize the difference between predicted Q-values and target Q-values. The target network is a separate copy of the Q-network that is used to estimate the target Q-values during training. It helps stabilize the learning process and improve the convergence of the Q-network. The weights of the target network are not updated during training; instead, they are periodically updated to match the weights of the Q-network.

The key idea behind the DDQN is to decouple the selection of actions from the estimation of their values.

**NOTE**

The algorithm updates the Q-network regularly and the target-network occasionally. This is done to avoid the issue of the same model being used to estimate the Q-value from the next state and then giving it to the Bellman equation to estimate the Q-value for the current state.

So, to put these elements into an ordered sequence, here’s how the deep reinforcement learning architecture may look:

1. Initialization of the the environment.
2. Selecting the epsilon value. Remember, epsilon is the exploration-exploitation trade-off parameter used to control the agent’s behavior during training.
3. Fetching the current state. Remember, an example of the current state may be the OHLC data, the RSI, the standard deviation of the returns, and even the day of the week.
4. In the first round, the algorithm selects the action by exploration as the model is not trained yet, therefore, the action is randomly selected (for example, from a choice panel of buy, sell, and hold). If it’s not the first game, then exploitation may be used to select the action. Exploitation is where the action is determined by the neural network model.
5. Applying the action.
6. Storing the previous elements in replay memory.
7. Fetching the inputs and the target array, then train Q-network.
8. Unless the round is over, repeating the process from step 3. Otherwise, if the round is over, training the target-network and repeating from the beginning.

To illustrate the algorithm, let’s use it on synthetic data like the sinewave time series. The first step therefore is to create the time series and then apply the deep reinforcement learning algorithm with the aim of predicting the future values.

The full code can be found in the GitHub repository[3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#id261) (for replication purposes).

[Figure 10-1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#figure-10-1) shows the test data (in the solid line) versus the predicted data (in the dashed line) using one epoch, 5 inputs (lagged values), a batch size of 64, and two hidden layers with each having 6 neurons.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1001.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 10-1. Predicted values (dashed line) versus actual values**

[Figure 10-2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#figure-10-2) shows the test data (in the solid line) versus the predicted data (in the dashed line) using one epoch, 5 inputs (lagged values), a batch size of 64, and two hidden layers with each having 6 neurons.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1002.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 10-2. Predicted values (dashed line) versus actual values**

[Figure 10-3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#figure-10-3) shows the predictions using 10 epochs, 5 inputs (lagged values), a batch size of 32, and two hidden layers with each having 6 neurons.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1003.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 10-3. Predicted values (dashed line) versus actual values**

[Figure 10-4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#figure-10-4) shows the predictions using 10 epochs, 5 inputs (lagged values), a batch size of 32, and two hidden layers with each having 24 neurons.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1004.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 10-4. Predicted values (dashed line) versus actual values**

[Figure 10-5](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#figure-10-5) shows the predictions using 10 epochs, 8 inputs (lagged values), a batch size of 32, and two hidden layers with each having 64 neurons.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1005.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 10-5. Predicted values (dashed line) versus actual values**

As you know, the more epochs, the better the fit--up to a certain point where overfitting may start to become an issue. Luckily, by now you know how reduce that risk.

**NOTE**

Note that the sinewave example is a very basic one and more complex data can be used with the algorithm. The choice of the sinewave time series is for illustrative purposes only and you must use more sophisticated methods on more complex time series to be able to judge the algorithm.

Reinforcement learning is easily overfit and is more likely to learn simple patterns and not hidden and complicated ones. Also, you should now be aware of the difficulty of reward function design and choice of features. Furthermore, such models are often considered black boxes, making it difficult to explain the reasoning behind their predictions.

All of these issues are now a barrier to implementing a stable and profitable deep reinforcement learning algorithm for trading.

## Summary

Reinforcement learning can be applied to time series prediction tasks, where the goal is to make predictions about future values based on historical data. In this approach, an agent interacts with an environment representing the time series data. The agent receives observations of past values and takes actions to predict future values. The agent’s actions involve adjusting its internal model or parameters to make predictions. It uses reinforcement learning algorithms to learn from past experiences and improve its prediction accuracy over time.

The agent receives rewards or penalties based on the accuracy of its predictions. Rewards can be designed to reflect the prediction error or the utility of the predictions for the specific application. Through a process of trial and error, the agent learns to associate certain patterns or features in the time series data with future outcomes. It learns to make predictions that maximize rewards and minimize errors.

The reinforcement learning process involves a balance between exploration and exploitation. The agent explores different prediction strategies, trying to discover patterns and make accurate predictions. It also exploits its existing knowledge to make predictions based on what it has learned so far. The goal of reinforcement learning for time series prediction is to train the agent to make accurate and reliable predictions. By continually receiving feedback and updating its prediction strategies, the agent adapts to changing patterns in the time series and improves its forecasting abilities.

The next chapter will show how to employ more deep learning techniques and applications.

[1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#id259-marker) A buy-and-hold strategy is a passive action where the trader or the algorithm initiates one buy order and holds it for a long time in an attempt to replicate the market’s return and minimize transaction costs incurred from excessive trading.

[2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#id260-marker) Keep in mind epsilon decay as it will be used as a variable in the code later.

[3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch10.html#id261-marker) Link: https://github.com/sofienkaabar/deep-learning-for-finance
