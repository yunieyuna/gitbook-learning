# 11. Advanced Techniques and Strategies

## Chapter 11. Advanced Techniques and Strategies

This chapter presents a few advanced techniques and methods based on what has been seen so far in the book. By now, you should have a solid knowledge on deep learning algorithms and how to develop a model to predict time series data. Even though this is just a first step towards deploying a profitable algorithm, you should know that you have come a long way since the beginning of the book. The chapter is divided into independent sections that discuss interesting ways of applying deep learning for time series prediction and to enhance the process

## Using COT Data to Predict Long-Term Trends

The _Commitments of Traders_ (COT) report is a [weekly publication](https://www.cftc.gov/MarketReports/CommitmentsofTraders/HistoricalCompressed/index.htm) released by the U.S. Commodity Futures Trading Commission (CFTC). It provides information on the positions held by various market participants in futures markets. The report is based on data collected from futures exchanges, including the Chicago Mercantile Exchange (CME) and the Intercontinental Exchange (ICE). The COT report categorizes traders into three main groups:

1. _Commercial traders (also referred to as dealers or hedgers)_: These are typically companies that use the futures market to hedge their main business activities. For example, a grain producer may use futures contracts to protect against price fluctuations in the agricultural market. Their positions are generally negatively correlated to the underlying market.
2. _Non-commercial traders (also referred to as funds or leveraged money)_: This group consists of large speculators, such as hedge funds and commodity trading advisors. Non-commercial traders often take positions based on their market outlook and profit-seeking strategies. Their positions are generally positively correlated to the underlying market as they have a trend following nature.
3. _Non-reportable traders_: This category includes small speculators and traders whose positions do not meet the reporting requirements set by the CFTC. They do not have a clear correlation with the underlying market.

The report provides a breakdown of the positions held by each group, indicating whether they are _net long_ (holding more long positions than short positions) or _net short_ (holding more short positions than long positions) in a particular futures market.

Traders and investors analyze the COT report to gain insights into the sentiment and behavior of different market participants. By monitoring changes in positions, they attempt to identify potential trends or reversals in the market. The report is especially popular in the commodity and currency markets, where it is used as a tool for fundamental analysis and to gauge market sentiment. This section covers how to do the following:

1. Create an algorithm to download the COT data automatically and analyze it.
2. Chart and understand the correlations between the COT values and their respective underlying markets. Additionally, check for stationarity in the COT values to see if they can be used directly in the algorithms.
3. Create an LSTM algorithm to forecast the next COT value using lagged values and evaluate it. This will be referred to as the _indirect one-step COT model_.
4. Create an LSTM algorithm to forecast a few weeks’ worth of COT observations using the direct method. This will be referred to as the _MPF COT direct model_.
5. Create an LSTM algorithm to forecast a few weeks’ worth of COT observations using the recursive method. This will be referred to as the _MPF COT recursive model_.

There are mainly four columns of interest in the COT report: the long hedgers, the short hedgers, the long funds, and the short funds. They are the four basic pillars from where you calculate the net hedgers and the net funds. Some traders like to analyze every column before netting and seeing the bigger picture. It helps to understand the logic behind every column before proceeding to the netting process:

Long hedgers (the percentage long positions of commercial traders)You can consider these hedgers as _consumers_ of the asset. A long hedger is an entity that buys futures on a certain asset (such as wheat) to hedge its main business. The primary goal is to protect itself from the risk of rising prices by securing a fixed price they need in the future. By doing so, they can plan their production costs more accurately and avoid potential losses if wheat prices increase. Therefore, long hedgers buy the asset in fear that it goes up. They usually buy it on its way down which results in a negative correlation with the price of the asset.Short hedgers (the percentage short positions of commercial traders)You can consider these hedgers as _producers_ of the asset. A short hedger is an entity that sells short futures on a certain asset to hedge its main business. The primary goal is to protect itself from the risk of falling prices. Therefore, short hedgers sell short the asset in fear that it goes down. They usually sell short on its way up which results in a positive correlation with the price of the asset (this means that the number of short hedgers rises as the price of the asset rises).Long funds (the percentage long positions of non-commercial traders)Speculative long fund positions are buyers of futures contracts in anticipation that the price rises. They have a positive correlation with the price of the asset on account of their trend following nature.Short funds (the percentage short positions of non-commercial traders)Speculative short fund positions are sellers of futures contracts in anticipation that the price goes down. They have a negative correlation with the price of the asset on account of their trend following nature. An example of this would be the decreased number of short funds as the asset goes up.

**NOTE**

Netting the COT report can be done in different ways depending on your needs. If you prefer to focus on commercial traders, then you can simply take the difference between commercial longs and commercial shorts (or as you may call them, consumers and producers). If you prefer to focus on non-commercial traders, then you can take the difference between non-commercial longs and non-commercial shorts, and if you prefer a global image, then you can take the difference between the netted commercial and non-commercial traders, so that you’re left with only one time series that summarizes the global picture of the market positioning on a certain asset.

[Table 11-1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#table-11-1) sheds some light on the calculation of the net COT values:

| Hedger long | Hedger short | Fund long | Fund short | Net hedger | Net Fund  | Net COT   |
| ----------- | ------------ | --------- | ---------- | ---------- | --------- | --------- |
| A           | B            | C         | D          | E = A - B  | F = C - D | G = F - E |

The net COT value has the following formula:

������=��������-���ℎ������

It shares a positive correlation with the underlying price of the asset. [Figure 11-1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-1) shows the net COT positioning on the Canadian dollar (CAD).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1101.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 11-1. Net COT CAD since 2015. Notice the mean-reverting nature of the values**

Let’s see how to download COT values using Python. First, `pip install` the library that allows you to automatically download COT values from the CFTC website:

```
pip install cot_reports
```

Just in case the library has issues, you can use the pre-downloaded COT reports in Excel format found in the [GitHub repository](https://github.com/sofienkaabar/deep-learning-for-finance)[1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#id262) (a code block in the end of the section is provided for this manual import).

Import the required libraries to download the historical observations of the COT report. For simplicity, let’s choose CAD positioning:

```
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from master_function import import_cot_data
```

The `import_cot_data()` function that allows you to fetch the COT values of a selected market is defined as follows (found in _master\_function.py_):

```
def import_cot_data(start_year, end_year, market):
    df = pd.DataFrame()
    for i in range(start_year, end_year + 1):
        single_year = pd.DataFrame(cot.cot_year(i, 
                      cot_report_type='traders_in_financial_futures_fut'))
        df = pd.concat([single_year, df], ignore_index=True)
    new_df = df.loc[:, ['Market_and_Exchange_Names',
                        'Report_Date_as_YYYY-MM-DD',
                        'Pct_of_OI_Dealer_Long_All',
                        'Pct_of_OI_Dealer_Short_All',
                        'Pct_of_OI_Lev_Money_Long_All',                    
                        'Pct_of_OI_Lev_Money_Short_All']]
    new_df['Report_Date_as_YYYY-MM-DD'] = 
                       pd.to_datetime(new_df['Report_Date_as_YYYY-MM-DD'])
    new_df = new_df.sort_values(by='Report_Date_as_YYYY-MM-DD')
    data = new_df[new_df['Market_and_Exchange_Names'] == market]
    data['Net_COT'] = (data['Pct_of_OI_Lev_Money_Long_All'] - \
                       data['Pct_of_OI_Lev_Money_Short_All']) - \
                      (data['Pct_of_OI_Dealer_Long_All'] -\
                       data['Pct_of_OI_Dealer_Short_All'])                
    return data
```

To import CAD COT values, use the following syntax:

```
CAD = 'CANADIAN DOLLAR - CHICAGO MERCANTILE EXCHANGE'
data = import_cot_data(2015, 2023, CAD)
data = np.array(data.iloc[:, -1], dtype = np.float64)
```

It is worth mentioning that other markets have the following code names that you can use to import them:

```
EUR = 'EURO FX - CHICAGO MERCANTILE EXCHANGE'
GBP = 'BRITISH POUND STERLING - CHICAGO MERCANTILE EXCHANGE'
JPY = 'JAPANESE YEN - CHICAGO MERCANTILE EXCHANGE'
CHF = 'SWISS FRANC - CHICAGO MERCANTILE EXCHANGE'
AUD = 'AUSTRALIAN DOLLAR - CHICAGO MERCANTILE EXCHANGE'
MXN = 'MEXICAN PESO - CHICAGO MERCANTILE EXCHANGE'
BRL = 'BRAZILIAN REAL - CHICAGO MERCANTILE EXCHANGE'
BTC = 'BITCOIN - CHICAGO MERCANTILE EXCHANGE'
SPX = 'E-MINI S&P 500 - CHICAGO MERCANTILE EXCHANGE'
```

**NOTE**

In case you have a request error, try applying the following code before running the import section (remember to `pip install` the `proxy_requests` library):

```
from proxy_requests.proxy_requests import ProxyRequests
req = ProxyRequests("https://api.ipify.org")
req.get()
```

[Figure 11-2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-2) shows the net COT positioning on CAD versus CADUSD. Notice the strong positive correlation between the two. Calculating the Pearson correlation of the last 200 observations gives a whopping 0.66. In other words, tops on the net COT data coincide with tops on CADUSD. Similarly, troughs on the net COT data coincide with troughs on CADUSD.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1102.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 11-2. CADUSD in dashed line versus the CAD net COT positioning**

Be careful as the chart shows CADUSD and not USDCAD, the commonly used pair. This is because you are trying to understand CAD and therefore it helps to put it as the base currency so that you can see the positive correlation with the CAD speculators and the negative correlation with the CAD hedgers. To obtain CADUSD from USDCAD observations, take its reciprocal:

������=1������

The next step is to check for stationarity on the COT values so as to know if it requires transformation or not. Remember, transformation can either be differencing, taking the percentage returns, or even fractional differentiation (as discussed in [Chapter 9](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch09.html#ch09)):

```
from statsmodels.tsa.stattools import adfuller
print('p-value: %f' % adfuller(raw_data)[1])
```

The output is as follows:

```
p-value: 0.000717
```

The COT values seem to be stationary and ready to be used as inputs in the algorithms.

### Algorithm #1: Indirect One-Step COT Model

The indirect model will use LSTMs to predict the next COT value at every time step. The assumption is that by predicting a value that is directionally correlated to the underlying market, it may give a bias for the expected move during the coming week. For example, if the forecast is for a higher COT value in the coming week, then you may have a bullish CAD bias in preparation of your weekly trading.

First, import the required libraries:

```
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, LSTM
from master_function import import_cot_data, data_preprocessing
from master_function import plot_train_test_values, 
from master_function import calculate_directional_accuracy
from sklearn.metrics import mean_squared_error
```

Import the required data:

```
CAD = 'CANADIAN DOLLAR - CHICAGO MERCANTILE EXCHANGE'
data = import_cot_data(2015, 2023, CAD)
data = np.array(data.iloc[:, -1], dtype = np.float64)
```

Set the hyperparameters and create the arrays:

<pre><code>num_lags = 100
train_test_split = 0.80
num_neurons_in_hidden_layers = 200
num_epochs = 200
batch_size = 4
<strong># Creating the training and test sets
</strong>x_train, y_train, x_test, y_test = data_preprocessing(data, 
                                                      num_lags, 
                                                      train_test_split)
</code></pre>

To comply with the LSTM architecture, the independent variables must be transformed into three-dimensional arrays. This is done using the following code:

```
x_train = x_train.reshape((-1, num_lags, 1))
x_test = x_test.reshape((-1, num_lags, 1))
```

Next, create the architecture of the model and predict the values on the training set (only to understand the goodness-of-fit) and the test set:

<pre><code><strong># Create the LSTM model
</strong>model = Sequential()
<strong># Adding a first layer
</strong>model.add(LSTM(units = neurons, input_shape = (num_lags, 1)))
<strong># Adding a second layer
</strong>model.add(Dense(neurons, activation = 'relu')) 
<strong># Adding the output layer 
</strong>model.add(Dense(units = 1))
<strong># Compiling the model
</strong>model.compile(loss = 'mean_squared_error', optimizer = 'adam')
<strong># Fitting the model
</strong>model.fit(x_train, y_train, epochs = num_epochs, batch_size = batch_size)
<strong># Predicting in the training set for illustrative purposes
</strong>y_predicted_train = model.predict(x_train)
<strong># Predicting in the test set
</strong>y_predicted = model.predict(x_test)
</code></pre>

To plot the predictions along the real values, use the following syntax:

```
plot_train_test_values(100, 50, y_train, y_test, y_predicted)
```

[Figure 11-3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-3) compares the predicted values with the real test values. At a first glance, the model seems to capture the variations of the COT values well. Let’s have a look at the performance results.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1103.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 11-3. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

The following are the results of the model used on CADUSD from 2015 to 2023:

```
Directional Accuracy Train =  86.18 %
Directional Accuracy Test =  60.87 %
RMSE Train =  5.3655332132
RMSE Test =  14.7772701349
Correlation In-Sample Predicted/Train =  0.995
Correlation Out-of-Sample Predicted/Test =  0.88
```

**NOTE**

An interesting task for you would be to apply the model to forecast the returns of the underlying market using COT values as inputs. You can use either the net COT value or any of the six other series you have available such as long hedgers and net funds. However, make sure to always check for stationarity.

### Algorithm #2: MPF COT Direct Model

The MPF COT model will use LSTMs to project a trajectory for the COT values to lead the way for the main market moves to come. The assumption is that by predicting the next COT values, which are less noisy than the market itself, you may have a guidemap for the expected trajectory of the market. As COT values are stationary and are highly correlated with the market (which is not stationary), you have a better chance of having a decent forecast than by using the MPF directly on the market. This algorithm uses the direct method (for more information, have another look at [Chapter 9](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch09.html#ch09)). First, import the required libraries:

```
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, LSTM
from master_function import import_cot_data, direct_mpf
from master_function import calculate_directional_accuracy
from sklearn.metrics import mean_squared_error
```

Set the hyperparameters and create the arrays:

<pre><code><strong># Setting the hyperparameters
</strong>num_lags = 100
train_test_split = 0.80
neurons = 400
num_epochs = 200
batch_size = 10
forecast_horizon = 50
<strong># Creating the training and test sets
</strong>x_train, y_train, x_test, y_test = direct_mpf(data, 
                                              num_lags, 
                                              train_test_split, 
                                              forecast_horizon)
</code></pre>

To comply with the LSTM architecture, the independent variables must be transformed into 3-dimensional arrays. This is done using the following code:

```
x_train = x_train.reshape((-1, num_lags, 1))
x_test = x_test.reshape((-1, num_lags, 1))
```

Next, create the architecture of the model and predict the values using the recursive function:

<pre><code><strong># Create the LSTM model
</strong>model = Sequential()
<strong># Adding a first layer
</strong>model.add(LSTM(units = neurons, input_shape = (num_lags, 1)))
<strong># Adding a second layer
</strong>model.add(Dense(neurons, activation = 'relu')) 
<strong># Adding the output layer 
</strong>model.add(Dense(units = forecast_horizon))
<strong># Compiling the model
</strong>model.compile(loss = 'mean_squared_error', optimizer = 'adam')
<strong># Fitting the model
</strong>model.fit(x_train, y_train, epochs = num_epochs, batch_size = batch_size)
<strong># Predicting in the test set
</strong>y_predicted = model.predict(x_test)
</code></pre>

To plot the predictions along the real values, use the following syntax:

```
plt.plot(y_predicted[-1], label = 'Predicted data', color = 'red', 
         linewidth = 1)
plt.plot(y_test[-1], label = 'Test data', color = 'black', 
         linestyle = 'dashed', linewidth = 2)
plt.grid()
plt.legend()
```

[Figure 11-4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-4) compares the predicted values with the real test values.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1104.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 11-4. Predicted data versus test data (dashed line)**

The following are the results of the model used on CADUSD:

```
Directional Accuracy Test =  57.14 %
RMSE Test =  26.4021245739
Correlation Out-of-Sample Predicted/Test =  0.426
```

### Algorithm #3: MPF COT Recursive Model

This algorithm uses the recursive method (for more information, have another look at [Chapter 9](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch09.html#ch09)). First, import the required libraries:

```
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, LSTM
from master_function import data_preprocessing, import_cot_data
from master_function import plot_train_test_values, recursive_mpf
from master_function import calculate_directional_accuracy
```

Set the hyperparameters and create the arrays:

<pre><code>num_lags = 100
train_test_split = 0.80
neurons = 100
num_epochs = 200
batch_size = 2
<strong># Creating the training and test sets
</strong>x_train, y_train, x_test, y_test = data_preprocessing(data,
                                                      num_lags,
                                                      train_test_split)
</code></pre>

To comply with the LSTM architecture, the independent variables must be transformed into three-dimensional arrays. This is done using the following code:

```
x_train = x_train.reshape((-1, num_lags, 1))
x_test = x_test.reshape((-1, num_lags, 1))
```

Next, create the architecture of the model and predict the values using the recursive function:

<pre><code><strong># Create the LSTM model
</strong>model = Sequential()
<strong># Adding a first layer
</strong>model.add(LSTM(units = neurons, input_shape = (num_lags, 1)))
<strong># Adding a second layer
</strong>model.add(Dense(neurons, activation = 'relu')) 
<strong># Adding the output layer 
</strong>model.add(Dense(units = 1))
<strong># Compiling the model
</strong>model.compile(loss = 'mean_squared_error', optimizer = 'adam')
<strong># Fitting the model
</strong>model.fit(x_train, y_train, epochs = num_epochs, batch_size = batch_size)
<strong># Predicting in the test set on a recursive basis
</strong>x_test, y_predicted = recursive_mpf(x_test, y_test, num_lags, model)
</code></pre>

To plot the predictions along the real values, use the following syntax:

```
plot_train_test_values(100, 50, y_train, y_test, y_predicted)
```

[Figure 11-5](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-5) compares the predicted values with the test values.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1105.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 11-5. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

The following are the results of the model used on CADUSD from 2015 to 2023:

```
Directional Accuracy Test =  52.17 %
RMSE Test =  40.3120541504
Correlation Out-of-Sample Predicted/Test =  0.737
```

**NOTE**

Remember that while setting the `random_state` and ensuring reproducibility is useful for experimentation, it might also limit the model’s ability to generalize well if the data distribution is genuinely random. In many cases, it’s recommended to perform multiple runs with different random seeds to get a better sense of the model’s performance and robustness. This is why many of the models seen in this book do not have the `random_state` implementation in their code (with the exception of a few models in [Chapter 7](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#ch07)).

It does not hurt to remember the symptoms of overfitting. Unfortunately, overfitting is sometimes not that easily detectable, so consider these general rules:

* _High training accuracy and low test accuracy_: The model shows excellent performance on the training data but performs poorly on the test data. This is a clear indication that the model has memorized the training data rather than learning general patterns.
* _Large gap between training and test performance_: There is a significant difference between the training and test error rates. Ideally, the two measures should be close to each other, and a large gap suggests overfitting.
* _Unusually high model complexity_: If the model is overly complex with a large number of parameters or features, it becomes more prone to overfitting. A simpler model may generalize better to new data.
* _Noisy predictions_: Overfit models tend to make erratic and inconsistent predictions on new data. This is because they are highly sensitive to small variations in the input data, including noise.

### Putting It All Together

The COT report is released every Friday by the CFTC that outlines the positioning of key market participants. It can be transformed into a time series forecasting task with the aim of improving market forecasts. The interesting part in the report lies with two market participants known as commercial participants (also known as dealers or hedgers) and non-commercial participants (also known as leveraged money or funds).

**NOTE**

Make sure to put _master\_function.py_ in the directory of the interpreter. Alternatively, you can simply open _master\_function.py_ in Python and execute the whole file. However, the latter method requires you to do it every time you restart the kernel.

Deep learning techniques can be applied on COT values in order to forecast the market positioning through hidden patterns and seasonal configurations. This section has discussed three algorithm ideas to squeeze out value from the COT report. You can experiment with machine learning models, deep reinforcement learning, and even simple statistical techniques in order to better understand the COT data.

For the manual way to import COT values, refer to the historical data found in the [GitHub repository](https://github.com/sofienkaabar/deep-learning-for-finance) and use the following code (change the path of the interpreter to be the same as where the downloaded file is):

<pre><code>import pandas as pd
import numpy as np
<strong># Import the data using pandas
</strong>data = pd.read_excel('name_of_file.xlsx')
</code></pre>

It’s important to remember that the COT report provides a snapshot of market participant positioning and should be used in conjunction with other tools and analysis methods. While it can offer valuable information, it’s not a stand-alone trading strategy, and careful consideration of other factors is essential for making well-informed trading decisions.

## Using Technical Indicators as Inputs

You learned about technical indicators in [Chapter 5](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch05.html#ch05). It’s time to use them as inputs to predict the underlying market’s returns. Using lagged values implies that there must be value in the past observations, which may translate into decent forecasts. This section will explore past that assumption and will search for value in other price transformations. It is interesting to know that you can have the following price-derived calculations:

* _Mathematical transformation_: This type of transformation is likely to be a direct manipulation of the raw data. An example of this is a basic differencing or a simple moving average. Normalization is also part of mathematical transformation.
* _Technical transformation_: This type of transformation is less obvious and the result may not look at all like the raw data. An example of this is the RSI, a technical indicator created out of recursive rules based on moving averages and normalization.
* _Categorical transformation_: This type of transformation shifts the type of numerical data to categorical and vice versa. For example, an algorithm known as `OneHotEncoder` takes categorical data and transforms it into binary data so that machine learning algorithms are able to classify it.

Before proceeding, there is a data issue that is worth discussing. _Multicollinearity_ is a statistical phenomenon that occurs in regression analysis when two or more independent variables (inputs) in a multiple regression model are highly correlated with each other. In other words, it is a situation where there is a strong linear relationship between two or more of the predictors. This correlation can make it difficult for the regression model to separate the individual effects of each predictor on the dependent variable (the outcome variable). Obviously, if you are using multiple RSIs with different time periods, then you are likely to run into multicollinearity. Make sure to find weakly correlated indicators.

Two technical indicators (or transformations) are used in this example:

* The 5-week RSI, a stationary indicator that does not require any transformation.
* The difference between the weekly close price and the 20-week moving average. This is also a stationary calculation that does not require any transformation.

Therefore, EURUSD’s weekly returns will be forecasted using the previous week’s RSI value and the distance between the previous week’s close price and the 20-week moving average.

First, import the required libraries:

```
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, LSTM
from master_function import mass_import, rsi, ma, calculate_accuracy
from master_function import plot_train_test_values, 
from master_function import multiple_data_preprocessing
from sklearn.metrics import mean_squared_error
from master_function import add_column, delete_column
```

Next, import the data using the `mass_import()` function:

```
data = mass_import(0, 'W1')[:, -1]
data = rsi(np.reshape(data, (-1, 1)), 5, 0, 1)
data = ma(data, 5, 0, 2)
data[:, 2] = data[:, 0] - data[:, 2]
data = add_column(data, 1)
for i in range(len(data)):
    data[i, 3] = data[i, 0] - data[i - 1, 0]
data[:, 0] = data[:, -1]
data = delete_column(data, 3, 1)
```

Define the multiple data preprocessing function which takes the values of the two technical indicators and lags them so that they can be used as inputs to predict EURUSD’s returns. [Table 11-2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#table-11-2) shows the training table `x_train` with 6 lagged values as independent variables to explain and predict the next EURUSD return.

| RSI t-1 | (Close - MA)t-1 | RSI t-2 | (Close - MA)t-2 | RSI t-3 | (Close - MA)t-3 |
| ------- | --------------- | ------- | --------------- | ------- | --------------- |
| 36.619  | -0.003804       | 48.5188 | 0.001044        | 42.4396 | -0.001714       |
| 46.7928 | 0.001674        | 36.619  | -0.003804       | 48.5188 | 0.001044        |
| 40.543  | -0.002518       | 46.7928 | 0.001674        | 36.619  | -0.003804       |
| 65.9614 | 0.01134         | 40.543  | -0.002518       | 46.7928 | 0.001674        |
| 47.2585 | -0.00039        | 65.9614 | 0.01134         | 40.543  | -0.002518       |
| 63.9755 | 0.011302        | 47.2585 | -0.00039        | 65.9614 | 0.01134         |

The next function is already defined, but it doesn’t hurt to discuss what it does. The `multiple_data_preprocessing()` function simply creates the four needed arrays for the back-test, but uses the two technical indicators as inputs. You can define the function for 6 lagged values (3 for RSI and 3 for the difference between the close and the moving average) as follows:

```
def multiple_data_preprocessing(data, train_test_split):
    data = add_column(data, 4)
    data[:, 1] = np.roll(data[:, 1], 1, axis = 0)
    data[:, 2] = np.roll(data[:, 2], 1, axis = 0)
    data[:, 3] = np.roll(data[:, 1], 1, axis = 0)
    data[:, 4] = np.roll(data[:, 2], 1, axis = 0)
    data[:, 5] = np.roll(data[:, 3], 1, axis = 0)
    data[:, 6] = np.roll(data[:, 4], 1, axis = 0)
    data = data[1:, ]
    x = data[:, 1:]
    y = data[:, 0]
    split_index = int(train_test_split * len(x))
    x_train = x[:split_index]
    y_train = y[:split_index]
    x_test = x[split_index:]
    y_test = y[split_index:]
    return x_train, y_train, x_test, y_test
```

Set the hyperparameters and create the arrays:

<pre><code><strong>num_lags = 6 # Must equal the number of the lagged values you create
</strong>train_test_split = 0.80
neurons = 500
num_epochs = 500
batch_size = 200
<strong># Creating the training and test sets
</strong>x_train, y_train, x_test, y_test = multiple_data_preprocessing(data, 
                                                   train_test_split)
</code></pre>

To comply with the LSTM architecture, the independent variables must be transformed into 3-dimensional arrays. This is done using the following code:

```
x_train = x_train.reshape((-1, num_lags, 1))
x_test = x_test.reshape((-1, num_lags, 1))
```

Next, create the architecture of the model and predict the values on the training set (only to understand the goodness-of-fit) and the test set:

<pre><code><strong># Create the LSTM model
</strong>model = Sequential()
<strong># Adding a first layer
</strong>model.add(LSTM(units = neurons, input_shape = (num_lags, 1)))
<strong># Adding a second layer
</strong>model.add(Dense(neurons, activation = 'relu')) 
<strong># Adding a third layer
</strong>model.add(Dense(neurons, activation = 'relu')) 
<strong># Adding a fourth layer
</strong>model.add(Dense(neurons, activation = 'relu')) 
<strong># Adding a fifth layer
</strong>model.add(Dense(neurons, activation = 'relu')) 
<strong># Adding the output layer 
</strong>model.add(Dense(units = 1))
<strong># Compiling the model
</strong>model.compile(loss = 'mean_squared_error', optimizer = 'ada
<strong># Fitting the model
</strong>model.fit(x_train, y_train, epochs = num_epochs, batch_size = batch_size)
<strong># Predicting in the training set for illustrative purposes
</strong>y_predicted_train = model.predict(x_train)
<strong># Predicting in the test set
</strong>y_predicted = model.predict(x_test)
</code></pre>

To plot the predictions along the real values, use the following syntax:

```
plot_train_test_values(100, 50, y_train, y_test, y_predicted)
```

[Figure 11-6](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-6) compares the predicted values with the real test values.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1106.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 11-6. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

The following are the results of the model:

```
Accuracy Train =  59.39 %
Accuracy Test =  51.82 %
RMSE Train =  0.0163232045
RMSE Test =  0.0122093494
Correlation In-Sample Predicted/Train =  0.255
Correlation Out-of-Sample Predicted/Test =  0.015
```

It is very interesting to tweak the model and see how to improve it. As these are weekly predictions, good accuracy could be the first step in shaping your swing trading[2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#id263) on the condition of optimizing the model and making sure it is not overfit. It is worth noting that since this algorithm is trying to predict the financial returns of an instrument, the `calculate_accuracy()` function is used as opposed to `calculate_directional_accuracy()`.

**NOTE**

Try running the algorithm seen in this section on MPF mode and see what you can extract out of it. Remember the algorithm’s limitations while doing so.

## Predicting Bitcoin’s Volatility Using Deep Learning

_Bitcoin_ is a decentralized digital currency that was created in 2009 by an unknown person or entity using the pseudonym _Satoshi Nakamoto_. It was the first cryptocurrency to be introduced and remains the most well-known and widely traded cryptocurrency to date. You probably do not need an introduction to Bitcoin considering the immense hype over it during these past years, but more knowledge never hurts.

Bitcoin operates on a technology called _blockchain_, which is a distributed ledger system. Unlike traditional currencies that are issued and regulated by governments or central banks, Bitcoin is not controlled by any central authority. Instead, it relies on a peer-to-peer network of computers, known as nodes, to validate and record transactions. Nowadays, Bitcoin is heavily traded on cryptocurrency exchanges and is used for speculative but also hedging operations. The most commonly traded pair is BTCUSD, which is the value of 1 Bitcoin relative to USD.

[Figure 11-7](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-7) shows the evolution of BTCUSD (Bitcoin’s value priced in US dollars):

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1107.png" alt="" height="448" width="600"><figcaption></figcaption></figure>

**Figure 11-7. Historical evolution of BTCUSD since 2014 in a linear scale**

You can generate [Figure 11-7](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-7) using the following code:

<pre><code>import pandas as pd
<strong># Manually importing BTCUSD values
</strong>my_data = pd.read_excel('Bitcoin_Daily_Historical_Data.xlsx')
<strong># Renaming the columns
</strong>my_data.columns = ['Date', 'Open', 'High', 'Low', 'Close']
<strong># Setting the date column
</strong>my_data['Date'] = pd.to_datetime(my_data['Date'])
<strong># Charting
</strong>plt.plot(my_data['Date'], my_data['Close'], label = 'BTCUSD', 
         color = 'black')
plt.legend()
plt.grid()
</code></pre>

Make sure you download the historical BTCUSD values from the [GitHub repository](https://github.com/sofienkaabar/deep-learning-for-finance)[3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#id264) and to set the directory of the interpreter in the same folder as the downloaded file.

[Figure 11-7](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-7) is charted using a linear scale which means that data is represented on a straight and evenly spaced axis (the space between 10 and 20 is the same as the space between 1230 and 1240). It is also possible to use what is known as a logarithmic scale to chart time series that experiences big jumps.

## TO LOG OR TO LIN

The _logarithmic scale_ is a scale used in charting that represents the data using logarithmic transformations. In a logarithmic scale, the spacing between values is based on the logarithm of the actual values rather than their linear scale. This means that the distance between, for example, 1 and 10 on a logarithmic scale is the same as the distance between 10 and 100, rather than being ten times larger as in a linear scale.

The importance of using a logarithmic scale in charting financial time series stems from the nature of financial data and its tendency to exhibit exponential growth or decay (as is the case on BTCUSD).

Financial markets often experience large variations in prices or returns, and using a logarithmic scale can help to better visualize and understand these changes. In trading jargon, linear scale charts are typically referred to as lin charts and logarithmic scale charts are referred to as log charts

Using a logarithmic scale in charting financial time series helps provide a more accurate representation of the data, facilitates the analysis of growth rates, and enhances the ability to identify trends and patterns in financial markets.

[Figure 11-8](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-8) shows the evolution of BTCUSD (Bitcoin’s value priced in US dollars) in logarithmic scale:

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1108.png" alt="" height="448" width="600"><figcaption></figcaption></figure>

**Figure 11-8. Historical evolution of BTCUSD since 2014 in a semilog scale**

Use the following code to generate [Figure 11-8](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-8) using `plt.semilogy` function:

```
plt.semilogy(my_data['Date'], my_data['Close'], label = 'BTCUSD', 
             color = 'black')
plt.legend()
plt.grid()
```

**NOTE**

The word _semilog_ refers to transforming one of the two axes into a logarithmic scale while the word _log_ refers to transforming both axes into a logarithmic scale.

Since time is linear, you only need to transform the _y_-axis (which means the values), and therefore, you are technically using semilog charts. This is why the `matplotlib` function is called `plt.semilogy`.

If you prefer to import BTCUSD values using a Python script, use the following:

<pre><code>import requests
import json
import pandas as pd
import numpy as np  
import datetime as dt

<strong># Selecting the time frame
</strong>frequency = '1h'
<strong># Defining the import function
</strong>def import_crypto(symbol, interval = frequency):
<strong>    # Getting the original link from Binance
</strong>    url = 'https://api.binance.com/api/v1/klines'
<strong>    # Linking the link with the Cryptocurrency and the time frame
</strong>    link = url + '?symbol=' + symbol + '&#x26;interval=' + interval
<strong>    # Requesting the data in the form of text
</strong>    data = json.loads(requests.get(link).text)
<strong>    # Converting the text data to dataframe
</strong>    data = np.array(data)
    data = data.astype(np.float)
    data = data[:, 1:5]
    return data
<strong># Importing hourly BTCUSD data
</strong>data = import_crypto('BTCUSDT')
</code></pre>

Let’s see if deep learning helps forecast BTCUSD’s volatility using its lagged values. But first, two questions are begging an answer:

* What is the use of predicting Bitcoin’s volatility?
* How to calculate Bitcoin’s volatility?

To answer the first question, by predicting Bitcoin’s volatility, traders can potentially identify periods of increased price swings and capitalize on them. Similarly, volatility predictions can also provide insights into market sentiment. When investors expect uncertain or turbulent market conditions, it may indicate a lack of confidence, leading to potential changes in market dynamics.

To answer the second question, you can use a rolling standard deviation measure on the close prices. This allows you to create a new time series that reflects the historical volatility of Bitcoin.

**NOTE**

Typically, an increase in volatility is a sign of market stress and fear which translates into a bearish tone. In contrast, a decrease in volatility is a sign of a healthy and stable market which is a sign of a bullish market. This relationship is not perfect and other variables may impact it. For example, if you calculate the correlation coefficient of Bitcoin’s daily close prices and the 10-day rolling volatility, you will find that it’s positive (at some periods, it’s extremely positive). Bitcoin is known for being a euphoric asset where rises are accompanied by a phenomenon called FOMO, an abbreviation of _fear of missing out_. This psychological bias is one of the building blocks of a market bubble where everyone keeps buying in hopes to profit from the move.

The first step of predictive analytics is to understand the data you’re dealing with. As a reminder, volatility refers to the degree of variation or fluctuation in the price of a financial instrument, such as a stock, bond, commodity, or currency, over a specific period of time. It is a statistical measure of the dispersion for that particular asset. [Figure 11-9](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-9) shows the latest values of Bitcoin’s 10-day volatility as measured by the rolling standard deviation calculation. The latest values say that recently, the price variations hovered around $500 from the 10-day mean most of the time (refer to [Chapter 3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#ch03) for more in-depth comprehension on standard deviation).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1109.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 11-9. Bitcoin’s rolling 10-day standard deviation as a proxy for volatility**

Let’s get started. For this task, let’s manually import BTCUSD into the interpreter. First, start by importing the required libraries:

```
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, LSTM
from master_function import add_column, delete_row, volatility
from master_function import data_preprocessing, plot_train_test_values
from master_function import calculate_directional_accuracy
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.stattools import adfuller
```

Next, import the data using `pandas`:

```
data = pd.read_excel('Bitcoin_Daily_Historical_Data.xlsx').values
```

The next step is to calculate volatility. Its function is defined in master\_function and as you have imported it already in, it should work directly:

```
data = volatility(data, 10, 0, 1)
data = data[:, -1]
```

You have to check for stationarity. If the data is stationary, then it’s ready to be deployed for training. Otherwise, you may have to transform it. The following code applies the ADF test on the 10-day volatility of Bitcoin:

```
print('p-value: %f' % adfuller(data)[1])
```

Here is the output:

```
p-value: 0.120516
```

It seems that the volatility values are non-stationary. Let’s try differencing:

```
data = np.diff(data)
```

**NOTE**

You can also apply fractional differentation in order to preserve a hint of memory.

[Figure 11-10](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-10) shows the latest values of Bitcoin’s 10-day differenced volatility.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1110.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 11-10. Bitcoin’s rolling 10-day standard deviation (differenced)**

The next step is to set the hyperparameters and prepare the arrays as usual:

<pre><code>num_lags = 300
train_test_split = 0.80
neurons = 80
num_epochs = 100
batch_size = 500
<strong># Prepare the arrays
</strong>x_train, y_train, x_test, y_test = direct_mpf(data, 
                                              num_lags, 
                                              train_test_split, 
                                              forecast_horizon)
</code></pre>

To comply with the LSTM architecture, the independent variables must be transformed into 3-dimensional arrays. This is done using the following code:

```
x_train = x_train.reshape((-1, num_lags, 1))
x_test = x_test.reshape((-1, num_lags, 1))
```

Create the deep neural network architecture with a few extra layers:

<pre><code><strong># Create the LSTM model
</strong>model = Sequential()
<strong># Adding a first layer
</strong>model.add(LSTM(units = neurons, input_shape = (num_lags, 1)))
<strong># Adding a second layer
</strong>model.add(Dense(neurons, activation = 'relu')) 
<strong># Adding a third layer
</strong>model.add(Dense(neurons, activation = 'relu')) 
<strong># Adding a fourth layer
</strong>model.add(Dense(neurons, activation = 'relu')) 
<strong># Adding the output layer 
</strong>model.add(Dense(units = forecast_horizon))
<strong># Compiling the model
</strong>model.compile(loss = 'mean_squared_error', optimizer = 'adam')
</code></pre>

Now, fit and predict the data:

```
model.fit(x_train, y_train, epochs = num_epochs, batch_size = batch_size)
y_predicted = model.predict(x_test)
```

[Figure 11-11](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-11) compares the predicted values with the test values.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1111.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 11-11. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

The following are the results of the model:

```
Accuracy Train =  66.56 %
Accuracy Test =  63.59 %
RMSE Train =  95.6086778521
RMSE Test =  186.1401365824
Correlation In-Sample Predicted/Train =  0.807
Correlation Out-of-Sample Predicted/Test =  0.56
```

Keep in mind that the back-testing results may differ significantly due to the back-tested period, time granularity, transaction costs, different quotations from different brokers, different hyperparameters, and different randomization. Optimization is a key component and validation of the results must be done before forming an opinion on the algorithm. Your task is therefore to improve the results and get a better prediction on volatility.

You may also wonder if you can predict Bitcoin’s returns directly with an accuracy better than random. The answer is yes and if you are in need for a few ideas of inputs that may help predict Bitcoin’s returns, check out the following non-exhaustive list:

* _Historical price data and its derivatives_: Using historical price data is a fundamental aspect of predicting returns. You can include features such as daily, weekly, or monthly price changes, moving averages, and price volatility.
* _Trading volume_: The trading volume of Bitcoin provides valuable information about the level of market activity and liquidity. Higher trading volumes often accompany significant price movements.
* _Market sentiment indicators_: Sentiment analysis from social media platforms, news articles, or forums can help gauge the overall market sentiment towards Bitcoin. Bitcoin’s _Fear & Greed index_[_4_](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#id266) is a good candidate as it’s published on a daily basis and uses many fundamental variables to calculate its values.
* _Network metrics_: Bitcoin’s blockchain data provides useful metrics, such as the number of transactions, hash rate, and difficulty level. These metrics reflect the activity and health of the Bitcoin network, which can influence prices.
* _Market indicators_: Consider using general market indicators like the S\&P 500 or the VIX as external variables. Cryptocurrencies, including Bitcoin, can sometimes exhibit correlations with traditional financial markets.
* _Cryptocurrency-specific indicators_: Other indicators specific to the cryptocurrency space, such as total market capitalization of all cryptocurrencies and dominance ratio of Bitcoin, may provide insights into broader market conditions.
* _Technical indicators_: Various technical analysis indicators, such as the RSI and volatility, can offer insights into potential price trends and reversals.
* _Google trends_: Monitoring the popularity of search terms related to Bitcoin on Google Trends can provide insights into public interest and potential price movements.
* _Cryptocurrency exchange data_: Data from cryptocurrency exchanges, such as open interest, funding rates, and liquidation data, can offer insights into market dynamics and potential price shifts.

## Real Time Visualization of Training

What happens during training? Sure, you can see that the training process is on-going when you look at the progress bar of every epoch such as the following:

Epoch 1/100\
9/9 \[=============================] - 3s 77ms/step - loss: 0.0052\
Epoch 2/100\
9/9 \[=============================] - 1s 78ms/step - loss: 0.0026\
Epoch 3/100\
9/9 \[=============================] - 1s 68ms/step - loss: 0.0015

However, you can also code a dynamic plot that shows you the in-sample predictions getting updated through epochs as they approach the in-sample real values. This will be the first aim of this section. Before proceeding, refresh your knowledge of the terminologies:

* _In-sample real values_: These are the values contained in `y_train`. They are the real values which the model use to calibrate its training. They belong to the training set.
* _In-sample predictions_: These are the values contained in `y_predicted_train`. They are the predictions which the model outputs during its training. They belong to the training set and suffer from _look ahead bias_, which happens when future information is used to make decisions or predictions that should have been made in the past based only on historical data available at that time.&#x20;
* _Out-of-sample real values_: These are the test values used to test the model’s ability to predict values on never-seen-before data. They belong to the test set.
* _Out-of-sample predictions_: These are the predictions that follow the training phase. They belong to the test set.

Let’s take an example that you are familiar with from [Chapter 9](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch09.html#ch09), the ISM PMI data. The aim is to create a one-step forecast LSTM model of the differenced ISM PMI values while creating a dynamic plot during training that shows predictions being calibrated to the training set. First, import the required libraries:

```
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, LSTM
from master_function import data_preprocessing
from master_function import calculate_directional_accuracy
from sklearn.metrics import mean_squared_error
import tensorflow as tf
import random
```

Import and difference the ISM PMI data from the [GitHub repository](https://github.com/sofienkaabar/deep-learning-for-finance)[5](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#id267):

```
 data = np.diff(np.reshape(pd.read_excel('ISM_PMI.xlsx').values, (-1)))
```

The next step is to set the hyperparameters and prepare the arrays as usual:

<pre><code>num_lags = 100
train_test_split = 0.80
neurons = 200
num_epochs = 200
batch_size = 4
<strong># Creating the training and test sets
</strong>x_train, y_train, x_test, y_test = data_preprocessing(data, 
                                                      num_lags, 
                                                      train_test_split)
</code></pre>

To comply with the LSTM architecture, the independent variables must be transformed into three-dimensional arrays. This is done using the following code:

```
x_train = x_train.reshape((-1, num_lags, 1))
x_test = x_test.reshape((-1, num_lags, 1))
```

Create the deep neural network architecture:

<pre><code><strong># Create the LSTM model
</strong>model = Sequential()
<strong># Adding a first layer
</strong>model.add(LSTM(units = neurons, input_shape = (num_lags, 1)))
<strong># Adding a second layer
</strong>model.add(Dense(neurons, activation = 'relu')) 
<strong># Adding a third layer
</strong>model.add(Dense(neurons, activation = 'relu')) 
<strong># Adding a fourth layer
</strong>model.add(Dense(neurons, activation = 'relu')) 
<strong># Adding the output layer 
</strong>model.add(Dense(units = 1))
<strong># Compiling the model
</strong>model.compile(loss = 'mean_squared_error', optimizer = 'adam')
</code></pre>

The following code fits the `x_train` data to the `y_train` data while plotting the predictions at every epoch:

<pre><code>def update_plot(epoch, logs):
    if epoch % 1 == 0:
        plt.cla()
        y_predicted_train = model.predict(x_train)
        plt.plot(y_train, label = 'Training data', 
                 color = 'black', linewidth = 2.5)
        plt.plot(y_predicted_train, label = 'Predicted data', 
                 color = 'red', linewidth = 1)
        plt.title(f'Training Epoch: {epoch}')
        plt.xlabel('Time')
        plt.ylabel('Value')
        plt.legend()
        plt.savefig(str(random.randint(1, 1000)))
<strong># Create the dynamic plot
</strong>fig = plt.figure()
<strong># Train the model using the on_epoch_end callback
</strong>class PlotCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs = None):
        update_plot(epoch, logs)
        plt.pause(0.001)
plot_callback = PlotCallback()
history = model.fit(x_train, y_train, epochs = num_epochs, 
                    batch_size = batch_size, callbacks = [plot_callback])
</code></pre>

[Figure 11-12](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-12) shows the training at epoch 0. Notice how the algorithm is just starting out and is not quite capturing the relationship between the independent and dependent variables yet.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1112.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 11-12. ISM PMI data training in-progress during epoch 0**

[Figure 11-13](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-13) shows the training at epoch 24. It looks like the algorithm is still calibrating itself to the features.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1113.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 11-13. ISM PMI data training in-progress during epoch 24**

[Figure 11-14](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-14) shows the training at epoch 56. The model is starting to properly fit the data.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1114.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 11-14. ISM PMI data training in-progress during epoch 56**

[Figure 11-15](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-15) shows the training at epoch 93. The model seems to fit the data well although with some errors, but this is not the main aim of this section.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1115.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 11-15. ISM PMI data training in-progress during epoch 93**

Dynamic training plotting can be an interesting way to see how the model is learning, and if it’s actually learning something. This helps with the problem of _constant predictions_. The latter issue occurs when the model fails to capture any relationship between the dependent and independent variables.

Several factors can lead to the problem of constant predictions in deep learning:

* _Poor model architecture_: If the model is not expressive enough, it may struggle to learn meaningful patterns, resorting to a simple, constant prediction instead.
* _Limited or noisy data_: Insufficient or noisy data can hinder the model’s ability to learn meaningful patterns. If the data lacks diversity or contains significant errors, the model may converge to a constant forecast as the simplest way to minimize the loss.
* _Improper loss function_: The choice of the loss function plays a vital role in guiding the model during training. If the loss function is not appropriate for the task or the model architecture, it may not encourage the model to make varied predictions, leading to constant forecasts.
* _Poor hyperparameter tuning_: Hyperparameters, such as the batch size and the number of neurons, can significantly impact the training process. If these hyperparameters are not appropriately tuned, the model might not converge effectively, leading to constant predictions.

Finally, before ending this chapter, you must have wondered what the architecture you built using `Sequential()` and `Dense()` functions looks like, this is the second aim of the section. Naturally, it should look like the neural network graphs you have seen previously. To do this, you must `pip install` the required libraries from the prompt:

```
pip install pydot
pip install pydotplus
pip install graphviz
```

Then, download `graphviz` binaries folder from the [official website](https://graphviz.gitlab.io/download/). After that, extract the contents of the file, then set the _bin_ folder as one of the paths of the Python interpreter (for example, Spyder). Restart the kernel, and proceed to compile your model as usual. Finally, use the following code to print out the architecture:

```
from tensorflow.keras.utils import plot_model
from IPython.display import Image
plot_model(model, to_file = 'Architecture.png', 
           show_shapes = True, 
           show_layer_names = True)
Image('Architecture.png')
```

[Figure 11-16](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#figure-11-16) shows the output of the code:

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_1116.png" alt="" height="516" width="328"><figcaption></figcaption></figure>

**Figure 11-16. Model architecture example**

The code outputs the current architecture of the compiled LSTM model.

## Summary

This information-heavy chapter has shown you a few ideas on using a selection of trading algorithms to forecast returns. It is mostly a way to stimulate critical and innovative thinking and finding new and exotic ideas from where trading signals may be derived. For example, you can try applying filters with the signals you get from the algorithms. _Filters_ are like ON/OFF switches that allow the signal if a final condition is met or not. An example of a hypothetical trading strategy with a filter can be to take the bullish signals only if the market is above its 200-day moving average and to take the bearish signals only if the market is below its 200-day moving average.

Your main takeaway should be the way on how things must be structured to be able to understand the back-testing process. [Chapter 12](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch12.html#ch12) will be a gentle breeze as it explores risk management and fundamental tools aimed at enhancing the research process.

[1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#id262-marker) Link: https://github.com/sofienkaabar/deep-learning-for-finance

[2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#id263-marker) Swing trading involves holding positions for a short to medium-term period, typically a few days to a few weeks, to profit from price swings or price movements within a larger trend.

[3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#id264-marker) Link: https://github.com/sofienkaabar/deep-learning-for-finance

[4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#id266-marker) The index is used to gauge the emotions and sentiments of investors in the cryptocurrency market. It provides a numerical value on a scale from 0 to 100, where lower values indicate extreme fear, and higher values indicate extreme greed. The index is designed to help traders and investors identify potential market turning points based on prevailing emotions.

[5](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch11.html#id267-marker) Link: https://github.com/sofienkaabar/deep-learning-for-finance
