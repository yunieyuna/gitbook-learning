# 7. Machine Learning Models for Time Series Prediction

## Chapter 7. Machine Learning Models for Time Series Prediction

Machine learning is a subfield of artificial intelligence (AI) that focuses on the development of algorithms and models that enable computers to learn and make predictions or decisions without being explicitly programmed, hence the term _learning_. Machine learning deals with the design and construction of systems that can automatically learn and improve from experience, typically by analyzing and extracting patterns from large amounts of data.

This chapter presents the framework of using machine learning models for time series prediction and discusses a selection of known algorithms.

## The Framework

The _framework_ is very important as it organizes the way the whole research process is done (from data collection to performance evaluation). Having a proper framework ensures harmony across the back-tests, which allows for proper comparison between different machine learning models. The framework may follow these chronological steps:

1. Import and preprocess the historical data, which must contain a sufficient number of values to ensure a decent back-test and evaluation.
2. Perform a _train-test_ split, which splits the data into two parts where the first part of the data (for example, from 2000 to 2020) is reserved for training the algorithm so that it understands the mathematical formula to predict the future values, and the second part of the data (for example, from 2020 to 2023) is reserved for testing the algorithm’s performance on data that it has never seen before.
3. Fit (train) and predict (test) the data using the algorithm.
4. Run a performance evaluation algorithm in order to understand the model’s performance in the past.

**NOTE**

The _training set_ is also called the _in-sample data_ and the _test set_ is also called the _out-of-sample data_.

The first step of the framework has been discussed in [Chapter 6](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch06.html#ch06). You should now be able to easily import historical data using Python. The train-test split divides the historical data into a training (in-sample) set where the model is fitted (trained) so that an implied forecasting function is found, and a test (out-of-sample) set where the forecasting function that has been calculated on the training set is applied and evaluated. Theoretically, if the model does well on the test set, it is likely that you have a potential candidate for a trading strategy, but this is just a first step and the reality is much more complicated than that.

So that everything goes smoothly, download _master\_function.py_ from the GitHub repository[1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#id251) and then setting the directory of the Python interpreter (for example, Spyder) in the same location as the downloaded file so that you can import it as a library and use its functions. For example, if you download the file in the desktop, you may want to set the directory as [Figure 6-5](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch06.html#figure-6-5) shows in [Chapter 6](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch06.html#ch06). The choice of the directory is typically found on the upper right corner in Spyder (above the variable explorer).

**NOTE**

If you prefer not to import _master\_function.py_, you can just open it in the interpreter as a normal file, and execute it so that Python defines the functions inside. However, you have to do this every time you restart the kernel.

Now, preprocess (transform) and split the time series into four different arrays (or data frames if you wish) with each array having a utility:

* Array `x_train` is the in-sample set of features (independent variables) that explain the variations of the variable that you want to forecast. They are the predictors.
* Array `y_train` is the in-sample set of dependent variables (in other words, the right answers) that you want the model to calibrate its forecasting function on.
* Array `x_test` is the out-of-sample set of features that will be used as a test of the model to see how it performs on this never-seen-before data.
* Array `y_test` contains the real values that the model must approach. In other words, these are the right answers that will be compared with the model’s forecasts.

Before the split, it is important to know what is being forecasted and what is being used to forecast it. In this chapter, lagged price differences (returns) will be used for the forecast. Normally, a few tests must be made before doing this, but for simplicity, let’s leave them out and suppose that the last five hundred daily EURUSD returns have predictive power over the current return, which means that you can find a predictive formula that uses the last five hundred observations to observe the next one:

* _Dependent variable (forecast)_: The t+1 return of EURUSD in the daily time frame. This is also referred to as the _y_ variable.
* _Independent variables (inputs)_: The last five hundred daily returns of EURUSD. These are also referred to as the _x_ variables.

[Figure 7-1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#figure-7-1) shows EURUSD daily returns over a certain time period. Notice the stationary appearance it has. According the ADF test (seen in [Chapter 3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#ch03)), the returns dataset seems stationary and is valid for a regression analysis.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0701.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 7-1. EURUSD daily returns**

**NOTE**

In this chapter, the features (_x_ values) will be the lagged daily price differences of EURUSD[2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#id252). In subsequent chapters, the features used will be either lagged returns or values of technical indicators. Note that you can use whichever features you think are worthy of being considered as predictive.

The choice of the time frame (daily) is ideal for traders who want an intraday view that will help them trade the market and close the position before the end of the day

Let’s use the dummy regression model as a first basic example. _Dummy regression_ is a comparison machine learning algorithm that is only used as a benchmark, as it uses very simple rules for predictions which are unlikely to add any real forecasting value. The real utility of the dummy regression is to see if your real model outperforms it or not. As a reminder, the process of the machine learning algorithms is composed of the following steps:

1. Import the data.
2. Preprocess and split the data.
3. Train the algorithm.
4. Predict on test data using the training parameters. Also, predict on training data for comparison.
5. Plot and evaluate the results.

Start by importing the required libraries for this chapter:

```
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from master_function import data_preprocessing, mass_import
from master_function import plot_train_test_values, 
from master_function import calculate_accuracy, model_bias
from sklearn.metrics import mean_squared_error
```

Now, import the specific library for the algorithm you will use:

```
from sklearn.dummy import DummyRegressor
```

The next step is to import and transform the close price data. Remember, you are trying to forecast daily returns, which means that you must select the close column only and then apply a differencing function on it so that prices become differenced:

**NOTE**

In finance, the term _returns_ typically refers to the gain or loss generated by an investment or a certain asset, and it can be calculated by taking the difference between the current value of an asset and its value at a previous point in time. This is essentially a form of differencing, as you are calculating the change or difference in the asset’s value.

In time series analysis, differencing is a common technique used to make a time series data stationary, which can be helpful for various analyses. Differencing involves subtracting consecutive observations from each other to remove trends or seasonality, thereby focusing on the changes in the data.

<pre><code><strong># Importing the differenced close price of EURUSD daily time frame
</strong>data = np.diff(mass_import(0, 'H1')[:, 3])
</code></pre>

Next, set the hyperparameters of the algorithm. In the case of these basic algorithms, it would be the number of lags (number of predictors) and the percentage split of data:

<pre><code><strong># Setting the hyperparameters
</strong>num_lags = 500
train_test_split = 0.80
</code></pre>

A train\_test\_split of 0.80 means that 80% of the data will be used for training while the remaining 20% will be used for testing.

The function to split and define the four necessary arrays for the back-test can be defined as follows:

<pre><code>def data_preprocessing(data, num_lags, train_test_split):
<strong>    # Prepare the data for training
</strong>    x = []
    y = []
    for i in range(len(data) - num_lags):
        x.append(data[i:i + num_lags])
        y.append(data[i+ num_lags])
<strong>    # Convert the data to numpy arrays
</strong>    x = np.array(x)
    y = np.array(y)
<strong>    # Split the data into training and testing sets
</strong>    split_index = int(train_test_split * len(x))
    x_train = x[:split_index]
    y_train = y[:split_index]
    x_test = x[split_index:]
    y_test = y[split_index:]
    return x_train, y_train, x_test, y_test
</code></pre>

Call the function to create the four arrays:

<pre><code><strong># Creating the training and test sets
</strong>x_train, y_train, x_test, y_test = data_preprocessing(data, 
                                                      num_lags, 
                                                      train_test_split)
</code></pre>

You should now see four new arrays appearing in the variable explorer. The next step is to train the data using the chosen algorithm:

<pre><code><strong># Fitting the model
</strong>model = DummyRegressor(strategy = 'mean')
model.fit(x_train, y_train)
</code></pre>

Note that the dummy regression can take any of the following strategies as arguments:

* `mean`: Always predicts the mean of the training set.
* `median`: Always predicts the median of the training set.
* `quantile`: Always predicts a specified quantile of the training set, provided with the quantile parameter.
* `constant`: Always predicts a constant value that is provided by the user.

As you can see from the previous code, the selected parameter is `mean`. Naturally, this signifies that all the predictions made will be simply the mean of the training set (`y_train`). This is why the dummy regression is only used as a benchmark and not as a serious machine learning model.

After that, predict on the test data, but also on the training data as a means of comparison. Note that the predictions on the training data have no value since the algorithm has already seen the data during training, but it is interesting to know how worse or better the algorithm performs on never seen before data:

<pre><code><strong># Predicting in-sample
</strong>y_predicted_train = np.reshape(model.predict(x_train), (-1, 1))
<strong># Predicting out-of-sample
</strong>y_predicted = np.reshape(model.predict(x_test), (-1, 1))
</code></pre>

To make sure your reasoning is correct with regards to using the dummy regresison algorithm, calculate manually the mean of `y_train` and compare it to the value you get in every `y_predicted`. You will see that it’s the same:

<pre><code><strong># Comparing the mean of y_train to an arbitrary value in y_predicted
</strong>y_train.mean() == y_predicted[123]
</code></pre>

The output should be as follows:

```
True
```

Finally, use the following function to plot the last training data followed by the first test data and the equivalent predicted data:

<pre><code><strong># Plotting
</strong>plot_train_test_values(100, 50, y_train, y_test, y_predicted)
</code></pre>

**NOTE**

You can find the definition of `plot_train_test_values()` function in the GitHub repository.

[Figure 7-2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#figure-7-2) shows the evolution of the forecasting task from the last values of `y_train` to the first values of `y_test` and `y_predicted`. Naturally, the dummy regression algorithm predicts a constant value which is why the prediction line alongside the test values is a straight line.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0702.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 7-2. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

**NOTE**

If you want the figures to be plotted in a separate window, type `%matplotlib qt` in the console. If you want the figures to be inside the plots explorer, type `%matplotlib inline` in the console.

How can you tell if a model is performing well or not? _Performance evaluation_ is a key concept in trading and algorithmic development as it ensures you pick the right model and take it live. However, the task is not simple due to an ironically simple question: _If the past performance was good, what guarantees it continues to be good?_

This question is painful but it points towards the right direction. The answer to this question is subjective. For now, let’s talk about the different ways to measure the performance of a model. To simplify the task, I will split the performance and evaluation metrics into two: model evaluation and trading evaluation. _Model evaluation_ deals with the algorithm’s performance in its forecasts while _trading evaluation_ deals with the financial performance of a system that trades using the algorithm (an example of a trading evaluation metric is the net profit).

Let’s start with model evaluation. _Accuracy_ is the first metric that comes to mind when comparing forecasts to real values, especially in the financial markets. Theoretically, if you predict the direction (up or down) and you get it right, you should make money (excluding transaction costs). Accuracy is also referred to as the _hit ratio_ in financial jargon and is calculated as follows:

��������=����������������������������������×100

For example, if you have made 100 predictions last year and had 73 of them correct, you would have a 73% accuracy.

Forecasting can also be evaluated by how close the predicted values (`y_predicted`) are to the real values (`y_test`). This is done by loss functions. A _loss function_ is a mathematical calculation that measures the difference between the predictions and the real (test) values. The most basic loss function is the _mean absolute error_ (MAE). It measures the average of the absolute differences between the predicted and actual values. The mathematical representaion of MAE is as follows:

���=∑�=1�|�^-��|�

�^���ℎ�������������������ℎ����������

Therefore, MAE calculates the average distance (or positive difference) between the predicted and the real values. The lower the MAE, the more accurate the model.

The _mean squared error_ (MSE) is one of the commonly used loss functions for regression. It measures the average of the squared differences between the predicted and actual values. You can think of MSE as the equivalent of the variance metric seen in [Chapter 3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#ch03). The mathematical representaion of MSE is as follows:

���=∑�=1�(�^-��)2�

Hence, MSE calculates the average squared distance between the predicted and the real values. Similar to the MAE, the lower the MSE, the more accurate the model. With this in mind, it helps to compare apples to apples (such as with variance and standard deviation as seen in [Chapter 3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch03.html#ch03)). Therefore, the _root mean squared error_ (RMSE) has been developed to tackle this problem (hence, scaling the error metric back to the same units as the target variable). The mathematical representaion of RMSE is as follows:

����=∑�=1�(�^-��)2�

The RMSE is the equivalent of the standard deviation in descriptive statistics.

**NOTE**

MAE is relatively less sensitive to outliers than MSE, and is often used when the data contains extreme values or when the absolute magnitude of the error is more important than its squared value. On the other hand, as MSE gives more weight to larger errors, it is the go-to loss function when trying to improve the performance of the model.

When evaluating models using MAE, MSE, or RMSE, it is important to have a baseline for comparison:

* If you have built multiple regression models, you can compare their metrics to determine which model performs better. The model with the lower metric is generally considered to be more accurate in its predictions.
* Depending on the specific problem, you may have a threshold value for what is considered an acceptable level of prediction error. For example, in some cases, an RMSE below a certain threshold may be considered satisfactory, while values above that threshold may be considered unacceptable.
* You can compare the loss functions of the training data with the loss functions of the test data.

Algorithms may sometimes be directionally biased for many reasons (either structurally or externally). A _biased model_ takes on significantly more trades in one direction than the other (an example would be an algorithm having 200 long positions and 30 short positions). _Model bias_ measures this as a ratio by dividing the number of long positions by the number of short positions. The ideal model bias is around 1.00 which implies a balanced trading system. The mathematical representation of model bias is as follows:

���������=��������������ℎ���������������������ℎ�������

If a model has had 934 long positions and 899 short positions this year, then the model bias metric is 1.038 which is acceptable. This means that the model is not really biased. It is worth noting that a model bias of 0.0 represents the absence of any bullish signals and a model bias that has an undefined value represents the absence of any bearish signals (due to the division by zero).

Optionally, let’s discuss trading evaluation. Finance pioneers have been developing metrics that measure the performance of strategies and portfolios. Let’s discuss the most common and most useful ones. The first and most basic metric is the _net return_, which is basically the return over the invested capital after a trading period that has at least one closed trade. The mathematical representation of the net return is as follows:

���������=(����������������������-1)×100

The word _net_ implies a result after deducting any type of fees, otherwise it is referred to as a _gross_ return. For example, if you start the year with $52,000 and finish at $67,150, you would have made 29.13% (a net profit of $15,150).

Another profitability metric is the _profit factor,_ which is the ratio between the total gross profits to the total gross losses. Intuitively, a profit factor above 1.00 implies a profitable strategy and a profit factor below 1.00 implies a losing strategy. The mathematical representation of the profit factor is as follows:

������������=�����������������������

The profit factor is a useful metric for evaluating the profitability of a trading strategy because it takes into account both the profits and losses generated by the strategy, rather than just looking at one side of the equation. The profit factor of a trading strategy that has generated $54,012 in profits and $29,988 in losses is 1.80.

The next interesting metric relates to individual trades. The _average gain_ per trade calculates the average profits (or positive returns) per trade based on historical data and the _average loss_ per trade calculates losses (or negative returns) per trade based on historical data. These two metrics give an expected return depending on the outcome. Both metrics are calculated following these formulae:

�����������=�������������������������������������������=�������������������������������

The next metric relates to risk and is one of the most important measures of evaluation. _Maximum drawdown_ is a metric that measures the largest percentage decline in the value of an investment or portfolio from its highest historical peak to its lowest point. It is commonly used to assess the downside risk of an investment or portfolio. For example, if an investment has a peak value of $100,000 and its value subsequently drops to $50,000 before recovering, the maximum drawdown would be 50%, which is the percentage decline from the peak value to the trough.

���������������=(�����ℎ�����-������������������)×100

Finally, let’s discuss a famous profitability ratio called the _Sharpe ratio_. It is how much return generated by units of excess risk. The formula of the ratio is as follows:

�ℎ����=�-��

����ℎ��������������ℎ�����������������ℎ��������������������

So, if the net return is 5% and the risk-free rate is 2% while the volatility of returns is 2.5%, the Sharpe ratio becomes 1.20. Anything above 1.00 is desirable as it implies that the strategy is generating positive excess risk-adjusted return.

## RISK-FREE?

The _risk-free rate_ refers to the theoretical rate of return on an investment that carries no risk. It serves as a benchmark for evaluating the potential returns of other investments that do involve risk. In practice, the risk-free rate is typically based on the yield of a government-issued bond, usually one with a short-term maturity.

The specific risk-free rate can vary depending on the country and currency involved. In the United States, for example, the risk-free rate is often associated with the yield on U.S. Treasury securities. The most commonly used benchmark is the yield on the 10-year Treasury note, as it is considered a relatively low-risk investment.

The focus of this book is on the developing machine and deep learning algorithm, therefore, the performance evaluation step will solely focus on accuracy, RMSE, model bias (and also correlation between predicted variables as an extra metric). The performance functions can be found in the GitHub along with the complete scripts.

The model’s results on EURUSD after applying the performance metrics are as follows:

```
Accuracy Train =  49.28 %
Accuracy Test =  49.33 %
RMSE Train =  0.0076467838
RMSE Test =  0.0053250347
Model Bias =  0.0
```

With a bias of 0.0, it’s easy to see that this is a dummy regression model. The bias means that according to the formula, all the forecasts are bearish. Taking a look into the details of the predictions, you will see that they are all constant values.

## MORE WAYS TO IMPORT DATA

Some readers may not have the right operating system to run the importing algorithm through MetaTrader5. For this, you have alternative ways to import the data. The first method is to use `pandas_datareader` library which does not require a third party software installation. It does however require a pip installation. To import daily EURUSD daily, use the following code (also found in the GitHub repository):

<pre><code><strong># Importing the required libraries
</strong>import pandas_datareader as pdr
import numpy as np
<strong># Set the start and end dates for the data
</strong>start_date = '2000-01-01'
end_date   = '2023-06-01'
# Fetch EURUSD price data
data = np.array((pdr.get_data_fred('DEXUSEU', 
                                   start = start_date, 
                                   end = end_date)).dropna())
<strong># Difference the data and make it stationary
</strong>data = np.diff(data[:, 0])
</code></pre>

The second method is to simply refer to the folder called Historical Data in the repository which contains multiple excel files with the historical data. By downloading them, you can import them into Spyder using the following code:

<pre><code><strong># Importing the required libraries
</strong>import pandas as pd
import numpy as np
<strong># Import the data (write the code in one line)
</strong>data = np.array(pd.read_excel('Daily_EURUSD_Historical_Data.xlsx')
       ['&#x3C;CLOSE>'])
<strong># Difference the data and make it stationary
</strong>data = np.diff(data)
</code></pre>

Make sure to check that the directory of the interpreter is the same as the downloaded files, otherwise you will get an error.

The key takeaways from this section are as follows:

* Automatic data import and creation saves you time and allows you to concentrate on the main issues of the algorithm.
* For a proper back-test, the data must be split into a training set and a test set.
* The training set contains `x_train` and `y_train` with the former containing the values that are supposed to have a predictive power over the latter.
* The test set contains `x_test` and `y_test` with the former containing the values that are supposed to have a predictive power (even though the model hasn’t encountered them in its training) over the latter.
* Fitting the data is when the algorithm runs on the training set and predicting the data is when the algorithm runs on the test set.
* The predictions are stored in a variable called `y_predicted` which is compared to `y_test` for performance evaluation purposes.
* The main aim of the algorithms is to have good accuracy and stable low volatility returns.

## Machine Learning Models

This section presents a selection of machine learning models using the framework developed so far. It is important to understand every model’s intuition and assumptions, but also its weaknesses so that you know which model to choose depending on the forecasting task.

### Linear Regression

The _linear regression_ algorithm works by finding the best-fitting line that minimizes the sum of squared differences between the predicted and actual target values. The most used optimization technique in this algorithm is the _ordinary least squares (OLS)_ method[3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#id255).

The model is trained on the training set using the OLS method which estimates the coefficients that minimize the sum of squared differences between the predicted and actual target values in order to find the optimal coefficients for the independent variables (the coefficients represent the _y_-intercept and the slope of the best-fitting line, respectively). The output is a linear function that gives the expected return given the explanatory variables weighted by the coefficient with an adjustment for noise and the intercept.

Import the linear regression library from `sklearn` as follows:

```
from sklearn.linear_model import LinearRegression
```

Fit the model:

<pre><code><strong># Fitting the model
</strong>model = LinearRegression()
model.fit(x_train, y_train)
<strong># Predicting in-sample
</strong>y_predicted_train = np.reshape(model.predict(x_train), (-1, 1))
<strong># Predicting out-of-sample
</strong>y_predicted = np.reshape(model.predict(x_test), (-1, 1))
</code></pre>

The model assumes that the linear relationship that has held in the past will still hold in the future. This is unrealistic and it ignores the fact that market dynamics and drivers are constantly shifting whether on the short-term or the long-term. They are also non-linear.

[Figure 7-3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#figure-7-3) shows the evolution of the forecasting task from the last values of `y_train` to the first values of `y_test` and `y_predicted`.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0703.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 7-3. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

The model’s results on EURUSD after applying the performance metrics are as follows:

```
Accuracy Train =  58.52 %
Accuracy Test =  49.54 %
RMSE Train =  0.007096094
RMSE Test =  0.0055932632
Correlation In-Sample Predicted/Train =  0.373
Correlation Out-of-Sample Predicted/Test =  0.014
Model Bias =  0.93
```

The results indicate a poor performance coming from the linear regression algorithm with an accuracy below 50.00%. As you can see, the accuracy generally drops after switching to the test set. The correlation between the in-sample predictions and the real in-sample values also drops from 0.373 to 0.014. The model bias is close to equilibrium which means that the number of long signals is close to the number of short signals.

There are a few things to note regarding the model’s results:

* The transaction costs have not been incorporated and therefore the results are gross.
* There is no risk management system as this is a pure time series machine learning model and not a full trading algorithm that incorporates stops and targets. Therefore, by being a purely directional model, the job is to try to maximize the number of correct forecasts. With a daily horizon, you are searching for accuracy.
* Different FX data providers may have small differences in the historical data which may cause some differences between back-tests.

Models are made to be optimized and tweaked. The process of optimization may include any of the following techniques:

* _Choosing the right predictors is paramount to a model’s success_: In this chapter, the predictors used are the lagged returns. This has been chosen arbitrarily and is not necessarily the right choice. Predictors must be chosen based on economic and statistical intuition. For example, it may be reasonable to choose the returns of gold to explain (predict) the variations on the S\&P 500 index as they are economically linked. Safe haven assets like gold rise during periods of economic uncertainty while the stock market tends to fall. This negative correlation may harbor hidden patterns between the returns of both instruments. Another way of choosing predictors is to use technical indicators such as the RSI and moving averages.
* _Proper splitting is crucial to evaluate the model properly_: Train-test splits are important as they determine the window of evaluation. Typically, 20/80 and 30/70 are used, which means that 20% (30%) of the data is used for the testing sample and 80% (70%) is used for the training sample.
* _Regularization techniques can help prevent biases_: Ridge regression and Lasso regression are two common regularization methods used in linear regression. _Ridge regression_ adds a penalty term to the OLS function to reduce the impact of large coefficients, while _Lasso regression_ can drive some coefficients to zero, effectively performing feature selection.

The model seen in this section is called an _autoregressive model_ since the dependent variable depends on its past values and not on exogenous data. Also, since at every time step, five hundred different variables (with their coefficients) have been used to predict the next variable, the model is referred to as a _multiple linear regression_. In contrast, when the model only uses one dependent variable to predict the dependent variable, it is referred to as a _simple linear regression_.

The advantages of linear regression are:

* It is easy to implement and train. It also does not consume a lot of memory.
* It outperforms when the data has a linear dependency.

The disadvantages of linear regression are:

* It is sensitive to outliers.
* It is easily biased (more on this type of bias in the last section of this chapter).
* It has unrealistic assumptions such as the independence of data.

Before moving on to the next section, you must be careful from linear regression models that do not transform the data. You may see extremely high accuracy and a prediction that is very close to the real data, but the reality is that it lags it with one time step. This means that at every time step, the prediction is simply the last real value. Let’s prove this using the same example. Use the same code as before but omit the price differencing code. You should see [Figure 7-4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#figure-7-4).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0704.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 7-4. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

Notice how it’s simply lagging the real values and not adding any predictive information. Always transform non-stationary data when dealing with such models. Non-stationary cannot be forecasted using this type of algorithms (there are exceptions which you will see later on).

When using linear regression on non-stationary data, such as market prices, and observing that the forecasts are the same as the last value, it might be an indication of a specific issue known as _naive forecasting_. This occurs when the most recent observation (in this case, the last value) is simply used as the forecast for the next time period. While this approach can sometimes work for certain types of data, it is generally not a sophisticated forecasting method and may not capture the underlying patterns or trends in the data. There are a few reasons why this might happen:

* _Lack of predictive power_: Linear regression assumes that there is a linear relationship between the independent variable(s) and the dependent variable. If the data is highly non-stationary and lacks a clear linear relationship, then the linear regression model may not be able to capture meaningful patterns and will default to a simplistic forecast like naive forecasting.
* _Lagging indicators_: Market prices often exhibit strong auto-correlation, meaning that the current price is highly correlated with the previous price. In such cases, if the model only takes into account lagged values as predictors, it might simply replicate the last value as the forecast.
* _Lack of feature engineering_: Linear regression models rely on the features (predictors) you provide to make forecasts. If you’re using only lagged values as predictors and not incorporating other relevant features, the model might struggle to generate meaningful forecasts.
* _Model complexity_: Linear regression is a relatively simple modeling technique. If the underlying relationship in the data is more complex than can be captured by a linear equation, the model might not be able to make accurate forecasts.

### Support Vector Regression

_Support vector regression_ (SVR) is a machine learning algorithm that belongs to the family of _Support vector machines_ (SVMs). SVR is specifically designed for regression problems, where the goal is to predict continuous numerical values (such as return values).

SVR performs regression by finding a hyperplane in a high-dimensional feature space that best approximates the relationship between the input features and the target variable. Unlike traditional regression techniques that aim to minimize the errors between the predicted and actual values, SVR focuses on finding a hyperplane that captures the majority of the data within a specified margin, known as the _epsilon tube_ (loss function).

The key idea behind SVR is to transform the original input space into a higher-dimensional space using a kernel function. This transformation allows SVR to implicitly map the data into a higher-dimensional feature space, where it becomes easier to find a linear relationship between the features and the target variable. The kernel function calculates the similarity between two data points, enabling the SVR algorithm to work effectively in non-linear regression problems. The steps performed in the SVR process are as follows:

1. The algorithm employs a kernel function to transform the input features into a higher-dimensional space. Common kernel functions include the _linear kernel_, _polynomial kernel_, _radial basis function kernel (RBF)_, and _sigmoid kernel_. The choice of kernel depends on the data and the underlying problem.
2. The algorithm then aims to find the hyperplane that best fits the data points within the epsilon tube. The training process involves solving an optimization problem to minimize the error (using a loss function such as MSE) while controlling the margin.

**NOTE**

The _RBF kernel_ is a popular choice for SVR because it can capture non-linear relationships effectively. It is suitable when there is no prior knowledge about the specific form of the relationship. The RBF kernel calculates the similarity between feature vectors based on their distance in the input space. It uses a parameter called _gamma_, which determines the influence of each training example on the model. Higher gamma values make the model focus more on individual data points, potentially leading to errors.

By finding an optimal hyperplane within the epsilon-tube, SVR can effectively capture the underlying patterns and relationships in the data, even in the presence of noise or outliers. It is a powerful technique for regression tasks, especially when dealing with non-linear relationships between features and target variables.

As SVR is sensitive to the scale of the features, it’s important to bring all the features to a similar scale. Common scaling methods include _standardization_ (mean subtraction and division by standard deviation) or _normalization_ (scaling features to a range, e.g., \[0, 1]).

Let’s take a look at the SVR in action. Once again, the aim is to solve the same issue, that is predicting the next EURUSD return given the previous returns. To import the SVR library and the scaling library, use the following code:

```
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
```

For the SVR algorithm, a little tweaking was done to get acceptable forecasts. The tweak came from reducing the number of lagged values from 500 to 50:

```
num_lags = 50
```

This allows the SVR to improve its forecasts. You will see throughout the book that a part of performing these types of back-tests is all about tweaking and calibrating the models.

Next, to implement the algorithm, use the following code:

<pre><code><strong># Fitting the model
</strong>model = make_pipeline(StandardScaler(), 
                      SVR(kernel = 'rbf', C = 1, gamma = 0.04, 
                      epsilon = 0.01))
model.fit(x_train, y_train)
<strong># Predicting in-sample
</strong>y_predicted_train = np.reshape(model.predict(x_train), (-1, 1))
<strong># Predicting out-of-sample
</strong>y_predicted = np.reshape(model.predict(x_test), (-1, 1))
</code></pre>

[Figure 7-5](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#figure-7-5) shows the evolution of the forecasting task from the last values of `y_train` to the first values of `y_test` and `y_predicted`.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0705.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 7-5. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

The model’s results are as follows:

```
Accuracy Train =  57.94 %
Accuracy Test =  50.14 %
RMSE Train =  0.0060447699
RMSE Test =  0.0054036167
Correlation In-Sample Predicted/Train =  0.686
Correlation Out-of-Sample Predicted/Test =  0.024
Model Bias =  0.98
```

The advantages of SVR are:

* It performs well even in high-dimensional feature spaces, where the number of features is large compared to the number of samples. It is particularly useful when dealing with complex datasets.
* It can capture non-linear relationships between input features and the target variable by using kernel functions.
* It is robust to outliers in the training data due to the epsilon-tube formulation. The model focuses on fitting the majority of the data within the specified margin, reducing the influence of outliers.

The disadvantages of SVR are:

* It has several hyperparameters that need to be tuned for optimal performance. Selecting appropriate hyperparameters can be a challenging task and may require extensive experimentation.
* It can be computationally expensive, especially for large datasets or when using complex kernel functions.
* It can be sensitive to the choice of hyperparameters. Poorly chosen hyperparameters can lead to fitting issues.

### Stochastic Gradient Descent Regression

_Gradient descent_ (GD) is a general optimization algorithm used to minimize the cost or loss function of a model, and it serves as the foundation for various optimization algorithms.

**NOTE**

Gradient simply refers to a surface’s slope or tilt. In order to get to the lowest point on the surface, one must literally descend a slope.

_Stochastic gradient descent_ (SGD) is an iterative optimization algorithm commonly used for training machine learning models, including regression models. It is particularly useful for large datasets and online learning scenarios. When applied to time series prediction, SGD can be used to train regression models that capture temporal patterns and make predictions based on historical data. SGD is therefore a type of linear regression that uses stochastic gradient descent optimization to find the best-fitting line.

Unlike ordinary least squares, SGD updates the model’s parameters iteratively, making it more suitable for large datasets (which are treated in small batches). Instead of using the entire dataset for each update step, SGD randomly selects a small batch of samples or individual samples from the training dataset. This random selection helps to introduce randomness and avoid getting stuck in local optima (you can refer to [Chapter 4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch04.html#ch04) for more information on optimization). The main difference between GD and SGD lies in how they update the model’s parameters during optimization.

**NOTE**

SGD does not belong to any particular family of machine learning models; it is essentially an optimization technique.

GD computes the gradients over the entire training dataset, updating the model’s parameters once per epoch, while SGD computes the gradients based on a single training example or mini-batch, updating the parameters more frequently. SGD is faster but exhibits more erratic behavior, while GD is slower but has a smoother convergence trajectory. SGD is also more robust to local minima. The choice between GD and SGD depends on the specific requirements of the problem, the dataset size, and the trade-off between computational efficiency and convergence behavior. As usual, the first step is to import the necessary libraries:

```
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
```

Next, to implement the algorithm, use the following code:

<pre><code><strong># Fitting the model
</strong>model = make_pipeline(StandardScaler(), SGDRegressor(max_iter = 50, 
                                                     tol = 1e-3))
model.fit(x_train, y_train)
<strong># Predicting in-sample
</strong>y_predicted_train = np.reshape(model.predict(x_train), (-1, 1))
<strong># Predicting out-of-sample
</strong>y_predicted = np.reshape(model.predict(x_test), (-1, 1))
</code></pre>

[Figure 7-6](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#figure-7-6) shows the evolution of the forecasting task from the last values of `y_train` to the first values of `y_test` and `y_predicted`.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0706.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 7-6. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

The model’s results are as follows:

```
Accuracy Train =  55.59 %
Accuracy Test =  46.45 %
RMSE Train =  0.007834505
RMSE Test =  0.0059334014
Correlation In-Sample Predicted/Train =  0.235
Correlation Out-of-Sample Predicted/Test =  -0.001
Model Bias =  0.95
```

The advantages of SGD are:

* It performs well with large datasets since it updates the model parameters incrementally based on individual or small subsets of training examples.
* It can escape local minima and find better global optima (due to its stochastic nature).
* By exposing the model to different training examples in each iteration, SGD can improve generalization by reducing overfitting.

The disadvantages of SGD are:

* The convergence path can be noisy and exhibit more fluctuations compared to deterministic optimization algorithms. This can result in slower convergence or oscillations around the optimal solution.
* It is impacted by feature scaling, which means it is sensitive to such techniques.

### Nearest Neighbors Regression

The _nearest neighbors regression_ algorithm, also known as _k_-nearest neighbors (KNN) regression, is a non-parametric[4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#id256) algorithm used for regression tasks. It predicts the value of a target variable based on the values of its nearest neighbors in the feature space. The algorithm starts by determining _k_ which is the number of nearest neighbors to consider when making predictions. This is a hyperparameter that you need to choose based on the problem at hand.

**NOTE**

A larger _k_ value provides a smoother prediction, while a smaller _k_ value captures more local variations but may be more prone to noise.

Then, the model calculates the distance between the new, unseen data point and all the data points in the training set. The choice of distance metric depends on the nature of the input features. Common distance metrics include _Euclidean distance_, _Manhattan distance_, or _Minkowski distance_. Afterwards, the algorithm selects the _k_ data points with the shortest distances to the query point. These data points are the _nearest neighbors_ and will be used to make predictions.

Import the KNN regressor:

```
from sklearn.neighbors import KNeighborsRegressor
```

Fit the model with _k_ = 10:

<pre><code><strong># Fitting the model
</strong>model = KNeighborsRegressor(n_neighbors = 10)
model.fit(x_train, y_train)
<strong># Predicting in-sample
</strong>y_predicted_train = np.reshape(model.predict(x_train), (-1, 1))
<strong># Predicting out-of-sample
</strong>y_predicted = np.reshape(model.predict(x_test), (-1, 1))
</code></pre>

[Figure 7-7](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#figure-7-7) shows the evolution of the forecasting task from the last values of `y_train` to the first values of `y_test` and `y_predicted`.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0707.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 7-7. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

The choice of the number of neighbors in a time series prediction using the KNN regressor depends on several factors, including the characteristics of your dataset and the desired level of accuracy. There is no definitive answer to how many neighbors to choose, as it is often determined through experimentation and validation. Typically, selecting an appropriate value for the number of neighbors involves a trade-off between bias and variance:

* Small _k_ values are associated with a model that can capture local patterns in the data, but it may also be sensitive to noise or outliers.
* Larger _k_ values are associated with a model that can become more robust to noise or outliers but may overlook local patterns in the data.

**NOTE**

If you take the limit as _k_ approaches the size of the dataset, you will get a model that just predicts the class that appears more frequently in the dataset. This is known as the _Bayes error_.

The model’s results are as follows:

```
Accuracy Train =  67.69 %
Accuracy Test =  50.77 %
RMSE Train =  0.0069584171
RMSE Test =  0.0054027335
Correlation In-Sample Predicted/Train =  0.599
Correlation Out-of-Sample Predicted/Test =  0.002
Model Bias =  0.76
```

**NOTE**

It’s essential to consider the temporal aspect of your time series data. If there are clear trends or patterns that span multiple data points, a larger _k_ value might be appropriate to capture those dependencies. However, if the time series exhibits rapid changes or short-term fluctuations, a smaller k value could be more suitable.

The size of your dataset can also influence the choice of _k_. If you have a small dataset, choosing a smaller value for _k_ might be preferable to avoid overfitting. Conversely, a larger dataset can tolerate a higher value for _k_.

The advantages of KNN are:

* Its non-linearity allows it to capture complex patterns in financial data, which can be advantageous for predicting returns series that may exhibit non-linear behavior.
* It can adapt to changing market conditions or patterns. As the algorithm is instance-based, it does not require retraining the model when new data becomes available. This adaptability can be beneficial in the context of financial returns, where market dynamics can change over time.
* It provides intuitive interpretations for predictions. Since the algorithm selects the _k_ nearest neighbors to make predictions, it can be easier to understand and explain compared to more complex algorithms.

The disadvantages of KNN are:

* Its performance can degrade when dealing with high-dimensional data. Financial returns series often involve multiple predictors (such as technical indicators and other correlated returns), and KNN may struggle to find meaningful neighbors in high-dimensional spaces.
* As the dataset grows in size, the computational requirements of KNN can become significant.
* It is sensitive to noisy or outlier data points since the algorithm considers all neighbors equally.

### Decision Trees Regression

_Decision trees_ are versatile and intuitive machine learning models. They are graphical representations of a series of decisions or choices based on feature values that lead to different outcomes. Decision trees are structured as a hierarchical flowchart-like structure, where each internal node represents a decision based on a feature, each branch represents an outcome of that decision, and each leaf node represents the final prediction or class label.

At the root of the decision tree, consider all the input features and choose the one that best separates the data based on a specific criterion (for example, the information gain metric discussed in [Chapter 2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch02.html#ch02)). Create a decision node associated with the selected feature. Split the data based on the possible values of the chosen feature. Repeat the above steps recursively for each subset of data created by the splits, considering the remaining features at each node. Stop the recursion when a stopping criterion is met, such as reaching a maximum depth, reaching a minimum number of samples in a node, or no further improvement in impurity or gain.

To import the decision trees regressor, use the following code:

```
from sklearn.tree import DecisionTreeRegressor
```

Fit the model:

<pre><code><strong># Fitting the model
</strong>model = DecisionTreeRegressor(random_state = 123)
model.fit(x_train, y_train)
<strong># Predicting in-sample
</strong>y_predicted_train = np.reshape(model.predict(x_train), (-1, 1))
<strong># Predicting out-of-sample
</strong>y_predicted = np.reshape(model.predict(x_test), (-1, 1))
</code></pre>

**NOTE**

The argument `random_state` is often used to initialize the randomization within algorithms that involve randomness such as initializing weights. This ensures that if you train a model multiple times with the same `random_state`, you’ll get the same results, which is important for comparing different algorithms or hyperparameters.

[Figure 7-8](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#figure-7-8) shows the evolution of the forecasting task from the last values of `y_train` to the first values of `y_test` and `y_predicted`.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0708.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 7-8. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

The model’s results are as follows:

```
Accuracy Train =  100.0 %
Accuracy Test =  47.37 %
RMSE Train =  0.0
RMSE Test =  0.007640736
Correlation In-Sample Predicted/Train =  1.0
Correlation Out-of-Sample Predicted/Test =  -0.079
Model Bias =  0.94
```

Notice how the accuracy of the training set is extremely high. This is clearly evidence of overfitting (supported by the RMSE of the training data).

The advantages of decision trees are:

* They require minimal data preprocessing, and can handle missing values.
* They can capture nonlinear relationships, interactions, and variable importance.

The disadvantages of decision trees are:

* They can be sensitive to small changes in the data and can easily overfit if not properly regularized.
* They may struggle to capture complex relationships that require deeper trees.

The next section presents another breed of machine learning algorithms. These are called _ensemble algorithms._ Decision trees can be combined using ensemble methods to create more robust and accurate models. Random forest, an algorithm seen in the next section, combines multiple decision trees to enhance the predictive ability, but especially to reduce the risk of overfitting.

### Random Forest Regression

_Random forest_ is a machine learning algorithm that harnesses the power of multiple decision trees to form a single output (prediction). It is flexible and does not require much tuning. It is also less prone to overfitting due to its ensemble learning technique. _Ensemble learning_ refers to the combination of multiple learners (models) to improve the final prediction.

With random forest, the multiple learners are different decision trees that converge towards a single prediction.

Therefore, one of the hyperparameters that can be tuned in random forest algorithms is the number of decision trees. The algorithm uses the bagging method. In the context of random forests, _bagging_ refers to the technique of _bootstrap aggregating_ that aims to improve the performance and robustness of machine learning models, such as decision trees, by reducing biases. Here’s how bagging works within the random forest algorithm:

1. _Bootstrap sampling_: Random forest employs bootstrapping, which means creating multiple subsets of the original training data by sampling with replacement. Each subset has the same size as the original dataset but may contain duplicate instances and exclude some of them. This process is performed independently for each tree in the random forest.
2. _Tree construction and feature selection_: For each bootstrap sample, a decision tree is constructed using a process called _recursive partitioning_ where data is split based on features in order to create branches that optimize the separation of the target variables. At each node of the decision tree, a random subset of features is considered for splitting. This helps introduce diversity among the trees in the forest and prevents them from relying too heavily on a single dominant feature.
3. _Ensemble prediction_: Once all the trees are constructed, predictions are made by aggregating the outputs of individual trees. For regression tasks, the predictions are averaged.

To import the random forest regressor, use the following code:

```
from sklearn.ensemble import RandomForestRegressor
```

Now, let’s look at the algorithm’s implementation:

<pre><code><strong># Fitting the model
</strong>model = RandomForestRegressor(max_depth = 20, random_state = 123)
model.fit(x_train, y_train)
<strong># Predicting in-sample
</strong>y_predicted_train = np.reshape(model.predict(x_train), (-1, 1))
<strong># Predicting out-of-sample
</strong>y_predicted = np.reshape(model.predict(x_test), (-1, 1))
</code></pre>

**NOTE**

The `max_depth` hyperparameter controls the depth of each decision tree in the random forest. A decision tree with a larger depth can capture more intricate patterns in the data, but it also becomes more prone to overfitting, which means it might perform very well on the training data but poorly on unseen data. On the other hand, a shallower tree might not capture all the details of the data but could generalize better to new, unseen data.

[Figure 7-9](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#figure-7-9) shows the evolution of the forecasting task from the last values of `y_train` to the first values of `y_test` and `y_predicted`.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0709.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 7-9. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

The model’s results are as follows:

```
Accuracy Train =  82.72 %
Accuracy Test =  50.15 %
RMSE Train =  0.0058106512
RMSE Test =  0.0053452064
Correlation In-Sample Predicted/Train =  0.809
Correlation Out-of-Sample Predicted/Test =  -0.049
Model Bias =  0.63
```

The advantages of random forest regression are:

* It generally has accurate forecasts on data due to its ensemble nature. With financial time series being highly noisy and borderline random, its results need to be optimized nevertheless.
* Robustness to noise and outliers due to its averaging nature.

The disadvantages of random forest regression are:

* It may be difficult to interpret from time to time. Since it uses an aggregating method, the true and final decision may be lost when a large number of trees is used.
* As the number of trees increases, the computational time of the algorithm takes more time to train, resulting in a slow process.

### AdaBoost Regression

Before understanding what AdaBoost is about, let’s discuss gradient boosting so that it becomes easier to comprehend the algorithm behind it. _Gradient boosting_ is a technique to build models based on the idea of improving weak learners (which means models that perform only slightly better than random).

The way of improving these weak learners is by targeting their weak spots by creating other weak learners that can handle such weak spots. This gave birth to what is known as _Adaptive Boosting_ or _AdaBoost_ for short. Hence, in layperson’s terms, _boosting_ is all about combining weak learners to form better models.

The learners in AdaBoost (which are as discussed, weak) are single-split decision trees (referred to as _stumps_). They are weighted, and more weight is put on instances that are more difficult to classify and less weight on the rest. At the same time, new learners are incorporated with the aim to be trained on the difficult parts, thus creating a more powerful model. So, the difficult instances receive greater weights until they are solved by new weak learners.

Predictions are based on votes from the weak learners. The majority rule is applied in order to maximize accuracy. Gradient boosting therefore can be summed up in three steps:

1. It builds an ensemble of weak predictive models, typically decision trees, in a sequential manner.
2. Each subsequent model is built to correct the errors or residuals of the previous models using gradient descent, which adjusts the predictions to minimize overall error.
3. The predictions from all the models are combined by taking a weighted average or sum, determined by the learning rate, to produce the final prediction.

Import the required library:

```
from sklearn.ensemble import AdaBoostRegressor
```

Fit the model:

<pre><code><strong># Fitting the model
</strong>model = AdaBoostRegressor(random_state = 123)
model.fit(x_train, y_train)
<strong># Predicting in-sample
</strong>y_predicted_train = np.reshape(model.predict(x_train), (-1, 1))
<strong># Predicting out-of-sample
</strong>y_predicted = np.reshape(model.predict(x_test), (-1, 1))
</code></pre>

[Figure 7-10](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#figure-7-10) shows the evolution of the forecasting task from the last values of `y_train` to the first values of `y_test` and `y_predicted`.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0710.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 7-10. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

The model’s results are as follows:

```
Accuracy Train =  53.27 %
Accuracy Test =  51.7 %
RMSE Train =  0.0070124217
RMSE Test =  0.0053582343
Correlation In-Sample Predicted/Train =  0.461
Correlation Out-of-Sample Predicted/Test =  0.017
Model Bias =  0.72
```

The advantages of AdaBoost are:

* It generally has good accuracy.
* It is easy to comprehend.

The disadvantages of AdaBoost are:

* It is impacted by outliers and sensitive to noise.&#x20;
* It is slow and not optimized.

### XGBoost Regression

_XGBoost_ is a fast and performant gradient boosted decision trees algorithm. The name may be complicated but the concept is not hard if you have understood gradient boosting from the previous section on AdaBoost. XGBoost stands for _extreme gradient boosting_ and was created by Tianqi Chen. Here’s how it works:

1. XGBoost starts with a simple base model, usually a decision tree.
2. It defines an objective function that measures the performance of the model.
3. Using gradient descent optimization, it iteratively improves the model’s predictions by adjusting the model based on the gradient of the objective function.
4. New decision trees are added to the ensemble to correct errors made by previous models.
5. Regularization techniques, such as learning rate and column subsampling, are employed to enhance performance and prevent fitting issues.
6. The final prediction is obtained by combining the predictions from all the models in the ensemble.

The implementation of XGBoost in Python takes more steps than the previous algorithms. The first step is to `pip install` the required module. Type the following command in the prompt[5](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#id257):

```
pip install xgboost
```

To import the XGBoost library, use the following code:

```
from xgboost import XGBRegressor
```

The implementation of the algorithm is as follows:

<pre><code><strong># Fitting the model
</strong>model = XGBRegressor(random_state = 123, n_estimators = 16, 
                     max_depth = 12)
model.fit(x_train, y_train)
<strong># Predicting in-sample
</strong>y_predicted_train = np.reshape(model.predict(x_train), (-1, 1))
<strong># Predicting out-of-sample
</strong>y_predicted = np.reshape(model.predict(x_test), (-1, 1))
</code></pre>

**NOTE**

The argument `n_estimators` is a hyperparameter that determines the number of boosting rounds or trees to be built in the ensemble. As the algorithm combines the predictions of multiple weak learners (individual decision trees) to create a strong predictive model, each boosting round (iteration) adds a new decision tree to the ensemble, and the algorithm learns from the mistakes made by previous trees. The `n_estimators` hyperparameter controls the maximum number of trees that will be added to the ensemble during the training process.

AdaBoost and XGBoost are both boosting algorithms used to enhance the predictive power of weak learners, usually decision trees. AdaBoost focuses on iteratively emphasizing misclassified samples, using exponential loss, lacks built-in regularization, and has limited parallelization. In contrast, XGBoost leverages gradient boosting, supports various loss functions, offers regularization, handles missing values, scales better through parallelization, provides comprehensive feature importance, and allows for more extensive hyperparameter tuning.

XGBoost therefore offers more advanced features. It is often preferred for its overall better performance and ability to handle complex tasks. However, the choice between the two depends on the specific problem, dataset, and computational resources available.

[Figure 7-11](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#figure-7-11) shows the evolution of the forecasting task from the last values of `y_train` to the first values of `y_test` and `y_predicted`.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0711.png" alt="" height="450" width="600"><figcaption></figcaption></figure>

**Figure 7-11. Training data followed by test data (dashed line) and the predicted data (thin line). The vertical dashed line represents the start of the test period**

The model’s results are as follows:

```
Accuracy Train =  75.77 %
Accuracy Test =  53.04 %
RMSE Train =  0.0042354698
RMSE Test =  0.0056622704
Correlation In-Sample Predicted/Train =  0.923
Correlation Out-of-Sample Predicted/Test =  0.05
Model Bias =  6.8
```

## Overfitting and Underfitting

Issues will arise in machine-based predictive analytics and this is completely normal, since _perfection_ is an impossible word in the world of data science (and finance). This section covers the most important issue when it comes to predicting data, and that is the _fitting problem_. Overfitting and underfitting are two terms that you must thoroughly understand so that you avoid their consequences when running your models.

_Overfitting_ occurs when a model performs extremely well on the training data but has bad results on the test data. It is a sign that the model has learned not only the details of the in-sample data but also the noise that occurred. Overfitting is generally associated with a high variance and low bias model, but what are those two terms?

_Bias_ refers to the difference between the expected value of the model’s predictions and the real value of the target variable. A low bias model is one that is complex enough to capture the underlying patterns in the data.

_Variance_, on the other hand, refers to the variability of the model’s predictions for different training sets. A high variance model is one that is overly complex and can capture random noise and fluctuations in the training data. This can lead to overfitting, as the model may be fitting the noise in the data.

To prevent overfitting, it’s important to strike a balance between bias and variance by selecting a model that is complex enough to capture the underlying patterns in the data, but not so complex that it captures random noise and fluctuations in the data. Regularization techniques can also be used to reduce variance and prevent overfitting.

Overfitting occurs for a number of reasons, notably:

* _Insufficient data_: If the training data is not diverse enough, or if there is not enough of it, the model may overfit to the training data.
* _Overly complex model_: If the model is too complex, it may learn the noise in the data rather than the underlying patterns.
* _Feature overload_: If the model is trained on too many features, it may learn irrelevant or noisy features that do not generalize to new data.
* _Lack of regularization_: If the model is not regularized properly, it may overfit to the training data.
* _Leakage_: Leakage occurs when information from the test set is inadvertently included in the training set. This can lead to overfitting as the model is learning from data that it will later see during testing.

A high bias model is one that is overly simplified and cannot capture the true underlying patterns in the data. This can lead to underfitting. Similarly, a low variance model is one that is not affected much by small changes in the training data, and can generalize well to new, unseen data.

Underfitting occurs for a number of reasons, notably:

* _Insufficient model complexity_: If the model used is too simple to capture the underlying patterns in the data, it may result in underfitting. For example, a linear regression model might not be able to capture the nonlinear relationship between the features and the target variable.
* _Insufficient training_: If the model is not trained for long enough, or with enough data, it may not be able to capture the underlying patterns in the data.
* _Over-regularization:_ Regularization is a technique used to prevent overfitting, but if it’s used excessively, it can lead to underfitting.
* _Poor feature selection_: If the features selected for the model are not informative or relevant, the model may underfit.

[Figure 7-12](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#figure-7-12) shows a comparison between the different fits of a model to the data. An underfit model fails to capture the real relationship from the start, thus it is bad at predicting the past values and the future ones as well. A well fit model captures the general tendency of the data. It is not an exact nor a perfect model but one that generally has satisfactory predictions across the time. An overfit model captures every detail of the past even if it’s noise or random dislocations. The danger of an overfit model is that it inhibits a false promise of the future.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098148386/files/assets/dlff_0712.png" alt="" height="212" width="600"><figcaption></figcaption></figure>

**Figure 7-12. Different fitting situations**

Therefore, when building machine learning models for time series prediction, you have to make sure not to tune the parameters to perfectly fit the past values. To reduce fitting biases, make sure to incorporate the following best practices in your back-tests:

* _Increase training data:_ Collecting more training data helps to capture a broader range of patterns and variations in the data, reducing the chances of overfitting.
* _Feature selection:_ Carefully select relevant and informative features for your model. Removing irrelevant or redundant features reduces noise and complexity in the data, making it easier for the model to generalize well to unseen examples.
* _Regularization techniques:_ Regularization methods explicitly control the complexity of the model to prevent overfitting.
* _Hyperparameter tuning:_ Optimize the hyperparameters of your model to find the best configuration. Hyperparameters control the behavior and complexity of the model.
* _Ensemble methods:_ Employ ensemble methods, such as random forests, to combine predictions from multiple models. Ensemble methods can reduce overfitting by aggregating the predictions of multiple models, smoothing out individual model biases, and improving generalization.
* _Regular model evaluation:_ Regularly evaluate your model’s performance on unseen data or a dedicated validation set. This helps monitor the model’s generalization ability and detect any signs of overfitting or degradation in performance.

## Summary

By properly understanding where machine learning algorithms come from, it becomes simpler to interpret them and understand their limitations. This chapter gave you the required knowledge (theory and practice) to build time series models using a few known machine learning algorithms in the hopes of forecasting values using past values.

What you must imperatively know is that past values are not necessarily indicative of future outcomes. Back-tests are always biased somehow since a lot of tweaking is needed to tune the results which may cause overfitting. Patterns do occur but their results are not necessarily the same. Machine learning for financial time series prediction is constantly evolving and most of the algorithms (in the raw form and with their basic inputs) are not very predictive, but with proper combination and the addition of risk management tools and filters, you may have a sustainable algorithm that adds value to your whole framework.

[1](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#id251-marker) Link to the repository: https://github.com/sofienkaabar/deep-learning-for-finance

[2](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#id252-marker) Price differences will be referred to as returns for simplicity. In general, returns can also represent a percentage return of a time series.

[3](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#id255-marker) The ordinary least squares method uses a mathematical formula to estimate the coefficients. It involves matrix algebra and calculus to solve for the coefficients that minimize the the sum of squared residuals.

[4](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#id256-marker) A class of statistical methods that do not rely on specific assumptions about the underlying probability distribution.

[5](https://learning.oreilly.com/library/view/deep-learning-for/9781098148386/ch07.html#id257-marker) The prompt is a command-line interface that can generally be accessed in the start menu. It is not the same as the area where you type the Python code that will later be executed.
