# 12. Advanced Query Techniques

### **12** **ADVANCED QUERY TECHNIQUES** <a href="#ch12" id="ch12"></a>

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/common01.jpg)

Sometimes data analysis requires advanced SQL techniques that go beyond a table join or basic SELECT query. For example, to find the story in your data, you might need to write a query that uses the results of other queries as inputs. Or you might need to reclassify numerical values into categories before counting them. Like other programming languages, SQL provides a collection of functions and options essential for solving more complex problems, and that is what we’ll explore in this chapter.

For the exercises, I’ll introduce a data set of temperatures recorded in select U.S. cities and we’ll revisit data sets you’ve created in previous chapters. The code for the exercises is available, along with all the book’s resources, at [_https://www.nostarch.com/practicalSQL/_](https://www.nostarch.com/practicalSQL/). You’ll continue to use the analysis database you’ve already built. Let’s get started.

#### Using Subqueries <a href="#lev202" id="lev202"></a>

A _subquery_ is nested inside another query. Typically, it’s used for a calculation or logical test that provides a value or set of data to be passed into the main portion of the query. Its syntax is not unusual: we just enclose the subquery in parentheses and use it where needed. For example, we can write a subquery that returns multiple rows and treat the results as a table in the FROM clause of the main query. Or we can create a _scalar subquery_ that returns a single value and use it as part of an _expression_ to filter rows via WHERE, IN, and HAVING clauses. These are the most common uses of subqueries.

You first encountered a subquery in [Chapter 9](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch09.xhtml#ch09) in the ANSI SQL standard syntax for a table UPDATE, which is shown again here. Both the data for the update and the condition that specifies which rows to update are generated by subqueries that look for values that match the columns in table and table\_b:

&#x20; UPDATE _table_\
➊ SET _column_ = (SELECT _column_\
&#x20;               FROM _table\_b_\
&#x20;               WHERE _table.column_ = _table\_b.column_)\
➋ WHERE EXISTS (SELECT _column_\
&#x20;               FROM _table\_b_\
&#x20;               WHERE _table.column_ = _table\_b.column_);

This example query has two subqueries that use the same syntax. We use the SELECT statement inside parentheses ➊ as the first subquery in the SET clause, which generates values for the update. Similarly, we use a second subquery in the WHERE EXISTS clause, again with a SELECT statement ➋ to filter the rows we want to update. Both subqueries are _correlated subqueries_ and are so named because they depend on a value or table name from the main query that surrounds them. In this case, both subqueries depend on _table_ from the main UPDATE statement. An _uncorrelated subquery_ has no reference to objects in the main query.

It’s easier to understand these concepts by working with actual data, so let’s look at some examples. We’ll revisit two data sets from earlier chapters: the Decennial 2010 Census table us\_counties\_2010 you created in [Chapter 4](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch04.xhtml#ch04) and the meat\_poultry\_egg\_inspect table in [Chapter 9](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch09.xhtml#ch09).

_**Filtering with Subqueries in a WHERE Clause**_

You know that a WHERE clause lets you filter query results based on criteria you provide, using an expression such as WHERE quantity > 1000. But this requires that you already know the value to use for comparison. What if you don’t? That’s one way a subquery comes in handy: it lets you write a query that generates one or more values to use as part of an expression in a WHERE clause.

**Generating Values for a Query Expression**

Say you wanted to write a query to show which U.S. counties are at or above the 90th percentile, or top 10 percent, for population. Rather than writing two separate queries—one to calculate the 90th percentile and the other to filter by counties—you can do both at once using a subquery in a WHERE clause, as shown in [Listing 12-1](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list1):

&#x20; SELECT geo\_name,\
&#x20;        state\_us\_abbreviation,\
&#x20;        p0010001\
&#x20; FROM us\_counties\_2010\
➊ WHERE p0010001 >= (\
&#x20;     SELECT percentile\_cont(.9) WITHIN GROUP (ORDER BY p0010001)\
&#x20;     FROM us\_counties\_2010\
&#x20;     )\
&#x20; ORDER BY p0010001 DESC;

_Listing 12-1: Using a subquery in a WHERE clause_

This query is standard in terms of what we’ve done so far except that the WHERE clause ➊, which filters by the total population column p0010001, doesn’t include a value like it normally would. Instead, after the >= comparison operators, we provide a second query in parentheses. This second query uses the percentile\_cont() function in [Chapter 5](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch05.xhtml#ch05) to generate one value: the 90th percentile cut-off point in the p0010001 column, which will then be used in the main query.

**NOTE**

_Using percentile\_cont() to filter with a subquery works only if you pass in a single input, as shown. If you pass in an array, as in_ [_Listing 5-12_](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch05.xhtml#ch05list12) _on_ [_page 68_](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch05.xhtml#page\_68)_, percentile\_cont() returns an array, and the query will fail to evaluate the >= against an array type._

If you run the subquery separately by highlighting it in pgAdmin, you should see the results of the subquery, a value of 197444.6. But you won’t see that number when you run the entire query in [Listing 12-1](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list1), because the result of that subquery is passed directly to the WHERE clause to use in filtering the results.

The entire query should return 315 rows, or about 10 percent of the 3,143 rows in us\_counties\_2010.

geo\_name              state\_us\_abbreviation    p0010001\
\------------------    ---------------------    --------\
Los Angeles County    CA                        9818605\
Cook County           IL                        5194675\
Harris County         TX                        4092459\
Maricopa County       AZ                        3817117\
San Diego County      CA                        3095313\
_--snip--_\
Elkhart County        IN                         197559\
Sangamon County       IL                         197465

The result includes all counties with a population greater than or equal to 197444.6, the value the subquery generated.

**Using a Subquery to Identify Rows to Delete**

Adding a subquery to a WHERE clause can be useful in query statements other than SELECT. For example, we can use a similar subquery in a DELETE statement to specify what to remove from a table. Imagine you have a table with 100 million rows that, because of its size, takes a long time to query. If you just want to work on a subset of the data (such as a particular state), you can make a copy of the table and delete what you don’t need from it.

[Listing 12-2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list2) shows an example of this approach. It makes a copy of the census table using the method you learned in [Chapter 9](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch09.xhtml#ch09) and then deletes everything from that backup except the 315 counties in the top 10 percent of population:

CREATE TABLE us\_counties\_2010\_top10 AS\
SELECT \* FROM us\_counties\_2010;\
\
DELETE FROM us\_counties\_2010\_top10\
WHERE p0010001 < (\
&#x20;   SELECT percentile\_cont(.9) WITHIN GROUP (ORDER BY p0010001)\
&#x20;   FROM us\_counties\_2010\_top10\
&#x20;   );

_Listing 12-2: Using a subquery in a WHERE clause with DELETE_

Run the code in [Listing 12-2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list2), and then execute SELECT count(\*) FROM us\_counties\_2010\_top10; to count the remaining rows in the table. The result should be 315 rows, which is the original 3,143 minus the 2,828 the subquery deleted.

_**Creating Derived Tables with Subqueries**_

If your subquery returns rows and columns of data, you can convert that data to a table by placing it in a FROM clause, the result of which is known as a _derived table_. A derived table behaves just like any other table, so you can query it or join it to other tables, even other derived tables. This approach is helpful when a single query can’t perform all the operations you need.

Let’s look at a simple example. In [Chapter 5](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch05.xhtml#ch05), you learned the difference between average and median values. I explained that a median can often better indicate a data set’s central value because a few very large or small values (or outliers) can skew an average. For that reason, I often recommend comparing the average and median. If they’re close, the data probably falls in a _normal distribution_ (the familiar bell curve), and the average is a good representation of the central value. If the average and median are far apart, some outliers might be having an effect or the distribution is skewed, not normal.

Finding the average and median population of U.S. counties as well as the difference between them is a two-step process. We need to calculate the average and the median, and then we need to subtract the two. We can do both operations in one fell swoop with a subquery in the FROM clause, as shown in [Listing 12-3](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list3).

&#x20; SELECT round(calcs.average, 0) AS average,\
&#x20;        calcs.median,\
&#x20;        round(calcs.average - calcs.median, 0) AS median\_average\_diff\
&#x20; FROM (\
&#x20;    ➊ SELECT avg(p0010001) AS average,\
&#x20;             percentile\_cont(.5)\
&#x20;                 WITHIN GROUP (ORDER BY p0010001)::numeric(10,1) AS median\
&#x20;      FROM us\_counties\_2010\
&#x20;      )\
➋ AS calcs;

_Listing 12-3: Subquery as a derived table in a FROM clause_

The subquery ➊ is straightforward. We use the avg() and percentile\_cont() functions to find the average and median of the census table’s p0010001 total population column and name each column with an alias. Then we name the subquery with an alias ➋ of calcs so we can reference it as a table in the main query.

Subtracting the median from the average, both of which are returned by the subquery, is done in the main query; then the main query rounds the result and labels it with the alias median\_average\_diff. Run the query, and the result should be the following:

average    median     median\_average\_diff\
\-------    -------    -------------------\
&#x20; 98233    25857.0                  72376

The difference between the median and average, 72,376, is nearly three times the size of the median. That helps show that a relatively small number of high-population counties push the average county size over 98,000, whereas the median of all counties is much less at 25,857.

_**Joining Derived Tables**_

Because derived tables behave like regular tables, you can join them. Joining derived tables lets you perform multiple preprocessing steps before arriving at the result. For example, say we wanted to determine which states have the most meat, egg, and poultry processing plants per million population; before we can calculate that rate, we need to know the number of plants in each state and the population of each state.

We start by counting producers by state using the meat\_poultry\_egg\_inspect table in [Chapter 9](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch09.xhtml#ch09). Then we can use the us\_counties\_2010 table to count population by state by summing and grouping county values. [Listing 12-4](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list4) shows how to write subqueries for both tasks and join them to calculate the overall rate.

&#x20; SELECT census.state\_us\_abbreviation AS st,\
&#x20;        census.st\_population,\
&#x20;        plants.plant\_count,\
&#x20;      ➊ round((plants.plant\_count/census.st\_population::numeric(10,1))\*1000000, 1)\
&#x20;            AS plants\_per\_million\
&#x20; FROM\
&#x20;     (\
&#x20;       ➋ SELECT st,\
&#x20;                count(\*) AS plant\_count\
&#x20;         FROM meat\_poultry\_egg\_inspect\
&#x20;         GROUP BY st\
&#x20;     )\
&#x20;     AS plants\
&#x20; JOIN\
&#x20;     (\
&#x20;       ➌ SELECT state\_us\_abbreviation,\
&#x20;                sum(p0010001) AS st\_population\
&#x20;         FROM us\_counties\_2010\
&#x20;         GROUP BY state\_us\_abbreviation\
&#x20;     )\
&#x20;     AS census\
➍ ON plants.st = census.state\_us\_abbreviation\
&#x20; ORDER BY plants\_per\_million DESC;

_Listing 12-4: Joining two derived tables_

You learned how to calculate rates in [Chapter 10](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch10.xhtml#ch10), so the math and syntax in the main query for finding plants\_per\_million ➊ should be familiar. We divide the number of plants by the population, and then multiply that quotient by 1 million. For the inputs, we use the values generated from derived tables using subqueries.

The first subquery ➋ finds the number of plants in each state using the count() aggregate function and then groups them by state. We label this subquery with the plants alias for reference in the main part of the query. The second subquery ➌ finds the total population by state by using sum() on the p0010001 total population column and then groups those by state\_us\_abbreviation. We alias this derived table as census.

Next, we join the derived tables ➍ by linking the st column in plants to the state\_us\_abbreviation column in census. We then list the results in descending order based on the calculated rates. Here’s a sample output of 51 rows showing the highest and lowest rates:

st    st\_population    plant\_count    plants\_per\_million\
\--    -------------    -----------    ------------------\
NE          1826341            110                  60.2\
IA          3046355            149                  48.9\
VT           625741             27                  43.1\
HI          1360301             47                  34.6\
ND           672591             22                  32.7\
_--snip--_\
SC          4625364             55                  11.9\
LA          4533372             49                  10.8\
AZ          6392017             37                   5.8\
DC           601723              2                   3.3\
WY           563626              1                   1.8

The results line up with what we might expect. The top states are well-known meat producers. For example, Nebraska is one of the nation’s top cattle exporters, and Iowa leads the United States in pork production. Washington, D.C., and Wyoming at the bottom of the list are among those states with the fewest plants per million.

**NOTE**

_Your results will differ slightly if you didn’t add missing state values to the meat\_poultry\_egg\_inspect table as noted in_ [_“Updating Rows Where Values Are Missing”_](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch09.xhtml#lev146) _on_ [_page 141_](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch09.xhtml#page\_141)_._

_**Generating Columns with Subqueries**_

You can also generate new columns of data with subqueries by placing a subquery in the column list after SELECT. Typically, you would use a single value from an aggregate. For example, the query in [Listing 12-5](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list5) selects the geo\_name and total population column p0010001 from us\_counties\_2010, and then adds a subquery to add the median of all counties to each row in the new column us\_median:

SELECT geo\_name,\
&#x20;      state\_us\_abbreviation AS st,\
&#x20;      p0010001 AS total\_pop,\
&#x20;      (SELECT percentile\_cont(.5) WITHIN GROUP (ORDER BY p0010001)\
&#x20;       FROM us\_counties\_2010) AS us\_median\
FROM us\_counties\_2010;

_Listing 12-5: Adding a subquery to a column list_

The first rows of the result set should look like this:

geo\_name          st    total\_pop    us\_median\
\--------------    --    ---------    ---------\
Autauga County    AL        54571        25857\
Baldwin County    AL       182265        25857\
Barbour County    AL        27457        25857\
Bibb County       AL        22915        25857\
Blount County     AL        57322        25857\
_--snip--_

On its own, that repeating us\_median value isn’t very helpful because it’s the same each time. It would be more interesting and useful to generate values that indicate how much each county’s population deviates from the median value. Let’s look at how we can use the same subquery technique to do that. [Listing 12-6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list6) builds on [Listing 12-5](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list5) by adding a subquery expression after SELECT that calculates the difference between the population and the median for each county:

&#x20; SELECT geo\_name,\
&#x20;        state\_us\_abbreviation AS st,\
&#x20;        p0010001 AS total\_pop,\
&#x20;        (SELECT percentile\_cont(.5) WITHIN GROUP (ORDER BY p0010001)\
&#x20;        FROM us\_counties\_2010) AS us\_median,\
&#x20;    ➊ p0010001 - (SELECT percentile\_cont(.5) WITHIN GROUP (ORDER BY p0010001)\
&#x20;                  FROM us\_counties\_2010) AS diff\_from\_median\
&#x20; FROM us\_counties\_2010\
➋ WHERE (p0010001 - (SELECT percentile\_cont(.5) WITHIN GROUP (ORDER BY p0010001)\
&#x20;                    FROM us\_counties\_2010))\
&#x20;        BETWEEN -1000 AND 1000;

_Listing 12-6: Using a subquery expression in a calculation_

The added subquery ➊ is part of a column definition that subtracts the subquery’s result from p0010001, the total population. It puts that new data in a column with an alias of diff\_from\_median. To make this query even more useful, we can narrow the results further to show only counties whose population falls within 1,000 of the median. This would help us identify which counties in America have close to the median county population. To do this, we repeat the subquery expression in the WHERE clause ➋ and filter results using the BETWEEN -1000 AND 1000 expression.

The outcome should reveal 71 counties with a population relatively close to the U.S. median. Here are the first five rows of the results:

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/prog\_page\_198.jpg)

Bear in mind that subqueries add to overall query execution time; therefore, if we were working with millions of rows, we could simplify [Listing 12-6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list6) by eliminating the subquery that displays the column us\_median. I’ve left it in this example for your reference.

_**Subquery Expressions**_

You can also use subqueries to filter rows by evaluating whether a condition evaluates as true or false. For this, we can use several standard ANSI SQL _subquery expressions_, which are a combination of a keyword with a subquery and are generally used in WHERE clauses to filter rows based on the existence of values in another table.

The PostgreSQL documentation at [_https://www.postgresql.org/docs/current/static/functions-subquery.html_](https://www.postgresql.org/docs/current/static/functions-subquery.html) lists available subquery expressions, but here we’ll examine the syntax for just two of them.

**Generating Values for the IN Operator**

The subquery expression IN (subquery) is like the IN comparison operator in [Chapter 2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch02.xhtml#ch02) except we use a subquery to provide the list of values to check against rather than having to manually provide one. In the following example, we use a subquery to generate id values from a retirees table, and then use that list for the IN operator in the WHERE clause. The NOT IN expression does the opposite to find employees whose id value does _not_ appear in retirees.

SELECT first\_name, last\_name\
FROM employees\
WHERE id IN (\
&#x20;   SELECT id\
&#x20;   FROM retirees);

We would expect the output to show the names of employees who have id values that match those in retirees.

**NOTE**

_The presence of NULL values in a subquery result set will cause a query with a NOT IN expression to return no rows. If your data contains NULL values, use the WHERE NOT EXISTS expression described in the next section._

**Checking Whether Values Exist**

Another subquery expression, EXISTS (_subquery_), is a true/false test. It returns a value of true if the subquery in parentheses returns at least one row. If it returns no rows, EXISTS evaluates to false. In the following example, the query returns all names from an employees table as long as the subquery finds at least one value in id in a retirees table.

SELECT first\_name, last\_name\
FROM employees\
WHERE EXISTS (\
&#x20;   SELECT id\
&#x20;   FROM retirees);

Rather than return all names from employees, we instead could mimic the behavior of IN and limit names to where the subquery after EXISTS finds at least one corresponding id value in retirees. The following is a correlated subquery because the table named in the main query is referenced in the subquery.

SELECT first\_name, last\_name\
FROM employees\
WHERE EXISTS (\
&#x20;   SELECT id\
&#x20;   FROM retirees\
&#x20;   WHERE id = employees.id);

This approach is particularly helpful if you need to join on more than one column, which you can’t do with the IN expression.

You can also use the NOT keyword with EXISTS. For example, to find employees with no corresponding record in retirees, you would run this query:

SELECT first\_name, last\_name\
FROM employees\
WHERE NOT EXISTS (\
&#x20;   SELECT id\
&#x20;   FROM retirees\
&#x20;   WHERE id = employees.id);

The technique of using NOT with EXISTS is helpful for assessing whether a data set is complete.

#### Common Table Expressions <a href="#lev211" id="lev211"></a>

Earlier in this chapter, you learned how to create derived tables by placing subqueries in a FROM clause. A second approach to creating temporary tables for querying uses the _Common Table Expression (CTE)_, a relatively recent addition to standard SQL that’s informally called a “WITH clause.” Using a CTE, you can define one or more tables up front with subqueries. Then you can query the table results as often as needed in a main query that follows.

[Listing 12-7](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list7) shows a simple CTE called large\_counties based on our census data, followed by a query of that table. The code determines how many counties in each state have 100,000 people or more. Let’s walk through the example.

➊ WITH\
&#x20;     large\_counties (geo\_name, st, p0010001)\
&#x20; AS\
&#x20;     (\
&#x20;       ➋ SELECT geo\_name, state\_us\_abbreviation, p0010001\
&#x20;         FROM us\_counties\_2010\
&#x20;         WHERE p0010001 >= 100000\
&#x20;     )\
➌ SELECT st, count(\*)\
&#x20; FROM large\_counties\
&#x20; GROUP BY st\
&#x20; ORDER BY count(\*) DESC;

_Listing 12-7: Using a simple CTE to find large counties_

The WITH ... AS block ➊ defines the CTE’s temporary table large\_counties. After WITH, we name the table and list its column names in parentheses. Unlike column definitions in a CREATE TABLE statement, we don’t need to provide data types, because the temporary table inherits those from the subquery ➋, which is enclosed in parentheses after AS. The subquery must return the same number of columns as defined in the temporary table, but the column names don’t need to match. Also, the column list is optional if you’re not renaming columns, although including the list is still a good idea for clarity even if you don’t rename columns.

The main query ➌ counts and groups the rows in large\_counties by st, and then orders by the count in descending order. The top five rows of the results should look like this:

st    count\
\--    -----\
TX       39\
CA       35\
FL       33\
PA       31\
OH       28\
_--snip--_

As you can see, Texas, California, and Florida are among the states with the highest number of counties with a population of 100,000 or more.

You could find the same results using a SELECT query instead of a CTE, as shown here:

SELECT state\_us\_abbreviation, count(\*)\
FROM us\_counties\_2010\
WHERE p0010001 >= 100000\
GROUP BY state\_us\_abbreviation\
ORDER BY count(\*) DESC;

So why use a CTE? One reason is that by using a CTE, you can pre-stage subsets of data to feed into a larger query for more complex analysis. Also, you can reuse each table defined in a CTE in multiple places in the main query, which means you don’t have to repeat the SELECT query each time. Another commonly cited advantage is that the code is more readable than if you performed the same operation with subqueries.

[Listing 12-8](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list8) uses a CTE to rewrite the join of derived tables in [Listing 12-4](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list4) (finding the states that have the most meat, egg, and poultry processing plants per million population) into a more readable format:

&#x20; WITH\
&#x20;   ➊ counties (st, population) AS\
&#x20;     (SELECT state\_us\_abbreviation, sum(population\_count\_100\_percent)\
&#x20;      FROM us\_counties\_2010\
&#x20;      GROUP BY state\_us\_abbreviation),\
\
&#x20;   ➋ plants (st, plants) AS\
&#x20;     (SELECT st, count(\*) AS plants\
&#x20;      FROM meat\_poultry\_egg\_inspect\
&#x20;      GROUP BY st)\
\
&#x20; SELECT counties.st,\
&#x20;        population,\
&#x20;        plants,\
&#x20;        round((plants/population::numeric(10,1)) \* 1000000, 1) AS per\_million\
➌ FROM counties JOIN plants\
&#x20; ON counties.st = plants.st\
&#x20; ORDER BY per\_million DESC;

_Listing 12-8: Using CTEs in a table join_

Following the WITH keyword, we define two tables using subqueries. The first subquery, counties ➊, returns the population of each state. The second, plants ➋, returns the number of plants per state. With those tables defined, we join them ➌ on the st column in each table and calculate the rate per million. The results are identical to the joined derived tables in [Listing 12-4](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list4), but [Listing 12-8](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list8) is easier to read.

As another example, you can use a CTE to simplify queries with redundant code. For example, in [Listing 12-6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list6), we used a subquery with the percentile\_cont() function in three different locations to find median county population. In [Listing 12-9](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list9), we can write that subquery just once as a CTE:

➊ WITH us\_median AS\
&#x20;     (SELECT percentile\_cont(.5)\
&#x20;      WITHIN GROUP (ORDER BY p0010001) AS us\_median\_pop\
&#x20;      FROM us\_counties\_2010)\
\
&#x20; SELECT geo\_name,\
&#x20;        state\_us\_abbreviation AS st,\
&#x20;        p0010001 AS total\_pop,\
&#x20;      ➋ us\_median\_pop,\
&#x20;      ➌ p0010001 - us\_median\_pop AS diff\_from\_median\
➍ FROM us\_counties\_2010 CROSS JOIN us\_median\
➎ WHERE (p0010001 - us\_median\_pop)\
&#x20;        BETWEEN -1000 AND 1000;

_Listing 12-9: Using CTEs to minimize redundant code_

After the WITH keyword, we define us\_median ➊ as the result of the same subquery used in [Listing 12-6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list6), which finds the median population using percentile\_cont(). Then we reference the us\_median\_pop column on its own ➋, as part of a calculated column ➌, and in a WHERE clause ➎. To make the value available to every row in the us\_counties\_2010 table during SELECT, we use the CROSS JOIN query ➍ you learned in [Chapter 6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch06.xhtml#ch06).

This query provides identical results to those in [Listing 12-6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list6), but we only had to write the subquery once to find the median. Not only does this save time, but it also lets you revise the query more easily. For example, to find counties whose population is close to the 90th percentile, you can substitute .9 for .5 as input to percentile\_cont() in just one place.

#### Cross Tabulations <a href="#lev212" id="lev212"></a>

_Cross tabulations_ provide a simple way to summarize and compare variables by displaying them in a table layout, or matrix. In a matrix, rows represent one variable, columns represent another variable, and each cell where a row and column intersects holds a value, such as a count or percentage.

You’ll often see cross tabulations, also called _pivot tables_ or _crosstabs_, used to report summaries of survey results or to compare sets of variables. A frequent example happens during every election when candidates’ votes are tallied by geography:

candidate    ward 1    ward 2    ward 3\
\---------    ------    ------    ------\
Dirk            602     1,799     2,112\
Pratt           599     1,398     1,616\
Lerxst          911       902     1,114

In this case, the candidates’ names are one variable, the wards (or city districts) are another variable, and the cells at the intersection of the two hold the vote totals for that candidate in that ward. Let’s look at how to generate cross tabulations.

_**Installing the crosstab() Function**_

Standard ANSI SQL doesn’t have a crosstab function, but PostgreSQL does as part of a _module_ you can install easily. Modules include PostgreSQL extras that aren’t part of the core application; they include functions related to security, text search, and more. You can find a list of PostgreSQL modules at [_https://www.postgresql.org/docs/current/static/contrib.html_](https://www.postgresql.org/docs/current/static/contrib.html).

PostgreSQL’s crosstab() function is part of the tablefunc module. To install tablefunc in the pgAdmin Query Tool, execute this command:

CREATE EXTENSION tablefunc;

PostgreSQL should return the message CREATE EXTENSION when it’s done installing. (If you’re working with another database management system, check the documentation to see whether it offers a similar functionality. For example, Microsoft SQL Server has the PIVOT command.)

Next, we’ll create a basic crosstab so you can learn the syntax, and then we’ll handle a more complex case.

_**Tabulating Survey Results**_

Let’s say your company needs a fun employee activity, so you coordinate an ice cream social at your three offices in the city. The trouble is, people are particular about ice cream flavors. To choose flavors people will like, you decide to conduct a survey.

The CSV file _ice\_cream\_survey.csv_ contains 200 responses to your survey. You can download this file, along with all the book’s resources, at [_https://www.nostarch.com/practicalSQL/_](https://www.nostarch.com/practicalSQL/). Each row includes a response\_id, office, and flavor. You’ll need to count how many people chose each flavor at each office and present the results in a readable way to your colleagues.

In your analysis database, use the code in [Listing 12-10](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list10) to create a table and load the data. Make sure you change the file path to the location on your computer where you saved the CSV file.

CREATE TABLE ice\_cream\_survey (\
&#x20;   response\_id integer PRIMARY KEY,\
&#x20;   office varchar(20),\
&#x20;   flavor varchar(20)\
);\
\
COPY ice\_cream\_survey\
FROM '_C:\YourDirectory\\_ice\_cream\_survey.csv'\
WITH (FORMAT CSV, HEADER);

_Listing 12-10: Creating and filling the ice\_cream\_survey table_

If you want to inspect the data, run the following to view the first five rows:

SELECT \*\
FROM ice\_cream\_survey\
LIMIT 5;

The data should look like this:

response\_id    office      flavor\
\-----------    --------    ----------\
&#x20;         1    Uptown      Chocolate\
&#x20;         2    Midtown     Chocolate\
&#x20;         3    Downtown    Strawberry\
&#x20;         4    Uptown      Chocolate\
&#x20;         5    Midtown     Chocolate

It looks like chocolate is in the lead! But let’s confirm this choice by using the code in [Listing 12-11](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list11) to generate a crosstab from the table:

&#x20; SELECT \*\
➊ FROM crosstab('SELECT ➋office,\
&#x20;                       ➌flavor,\
&#x20;                       ➍count(\*)\
&#x20;                 FROM ice\_cream\_survey\
&#x20;                 GROUP BY office, flavor\
&#x20;                 ORDER BY office',\
\
&#x20;             ➎ 'SELECT flavor\
&#x20;                 FROM ice\_cream\_survey\
&#x20;                 GROUP BY flavor\
&#x20;                 ORDER BY flavor')\
\
➏ AS (office varchar(20),\
&#x20;     chocolate bigint,\
&#x20;     strawberry bigint,\
&#x20;     vanilla bigint);

_Listing 12-11: Generating the ice cream survey crosstab_

The query begins with a SELECT \* statement that selects everything from the contents of the crosstab() function ➊. We place two subqueries inside the crosstab() function. The first subquery generates the data for the crosstab and has three required columns. The first column, office ➋, supplies the row names for the crosstab, and the second column, flavor ➌, supplies the category columns. The third column supplies the values for each cell where row and column intersect in the table. In this case, we want the intersecting cells to show a count() ➍ of each flavor selected at each office. This first subquery on its own creates a simple aggregated list.

The second subquery ➎ produces the set of category names for the columns. The crosstab() function requires that the second subquery return only one column, so here we use SELECT to retrieve the flavor column, and we use GROUP BY to return that column’s unique values.

Then we specify the names and data types of the crosstab’s output columns following the AS keyword ➏. The list must match the row and column names in the order the subqueries generate them. For example, because the second subquery that supplies the category columns orders the flavors alphabetically, the output column list does as well.

When we run the code, our data displays in a clean, readable crosstab:

office      chocolate    strawberry    vanilla\
\--------    ---------    ----------    -------\
Downtown           23            32         19\
Midtown            41                       23\
Uptown             22            17         23

It’s easy to see at a glance that the Midtown office favors chocolate but has no interest in strawberry, which is represented by a NULL value showing that strawberry received no votes. But strawberry is the top choice Downtown, and the Uptown office is more evenly split among the three flavors.

_**Tabulating City Temperature Readings**_

Let’s create another crosstab, but this time we’ll use real data. The _temperature\_readings.csv_ file, also available with all the book’s resources at [_https://www.nostarch.com/practicalSQL/_](https://www.nostarch.com/practicalSQL/), contains a year’s worth of daily temperature readings from three observation stations around the United States: Chicago, Seattle, and Waikiki, a neighborhood on the south shore of the city of Honolulu. The data come from the U.S. National Oceanic and Atmospheric Administration (NOAA) at [_https://www.ncdc.noaa.gov/cdo-web/datatools/findstation/_](https://www.ncdc.noaa.gov/cdo-web/datatools/findstation/).

Each row in the CSV file contains four values: the station name, the date, the day’s maximum temperature, and the day’s minimum temperature. All temperatures are in Fahrenheit. For each month in each city, we want to calculate the median high temperature so we can compare climates. [Listing 12-12](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list12) contains the code to create the temperature\_readings table and import the CSV file:

CREATE TABLE temperature\_readings (\
&#x20;   reading\_id bigserial,\
&#x20;   station\_name varchar(50),\
&#x20;   observation\_date date,\
&#x20;   max\_temp integer,\
&#x20;   min\_temp integer\
);\
\
COPY temperature\_readings\
&#x20;    (station\_name, observation\_date, max\_temp, min\_temp)\
FROM '_C:\YourDirectory\\_temperature\_readings.csv'\
WITH (FORMAT CSV, HEADER);

_Listing 12-12: Creating and filling a temperature\_readings table_

The table contains the four columns from the CSV file along with an added reading\_id of type bigserial that we use as a surrogate primary key. If you perform a quick count on the table, you should have 1,077 rows. Now, let’s see what cross tabulating the data does using [Listing 12-13](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list13):

SELECT \*\
FROM crosstab('SELECT\
&#x20;              ➊ station\_name,\
&#x20;              ➋ date\_part(''month'', observation\_date),\
&#x20;              ➌ percentile\_cont(.5)\
&#x20;                     WITHIN GROUP (ORDER BY max\_temp)\
&#x20;              FROM temperature\_readings\
&#x20;              GROUP BY station\_name,\
&#x20;                       date\_part(''month'', observation\_date)\
&#x20;              ORDER BY station\_name',\
\
&#x20;             'SELECT month\
&#x20;              FROM ➍generate\_series(1,12) month')\
\
AS (station varchar(50),\
&#x20;   jan numeric(3,0),\
&#x20;   feb numeric(3,0),\
&#x20;   mar numeric(3,0),\
&#x20;   apr numeric(3,0),\
&#x20;   may numeric(3,0),\
&#x20;   jun numeric(3,0),\
&#x20;   jul numeric(3,0),\
&#x20;   aug numeric(3,0),\
&#x20;   sep numeric(3,0),\
&#x20;   oct numeric(3,0),\
&#x20;   nov numeric(3,0),\
&#x20;   dec numeric(3,0)\
);

_Listing 12-13: Generating the temperature readings crosstab_

The structure of the crosstab is the same as in [Listing 12-11](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list11). The first subquery inside the crosstab() function generates the data for the crosstab, calculating the median maximum temperature for each month. It supplies the three required columns. The first column, station\_name ➊, names the rows. The second column uses the date\_part() function ➋ you learned in [Chapter 11](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch11.xhtml#ch11) to extract the month from observation\_date, which provides the crosstab columns. Then we use percentile\_cont(.5) ➌ to find the 50th percentile, or the median, of the max\_temp. We group by station name and month so we have a median max\_temp for each month at each station.

As in [Listing 12-11](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list11), the second subquery produces the set of category names for the columns. I’m using a function called generate\_series() ➍ in a manner noted in the official PostgreSQL documentation to create a list of numbers from 1 to 12 that match the month numbers date\_part() extracts from observation\_date.

Following AS, we provide the names and data types for the crosstab’s output columns. Each is a numeric type, matching the output of the percentile function. The following output is practically poetry:

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/prog\_page\_207.jpg)

We’ve transformed a raw set of daily readings into a compact table showing the median maximum temperature each month for each station. You can see at a glance that the temperature in Waikiki is consistently balmy, whereas Chicago’s median high temperatures vary from just above freezing to downright pleasant. Seattle falls between the two.

Crosstabs do take time to set up, but viewing data sets in a matrix often makes comparisons easier than viewing the same data in a vertical list. Keep in mind that the crosstab() function is CPU-intensive, so tread carefully when querying sets that have millions or billions of rows.

#### Reclassifying Values with CASE <a href="#lev216" id="lev216"></a>

The ANSI Standard SQL CASE statement is a _conditional expression_, meaning it lets you add some “if this, then . . .” logic to a query. You can use CASE in multiple ways, but for data analysis, it’s handy for reclassifying values into categories. You can create categories based on ranges in your data and classify values according to those categories.

The CASE syntax follows this pattern:

➊ CASE WHEN _condition_ THEN _result_\
&#x20;    ➋ WHEN _another\_condition_ THEN _result_\
&#x20;    ➌ ELSE _result_\
➍ END

We give the CASE keyword ➊, and then provide at least one WHEN _condition_ THEN _result_ clause, where _condition_ is any expression the database can evaluate as true or false, such as county = 'Dutchess County' or date > '1995-08-09'. If the condition is true, the CASE statement returns the _result_ and stops checking any further conditions. The result can be any valid data type. If the condition is false, the database moves on to evaluate the next condition.

To evaluate more conditions, we can add optional WHEN ... THEN clauses ➋. We can also provide an optional ELSE clause ➌ to return a result in case no condition evaluates as true. Without an ELSE clause, the statement would return a NULL when no conditions are true. The statement finishes with an END keyword ➍.

[Listing 12-14](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list14) shows how to use the CASE statement to reclassify the temperature readings data into descriptive groups (named according to my own bias against cold weather):

SELECT max\_temp,\
&#x20;      CASE WHEN max\_temp >= 90 THEN 'Hot'\
&#x20;           WHEN max\_temp BETWEEN 70 AND 89 THEN 'Warm'\
&#x20;           WHEN max\_temp BETWEEN 50 AND 69 THEN 'Pleasant'\
&#x20;           WHEN max\_temp BETWEEN 33 AND 49 THEN 'Cold'\
&#x20;           WHEN max\_temp BETWEEN 20 AND 32 THEN 'Freezing'\
&#x20;           ELSE 'Inhumane'\
&#x20;       END AS temperature\_group\
FROM temperature\_readings;

_Listing 12-14: Reclassifying temperature data with CASE_

We create five ranges for the max\_temp column in temperature\_readings, which we define using comparison operators. The CASE statement evaluates each value to find whether any of the five expressions are true. If so, the statement outputs the appropriate text. Note that the ranges account for all possible values in the column, leaving no gaps. If none of the statements is true, then the ELSE clause assigns the value to the category Inhumane. The way I’ve structured the ranges, this happens only when max\_temp is below 20 degrees. Alternatively, we could replace ELSE with a WHEN clause that looks for temperatures less than or equal to 19 degrees by using max\_temp <= 19.

Run the code; the first five rows of output should look like this:

max\_temp    temperature\_group\
\--------    -----------------\
&#x20;     31    Freezing\
&#x20;     34    Cold\
&#x20;     32    Freezing\
&#x20;     32    Freezing\
&#x20;     34    Cold\
&#x20;     _--snip--_

Now that we’ve collapsed the data set into six categories, let’s use those categories to compare climate among the three cities in the table.

#### Using CASE in a Common Table Expression <a href="#lev217" id="lev217"></a>

The operation we performed with CASE on the temperature data in the previous section is a good example of a preprocessing step you would use in a CTE. Now that we’ve grouped the temperatures in categories, let’s count the groups by city in a CTE to see how many days of the year fall into each temperature category.

[Listing 12-15](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list15) shows the code for reclassifying the daily maximum temperatures recast to generate a temps\_collapsed CTE and then use it for an analysis:

➊ WITH temps\_collapsed (station\_name, max\_temperature\_group) AS\
&#x20;     (SELECT station\_name,\
&#x20;            CASE WHEN max\_temp >= 90 THEN 'Hot'\
&#x20;                 WHEN max\_temp BETWEEN 70 AND 89 THEN 'Warm'\
&#x20;                 WHEN max\_temp BETWEEN 50 AND 69 THEN 'Pleasant'\
&#x20;                 WHEN max\_temp BETWEEN 33 AND 49 THEN 'Cold'\
&#x20;                 WHEN max\_temp BETWEEN 20 AND 32 THEN 'Freezing'\
&#x20;                 ELSE 'Inhumane'\
&#x20;             END\
&#x20;      FROM temperature\_readings)\
\
➋ SELECT station\_name, max\_temperature\_group, count(\*)\
&#x20; FROM temps\_collapsed\
&#x20; GROUP BY station\_name, max\_temperature\_group\
&#x20; ORDER BY station\_name, count(\*) DESC;

_Listing 12-15: Using CASE in a CTE_

This code reclassifies the temperatures, and then counts and groups by station name to find general climate classifications of each city. The WITH keyword defines the CTE of temps\_collapsed ➊, which has two columns: station\_name and max\_temperature\_group. We then run a SELECT query on the CTE ➋, performing straightforward count(\*) and GROUP BY operations on both columns. The results should look like this:

station\_name                      max\_temperature\_group    count\
\------------------------------    ---------------------    -----\
CHICAGO NORTHERLY ISLAND IL US    Warm                       133\
CHICAGO NORTHERLY ISLAND IL US    Cold                        92\
CHICAGO NORTHERLY ISLAND IL US    Pleasant                    91\
CHICAGO NORTHERLY ISLAND IL US    Freezing                    30\
CHICAGO NORTHERLY ISLAND IL US    Inhumane                     8\
CHICAGO NORTHERLY ISLAND IL US    Hot                          8\
SEATTLE BOEING FIELD WA US        Pleasant                   198\
SEATTLE BOEING FIELD WA US        Warm                        98\
SEATTLE BOEING FIELD WA US        Cold                        50\
SEATTLE BOEING FIELD WA US        Hot                          3\
WAIKIKI 717.2 HI US               Warm                       361\
WAIKIKI 717.2 HI US               Hot                          5

Using this classification scheme, the amazingly consistent Waikiki weather, with Warm maximum temperatures 361 days of the year, confirms its appeal as a vacation destination. From a temperature standpoint, Seattle looks good too, with nearly 300 days of high temps categorized as Pleasant or Warm (although this belies Seattle’s legendary rainfall). Chicago, with 30 days of Freezing max temps and 8 days Inhumane, probably isn’t for me.

#### Wrapping Up <a href="#lev218" id="lev218"></a>

In this chapter, you learned to make queries work harder for you. You can now add subqueries in multiple locations to provide finer control over filtering or preprocessing data before analyzing it in a main query. You also can visualize data in a matrix using cross tabulations and reclassify data into groups; both techniques give you more ways to find and tell stories using your data. Great work!

Throughout the next chapters, we’ll dive into SQL techniques that are more specific to PostgreSQL. We’ll begin by working with and searching text and strings.

**TRY IT YOURSELF**

Here are two tasks to help you become more familiar with the concepts introduced in the chapter:

1.  Revise the code in [Listing 12-15](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list15) to dig deeper into the nuances of Waikiki’s high temperatures. Limit the temps\_collapsed table to the Waikiki maximum daily temperature observations. Then use the WHEN clauses in the CASE statement to reclassify the temperatures into seven groups that would result in the following text output:

    '90 or more'\
    '88-89'\
    '86-87'\
    '84-85'\
    '82-83'\
    '80-81'\
    '79 or less'

    In which of those groups does Waikiki’s daily maximum temperature fall most often?
2. Revise the ice cream survey crosstab in [Listing 12-11](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#ch12list11) to flip the table. In other words, make flavor the rows and office the columns. Which elements of the query do you need to change? Are the counts different?
