# 8. Extracting Information By Grouping And Summarizing

### **8** **EXTRACTING INFORMATION BY GROUPING AND SUMMARIZING** <a href="#ch08" id="ch08"></a>

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/common01.jpg)

Every data set tells a story, and it’s the data analyst’s job to find out what that story is. In [Chapter 2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch02.xhtml#ch02), you learned about interviewing data using SELECT statements, which included sorting columns, finding distinct values, and filtering results. You’ve also learned the fundamentals of SQL math, data types, table design, and joining tables. With all these tools under your belt, you’re ready to summarize data using grouping and SQL functions.

Summarizing data allows us to identify useful information we wouldn’t be able to see otherwise. In this chapter, we’ll use the well-known institution of your local library as our example.

Despite changes in the way people consume information, libraries remain a vital part of communities worldwide. But the internet and advancements in library technology have changed how we use libraries. For example, ebooks and online access to digital materials now have a permanent place in libraries along with books and periodicals.

In the United States, the Institute of Museum and Library Services (IMLS) measures library activity as part of its annual Public Libraries Survey. The survey collects data from more than 9,000 library administrative entities, defined by the survey as agencies that provide library services to a particular locality. Some agencies are county library systems, and others are part of school districts. Data on each agency includes the number of branches, staff, books, hours open per year, and so on. The IMLS has been collecting data each year since 1988 and includes all public library agencies in the 50 states plus the District of Columbia and several territories, such as American Samoa. (Read more about the program at [_https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey/_](https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey/).)

For this exercise, we’ll assume the role of an analyst who just received a fresh copy of the library data set to produce a report describing trends from the data. We’ll need to create two tables, one with data from the 2014 survey and the second from the 2009 survey. Then we’ll summarize the more interesting data in each table and join the tables to see the five-year trends. During the analysis, you’ll learn SQL techniques for summarizing data using _aggregate functions_ and _grouping_.

#### Creating the Library Survey Tables <a href="#lev120" id="lev120"></a>

Let’s create the 2014 and 2009 library survey tables and import the data. We’ll use appropriate data types for each column and add constraints and an index to each table to preserve data integrity and speed up queries.

_**Creating the 2014 Library Data Table**_

We’ll start by creating the table for the 2014 library data. Using the CREATE TABLE statement, [Listing 8-1](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list1) builds pls\_fy2014\_pupld14a, a table for the fiscal year 2014 Public Library Data File from the Public Libraries Survey. The Public Library Data File summarizes data at the agency level, counting activity at all agency outlets, which include central libraries, branch libraries, and bookmobiles. The annual survey generates two additional files we won’t use: one summarizes data at the state level, and the other has data on individual outlets. For this exercise, those files are redundant, but you can read about the data they contain in the 2014 data dictionary, available from the IMLS at [_https://www.imls.gov/sites/default/files/fy2014\_pls\_data\_file\_documentation.pdf_](https://www.imls.gov/sites/default/files/fy2014\_pls\_data\_file\_documentation.pdf).

For convenience, I’ve created a naming scheme for the tables: pls refers to the survey title, fy2014 is the fiscal year the data covers, and pupld14a is the name of the particular file from the survey. For simplicity, I’ve selected just 72 of the more relevant columns from the 159 in the original survey file to fill the pls\_fy2014\_pupld14a table, excluding data like the codes that explain the source of individual responses. When a library didn’t provide data, the agency derived the data using other means, but we don’t need that information for this exercise.

Note that [Listing 8-1](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list1) is abbreviated for convenience. The full data set and code for creating and loading this table is available for download with all the book’s resources at [_https://www.nostarch.com/practicalSQL/_](https://www.nostarch.com/practicalSQL/).

&#x20; CREATE TABLE pls\_fy2014\_pupld14a (\
&#x20;     stabr varchar(2) NOT NULL,\
&#x20;   ➊ fscskey varchar(6) CONSTRAINT fscskey2014\_key PRIMARY KEY,\
&#x20;     libid varchar(20) NOT NULL,\
&#x20;     libname varchar(100) NOT NULL,\
&#x20;     obereg varchar(2) NOT NULL,\
&#x20;     rstatus integer NOT NULL,\
&#x20;     statstru varchar(2) NOT NULL,\
&#x20;     statname varchar(2) NOT NULL,\
&#x20;     stataddr varchar(2) NOT NULL,\
&#x20;     _--snip--_\
&#x20;     wifisess integer NOT NULL,\
&#x20;     yr\_sub integer NOT NULL\
&#x20; );\
➋ CREATE INDEX libname2014\_idx ON pls\_fy2014\_pupld14a (libname);\
&#x20; CREATE INDEX stabr2014\_idx ON pls\_fy2014\_pupld14a (stabr);\
&#x20; CREATE INDEX city2014\_idx ON pls\_fy2014\_pupld14a (city);\
&#x20; CREATE INDEX visits2014\_idx ON pls\_fy2014\_pupld14a (visits);\
\
➌ COPY pls\_fy2014\_pupld14a\
&#x20; FROM '_C:\YourDirectory\\_pls\_fy2014\_pupld14a.csv'\
&#x20; WITH (FORMAT CSV, HEADER);

_Listing 8-1: Creating and filling the 2014 Public Libraries Survey table_

After finding the code and data file for [Listing 8-1](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list1), connect to your analysis database in pgAdmin and run it. Remember to change _C:\YourDirectory\\_ to the path where you saved the CSV file.

Here’s what it does: first, the code makes the table via CREATE TABLE. We assign a primary key constraint to the column named fscskey ➊, a unique code the data dictionary says is assigned to each library. Because it’s unique, present in each row, and unlikely to change, it can serve as a natural primary key.

The definition for each column includes the appropriate data type and NOT NULL constraints where the columns have no missing values. If you look carefully in the data dictionary, you’ll notice that I changed the column named database in the CSV file to databases in the table. The reason is that database is a SQL reserved keyword, and it’s unwise to use keywords as identifiers because it can lead to unintended consequences in queries or other functions.

The startdat and enddat columns contain dates, but we’ve set their data type to varchar(10) in the code because in the CSV file those columns include non-date values, and our import will fail if we try to use a date data type. In [Chapter 9](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch09.xhtml#ch09), you’ll learn how to clean up cases like these. For now, those columns are fine as is.

After creating the table, we add indexes ➋ to columns we’ll use for queries. This provides faster results when we search the column for a particular library. The COPY statement ➌ imports the data from a CSV file named _pls\_fy2014\_pupld14a.csv_ using the file path you provide.

_**Creating the 2009 Library Data Table**_

Creating the table for the 2009 library data follows similar steps, as shown in [Listing 8-2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list2). Most ongoing surveys will have a handful of year-to-year changes because the makers of the survey either think of new questions or modify existing ones, so the included columns will be slightly different in this table. That’s one reason the data providers create new tables instead of adding rows to a cumulative table. For example, the 2014 file has a wifisess column, which lists the annual number of Wi-Fi sessions the library provided, but this column doesn’t exist in the 2009 data. The data dictionary for this survey year is at [_https://www.imls.gov/sites/default/files/fy2009\_pls\_data\_file\_documentation.pdf_](https://www.imls.gov/sites/default/files/fy2009\_pls\_data\_file\_documentation.pdf).

After you build this table, import the CSV file _pls\_fy2009\_pupld09a_. This file is also available to download along with all the book’s resources at [_https://www.nostarch.com/practicalSQL/_](https://www.nostarch.com/practicalSQL/). When you’ve saved the file and added the correct file path to the COPY statement, execute the code in [Listing 8-2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list2):

&#x20; CREATE TABLE pls\_fy2009\_pupld09a (\
&#x20;     stabr varchar(2) NOT NULL,\
&#x20;   ➊ fscskey varchar(6) CONSTRAINT fscskey2009\_key PRIMARY KEY,\
&#x20;     libid varchar(20) NOT NULL,\
&#x20;     libname varchar(100) NOT NULL,\
&#x20;     address varchar(35) NOT NULL,\
&#x20;     city varchar(20) NOT NULL,\
&#x20;     zip varchar(5) NOT NULL,\
&#x20;     zip4 varchar(4) NOT NULL,\
&#x20;     cnty varchar(20) NOT NULL,\
&#x20;     _--snip--_\
&#x20;     fipsst varchar(2) NOT NULL,\
&#x20;     fipsco varchar(3) NOT NULL\
&#x20; );\
➋ CREATE INDEX libname2009\_idx ON pls\_fy2009\_pupld09a (libname);\
&#x20; CREATE INDEX stabr2009\_idx ON pls\_fy2009\_pupld09a (stabr);\
&#x20; CREATE INDEX city2009\_idx ON pls\_fy2009\_pupld09a (city);\
&#x20; CREATE INDEX visits2009\_idx ON pls\_fy2009\_pupld09a (visits);\
\
&#x20; COPY pls\_fy2009\_pupld09a\
&#x20; FROM '_C:\YourDirectory\\_pls\_fy2009\_pupld09a.csv'\
&#x20; WITH (FORMAT CSV, HEADER);

_Listing 8-2: Creating and filling the 2009 Public Libraries Survey table_

We use fscskey as the primary key again ➊, and we create an index on libname and other columns ➋. Now, let’s mine the two tables of library data from 2014 and 2009 to discover their stories.

#### **Exploring the Library Data Using Aggregate Functions** <a href="#lev123" id="lev123"></a>

Aggregate functions combine values from multiple rows and return a single result based on an operation on those values. For example, you might return the average of values with the avg() function, as you learned in [Chapter 5](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch05.xhtml#ch05). That’s just one of many aggregate functions in SQL. Some are part of the SQL standard, and others are specific to PostgreSQL and other database managers. Most of the aggregate functions used in this chapter are part of standard SQL (a full list of PostgreSQL aggregates is at [_https://www.postgresql.org/docs/current/static/functions-aggregate.html_](https://www.postgresql.org/docs/current/static/functions-aggregate.html)).

In this section, we’ll work through the library data using aggregates on single and multiple columns, and then explore how you can expand their use by grouping the results they return with values from additional columns.

_**Counting Rows and Values Using count()**_

After importing a data set, a sensible first step is to make sure the table has the expected number of rows. For example, the IMLS documentation for the 2014 data says the file we imported has 9,305 rows, and the 2009 file has 9,299 rows. When we count the number of rows in those tables, the results should match those counts.

The count() aggregate function, which is part of the ANSI SQL standard, makes it easy to check the number of rows and perform other counting tasks. If we supply an asterisk as an input, such as count(\*), the asterisk acts as a wildcard, so the function returns the number of table rows regardless of whether they include NULL values. We do this in both statements in [Listing 8-3](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list3):

SELECT count(\*)\
FROM pls\_fy2014\_pupld14a;\
\
SELECT count(\*)\
FROM pls\_fy2009\_pupld09a;

_Listing 8-3: Using count() for table row counts_

Run each of the commands in [Listing 8-3](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list3) one at a time to see the table row counts. For pls\_fy2014\_pupld14a, the result should be:

count\
\-----\
&#x20;9305

And for pls\_fy2009\_pupld09a, the result should be:

count\
\-----\
&#x20;9299

Both results match the number of rows we expected.

**NOTE**

_You can also check the row count using the pgAdmin interface, but it’s clunky. Right-clicking the table name in pgAdmin’s object browser and selecting **View/Edit Data**_ ▸ _**All Rows** executes a SQL query for all rows. Then, a pop-up message in the results pane shows the row count, but it disappears after a few seconds._

Comparing the number of table rows to what the documentation says is important because it will alert us to issues such as missing rows or cases where we might have imported the wrong file.

**Counting Values Present in a Column**

To return the number of rows in a specific column that contain values, we supply the name of a column as input to the count() function rather than an asterisk. For example, if you scan the CREATE TABLE statements for both library tables closely, you’ll notice that we omitted the NOT NULL constraint for the salaries column plus several others. The reason is that not every library agency reported salaries, and some rows have NULL values.

To count the number of rows in the salaries column from 2014 that have values, run the count() function in [Listing 8-4](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list4):

SELECT count(salaries)\
FROM pls\_fy2014\_pupld14a;

_Listing 8-4: Using count() for the number of values in a column_

The result shows 5,983 rows have a value in salaries:

count\
\-----\
&#x20;5983

This number is far lower than the number of rows that exist in the table. In the 2014 data, slightly less than two-thirds of the agencies reported salaries, and you’d want to note that fact when reporting any results of calculations performed on those columns. This check is important because the extent to which values are present in a column might influence your decision on whether to proceed with analysis at all. Checking with experts on the topic and digging deeper into the data is usually a good idea, and I recommend seeking expert advice as part of a broader analysis methodology (for more on this topic, see [Chapter 18](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch18.xhtml#ch18)).

**Counting Distinct Values in a Column**

In [Chapter 2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch02.xhtml#ch02), I covered the DISTINCT keyword, which is part of the SQL standard. When added after SELECT in a query, DISTINCT returns a list of unique values. We can use it to see unique values in one column, or we can see unique combinations of values from multiple columns. Another use of DISTINCT is to add it to the count() function, which causes the function to return a count of distinct values from a column.

[Listing 8-5](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list5) shows two queries. The first counts all values in the 2014 table’s libname column. The second does the same but includes DISTINCT in front of the column name. Run them both, one at a time.

SELECT count(libname)\
FROM pls\_fy2014\_pupld14a;\
\
SELECT count(DISTINCT libname)\
FROM pls\_fy2014\_pupld14a;

_Listing 8-5: Using count() for the number of distinct values in a column_

The first query returns a row count that matches the number of rows in the table that we found using [Listing 8-3](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list3):

count\
\-----\
&#x20;9305

That’s good. We expect to have the library agency name listed in every row. But the second query returns a smaller number:

count\
\-----\
&#x20;8515

Using DISTINCT to remove duplicates reduces the number of library names to the 8,515 that are unique. My closer inspection of the data shows that 530 library agencies share their name with one or more other agencies. As one example, nine library agencies are named OXFORD PUBLIC LIBRARY in the table, each one in a city or town named Oxford in different states, including Alabama, Connecticut, Kansas, and Pennsylvania, among others. We’ll write a query to see combinations of distinct values in [“Aggregating Data Using GROUP BY”](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#lev128) on [page 120](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#page\_120).

_**Finding Maximum and Minimum Values Using max() and min()**_

Knowing the largest and smallest numbers in a column is useful for a couple of reasons. First, it helps us get a sense of the scope of the values reported for a particular variable. Second, the functions used, max() and min(), can reveal unexpected issues with the data, as you’ll see now with the libraries data.

Both max() and min() work the same way: you use a SELECT statement followed by the function with the name of a column supplied. [Listing 8-6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list6) uses max() and min() on the 2014 table with the visits column as input. The visits column records the number of annual visits to the library agency and all of its branches. Run the code, and then we’ll review the output.

SELECT max(visits), min(visits)\
FROM pls\_fy2014\_pupld14a;

_Listing 8-6: Finding the most and fewest visits using max() and min()_

The query returns the following results:

max         min\
\--------    ---\
17729020     -3

Well, that’s interesting. The maximum value of more than 17.7 million is reasonable for a large city library system, but -3 as the minimum? On the surface, that result seems like a mistake, but it turns out that the creators of the library survey are employing a problematic yet common convention in data collection: using a negative number or some artificially high value as an indicator.

In this case, the survey creators used negative numbers to indicate the following conditions:

1. A value of -1 indicates a “nonresponse” to that question.
2. A value of -3 indicates “not applicable” and is used when a library agency has closed either temporarily or permanently.

We’ll need to account for and exclude negative values as we explore the data, because summing a column and including the negative values will result in an incorrect total. We can do this using a WHERE clause to filter them. It’s a good thing we discovered this issue now rather than later after spending a lot of time on deeper analysis!

**NOTE**

_A better alternative for this negative value scenario is to use NULL in rows in the visits column where response data is absent, and then create a separate visits\_flag column to hold codes explaining why. This technique separates number values from information about them._

_**Aggregating Data Using GROUP BY**_

When you use the GROUP BY clause with aggregate functions, you can group results according to the values in one or more columns. This allows us to perform operations like sum() or count() for every state in our table or for every type of library agency.

Let’s explore how using GROUP BY with aggregates works. On its own, GROUP BY, which is also part of standard ANSI SQL, eliminates duplicate values from the results, similar to DISTINCT. [Listing 8-7](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list7) shows the GROUP BY clause in action:

&#x20; SELECT stabr\
&#x20; FROM pls\_fy2014\_pupld14a\
➊ GROUP BY stabr\
&#x20; ORDER BY stabr;

_Listing 8-7: Using GROUP BY on the stabr column_

The GROUP BY clause ➊ follows the FROM clause and includes the column name to group. In this case, we’re selecting stabr, which contains the state abbreviation, and grouping by that same column. We then use ORDER BY stabr as well so that the grouped results are in alphabetical order. This will yield a result with unique state abbreviations from the 2014 table. Here’s a portion of the results:

stabr\
\-----\
AK\
AL\
AR\
AS\
AZ\
CA\
_--snip--_\
WV\
WY

Notice that there are no duplicates in the 56 rows returned. These standard two-letter postal abbreviations include the 50 states plus Washington, D.C., and several U.S. territories, such as American Samoa and the U.S. Virgin Islands.

You’re not limited to grouping just one column. In [Listing 8-8](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list8), we use the GROUP BY clause on the 2014 data to specify the city and stabr columns for grouping:

SELECT city, stabr\
FROM pls\_fy2014\_pupld14a\
GROUP BY city, stabr\
ORDER BY city, stabr;

_Listing 8-8: Using GROUP BY on the city and stabr columns_

The results get sorted by city and then by state, and the output shows unique combinations in that order:

city          stabr\
\----------    -----\
ABBEVILLE     AL\
ABBEVILLE     LA\
ABBEVILLE     SC\
ABBOTSFORD    WI\
ABERDEEN      ID\
ABERDEEN      SD\
ABERNATHY     TX\
_--snip--_

This grouping returns 9,088 rows, 217 fewer than the total table rows. The result indicates there are multiple occasions where the file includes more than one library agency for a particular city and state combination.

**Combining GROUP BY with count()**

If we combine GROUP BY with an aggregate function, such as count(), we can pull more descriptive information from our data. For example, we know 9,305 library agencies are in the 2014 table. We can get a count of agencies by state and sort them to see which states have the most. [Listing 8-9](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list9) shows how:

➊ SELECT stabr, count(\*)\
&#x20; FROM pls\_fy2014\_pupld14a\
➋ GROUP BY stabr\
➌ ORDER BY count(\*) DESC;

_Listing 8-9: Using GROUP BY with count() on the stabr column_

Unlike in earlier examples, we’re now asking for the values in the stabr column and a count of those values. In the list of columns to query ➊, we specify stabr and the count() function with an asterisk as its input. As before, the asterisk causes count() to include NULL values. Also, when we select individual columns along with an aggregate function, we must include the columns in a GROUP BY clause ➋. If we don’t, the database will return an error telling us to do so. The reason is that you can’t group values by aggregating and have ungrouped column values in the same query.

To sort the results and have the state with the largest number of agencies at the top, we can ORDER BY the count() function ➌ in descending order using DESC.

Run the code in [Listing 8-9](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list9). The results show New York, Illinois, and Texas as the states with the greatest number of library agencies in 2014:

stabr    count\
\-----    -----\
NY         756\
IL         625\
TX         556\
IA         543\
PA         455\
MI         389\
WI         381\
MA         370\
_--snip--_

Remember that our table represents library agencies that serve a locality. Just because New York, Illinois, and Texas have the greatest number of library agencies doesn’t mean they have the greatest number of outlets where you can walk in and peruse the shelves. An agency might have one central library only, or it might have no central libraries but 23 branches spread around a county. To count outlets, each row in the table also has values in the columns centlib and branlib, which record the number of central and branch libraries, respectively. To find totals, we would use the sum() aggregate function on both columns.

**Using GROUP BY on Multiple Columns with count()**

We can glean yet more information from our data by combining GROUP BY with the count() function and multiple columns. For example, the stataddr column in both tables contains a code indicating whether the agency’s address changed in the last year. The values in stataddr are:

**00** No change from last year

**07** Moved to a new location

**15** Minor address change

[Listing 8-10](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list10) shows the code for counting the number of agencies in each state that moved, had a minor address change, or had no change using GROUP BY with stabr and stataddr and adding count():

➊ SELECT stabr, stataddr, count(\*)\
&#x20; FROM pls\_fy2014\_pupld14a\
➋ GROUP BY stabr, stataddr\
➌ ORDER BY stabr ASC, count(\*) DESC;

_Listing 8-10: Using GROUP BY with count() of the stabr and stataddr columns_

The key sections of the query are the column names and the count() function after SELECT ➊, and making sure both columns are reflected in the GROUP BY clause ➋. The effect of grouping by two columns is that count() will show the number of unique combinations of stabr and stataddr.

To make the output easier to read, let’s sort first by the state code in ascending order and then by the count in descending order ➌. Here are the results:

stabr    stataddr    count\
\-----    --------    -----\
AK       00             70\
AK       15             10\
AK       07              5\
AL       00            221\
AL       07              3\
AR       00             58\
AS       00              1\
AZ       00             91\
_--snip--_

The first few rows of the results show that code 00 (no change in address) is the most common value for each state. We’d expect that because it’s likely there are more library agencies that haven’t changed address than those that have. The result helps assure us that we’re analyzing the data in a sound way. If code 07 (moved to a new location) was the most frequent in each state, that would raise a question about whether we’ve written the query correctly or whether there’s an issue with the data.

**Revisiting sum() to Examine Library Visits**

So far, we’ve combined grouping with aggregate functions, like count(), on columns within a single table to provide results grouped by a column’s values. Now let’s expand the technique to include grouping and aggregating across joined tables using the 2014 and 2009 libraries data. Our goal is to identify trends in library visits spanning that five-year period. To do this, we need to calculate totals using the sum() aggregate function.

Before we dig into these queries, let’s address the issue of using the values -3 and -1 to indicate “not applicable” and “nonresponse.” To prevent these negative numbers with no meaning as quantities from affecting the analysis, we’ll filter them out using a WHERE clause to limit the queries to rows where values in visits are zero or greater.

Let’s start by calculating the sum of annual visits to libraries from the individual 2014 and 2009 tables. Run each SELECT statement in [Listing 8-11](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list11) separately:

SELECT sum(visits) AS visits\_2014\
FROM pls\_fy2014\_pupld14a\
WHERE visits >= 0;\
\
SELECT sum(visits) AS visits\_2009\
FROM pls\_fy2009\_pupld09a\
WHERE visits >= 0;

_Listing 8-11: Using the sum() aggregate function to total visits to libraries in 2014 and 2009_

For 2014, visits totaled approximately 1.4 billion.

visits\_2014\
\-----------\
&#x20;1425930900

For 2009, visits totaled approximately 1.6 billion. We’re onto something here, but it may not be good news. The trend seems to point downward with visits dropping about 10 percent from 2009 to 2014.

visits\_2009\
\-----------\
&#x20;1591799201

These queries sum overall visits. But from the row counts we ran earlier in the chapter, we know that each table contains a different number of library agencies: 9,305 in 2014 and 9,299 in 2009 due to agencies opening, closing, or merging. So, let’s determine how the sum of visits will differ if we limit the analysis to library agencies that exist in both tables. We can do that by joining the tables, as shown in [Listing 8-12](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list12):

➊ SELECT sum(pls14.visits) AS visits\_2014,\
&#x20;        sum(pls09.visits) AS visits\_2009\
➋ FROM pls\_fy2014\_pupld14a pls14 JOIN pls\_fy2009\_pupld09a pls09\
&#x20; ON pls14.fscskey = pls09.fscskey\
➌ WHERE pls14.visits >= 0 AND pls09.visits >= 0;

_Listing 8-12: Using the sum() aggregate function to total visits on joined 2014 and 2009 library tables_

This query pulls together a few concepts we covered in earlier chapters, including table joins. At the top, we use the sum() aggregate function ➊ to total the visits columns from the 2014 and 2009 tables. When we join the tables on the tables’ primary keys, we’re declaring table aliases ➋ as we explored in [Chapter 6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch06.xhtml#ch06). Here, we declare pls14 as the alias for the 2014 table and pls09 as the alias for the 2009 table to avoid having to write the lengthier full table names throughout the query.

Note that we use a standard JOIN, also known as an INNER JOIN. That means the query results will only include rows where the primary key values of both tables (the column fscskey) match.

Using the WHERE clause ➌, we return rows where both tables have a value of zero or greater in the visits column. As we did in [Listing 8-11](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list11), we specify that the result should include only those rows where visits are greater than or equal to 0 in both tables. This will prevent the artificial negative values from impacting the sums.

Run the query. The results should look like this:

visits\_2014    visits\_2009\
\-----------    -----------\
&#x20;1417299241     1585455205

The results are similar to what we found by querying the tables separately, although these totals are six to eight million smaller. The reason is that the query referenced only agencies with an fscskey in both tables. Still, the downward trend holds. We’ll need to dig a little deeper to get the full story.

**NOTE**

_Although we joined the tables on fscskey, it’s entirely possible that some library agencies that appear in both tables merged or split between 2009 and 2014. A call to the IMLS asking about caveats for working with this data is a good idea._

**Grouping Visit Sums by State**

Now that we know library visits dropped for the United States as a whole between 2009 and 2014, you might ask yourself, “Did every part of the country see a decrease, or did the degree of the trend vary by region?” We can answer this question by modifying our preceding query to group by the state code. Let’s also use a percent-change calculation to compare the trend by state. [Listing 8-13](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list13) contains the full code:

➊ SELECT pls14.stabr,\
&#x20;        sum(pls14.visits) AS visits\_2014,\
&#x20;        sum(pls09.visits) AS visits\_2009,\
&#x20;        round( (CAST(sum(pls14.visits) AS decimal(10,1)) - sum(pls09.visits)) /\
&#x20;                     sum(pls09.visits) \* 100, 2 ) AS pct\_change➋\
&#x20; FROM pls\_fy2014\_pupld14a pls14 JOIN pls\_fy2009\_pupld09a pls09\
&#x20; ON pls14.fscskey = pls09.fscskey\
&#x20; WHERE pls14.visits >= 0 AND pls09.visits >= 0\
➌ GROUP BY pls14.stabr\
➍ ORDER BY pct\_change DESC;

_Listing 8-13: Using GROUP BY to track percent change in library visits by state_

We follow the SELECT keyword with the stabr column ➊ from the 2014 table; that same column appears in the GROUP BY clause ➌. It doesn’t matter which table’s stabr column we use because we’re only querying agencies that appear in both tables. After SELECT, we also include the now-familiar percent-change calculation you learned in [Chapter 5](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch05.xhtml#ch05), which gets the alias pct\_change ➋ for readability. We end the query with an ORDER BY clause ➍, using the pct\_change column alias.

When you run the query, the top of the results shows 10 states or territories with an increase in visits from 2009 to 2014. The rest of the results show a decline. Oklahoma, at the bottom of the ranking, had a 35 percent drop!

stabr    visits\_2014    visits\_2009    pct\_change\
\-----    -----------    -----------    ----------\
GU            103593          60763         70.49\
DC           4230790        2944774         43.67\
LA          17242110       15591805         10.58\
MT           4582604        4386504          4.47\
AL          17113602       16933967          1.06\
AR          10762521       10660058          0.96\
KY          19256394       19113478          0.75\
CO          32978245       32782247          0.60\
SC          18178677       18105931          0.40\
SD           3899554        3890392          0.24\
MA          42011647       42237888         -0.54\
AK           3486955        3525093         -1.08\
ID           8730670        8847034         -1.32\
NH           7508751        7675823         -2.18\
WY           3666825        3756294         -2.38\
_--snip--_\
RI           5259143        6612167        -20.46\
NC          33952977       43111094        -21.24\
PR            193279         257032        -24.80\
GA          28891017       40922598        -29.40\
OK          13678542       21171452        -35.39

This useful data should lead a data analyst to investigate what’s driving the changes, particularly the largest ones. Data analysis can sometimes raise as many questions as it answers, but that’s part of the process. It’s always worth a phone call to a person with knowledge about the data to provide context for the results. Sometimes, they may have a very good explanation. Other times, an expert will say, “That doesn’t sound right.” That answer might send you back to the keeper of the data or the documentation to find out if you overlooked a code or a nuance with the data.

**Filtering an Aggregate Query Using HAVING**

We can refine our analysis by examining a subset of states and territories that share similar characteristics. With percent change in visits, it makes sense to separate large states from small states. In a small state like Rhode Island, one library closing could have a significant effect. A single closure in California might be scarcely noticed in a statewide count. To look at states with a similar volume in visits, we could sort the results by either of the visits columns, but it would be cleaner to get a smaller result set in our query.

To filter the results of aggregate functions, we need to use the HAVING clause that’s part of standard ANSI SQL. You’re already familiar with using WHERE for filtering, but aggregate functions, such as sum(), can’t be used within a WHERE clause because they operate at the row level, and aggregate functions work across rows. The HAVING clause places conditions on groups created by aggregating. The code in [Listing 8-14](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list14) modifies the query in [Listing 8-13](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list13) by inserting the HAVING clause after GROUP BY:

&#x20; SELECT pls14.stabr,\
&#x20;        sum(pls14.visits) AS visits\_2014,\
&#x20;        sum(pls09.visits) AS visits\_2009,\
&#x20;        round( (CAST(sum(pls14.visits) AS decimal(10,1)) - sum(pls09.visits)) /\
&#x20;                     sum(pls09.visits) \* 100, 2 ) AS pct\_change\
&#x20; FROM pls\_fy2014\_pupld14a pls14 JOIN pls\_fy2009\_pupld09a pls09\
&#x20; ON pls14.fscskey = pls09.fscskey\
&#x20; WHERE pls14.visits >= 0 AND pls09.visits >= 0\
&#x20; GROUP BY pls14.stabr\
➊ HAVING sum(pls14.visits) > 50000000\
&#x20; ORDER BY pct\_change DESC;

_Listing 8-14: Using a HAVING clause to filter the results of an aggregate query_

In this case, we’ve set our query results to include only rows with a sum of visits in 2014 greater than 50 million. That’s an arbitrary value I chose to show only the very largest states. Adding the HAVING clause ➊ reduces the number of rows in the output to just six. In practice, you might experiment with various values. Here are the results:

stabr    visits\_2014    visits\_2009    pct\_change\
\-----    -----------    -----------    ----------\
TX          72876601       78838400         -7.56\
CA         162787836      182181408        -10.65\
OH          82495138       92402369        -10.72\
NY         106453546      119810969        -11.15\
IL          72598213       82438755        -11.94\
FL          73165352       87730886        -16.60

Each of the six states has experienced a decline in visits, but notice that the percent-change variation isn’t as wide as in the full set of states and territories. Depending on what we learn from library experts, looking at the states with the most activity as a group might be helpful in describing trends, as would looking at other groupings. Think of a sentence or bullet point you might write that would say, “In the nation’s largest states, visits decreased between 8 percent and 17 percent between 2009 and 2014.” You could write similar sentences about medium-sized states and small states.

#### Wrapping Up <a href="#lev134" id="lev134"></a>

If this chapter has inspired you to visit your local library and check out a couple of books, ask a librarian whether their branch has seen a rise or drop in visits over the last few years. Chances are, you can guess the answer now. In this chapter, you learned how to use standard SQL techniques to summarize data in a table by grouping values and using a handful of aggregate functions. By joining data sets, you were able to identify some interesting five-year trends.

You also learned that data doesn’t always come perfectly packaged. The use of negative values in columns as an indicator rather than as an actual numeric value forced us to filter out those rows. Unfortunately, data sets offer those kinds of challenges more often than not. In the next chapter, you’ll learn techniques to clean up a data set that has a number of issues. In subsequent chapters, you’ll also discover more aggregate functions to help you find the stories in your data.

**TRY IT YOURSELF**

Put your grouping and aggregating skills to the test with these challenges:

1. We saw that library visits have declined recently in most places. But what is the pattern in the use of technology in libraries? Both the 2014 and 2009 library survey tables contain the columns gpterms (the number of internet-connected computers used by the public) and pitusr (uses of public internet computers per year). Modify the code in [Listing 8-13](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08list13) to calculate the percent change in the sum of each column over time. Watch out for negative values!
2. Both library survey tables contain a column called obereg, a two-digit Bureau of Economic Analysis Code that classifies each library agency according to a region of the United States, such as New England, Rocky Mountains, and so on. Just as we calculated the percent change in visits grouped by state, do the same to group percent changes in visits by U.S. region using obereg. Consult the survey documentation to find the meaning of each region code. For a bonus challenge, create a table with the obereg code as the primary key and the region name as text, and join it to the summary query to group by the region name rather than the code.
3. Thinking back to the types of joins you learned in [Chapter 6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch06.xhtml#ch06), which join type will show you all the rows in both tables, including those without a match? Write such a query and add an IS NULL filter in a WHERE clause to show agencies not included in one or the other table.
