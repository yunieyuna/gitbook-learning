# 7. Table Design That Works For You

### **7** **TABLE DESIGN THAT WORKS FOR YOU** <a href="#ch07" id="ch07"></a>

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/common01.jpg)

Obsession with detail can be a good thing. When you’re running out the door, it’s reassuring to know your keys will be hanging on the hook where you _always_ leave them. The same holds true for database design. When you need to excavate a nugget of information from dozens of tables and millions of rows, you’ll appreciate a dose of that same detail obsession. When you organize data into a finely tuned, smartly named set of tables, the analysis experience becomes more manageable.

In this chapter, I’ll build on [Chapter 6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch06.xhtml#ch06) by introducing _best practices_ for organizing and tuning SQL databases, whether they’re yours or ones you inherit for analysis. You already know how to create basic tables and add columns with the appropriate data type and a primary key. Now, we’ll dig deeper into table design by exploring naming rules and conventions, ways to maintain the integrity of your data, and how to add indexes to tables to speed up queries.

#### Naming Tables, Columns, and Other Identifiers <a href="#lev99" id="lev99"></a>

Developers tend to follow different SQL style patterns when naming tables, columns, and other objects (called _identifiers_). Some prefer to use _camel case_, as in berrySmoothie, where words are strung together and the first letter of each word is capitalized except for the first word. _Pascal case_, as in BerrySmoothie, follows a similar pattern but capitalizes the first letter of the first word too. With _snake case_, as in berry\_smoothie, all the words are lowercase and separated by underscores. So far, I’ve been using snake case in most of the examples, such as in the table us\_counties\_2010.

You’ll find passionate supporters of each naming convention, and some preferences are tied to individual database applications or programming languages. For example, Microsoft recommends Pascal case for its SQL Server users. Whichever convention you prefer, it’s most important to choose a style and apply it consistently. Be sure to check whether your organization has a style guide or offer to collaborate on one, and then follow it religiously.

Mixing styles or following none generally leads to a mess. It will be difficult to know which table is the most current, which is the backup, or the difference between two similarly named tables. For example, imagine connecting to a database and finding the following collection of tables:

Customers\
customers\
custBackup\
customer\_analysis\
customer\_test2\
customer\_testMarch2012\
customeranalysis

In addition, working without a consistent naming scheme makes it problematic for others to dive into your data and makes it challenging for you to pick up where you left off.

Let’s explore considerations related to naming identifiers and suggestions for best practices.

_**Using Quotes Around Identifiers to Enable Mixed Case**_

Standard ANSI SQL and many database-specific variants of SQL treat identifiers as case-insensitive unless you provide a delimiter around them—typically double quotes. Consider these two hypothetical CREATE TABLE statements for PostgreSQL:

CREATE TABLE customers (\
&#x20;   customer\_id serial,\
&#x20;   _--snip--_\
);\
\
CREATE TABLE Customers (\
&#x20;   customer\_id serial,\
&#x20;   _--snip--_\
);

When you execute these statements in order, the first CREATE TABLE command creates a table called customers. But rather than creating a second table called Customers, the second statement will throw an error: relation "customers" already exists. Because you didn’t quote the identifier, PostgreSQL treats customers and Customers as the same identifier, disregarding the case. If you want to preserve the uppercase letter and create a separate table named Customers, you must surround the identifier with quotes, like this:

CREATE TABLE "Customers" (\
&#x20;   customer\_id serial,\
&#x20;   _--snip--_\
);

Now, PostgreSQL retains the uppercase C and creates Customers as well as customers. Later, to query Customers rather than customers, you’ll have to quote its name in the SELECT statement:

SELECT \* FROM "Customers";

Of course, you wouldn’t want two tables with such similar names because of the high risk of a mix-up. This example simply illustrates the behavior of SQL in PostgreSQL.

_**Pitfalls with Quoting Identifiers**_

Using quotation marks also permits characters not otherwise allowed in an identifier, including spaces. But be aware of the negatives of using this method: for example, you might want to throw quotes around "trees planted" and use that as a column name in a reforestation database, but then all users will have to provide quotes on every subsequent reference to that column. Omit the quotes and the database will respond with an error, identifying trees and planted as separate columns missing a comma between them. A more readable and reliable option is to use snake case, as in trees\_planted.

Another downside to quoting is that it lets you use SQL _reserved keywords_, such as TABLE, WHERE, or SELECT, as an identifier. Reserved keywords are words SQL designates as having special meaning in the language. Most database developers frown on using reserved keywords as identifiers. At a minimum it’s confusing, and at worst neglecting or forgetting to quote that keyword later will result in an error because the database will interpret the word as a command instead of an identifier.

**NOTE**

_For PostgreSQL, you can find a list of keywords documented at_ [https://www.postgresql.org/docs/current/static/sql-keywords-appendix.html](https://www.postgresql.org/docs/current/static/sql-keywords-appendix.html). In addition, many code editors and database tools, including pgAdmin, will automatically highlight keywords in a particular color.

_**Guidelines for Naming Identifiers**_

Given the extra burden of quoting and its potential problems, it’s best to keep your identifier names simple, unquoted, and consistent. Here are my recommendations:

* **Use snake case.** Snake case is readable and reliable, as shown in the earlier trees\_planted example. It’s used throughout the official PostgreSQL documentation and helps make multiword names easy to understand: video\_on\_demand makes more sense at a glance than videoondemand.
* **Make names easy to understand and avoid cryptic abbreviations.** If you’re building a database related to travel, arrival\_time is a better reminder of the content as a column name than arv\_tm.
* **For table names, use plurals.** Tables hold rows, and each row represents one instance of an entity. So, use plural names for tables, such as teachers, vehicles, or departments.
* **Mind the length.** The maximum number of characters allowed for an identifier name varies by database application: the SQL standard is 128 characters, but PostgreSQL limits you to 63, and the Oracle system maximum is 30. If you’re writing code that may get reused in another database system, lean toward shorter identifier names.
* **When making copies of tables, use names that will help you manage them later.** One method is to append a YYYY\_MM\_DD date to the table name when you create it, such as tire\_sizes\_2017\_10\_20. An additional benefit is that the table names will sort in date order.

#### Controlling Column Values with Constraints <a href="#lev103" id="lev103"></a>

A column’s data type already broadly defines the kind of data it will accept: integers versus characters, for example. But SQL provides several additional constraints that let us further specify acceptable values for a column based on rules and logical tests. With constraints, we can avoid the “garbage in, garbage out” phenomenon, which is what happens when poor-quality data result in inaccurate or incomplete analysis. Constraints help maintain the quality of the data and ensure the integrity of the relationships among tables.

In [Chapter 6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch06.xhtml#ch06), you learned about _primary_ and _foreign keys_, which are two of the most commonly used constraints. Let’s review them as well as the following additional constraint types:

CHECK Evaluates whether the data falls within values we specify

UNIQUE Ensures that values in a column or group of columns are unique in each row in the table

NOT NULL Prevents NULL values in a column

We can add constraints in two ways: as a _column constraint_ or as a _table constraint_. A column constraint only applies to that column. It’s declared with the column name and data type in the CREATE TABLE statement, and it gets checked whenever a change is made to the column. With a table constraint, we can supply criteria that apply to one or more columns. We declare it in the CREATE TABLE statement immediately after defining all the table columns, and it gets checked whenever a change is made to a row in the table.

Let’s explore these constraints, their syntax, and their usefulness in table design.

_**Primary Keys: Natural vs. Surrogate**_

In [Chapter 6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch06.xhtml#ch06), you learned about giving a table a _primary key_: a column or collection of columns whose values uniquely identify each row in a table. A primary key is a constraint, and it imposes two rules on the column or columns that make up the key:

1. Each column in the key must have a unique value for each row.
2. No column in the key can have missing values.

Primary keys also provide a means of relating tables to each other and maintaining _referential integrity_, which is ensuring that rows in related tables have matching values when we expect them to. The simple primary key example in [“Relating Tables with Key Columns”](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch06.xhtml#lev82) on [page 74](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch06.xhtml#page\_74) had a single ID field that used an integer inserted by us, the user. However, as with most areas of SQL, you can implement primary keys in several ways. Often, the data will suggest the best path. But first we must assess whether to use a _natural key_ or a _surrogate key_ as the primary key.

**Using Existing Columns for Natural Keys**

You implement a natural key by using one or more of the table’s existing columns rather than creating a column and filling it with artificial values to act as keys. If a column’s values obey the primary key constraint—unique for every row and never empty—it can be used as a natural key. A value in the column can change as long as the new value doesn’t cause a violation of the constraint.

An example of a natural key is a driver’s license identification number issued by a local Department of Motor Vehicles. Within a governmental jurisdiction, such as a state in the United States, we’d reasonably expect that all drivers would receive a unique ID on their licenses. But if we were compiling a national driver’s license database, we might not be able to make that assumption; several states could independently issue the same ID code. In that case, the driver\_id column may not have unique values and cannot be used as the natural key unless it’s combined with one or more additional columns. Regardless, as you build tables, you’ll encounter many values suitable for natural keys: a part number, a serial number, or a book’s ISBN are all good examples.

**Introducing Columns for Surrogate Keys**

Instead of relying on existing data, a surrogate key typically consists of a single column that you fill with artificial values. This might be a sequential number auto-generated by the database; for example, using a serial data type (covered in [“Auto-Incrementing Integers”](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch03.xhtml#lev34) on [page 27](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch03.xhtml#page\_27)). Some developers like to use a _Universally Unique Identifier (UUID)_, which is a code comprised of 32 hexadecimal digits that identifies computer hardware or software. Here’s an example:

2911d8a8-6dea-4a46-af23-d64175a08237

**Pros and Cons of Key Types**

As with most SQL debates, there are arguments for using either type of primary key. Reasons cited for using natural keys often include the following:

* The data already exists in the table, and you don’t need to add a column to create a key.
*   Because the natural key data has meaning, it can reduce the need to join tables when searching.

    Alternatively, advocates of surrogate keys highlight these points in favor:
* Because a surrogate key doesn’t have any meaning in itself and its values are independent of the data in the table, if your data changes later, you’re not limited by the key structure.
* Natural keys tend to consume more storage than the integers typically used for surrogate keys.

A well-designed table should have one or more columns that can serve as a natural key. An example is a product table with a unique product code. But in a table of employees, it might be difficult to find any single column, or even multiple columns, that would be unique on a row-by-row basis to serve as a primary key. In that case, you can create a surrogate key, but you probably should reconsider the table structure.

**Primary Key Syntax**

In “JOIN Types” on [page 78](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch06.xhtml#page\_78), you created primary keys on the schools\_left and schools\_right tables to try out JOIN types. In fact, these were surrogate keys: in both tables, you created columns called id to use as the key and used the keywords CONSTRAINT key\_name PRIMARY KEY to declare them as primary keys. Let’s work through several more primary key examples.

In [Listing 7-1](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list1), we declare a primary key using the column constraint and table constraint methods on a table similar to the driver’s license example mentioned earlier. Because we expect the driver’s license IDs to always be unique, we’ll use that column as a natural key.

&#x20; CREATE TABLE natural\_key\_example (\
&#x20;   ➊ license\_id varchar(10) CONSTRAINT license\_key PRIMARY KEY,\
&#x20;     first\_name varchar(50),\
&#x20;     last\_name varchar(50)\
&#x20; );\
\
➋ DROP TABLE natural\_key\_example;\
\
&#x20; CREATE TABLE natural\_key\_example (\
&#x20;     license\_id varchar(10),\
&#x20;     first\_name varchar(50),\
&#x20;     last\_name varchar(50),\
&#x20;   ➌ CONSTRAINT license\_key PRIMARY KEY (license\_id)\
&#x20; );

_Listing 7-1: Declaring a single-column natural key as a primary key_

We first use the column constraint syntax to declare license\_id as the primary key by adding the CONSTRAINT keyword ➊ followed by a name for the key and then the keywords PRIMARY KEY. An advantage of using this syntax is that it’s easy to understand at a glance which column is designated as the primary key. Note that in the column constraint syntax you can omit the CONSTRAINT keyword and name for the key, and simply use PRIMARY KEY.

Next, we delete the table from the database by using the DROP TABLE command ➋ to prepare for the table constraint example.

To add the same primary key using the table constraint syntax, we declare the CONSTRAINT after listing the final column ➌ with the column we want to use as the key in parentheses. In this example, we end up with the same column for the primary key as we did with the column constraint syntax. However, you must use the table constraint syntax when you want to create a primary key using more than one column. In that case, you would list the columns in parentheses, separated by commas. We’ll explore that in a moment.

First, let’s look at how having a primary key protects you from ruining the integrity of your data. [Listing 7-2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list2) contains two INSERT statements:

INSERT INTO natural\_key\_example (license\_id, first\_name, last\_name)\
VALUES ('T229901', 'Lynn', 'Malero');\
\
INSERT INTO natural\_key\_example (license\_id, first\_name, last\_name)\
VALUES ('T229901', 'Sam', 'Tracy');

_Listing 7-2: An example of a primary key violation_

When you execute the first INSERT statement on its own, the server loads a row into the natural\_key\_example table without any issue. When you attempt to execute the second, the server replies with an error:

ERROR: duplicate key value violates unique constraint "license\_key"\
DETAIL: Key (license\_id)=(T229901) already exists.

Before adding the row, the server checked whether a license\_id of T229901 was already present in the table. Because it was, and because a primary key by definition must be unique for each row, the server rejected the operation. The rules of the fictional DMV state that no two drivers can have the same license ID, so checking for and rejecting duplicate data is one way for the database to enforce that rule.

**Creating a Composite Primary Key**

If we want to create a natural key but a single column in the table isn’t sufficient for meeting the primary key requirements for uniqueness, we may be able to create a suitable key from a combination of columns, which is called a _composite primary key_.

As a hypothetical example, let’s use a table that tracks student school attendance. The combination of a student ID column and a date column would give us unique data for each row, tracking whether or not the student was in school each day during a school year. To create a composite primary key from two or more columns, you must declare it using the table constraint syntax mentioned earlier. [Listing 7-3](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list3) creates an example table for the student attendance scenario. The school database would record each student\_id only once per school\_day, creating a unique value for the row. A present column of data type boolean indicates whether the student was there on that day.

CREATE TABLE natural\_key\_composite\_example (\
&#x20;   student\_id varchar(10),\
&#x20;   school\_day date,\
&#x20;   present boolean,\
&#x20;   CONSTRAINT student\_key PRIMARY KEY (student\_id, school\_day)\
);

_Listing 7-3: Declaring a composite primary key as a natural key_

The syntax in [Listing 7-3](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list3) follows the same table constraint format for adding a primary key for one column, but we pass two (or more) columns as arguments rather than one. Again, we can simulate a key violation by attempting to insert a row where the combination of values in the two key columns—student\_id and school\_day—is not unique to the table. Run the code in [Listing 7-4](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list4):

INSERT INTO natural\_key\_composite\_example (student\_id, school\_day, present)\
VALUES(775, '1/22/2017', 'Y');\
\
INSERT INTO natural\_key\_composite\_example (student\_id, school\_day, present)\
VALUES(775, '1/23/2017', 'Y');\
\
INSERT INTO natural\_key\_composite\_example (student\_id, school\_day, present)\
VALUES(775, '1/23/2017', 'N');

_Listing 7-4: Example of a composite primary key violation_

The first two INSERT statements execute fine because there’s no duplication of values in the combination of key columns. But the third statement causes an error because the student\_id and school\_day values it contains match a combination that already exists in the table:

ERROR: duplicate key value violates unique constraint "student\_key"\
DETAIL: Key (student\_id, school\_day)=(775, 2017-01-23) already exists.

You can create composite keys with more than two columns. The specific database you’re using imposes the limit to the number of columns you can use.

**Creating an Auto-Incrementing Surrogate Key**

If a table you’re creating has no columns suitable for a natural primary key, you may have a data integrity problem; in that case, it’s best to reconsider how you’re structuring the database. If you’re inheriting data for analysis or feel strongly about using surrogate keys, you can create a column and fill it with unique values. Earlier, I mentioned that some developers use UUIDs for this; others rely on software to generate a unique code. For our purposes, an easy way to create a surrogate primary key is with an auto-incrementing integer using one of the serial data types discussed in [“Auto-Incrementing Integers”](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch03.xhtml#lev34) on [page 27](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch03.xhtml#page\_27).

Recall the three serial types: smallserial, serial, and bigserial. They correspond to the integer types smallint, integer, and bigint in terms of the range of values they handle and the amount of disk storage they consume. For a primary key, it may be tempting to try to save disk space by using serial, which handles numbers as large as 2,147,483,647. But many a database developer has received a late-night call from a user frantic to know why their application is broken, only to discover that the database is trying to generate a number one greater than the data type’s maximum. For this reason, with PostgreSQL, it’s generally wise to use bigserial, which accepts numbers as high as 9.2 _quintillion_. You can set it and forget it, as shown in the first column defined in [Listing 7-5](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list5):

&#x20; CREATE TABLE surrogate\_key\_example (\
&#x20;   ➊ order\_number bigserial,\
&#x20;     product\_name varchar(50),\
&#x20;     order\_date date,\
&#x20;   ➋ CONSTRAINT order\_key PRIMARY KEY (order\_number)\
&#x20; );\
\
➌ INSERT INTO surrogate\_key\_example (product\_name, order\_date)\
&#x20; VALUES ('Beachball Polish', '2015-03-17'),\
&#x20;        ('Wrinkle De-Atomizer', '2017-05-22'),\
&#x20;        ('Flux Capacitor', '1985-10-26');\
&#x20; SELECT \* FROM surrogate\_key\_example;

_Listing 7-5: Declaring a bigserial column as a surrogate key_

[Listing 7-5](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list5) shows how to declare the bigserial ➊ data type for an order\_number column and set the column as the primary key ➋. When you insert data into the table ➌, you can omit the order\_number column. With order\_number set to bigserial, the database will create a new value for that column on each insert. The new value will be one greater than the largest already created for the column.

Run SELECT \* FROM surrogate\_key\_example; to see how the column fills in automatically:

order\_number    product\_name           order\_date\
\------------    -------------------    ----------\
&#x20;          1    Beachball Polish       2015-03-17\
&#x20;          2    Wrinkle De-Atomizer    2017-05-22\
&#x20;          3    Flux Capacitor         1985-10-26

The database will add one to order\_number each time a new row is inserted. But it won’t fill any gaps in the sequence created after rows are deleted.

_**Foreign Keys**_

With the _foreign key_ constraint, SQL very helpfully provides a way to ensure data in related tables doesn’t end up unrelated, or orphaned. A foreign key is one or more columns in a table that match the primary key of another table. But a foreign key also imposes a constraint: values entered must already exist in the primary key or other unique key of the table it references. If not, the value is rejected. This constraint ensures that we don’t end up with rows in one table that have no relation to rows in the other tables we can join them to.

To illustrate, [Listing 7-6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list6) shows two tables from a hypothetical database tracking motor vehicle activity:

&#x20; CREATE TABLE licenses (\
&#x20;     license\_id varchar(10),\
&#x20;     first\_name varchar(50),\
&#x20;     last\_name varchar(50),\
&#x20;   ➊ CONSTRAINT licenses\_key PRIMARY KEY (license\_id)\
&#x20; );\
\
&#x20; CREATE TABLE registrations (\
&#x20;     registration\_id varchar(10),\
&#x20;     registration\_date date,\
&#x20;   ➋ license\_id varchar(10) REFERENCES licenses (license\_id),\
&#x20;     CONSTRAINT registration\_key PRIMARY KEY (registration\_id, license\_id)\
&#x20; );\
\
➌ INSERT INTO licenses (license\_id, first\_name, last\_name)\
&#x20; VALUES ('T229901', 'Lynn', 'Malero');\
\
➍ INSERT INTO registrations (registration\_id, registration\_date, license\_id)\
&#x20; VALUES ('A203391', '3/17/2017', 'T229901');\
\
➎ INSERT INTO registrations (registration\_id, registration\_date, license\_id)\
&#x20; VALUES ('A75772', '3/17/2017', 'T000001');

_Listing 7-6: A foreign key example_

The first table, licenses, is similar to the natural\_key\_example table we made earlier and uses a driver’s unique license\_id ➊ as a natural primary key. The second table, registrations, is for tracking vehicle registrations. A single license ID might be connected to multiple vehicle registrations, because each licensed driver can register multiple vehicles over a number of years. Also, a single vehicle could be registered to multiple license holders, establishing, as you learned in [Chapter 6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch06.xhtml#ch06), a many-to-many relationship.

Here’s how that relationship is expressed via SQL: in the registrations table, we designate the column license\_id as a foreign key by adding the REFERENCES keyword, followed by the table name and column for it to reference ➋.

Now, when we insert a row into registrations, the database will test whether the value inserted into license\_id already exists in the license\_id primary key column of the licenses table. If it doesn’t, the database returns an error, which is important. If any rows in registrations didn’t correspond to a row in licenses, we’d have no way to write a query to find the person who registered the vehicle.

To see this constraint in action, create the two tables and execute the INSERT statements one at a time. The first adds a row to licenses ➌ that includes the value T229901 for the license\_id. The second adds a row to registrations ➍ where the foreign key contains the same value. So far, so good, because the value exists in both tables. But we encounter an error with the third insert, which tries to add a row to registrations ➎ with a value for license\_id that’s not in licenses:

ERROR: insert or update on table "registrations" violates foreign key\
constraint "registrations\_license\_id\_fkey"\
DETAIL: Key (license\_id)=(T000001) is not present in table "licenses".

The resulting error is good because it shows the database is keeping the data clean. But it also indicates a few practical implications: first, it affects the order we insert data. We cannot add data to a table that contains a foreign key before the other table referenced by the key has the related records, or we’ll get an error. In this example, we’d have to create a driver’s license record before inserting a related registration record (if you think about it, that’s what your local department of motor vehicles probably does).

Second, the reverse applies when we delete data. To maintain referential integrity, the foreign key constraint prevents us from deleting a row from licenses before removing any related rows in registrations, because doing so would leave an orphaned record. We would have to delete the related row in registrations first, and then delete the row in licenses. However, ANSI SQL provides a way to handle this order of operations automatically using the ON DELETE CASCADE keywords, which I’ll discuss next.

_**Automatically Deleting Related Records with CASCADE**_

To delete a row in licenses and have that action automatically delete any related rows in registrations, we can specify that behavior by adding ON DELETE CASCADE when defining the foreign key constraint.

When we create the registrations table, the keywords would go at the end of the definition of the license\_id column, like this:

CREATE TABLE registrations (\
&#x20;   registration\_id varchar(10),\
&#x20;   registration\_date date,\
&#x20;   license\_id varchar(10) REFERENCES licenses (license\_id) ON DELETE CASCADE,\
&#x20;   CONSTRAINT registration\_key PRIMARY KEY (registration\_id, license\_id)\
);

Now, deleting a row in licenses should also delete all related rows in registrations. This allows us to delete a driver’s license without first having to manually remove any registrations to it. It also maintains data integrity by ensuring deleting a license doesn’t leave orphaned rows in registrations.

_**The CHECK Constraint**_

A CHECK constraint evaluates whether data added to a column meets the expected criteria, which we specify with a logical test. If the criteria aren’t met, the database returns an error. The CHECK constraint is extremely valuable because it can prevent columns from getting loaded with nonsensical data. For example, a new employee’s birthdate probably shouldn’t be more than 120 years in the past, so you can set a cap on birthdates. Or, in most schools I know, Z isn’t a valid letter grade for a course (although my barely passing algebra grade felt like it), so we might insert constraints that only accept the values A–F.

As with primary keys, we can implement a CHECK constraint as a column constraint or a table constraint. For a column constraint, declare it in the CREATE TABLE statement after the column name and data type: CHECK (logical expression). As a table constraint, use the syntax CONSTRAINT constraint\_name CHECK (logical expression) after all columns are defined.

[Listing 7-7](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list7) shows a CHECK constraint applied to two columns in a table we might use to track the user role and salary of employees within an organization. It uses the table constraint syntax for the primary key and the CHECK constraint.

CREATE TABLE check\_constraint\_example (\
&#x20;   user\_id bigserial,\
&#x20;   user\_role varchar(50),\
&#x20;   salary integer,\
&#x20;   CONSTRAINT user\_id\_key PRIMARY KEY (user\_id),\
&#x20; ➊ CONSTRAINT check\_role\_in\_list CHECK (user\_role IN('Admin', 'Staff')),\
&#x20; ➋ CONSTRAINT check\_salary\_not\_zero CHECK (salary > 0)\
);

_Listing 7-7: Examples of CHECK constraints_

We create the table and set the user\_id column as an auto-incrementing surrogate primary key. The first CHECK ➊ tests whether values entered into the user\_role column match one of two predefined strings, Admin or Staff, by using the SQL IN operator. The second CHECK tests whether values entered in the salary column are greater than 0, because no one should be earning a negative amount ➋. Both tests are another example of a _Boolean expression_, a statement that evaluates as either true or false. If a value tested by the constraint evaluates as true, the check passes.

**NOTE**

_Developers may debate whether check logic belongs in the database, in the application in front of the database, such as a human resources system, or both. One advantage of checks in the database is that the database will maintain data integrity in the case of changes to the application, even if a new system gets built or users are given alternate ways to add data._

When values are inserted or updated, the database checks them against the constraint. If the values in either column violate the constraint—or, for that matter, if the primary key constraint is violated—the database will reject the change.

If we use the table constraint syntax, we also can combine more than one test in a single CHECK statement. Say we have a table related to student achievement. We could add the following:

CONSTRAINT grad\_check CHECK (credits >= 120 AND tuition = 'Paid')

Notice that we combine two logical tests by enclosing them in parentheses and connecting them with AND. Here, both Boolean expressions must evaluate as true for the entire check to pass. You can also test values across columns, as in the following example where we want to make sure an item’s sale price is a discount on the original, assuming we have columns for both values:

CONSTRAINT sale\_check CHECK (sale\_price < retail\_price)

Inside the parentheses, the logical expression checks that the sale price is less than the retail price.

_**The UNIQUE Constraint**_

We can also ensure that a column has a unique value in each row by using the UNIQUE constraint. If ensuring unique values sounds similar to the purpose of a primary key, it is. But UNIQUE has one important difference. In a primary key, no values can be NULL, but a UNIQUE constraint permits multiple NULL values in a column.

To show the usefulness of UNIQUE, look at the code in [Listing 7-8](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list8), which is a table for tracking contact info:

&#x20; CREATE TABLE unique\_constraint\_example (\
&#x20;     contact\_id bigserial CONSTRAINT contact\_id\_key PRIMARY KEY,\
&#x20;     first\_name varchar(50),\
&#x20;     last\_name varchar(50),\
&#x20;     email varchar(200),\
&#x20;   ➊ CONSTRAINT email\_unique UNIQUE (email)\
&#x20; );\
\
&#x20; INSERT INTO unique\_constraint\_example (first\_name, last\_name, email)\
&#x20; VALUES ('Samantha', 'Lee', 'slee@example.org');\
\
&#x20; INSERT INTO unique\_constraint\_example (first\_name, last\_name, email)\
&#x20; VALUES ('Betty', 'Diaz', 'bdiaz@example.org');\
\
&#x20; INSERT INTO unique\_constraint\_example (first\_name, last\_name, email)\
➋ VALUES ('Sasha', 'Lee', 'slee@example.org');

_Listing 7-8: A UNIQUE constraint example_

In this table, contact\_id serves as a surrogate primary key, uniquely identifying each row. But we also have an email column, the main point of contact with each person. We’d expect this column to contain only unique email addresses, but those addresses might change over time. So, we use UNIQUE ➊ to ensure that any time we add or update a contact’s email we’re not providing one that already exists. If we do try to insert an email that already exists ➋, the database will return an error:

ERROR: duplicate key value violates unique constraint "email\_unique"\
DETAIL: Key (email)=(slee@example.org) already exists.

Again, the error shows the database is working for us.

_**The NOT NULL Constraint**_

In [Chapter 6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch06.xhtml#ch06), you learned about NULL, a special value in SQL that represents a condition where no data is present in a row in a column or the value is unknown. You’ve also learned that NULL values are not allowed in a primary key, because primary keys need to uniquely identify each row in a table. But there will be other columns besides primary keys where you don’t want to allow empty values. For example, in a table listing each student in a school, it would be necessary for columns containing first and last names to be filled for each row. To require a value in a column, SQL provides the NOT NULL constraint, which simply prevents a column from accepting empty values.

[Listing 7-9](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list9) demonstrates the NOT NULL syntax:

CREATE TABLE not\_null\_example (\
&#x20;   student\_id bigserial,\
&#x20;   first\_name varchar(50) NOT NULL,\
&#x20;   last\_name varchar(50) NOT NULL,\
&#x20;   CONSTRAINT student\_id\_key PRIMARY KEY (student\_id)\
);

_Listing 7-9: A NOT NULL constraint example_

Here, we declare NOT NULL for the first\_name and last\_name columns because it’s likely we’d require those pieces of information in a table tracking student information. If we attempt an INSERT on the table and don’t include values for those columns, the database will notify us of the violation.

_**Removing Constraints or Adding Them Later**_

So far, we’ve been placing constraints on tables at the time of creation. You can also remove a constraint or later add one to an existing table using ALTER TABLE, the SQL command that makes changes to tables and columns. We’ll work with ALTER TABLE more in [Chapter 9](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch09.xhtml#ch09), but for now we’ll review the syntax for adding and removing constraints.

To remove a primary key, foreign key, or a UNIQUE constraint, you would write an ALTER TABLE statement in this format:

ALTER TABLE _table\_name_ DROP CONSTRAINT _constraint\_name_;

To drop a NOT NULL constraint, the statement operates on the column, so you must use the additional ALTER COLUMN keywords, like so:

ALTER TABLE _table\_name_ ALTER COLUMN _column\_name_ DROP NOT NULL;

Let’s use these statements to modify the not\_null\_example table you just made, as shown in [Listing 7-10](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list10):

ALTER TABLE not\_null\_example DROP CONSTRAINT student\_id\_key;\
ALTER TABLE not\_null\_example ADD CONSTRAINT student\_id\_key PRIMARY KEY (student\_id);\
ALTER TABLE not\_null\_example ALTER COLUMN first\_name DROP NOT NULL;\
ALTER TABLE not\_null\_example ALTER COLUMN first\_name SET NOT NULL;

_Listing 7-10: Dropping and adding a primary key and a NOT NULL constraint_

Execute the statements one at a time to make changes to the table. Each time, you can view the changes to the table definition in pgAdmin by clicking the table name once, and then clicking the **SQL** tab above the query window. With the first ALTER TABLE statement, we use DROP CONSTRAINT to remove the primary key named student\_id\_key. We then add the primary key back using ADD CONSTRAINT. We’d use that same syntax to add a constraint to any existing table.

**NOTE**

_You can only add a constraint to an existing table if the data in the target column obeys the limits of the constraint. For example, you can’t place a primary key constraint on a column that has duplicate or empty values._

In the third statement, ALTER COLUMN and DROP NOT NULL remove the NOT NULL constraint from the first\_name column. Finally, SET NOT NULL adds the constraint.

#### Speeding Up Queries with Indexes <a href="#lev113" id="lev113"></a>

In the same way that a book’s index helps you find information more quickly, you can speed up queries by adding an _index_ to one or more columns. The database uses the index as a shortcut rather than scanning each row to find data. That’s admittedly a simplistic picture of what, in SQL databases, is a nontrivial topic. I could write several chapters on SQL indexes and tuning databases for performance, but instead I’ll offer general guidance on using indexes and a PostgreSQL-specific example that demonstrates their benefits.

_**B-Tree: PostgreSQL’s Default Index**_

While following along in this book, you’ve already created several indexes, perhaps without knowing. Each time you add a primary key or UNIQUE constraint to a table, PostgreSQL (as well as most database systems) places an index on the column. Indexes are stored separately from the table data, but they’re accessed automatically when you run a query and are updated every time a row is added or removed from the table.

In PostgreSQL, the default index type is the _B-Tree index_. It’s created automatically on the columns designated for the primary key or a UNIQUE constraint, and it’s also the type created by default when you execute a CREATE INDEX statement. B-Tree, short for _balanced tree_, is so named because the structure organizes the data in a way that when you search for a value, it looks from the top of the tree down through branches until it locates the data you want. (Of course, the process is a lot more complicated than that. A good start on understanding more about the B-Tree is the B-Tree Wikipedia entry.) A B-Tree index is useful for data that can be ordered and searched using equality and range operators, such as <, <=, =, >=, >, and BETWEEN.

PostgreSQL incorporates additional index types, including the _Generalized Inverted Index (GIN)_ and the _Generalized Search Tree (GiST)_. Each has distinct uses, and I’ll incorporate them in later chapters on full text search and queries using geometry types.

For now, let’s see a B-Tree index speed a simple search query. For this exercise, we’ll use a large data set comprising more than 900,000 New York City street addresses, compiled by the OpenAddresses project at [_https://openaddresses.io/_](https://openaddresses.io/). The file with the data, _city\_of\_new\_york.csv_, is available for you to download along with all the resources for this book from [_https://www.nostarch.com/practicalSQL/_](https://www.nostarch.com/practicalSQL/).

After you’ve downloaded the file, use the code in [Listing 7-11](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list11) to create a new\_york\_addresses table and import the address data. You’re a pro at this by now, although the import will take longer than the tiny data sets you’ve loaded so far. The final, loaded table is 126MB, and on one of my systems, it took nearly a minute for the COPY command to complete.

CREATE TABLE new\_york\_addresses (\
&#x20;   longitude numeric(9,6),\
&#x20;   latitude numeric(9,6),\
&#x20;   street\_number varchar(10),\
&#x20;   street varchar(32),\
&#x20;   unit varchar(7),\
&#x20;   postcode varchar(5),\
&#x20;   id integer CONSTRAINT new\_york\_key PRIMARY KEY\
);\
\
COPY new\_york\_addresses\
FROM '_C:\YourDirectory\\_city\_of\_new\_york.csv'\
WITH (FORMAT CSV, HEADER);

_Listing 7-11: Importing New York City address data_

When the data loads, run a quick SELECT query to visually check that you have 940,374 rows and seven columns. A common use for this data might be to search for matches in the street column, so we’ll use that example for exploring index performance.

**Benchmarking Query Performance with EXPLAIN**

We’ll measure how well an index can improve query speed by checking the performance before and after adding one. To do this, we’ll use PostgreSQL’s EXPLAIN command, which is specific to PostgreSQL and not part of standard SQL. The EXPLAIN command provides output that lists the _query plan_ for a specific database query. This might include how the database plans to scan the table, whether or not it will use indexes, and so on. If we add the ANALYZE keyword, EXPLAIN will carry out the query and show the actual execution time, which is what we want for the current exercise.

**Recording Some Control Execution Times**

Run each of the three queries in [Listing 7-12](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list12) one at a time. We’re using typical SELECT queries with a WHERE clause but with the keywords EXPLAIN ANALYZE included at the beginning. Instead of showing the query results, these keywords tell the database to execute the query and display statistics about the query process and how long it took to execute.

EXPLAIN ANALYZE SELECT \* FROM new\_york\_addresses\
WHERE street = 'BROADWAY';\
\
EXPLAIN ANALYZE SELECT \* FROM new\_york\_addresses\
WHERE street = '52 STREET';\
\
EXPLAIN ANALYZE SELECT \* FROM new\_york\_addresses\
WHERE street = 'ZWICKY AVENUE';

_Listing 7-12: Benchmark queries for index performance_

On my system, the first query returns these stats:

➊ Seq Scan on new\_york\_addresses  (cost=0.00..20730.68 rows=3730 width=46)\
&#x20; (actual time=0.055..289.426 rows=3336 loops=1)\
&#x20;    Filter: ((street)::text = 'BROADWAY'::text)\
&#x20;    Rows Removed by Filter: 937038\
&#x20;  Planning time: 0.617 ms\
➋ Execution time: 289.838 ms

Not all the output is relevant here, so I won’t decode it all, but two lines are pertinent. The first indicates that to find any rows where street = 'BROADWAY', the database will conduct a sequential scan ➊ of the table. That’s a synonym for a full table scan: each row will be examined, and the database will remove any row that doesn’t match BROADWAY. The execution time (on my computer about 290 milliseconds) ➋ is how long this will take. Your time will depend on factors including your computer hardware.

Run each query in [Listing 7-12](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list12) and record the execution time for each.

**Adding the Index**

Now, let’s see how adding an index changes the query’s search method and how fast it works. [Listing 7-13](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list13) shows the SQL statement for creating the index with PostgreSQL:

CREATE INDEX street\_idx ON new\_york\_addresses (street);

_Listing 7-13: Creating a B-Tree index on the new\_york\_addresses table_

Notice that it’s similar to the commands for creating constraints we’ve covered in the chapter already. (Other database systems have their own variants and options for creating indexes, and there is no ANSI standard.) We give the CREATE INDEX keywords followed by a name we choose for the index, in this case street\_idx. Then ON is added, followed by the target table and column.

Execute the CREATE INDEX statement, and PostgreSQL will scan the values in the street column and build the index from them. We only need to create the index once. When the task finishes, rerun each of the three queries in [Listing 7-12](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07list12) and record the execution times reported by EXPLAIN ANALYZE. For example:

&#x20; Bitmap Heap Scan on new\_york\_addresses  (cost=65.80..5962.17 rows=2758\
&#x20; width=46) (actual time=1.792..9.816 rows=3336 loops=1)\
&#x20;   Recheck Cond: ((street)::text = 'BROADWAY'::text)\
&#x20;   Heap Blocks: exact=2157\
➊   ->  Bitmap Index Scan on street\_idx  (cost=0.00..65.11 rows=2758 width=0)\
&#x20;        (actual time=1.253..1.253 rows=3336 loops=1)\
&#x20;          Index Cond: ((street)::text = 'BROADWAY'::text)\
&#x20; Planning time: 0.163 ms\
➋ Execution time: 5.887 ms

Do you notice a change? First, instead of a sequential scan, the EXPLAIN ANALYZE statistics for each query show that the database is now using an index scan on street\_idx ➊ instead of visiting each row. Also, the query speed is now markedly faster ➋. [Table 7-1](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07tab1) shows the execution times (rounded) from my computer before and after adding the index.

**Table 7-1:** Measuring Index Performance

| **Query Filter**               | **Before Index** | **After Index** |
| ------------------------------ | ---------------- | --------------- |
| WHERE street = 'BROADWAY'      | 290 ms           | 6 ms            |
| WHERE street = '52 STREET'     | 271 ms           | 6 ms            |
| WHERE street = 'ZWICKY AVENUE' | 306 ms           | 1 ms            |

The execution times are much, much better, effectively a quarter second faster or more per query. Is a quarter second that impressive? Well, whether you’re seeking answers in data using repeated querying or creating a database system for thousands of users, the time savings adds up.

If you ever need to remove an index from a table—perhaps if you’re testing the performance of several index types—use the DROP INDEX command followed by the name of the index to remove.

_**Considerations When Using Indexes**_

You’ve seen that indexes have significant performance benefits, so does that mean you should add an index to every column in a table? Not so fast! Indexes are valuable, but they’re not always needed. In addition, they do enlarge the database and impose a maintenance cost on writing data. Here are a few tips for judging when to uses indexes:

* Consult the documentation for the database manager you’re using to learn about the kinds of indexes available and which to use on particular data types. PostgreSQL, for example, has five more index types in addition to B-Tree. One, called GiST, is particularly suited to the geometry data types I’ll discuss later in the book. Full text search, which you’ll learn in [Chapter 13](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13), also benefits from indexing.
* Consider adding indexes to any columns you’ll use in table joins. Primary keys are indexed by default in PostgreSQL, but foreign key columns in related tables are not and are a good target for indexes.
* Add indexes to columns that will frequently end up in a query WHERE clause. As you’ve seen, search performance is significantly improved via indexes.
* Use EXPLAIN ANALYZE to test performance under a variety of configurations if you’re unsure. Optimization is a process!

#### Wrapping Up <a href="#lev119" id="lev119"></a>

With the tools you’ve added to your toolbox in this chapter, you’re ready to ensure that the databases you build or inherit are best suited for your collection and exploration of data. Your queries will run faster, you can exclude unwanted values, and your database objects will have consistent organization. That’s a boon for you and for others who share your data.

This chapter concludes the first part of the book, which focused on giving you the essentials to dig into SQL databases. I’ll continue building on these foundations as we explore more complex queries and strategies for data analysis. In the next chapter, we’ll use SQL aggregate functions to assess the quality of a data set and get usable information from it.

**TRY IT YOURSELF**

Are you ready to test yourself on the concepts covered in this chapter? Consider the following two tables from a database you’re making to keep track of your vinyl LP collection. Start by reviewing these CREATE TABLE statements:

CREATE TABLE albums (\
&#x20;   album\_id bigserial,\
&#x20;   album\_catalog\_code varchar(100),\
&#x20;   album\_title text,\
&#x20;   album\_artist text,\
&#x20;   album\_release\_date date,\
&#x20;   album\_genre varchar(40),\
&#x20;   album\_description text\
);\
\
CREATE TABLE songs (\
&#x20;   song\_id bigserial,\
&#x20;   song\_title text,\
&#x20;   song\_artist text,\
&#x20;   album\_id bigint\
);

The albums table includes information specific to the overall collection of songs on the disc. The songs table catalogs each track on the album. Each song has a title and its own artist column, because each song might feature its own collection of artists.

Use the tables to answer these questions:

1. Modify these CREATE TABLE statements to include primary and foreign keys plus additional constraints on both tables. Explain why you made your choices.
2. Instead of using album\_id as a surrogate key for your primary key, are there any columns in albums that could be useful as a natural key? What would you have to know to decide?
3. To speed up queries, which columns are good candidates for indexes?
