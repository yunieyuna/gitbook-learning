# 13. Mining Text To Find Meaningful Data

### **13** **MINING TEXT TO FIND MEANINGFUL DATA** <a href="#ch13" id="ch13"></a>

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/common01.jpg)

Although it might not be obvious at first glance, you can extract data and even quantify data from text in speeches, reports, press releases, and other documents. Even though most text exists as _unstructured_ or _semi-structured data_, which is not organized in rows and columns, as in a table, you can use SQL to derive meaning from it.

One way to do this is to transform the text into _structured data_. You search for and extract elements such as dates or codes from the text, load them into a table, and analyze them. Another way to find meaning from textual data is to use advanced text analysis features, such as PostgreSQL’s full text search. Using these techniques, ordinary text can reveal facts or trends that might otherwise remain hidden.

In this chapter, you’ll learn how to use SQL to analyze and transform text. You’ll start with simple text wrangling using string formatting and pattern matching before moving on to more advanced analysis functions. We’ll use two data sets as examples: a small collection of crime reports from a sheriff’s department near Washington, D.C., and a set of State of the Union addresses delivered by former U.S. presidents.

#### Formatting Text Using String Functions <a href="#lev219" id="lev219"></a>

Whether you’re looking for data in text or simply want to change how it looks in a report, you first need to transform it into a format you can use. PostgreSQL has more than 50 built-in string functions that handle routine but necessary tasks, such as capitalizing letters, combining strings, and removing unwanted spaces. Some are part of the ANSI SQL standard, and others are specific to PostgreSQL. You’ll find a complete list of string functions at [_https://www.postgresql.org/docs/current/static/functions-string.html_](https://www.postgresql.org/docs/current/static/functions-string.html), but in this section we’ll examine several that you’ll likely use most often.

You can use these functions inside a variety of queries. Let’s try one now using a simple query that places a function after SELECT and runs it in the pgAdmin Query Tool, like this: SELECT upper('hello');. Examples of each function plus code for all the listings in this chapter are available at [_https://www.nostarch.com/practicalSQL/_](https://www.nostarch.com/practicalSQL/).

_**Case Formatting**_

The capitalization functions format the text’s case. The upper(string) function capitalizes all alphabetical characters of a string passed to it. Nonalphabet characters, such as numbers, remain unchanged. For example, upper('Neal7') returns NEAL7. The lower(string) function lowercases all alphabetical characters while keeping nonalphabet characters unchanged. For example, lower('Randy') returns randy.

The initcap(string) function capitalizes the first letter of each word. For example, initcap('at the end of the day') returns At The End Of The Day. This function is handy for formatting titles of books or movies, but because it doesn’t recognize acronyms, it’s not always the perfect solution. For example, initcap('Practical SQL') would return Practical Sql, because it doesn’t recognize SQL as an acronym.

The upper() and lower() functions are ANSI SQL standard commands, but initcap() is PostgreSQL-specific. These three functions give you enough options to rework a column of text into the case you prefer. Note that capitalization does not work with all locales or languages.

_**Character Information**_

Several functions return data about the string rather than transforming it. These functions are helpful on their own or combined with other functions. For example, the char\_length(string) function returns the number of characters in a string, including any spaces. For example, char\_length(' Pat ') returns a value of 5, because the three letters in Pat and the spaces on either end total five characters. You can also use the non-ANSI SQL function length(string) to count strings, which has a variant that lets you count the length of binary strings.

**NOTE**

_The length() function can return a different value than char\_length() when used with multibyte encodings, such as character sets covering the Chinese, Japanese, or Korean languages._

The position(substring in string) function returns the location of the substring characters in the string. For example, position(', ' in 'Tan, Bella') returns 4, because the comma and space characters (, ) specified in the substring passed as the first parameter start at the fourth index position in the main string Tan, Bella.

Both char\_length() and position() are in the ANSI SQL standard.

_**Removing Characters**_

The trim(characters from string) function removes unwanted characters from strings. To declare one or more characters to remove, add them to the function followed by the keyword from and the main string you want to change. Options to remove leading characters (at the front of the string), trailing characters (at the end of the string), or both make this function super flexible.

For example, trim('s' from 'socks') removes all s characters and returns ock. To remove only the s at the end of the string, add the trailing keyword before the character to trim: trim(trailing 's' from 'socks') returns sock.

If you don’t specify any characters to remove, trim() removes any spaces in the string by default. For example, trim(' Pat ') returns Pat without the leading or trailing spaces. To confirm the length of the trimmed string, we can nest trim() inside char\_length() like this:

SELECT char\_length(trim(' Pat '));

This query should return 3, the number of letters in Pat, which is the result of trim(' Pat ').

The ltrim(string, characters) and rtrim(string, characters) functions are PostgreSQL-specific variations of the trim() function. They remove characters from the left or right ends of a string. For example, rtrim('socks', 's') returns sock by removing only the s on the right end of the string.

_**Extracting and Replacing Characters**_

The left(string, number) and right(string, number) functions, both ANSI SQL standard, extract and return selected characters from a string. For example, to get just the 703 area code from the phone number 703-555-1212, use left('703-555-1212', 3) to specify that you want the first three characters of the string starting from the left. Likewise, right('703-555-1212', 8) returns eight characters from the right: 555-1212.

To substitute characters in a string, use the replace(string, from, to) function. To change bat to cat, for example, you would use replace('bat', 'b', 'c') to specify that you want to replace the b in bat with a c.

Now that you know basic functions for manipulating strings, let’s look at how to match more complex patterns in text and turn those patterns into data we can analyze.

#### Matching Text Patterns with Regular Expressions <a href="#lev224" id="lev224"></a>

_Regular expressions_ (or _regex_) are a type of notational language that describes text patterns. If you have a string with a noticeable pattern (say, four digits followed by a hyphen and then two more digits), you can write a regular expression that describes the pattern. You can then use the notation in a WHERE clause to filter rows by the pattern or use regular expression functions to extract and wrangle text that contains the same pattern.

Regular expressions can seem inscrutable to beginning programmers; they take practice to comprehend because they use single-character symbols that aren’t intuitive. Getting an expression to match a pattern can involve trial and error, and each programming language has subtle differences in the way it handles regular expressions. Still, learning regular expressions is a good investment because you gain superpower-like abilities to search text using many programming languages, text editors, and other applications.

In this section, I’ll provide enough regular expression basics to work through the exercises. To learn more, I recommend interactive online code testers, such as [_https://regexr.com/_](https://regexr.com/) or [_http://www.regexpal.com/_](http://www.regexpal.com/), which have notation references.

_**Regular Expression Notation**_

Matching letters and numbers using regular expression notation is straightforward because letters and numbers (and certain symbols) are literals that indicate the same characters. For example, Al matches the first two characters in Alicia.

For more complex patterns, you’ll use combinations of the regular expression elements in [Table 13-1](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13tab1).

**Table 13-1:** Regular Expression Notation Basics

| **Expression** | **Description**                                                |
| -------------- | -------------------------------------------------------------- |
| .              | A dot is a wildcard that finds any character except a newline. |
| \[FGz]         | Any character in the square brackets. Here, F, G, or z.        |
| \[a-z]         | A range of characters. Here, lowercase a to z.                 |
| \[^a-z]        | The caret negates the match. Here, not lowercase a to z.       |
| \w             | Any word character or underscore. Same as \[A-Za-z0-9\_].      |
| \d             | Any digit.                                                     |
| \s             | A space.                                                       |
|                | Tab character.                                                 |
|                | Newline character.                                             |
|                | Carriage return character.                                     |
| ^              | Match at the start of a string.                                |
| $              | Match at the end of a string.                                  |
| ?              | Get the preceding match zero or one time.                      |
| \*             | Get the preceding match zero or more times.                    |
| +              | Get the preceding match one or more times.                     |
| {_m_}          | Get the preceding match exactly _m_ times.                     |
| {_m_,_n_}      | Get the preceding match between _m_ and _n_ times.             |
| _a_\|_b_       | The pipe denotes alternation. Find either _a_ or _b_.          |
| ( )            | Create and report a capture group or set precedence.           |
| (?: )          | Negate the reporting of a capture group.                       |

Using these basic regular expressions, you can match various kinds of characters and also indicate how many times and where to match them. For example, placing characters inside square brackets (\[]) lets you match any single character or a range. So, \[FGz] matches a single F, G, or z, whereas \[A-Za-z] will match any uppercase or lowercase letter.

The backslash (\\) precedes a designator for special characters, such as a tab (), digit (\d), or newline (), which is a line ending character in text files.

There are several ways to indicate how many times to match a character. Placing a number inside curly brackets indicates you want to match it that many times. For example, \d{4} matches four digits in a row, and \d{1,4} matches a digit between one and four times.

The ?, \*, and + characters provide a useful shorthand notation for the number of matches. For example, the plus sign (+) after a character indicates to match it one or more times. So, the expression a+ would find the aa characters in the string aardvark.

Additionally, parentheses indicate a _capture group_, which you can use to specify just a portion of the matched text to display in the query results. This is useful for reporting back just a part of a matched expression. For example, if you were hunting for an HH:MM:SS time format in text and wanted to report only the hour, you could use an expression such as (\d{2}):\d{2}:\d{2}. This looks for two digits (\d{2}) of the hour followed by a colon, another two digits for the minutes and a colon, and then the two-digit seconds. By placing the first \d{2} inside parentheses, you can extract only those two digits, even though the entire expression matches the full time.

[Table 13-2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13tab2) shows examples of combining regular expressions to capture different portions of the sentence “The game starts at 7 p.m. on May 2, 2019.”

**Table 13-2:** Regular Expression Matching Examples

| **Expression**         | **What it matches**                                                              | **Result**                                |
| ---------------------- | -------------------------------------------------------------------------------- | ----------------------------------------- |
| .+                     | Any character one or more times                                                  | The game starts at 7 p.m. on May 2, 2019. |
| \d{1,2} (?:a.m.\|p.m.) | One or two digits followed by a space and _a.m._ or _p.m._ in a noncapture group | 7 p.m.                                    |
| ^\w+                   | One or more word characters at the start                                         | The                                       |
| \w+.$                  | One or more word characters followed by any character at the end                 | 2019.                                     |
| May\|June              | Either of the words _May_ or _June_                                              | May                                       |
| \d{4}                  | Four digits                                                                      | 2019                                      |
| May \d, \d{4}          | _May_ followed by a space, digit, comma, space, and four digits                  | May 2, 2019                               |

These results show the usefulness of regular expressions for selecting only the parts of the string that interest us. For example, to find the time, we use the expression \d{1,2} (?:a.m.|p.m.) to look for either one or two digits because the time could be a single or double digit followed by a space. Then we look for either a.m. or p.m.; the pipe symbol separating the terms indicates the either-or condition, and placing them in parentheses separates the logic from the rest of the expression. We need the ?: symbol to indicate that we don’t want to treat the terms inside the parentheses as a capture group, which would report a.m. or p.m. only. The ?: ensures that the full match will be returned.

You can use any of these regular expressions in pgAdmin by placing the text and regular expression inside the substring(string from pattern) function to return the matched text. For example, to find the four-digit year, use the following query:

SELECT substring('The game starts at 7 p.m. on May 2, 2019.' from '\d{4}');

This query should return 2019, because we specified that the pattern should look for any digit that is four characters long, and 2019 is the only digit in this string that matches these criteria. You can check out sample substring() queries for all the examples in [Table 13-2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13tab2) in the book’s code resources at [_https://www.nostarch.com/practicalSQL/_](https://www.nostarch.com/practicalSQL/).

The lesson here is that if you can identify a pattern in the text, you can use a combination of regular expression symbols to locate it. This technique is particularly useful when you have repeating patterns in text that you want to turn into a set of data to analyze. Let’s practice how to use regular expression functions using a real-world example.

_**Turning Text to Data with Regular Expression Functions**_

A sheriff’s department in one of the Washington, D.C., suburbs publishes daily reports that detail the date, time, location, and description of incidents the department investigates. These reports would be great to analyze, except they post the information in Microsoft Word documents saved as PDF files, which is not the friendliest format for importing into a database.

If I copy and paste incidents from the PDF into a text editor, the result is blocks of text that look something like [Listing 13-1](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list1):

➊ 4/16/17-4/17/17\
➋ 2100-0900 hrs.\
➌ 46000 Block Ashmere Sq.\
➍ Sterling\
➎ Larceny: ➏The victim reported that a\
&#x20; bicycle was stolen from their opened\
&#x20; garage door during the overnight hours.\
➐ C0170006614\
\
&#x20; 04/10/17\
&#x20; 1605 hrs.\
&#x20; 21800 block Newlin Mill Rd.\
&#x20; Middleburg\
&#x20; Larceny: A license plate was reported\
&#x20; stolen from a vehicle.\
&#x20; SO170006250

_Listing 13-1: Crime reports text_

Each block of text includes dates ➊, times ➋, a street address ➌, city or town ➍, the type of crime ➎, and a description of the incident ➏. The last piece of information is a code ➐ that might be a unique ID for the incident, although we’d have to check with the sheriff’s department to be sure. There are slight inconsistencies. For example, the first block of text has two dates (4/16/17-4/17/17) and two times (2100-0900 hrs.), meaning the exact time of the incident is unknown and likely occurred within that time span. The second block has one date and time.

If you compile these reports regularly, you can expect to find some good insights that could answer important questions: Where do crimes tend to occur? Which crime types occur most frequently? Do they happen more often on weekends or weekdays? Before you can start answering these questions, you’ll need to extract the text into table columns using regular expressions.

**Creating a Table for Crime Reports**

I’ve collected five of the crime incidents into a file named _crime\_reports.csv_ that you can download at [_https://www.nostarch.com/practicalSQL/_](https://www.nostarch.com/practicalSQL/). Download the file and save it on your computer. Then use the code in [Listing 13-2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list2) to build a table that has a column for each data element you can parse from the text using a regular expression.

CREATE TABLE crime\_reports (\
&#x20;   crime\_id bigserial PRIMARY KEY,\
&#x20;   date\_1 timestamp with time zone,\
&#x20;   date\_2 timestamp with time zone,\
&#x20;   street varchar(250),\
&#x20;   city varchar(100),\
&#x20;   crime\_type varchar(100),\
&#x20;   description text,\
&#x20;   case\_number varchar(50),\
&#x20;   original\_text text NOT NULL\
);\
\
COPY crime\_reports (original\_text)\
FROM '_C:\YourDirectory\\_crime\_reports.csv'\
WITH (FORMAT CSV, HEADER OFF, QUOTE '"');

_Listing 13-2: Creating and loading the crime\_reports table_

Run the CREATE TABLE statement in [Listing 13-2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list2), and then use COPY to load the text into the column original\_text. The rest of the columns will be NULL until we fill them.

When you run SELECT original\_text FROM crime\_reports; in pgAdmin, the results grid should display five rows and the first several words of each report. When you double-click on any cell, pgAdmin shows all the text in that row, as shown in [Figure 13-1](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13fig1).

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/f0218-01.jpg)

_Figure 13-1: Displaying additional text in the pgAdmin results grid_

Now that you’ve loaded the text you’ll be parsing, let’s explore this data using PostgreSQL regular expression functions.

**Matching Crime Report Date Patterns**

The first piece of data we want to extract from the report original\_text is the date or dates of the crime. Most of the reports have one date, although one has two. The reports also have associated times, and we’ll combine the extracted date and time into a timestamp. We’ll fill date\_1 with the first (or only) date and time in each report. In cases where a second date or second time exists, we’ll create a timestamp and add it to date\_2.

For extracting data, we’ll use the regexp\_match(string, pattern) function, which is similar to substring() with a few exceptions. One is that it returns each match as text in an array. Also, if there are no matches, it returns NULL. As you might recall from [Chapter 5](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch05.xhtml#ch05), arrays are a list of elements; in one exercise, you used an array to pass a list of values into the percentile\_cont() function to calculate quartiles. I’ll show you how to work with results that come back as an array when we parse the crime reports.

**NOTE**

_The regexp\_match() function was introduced in PostgreSQL 10 and is not available in earlier versions._

To start, let’s use regexp\_match() to find dates in each of the five incidents in crime\_reports. The general pattern to match is MM/DD/YY, although there may be one or two digits for both the month and date. Here’s a regular expression that matches the pattern:

\d{1,2}\\/\d{1,2}\\/\d{2}

In this expression, \d{1,2} indicates the month. The numbers inside the curly brackets specify that you want at least one digit and at most two digits. Next, you want to look for a forward slash (/), but because a forward slash can have special meaning in regular expressions, you must _escape_ that character by placing a backslash (\\) in front of it, like this \\/. Escaping a character in this context simply means we want to treat it as a literal rather than letting it take on special meaning. So, the combination of the backslash and forward slash (\\/) indicates you want a forward slash.

Another \d{1,2} follows for a single- or double-digit day of the month. The expression ends with a second escaped forward slash and \d{2} to indicate the two-digit year. Let’s pass the expression \d{1,2}\\/\d{1,2}\\/\d{2} to regexp\_match(), as shown in [Listing 13-3](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list3):

SELECT crime\_id,\
&#x20;      regexp\_match(original\_text, '\d{1,2}\\/\d{1,2}\\/\d{2}')\
FROM crime\_reports;

_Listing 13-3: Using regexp\_match() to find the first date_

Run that code in pgAdmin, and the results should look like this:

crime\_id    regexp\_match\
\--------    ------------\
&#x20;      1    {4/16/17}\
&#x20;      2    {4/8/17}\
&#x20;      3    {4/4/17}\
&#x20;      4    {04/10/17}\
&#x20;      5    {04/09/17}

Note that each row shows the first date listed for the incident, because regexp\_match() returns the first match it finds by default. Also note that each date is enclosed in curly brackets. That’s PostgreSQL indicating that regexp\_match() returns each result in an array, or list of elements. In “Extracting Text from the regexp\_match() Result” on [page 224](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#page\_224), I’ll show you how to access those elements from the array. You can also read more about using arrays in PostgreSQL at [_https://www.postgresql.org/docs/current/static/arrays.html_](https://www.postgresql.org/docs/current/static/arrays.html).

**Matching the Second Date When Present**

We’ve successfully extracted the first date from each report. But recall that one of the five incidents has a second date. To find and display all the dates in the text, you must use the related regexp\_matches() function and pass in an option in the form of the flag g, as shown in [Listing 13-4](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list4).

SELECT crime\_id,\
&#x20;      regexp\_matches(original\_text, '\d{1,2}\\/\d{1,2}\\/\d{2}', 'g'➊)\
FROM crime\_reports;

_Listing 13-4: Using the regexp\_matches() function with the 'g' flag_

The regexp\_matches() function, when supplied the g flag ➊, differs from regexp\_match() by returning each match the expression finds as a row in the results rather than returning just the first match.

Run the code again with this revision; you should now see two dates for the incident that has a crime\_id of 1, like this:

crime\_id    regexp\_matches\
\--------    --------------\
&#x20;      1    {4/16/17}\
&#x20;      1    {4/17/17}\
&#x20;      2    {4/8/17}\
&#x20;      3    {4/4/17}\
&#x20;      4    {04/10/17}\
&#x20;      5    {04/09/17}

Any time a crime report has a second date, we want to load it and the associated time into the date\_2 column. Although adding the g flag shows us all the dates, to extract just the second date in a report, we can use the pattern we always see when two dates exist. In [Listing 13-1](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list1), the first block of text showed the two dates separated by a hyphen, like this:

4/16/17-4/17/17

This means you can switch back to regexp\_match() and write a regular expression to look for a hyphen followed by a date, as shown in [Listing 13-5](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list5).

SELECT crime\_id,\
&#x20;      regexp\_match(original\_text, '-\d{1,2}\\/\d{1,2}\\/\d{2}')\
FROM crime\_reports;

_Listing 13-5: Using regexp\_match() to find the second date_

Although this query finds the second date in the first item (and returns a NULL for the rest), there’s an unintended consequence: it displays the hyphen along with it.

crime\_id    regexp\_match\
\--------    ------------\
&#x20;      1    {-4/17/17}\
&#x20;      2    \
&#x20;      3    \
&#x20;      4\
&#x20;      5

You don’t want to include the hyphen, because it’s an invalid format for the timestamp data type. Fortunately, you can specify the exact part of the regular expression you want to return by placing parentheses around it to create a capture group, like this:

\-(\d{1,2}/\d{1,2}/\d{1,2})

This notation returns only the part of the regular expression you want. Run the modified query in [Listing 13-6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list6) to report only the data in parentheses.

SELECT crime\_id,\
&#x20;      regexp\_match(original\_text, '-(\d{1,2}\\/\d{1,2}\\/\d{1,2})')\
FROM crime\_reports;

_Listing 13-6: Using a capture group to return only the date_

The query in [Listing 13-6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list6) should return just the second date without the leading hyphen, as shown here:

crime\_id    regexp\_match\
\--------    ------------\
&#x20;      1    {4/17/17}\
&#x20;      2\
&#x20;      3\
&#x20;      4\
&#x20;      5

The process you’ve just completed is typical. You start with text to analyze, and then write and refine the regular expression until it finds the data you want. So far, we’ve created regular expressions to match the first date and a second date, if it exists. Now, let’s use regular expressions to extract additional data elements.

**Matching Additional Crime Report Elements**

In this section, we’ll capture times, addresses, crime type, description, and case number from the crime reports. Here are the expressions for capturing this information:

**First hour \\/\d{2}\n(\d{4})**

The first hour, which is the hour the crime was committed or the start of the time range, always follows the date in each crime report, like this:

4/16/17-4/17/17\
2100-0900 hrs.

To find the first hour, we start with an escaped forward slash and \d{2}, which represents the two-digit year preceding the first date (17). The  character indicates the newline because the hour always starts on a new line, and \d{4} represents the four-digit hour (2100). Because we just want to return the four digits, we put \d{4} inside parentheses as a capture group.

**Second hour \\/\d{2}\n\d{4}-(\d{4})**

If the second hour exists, it will follow a hyphen, so we add a hyphen and another \d{4} to the expression we just created for the first hour. Again, the second \d{4} goes inside a capture group, because 0900 is the only hour we want to return.

**Street hrs.\n(\d+ .+(?:Sq.|Plz.|Dr.|Ter.|Rd.))**

In this data, the street always follows the time’s hrs. designation and a newline (), like this:

04/10/17\
1605 hrs.\
21800 block Newlin Mill Rd.

The street address always starts with some number that varies in length and ends with an abbreviated suffix of some kind. To describe this pattern, we use \d+ to match any digit that appears one or more times. Then we specify a space and look for any character one or more times using the dot wildcard and plus sign (.+) notation. The expression ends with a series of terms separated by the alternation pipe symbol that looks like this: (?:Sq.|Plz.|Dr.|Ter.|Rd.). The terms are inside parentheses, so the expression will match one or another of those terms. When we group terms like this, if we don’t want the parentheses to act as a capture group, we need to add ?: to negate that effect.

**NOTE**

_In a large data set, it’s likely roadway names would end with suffixes beyond the five in our regular expression. After making an initial pass at extracting the street, you can run a query to check for unmatched rows to find additional suffixes to match._

**City (?:Sq.|Plz.|Dr.|Ter.|Rd.)\n(\w+ \w+|\w+)**

Because the city always follows the street suffix, we reuse the terms separated by the alternation symbol we just created for the street. We follow that with a newline () and then use a capture group to look for two words or one word (\w+ \w+|\w+) before a final newline, because a town or city name can be more than a single word.

**Crime type \n(?:\w+ \w+|\w+)\n(.\*):**

The type of crime always precedes a colon (the only time a colon is used in each report) and might consist of one or more words, like this:

_--snip--_\
Middleburg\
Larceny: A license plate was reported\
stolen from a vehicle.\
SO170006250\
_--snip--_

To create an expression that matches this pattern, we follow a newline with a nonreporting capture group that looks for the one- or two-word city. Then we add another newline and match any character that occurs zero or more times before a colon using (.\*):.

**Description :\s(.+)(?:C0|SO)**

The crime description always comes between the colon after the crime type and the case number. The expression starts with the colon, a space character (\s), and then a capture group to find any character that appears one or more times using the .+ notation. The nonreporting capture group (?:C0|SO) tells the program to stop looking when it encounters either C0 or SO, the two character pairs that start each case number (a C followed by a zero, and an S followed by a capital O). We have to do this because the description might have one or more line breaks.

**Case number (?:C0|SO)\[0-9]+**

The case number starts with either C0 or SO, followed by a set of digits. To match this pattern, the expression looks for either C0 or SO in a nonreporting capture group followed by any digit from 0 to 9 that occurs one or more times using the \[0-9] range notation.

Now let’s pass these regular expressions to regexp\_match() to see them in action. [Listing 13-7](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list7) shows a sample regexp\_match() query that retrieves the case number, first date, crime type, and city:

SELECT\
&#x20;   regexp\_match(original\_text, '(?:C0|SO)\[0-9]+') AS case\_number,\
&#x20;   regexp\_match(original\_text, '\d{1,2}\\/\d{1,2}\\/\d{2}') AS date\_1,\
&#x20;   regexp\_match(original\_text, '\n(?:\w+ \w+|\w+)\n(.\*):') AS crime\_type,\
&#x20;   regexp\_match(original\_text, '(?:Sq.|Plz.|Dr.|Ter.|Rd.)\n(\w+ \w+|\w+)\n')\
&#x20;       AS city\
FROM crime\_reports;

_Listing 13-7: Matching case number, date, crime type, and city_

Run the code, and the results should look like this:

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/prog\_page\_224.jpg)

After all that wrangling, we’ve transformed the text into a structure that is more suitable for analysis. Of course, you would have to include many more incidents to count the frequency of crime type by city or the number of crimes per month to identify any trends.

To load each parsed element into the table’s columns, we’ll create an UPDATE query. But before you can insert the text into a column, you’ll need to learn how to extract the text from the array that regexp\_match() returns.

**Extracting Text from the regexp\_match() Result**

In [“Matching Crime Report Date Patterns”](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13lev3sec1) on [page 218](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#page\_218), I mentioned that regexp\_match() returns an array containing text values. Two clues reveal that these are text values. The first is that the data type designation in the column header shows text\[] instead of text. The second is that each result is surrounded by curly brackets. [Figure 13-2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13fig2) shows how pgAdmin displays the results of the query in [Listing 13-7](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list7).

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/f0224-01.jpg)

_Figure 13-2: Array values in the pgAdmin results grid_

The crime\_reports columns we want to update are not array types, so rather than passing in the array values returned by regexp\_match(), we need to extract the values from the array first. We do this by using array notation, as shown in [Listing 13-8](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list8).

SELECT\
&#x20;   crime\_id,\
&#x20; ➊ (regexp\_match(original\_text, '(?:C0|SO)\[0-9]+'))\[1]➋\
&#x20;       AS case\_number\
FROM crime\_reports;

_Listing 13-8: Retrieving a value from within an array_

First, we wrap the regexp\_match() function ➊ in parentheses. Then, at the end, we provide a value of 1, which represents the first element in the array, enclosed in square brackets ➋. The query should produce the following results:

crime\_id    case\_number\
\--------    -----------\
&#x20;      1    C0170006614\
&#x20;      2    C0170006162\
&#x20;      3    C0170006079\
&#x20;      4    SO170006250\
&#x20;      5    SO170006211

Now the data type designation in the pgAdmin column header should show text instead of text\[], and the values are no longer enclosed in curly brackets. We can now insert these values into crime\_reports using an UPDATE query.

**Updating the crime\_reports Table with Extracted Data**

With each element currently available as text, we can update columns in the crime\_reports table with the appropriate data from the original crime report. To start, [Listing 13-9](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list9) combines the extracted first date and time into a single timestamp value for the column date\_1.

&#x20; UPDATE crime\_reports\
➊ SET date\_1 =\
&#x20; (\
&#x20;   ➋ (regexp\_match(original\_text, '\d{1,2}\\/\d{1,2}\\/\d{2}'))\[1]\
&#x20;       ➌ || ' ' ||\
&#x20;   ➍ (regexp\_match(original\_text, '\\/\d{2}\n(\d{4})'))\[1]\
&#x20;       ➎ ||' US/Eastern'\
➏ )::timestamptz;\
\
&#x20; SELECT crime\_id,\
&#x20;        date\_1,\
&#x20;        original\_text\
&#x20; FROM crime\_reports;

_Listing 13-9: Updating the crime\_reports date\_1 column_

Because the date\_1 column is of type timestamp, we must provide an input in that data type. To do that, we’ll use the PostgreSQL double-pipe (||) concatenation operator to combine the extracted date and time in a format that’s acceptable for timestamp with time zone input. In the SET clause ➊, we start with the regex pattern that matches the first date ➋. Next, we concatenate the date with a space using two single quotes ➌ and repeat the concatenation operator. This step combines the date with a space before connecting it to the regex pattern that matches the time ➍. Then we include the time zone for the Washington, D.C., area by concatenating that at the end of the string ➎ using the US/Eastern designation. Concatenating these elements creates a string in the pattern of MM/DD/YY HHMM TIMEZONE, which is acceptable as a timestamp input. We cast the string to a timestamp with time zone data type ➏ using the PostgreSQL double-colon shorthand and the timestamptz abbreviation.

When you run the UPDATE portion of the code, PostgreSQL should return the message UPDATE 5. Running the SELECT statement in pgAdmin should show the now-filled date\_1 column alongside a portion of the original\_text column, like this:

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/prog\_page\_226.jpg)

At a glance, you can see that date\_1 accurately captures the first date and time that appears in the original text and puts it into a useable format that we can analyze. Note that if you’re not in the Eastern time zone, the timestamps will instead reflect your pgAdmin client’s time zone. As you learned in [“Setting the Time Zone”](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch11.xhtml#lev175) on [page 178](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch11.xhtml#page\_178), you can use the command SET timezone TO 'US/Eastern'; to change the client to reflect Eastern time.

**Using CASE to Handle Special Instances**

You could write an UPDATE statement for each remaining data element, but combining those statements into one would be more efficient. [Listing 13-10](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list10) updates all the crime\_reports columns using a single statement while handling inconsistent values in the data.

UPDATE crime\_reports\
SET date\_1➊ =\
&#x20;   (\
&#x20;     (regexp\_match(original\_text, '\d{1,2}\\/\d{1,2}\\/\d{2}'))\[1]\
&#x20;         || ' ' ||\
&#x20;     (regexp\_match(original\_text, '\\/\d{2}\n(\d{4})'))\[1]\
&#x20;         ||' US/Eastern'\
&#x20;   )::timestamptz,\
&#x20;           \
&#x20;   date\_2➋ =\
&#x20;   CASE➌\
&#x20;       WHEN➍ (SELECT regexp\_match(original\_text, '-(\d{1,2}\\/\d{1,2}\\/\d{1,2})') IS NULL➎)\
&#x20;               AND (SELECT regexp\_match(original\_text, '\\/\d{2}\n\d{4}-(\d{4})') IS NOT NULL➏)\
&#x20;       THEN➐\
&#x20;         ((regexp\_match(original\_text, '\d{1,2}\\/\d{1,2}\\/\d{2}'))\[1]\
&#x20;             || ' ' ||\
&#x20;         (regexp\_match(original\_text, '\\/\d{2}\n\d{4}-(\d{4})'))\[1]\
&#x20;             ||' US/Eastern'\
&#x20;         )::timestamptz\
&#x20;       WHEN➑ (SELECT regexp\_match(original\_text, '-(\d{1,2}\\/\d{1,2}\\/\d{1,2})') IS NOT NULL)\
&#x20;               AND (SELECT regexp\_match(original\_text, '\\/\d{2}\n\d{4}-(\d{4})') IS NOT NULL)\
&#x20;       THEN\
&#x20;         ((regexp\_match(original\_text, '-(\d{1,2}\\/\d{1,2}\\/\d{1,2})'))\[1]\
&#x20;             || ' ' ||\
&#x20;         (regexp\_match(original\_text, '\\/\d{2}\n\d{4}-(\d{4})'))\[1]\
&#x20;             ||' US/Eastern'\
&#x20;         )::timestamptz\
\
&#x20;       ELSE NULL➒\
&#x20;   END,\
&#x20;   street = (regexp\_match(original\_text, 'hrs.\n(\d+ .+(?:Sq.|Plz.|Dr.|Ter.|Rd.))'))\[1],\
&#x20;   city = (regexp\_match(original\_text,\
&#x20;                          '(?:Sq.|Plz.|Dr.|Ter.|Rd.)\n(\w+ \w+|\w+)\n'))\[1],\
&#x20;   crime\_type = (regexp\_match(original\_text, '\n(?:\w+ \w+|\w+)\n(.\*):'))\[1],\
&#x20;   description = (regexp\_match(original\_text, ':\s(.+)(?:C0|SO)'))\[1],\
&#x20;   case\_number = (regexp\_match(original\_text, '(?:C0|SO)\[0-9]+'))\[1];

_Listing 13-10: Updating all crime\_reports columns_

This UPDATE statement might look intimidating, but it’s not if we break it down by column. First, we use the same code from [Listing 13-9](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list9) to update the date\_1 column ➊. But to update date\_2 ➋, we need to account for the inconsistent presence of a second date and time. In our limited data set, there are three possibilities:

1. A second hour exists but not a second date. This occurs when a report covers a range of hours on one date.
2. A second date and second hour exist. This occurs when a report covers more than one date.
3. Neither a second date nor a second hour exists.

To insert the correct value in date\_2 for each scenario, we use the CASE statement syntax you learned in [“Reclassifying Values with CASE”](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#lev216) on [page 207](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch12.xhtml#page\_207) to test for each possibility. After the CASE keyword ➌, we use a series of WHEN ... THEN statements to check for the first two conditions and provide the value to insert; if neither condition exists, we use an ELSE keyword to provide a NULL.

The first WHEN statement ➍ checks whether regexp\_match() returns a NULL ➎ for the second date and a value for the second hour (using IS NOT NULL ➏). If that condition evaluates as true, the THEN statement ➐ concatenates the first date with the second hour to create a timestamp for the update.

The second WHEN statement ➑ checks that regexp\_match() returns a value for the second hour and second date. If true, the THEN statement concatenates the second date with the second hour to create a timestamp.

If neither of the two WHEN statements returns true, the ELSE statement ➒ provides a NULL for the update because there is only a first date and first time.

**NOTE**

_The WHEN statements handle the possibilities that exist in our small sample data set. If you are working with more data, you might need to handle additional variations, such as a second date but not a second time._

When we run the full query in [Listing 13-10](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list10), PostgreSQL should report UPDATE 5. Success! Now that we’ve updated all the columns with the appropriate data while accounting for elements that have additional data, we can examine all the columns of the table and find the parsed elements from original\_text. [Listing 13-11](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list11) queries four of the columns:

SELECT date\_1,\
&#x20;      street,\
&#x20;      city,\
&#x20;      crime\_type\
FROM crime\_reports;

_Listing 13-11: Viewing selected crime data_

The results of the query should show a nicely organized set of data that looks something like this:

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/prog\_page\_228.jpg)

You’ve successfully transformed raw text into a table that can answer questions and reveal storylines about crime in this area.

**The Value of the Process**

Writing regular expressions and coding a query to update a table can take time, but there is value to identifying and collecting data this way. In fact, some of the best data sets you’ll encounter are those you build yourself. Everyone can download the same data sets, but the ones you build are yours alone. You get to be first person to find and tell the story behind the data.

Also, after you set up your database and queries, you can use them again and again. In this example, you could collect crime reports every day (either by hand or by automating downloads using a programming language such as Python) for an ongoing data set that you can mine continually for trends.

In the next section, we’ll finish our exploration of regular expressions using additional PostgreSQL functions.

_**Using Regular Expressions with WHERE**_

You’ve filtered queries using LIKE and ILIKE in WHERE clauses. In this section, you’ll learn to use regular expressions in WHERE clauses so you can perform more complex matches.

We use a tilde (\~) to make a case-sensitive match on a regular expression and a tilde-asterisk (\~\*) to perform a case-insensitive match. You can negate either expression by adding an exclamation point in front. For example, !\~\* indicates to _not_ match a regular expression that is case-insensitive. [Listing 13-12](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list12) shows how this works using the 2010 Census table us\_counties\_2010 from previous exercises:

&#x20; SELECT geo\_name\
&#x20; FROM us\_counties\_2010\
➊ WHERE geo\_name \~\* '(.+lade.+|.+lare.+)'\
&#x20; ORDER BY geo\_name;\
\
&#x20; SELECT geo\_name\
&#x20; FROM us\_counties\_2010\
➋ WHERE geo\_name \~\* '.+ash.+' AND geo\_name !\~ 'Wash.+'\
&#x20; ORDER BY geo\_name;

_Listing 13-12: Using regular expressions in a WHERE clause_

The first WHERE clause ➊ uses the tilde-asterisk (\~\*) to perform a case-insensitive match on the regular expression (.+lade.+|.+lare.+) to find any county names that contain either the letters lade or lare between other characters. The results should show eight rows:

geo\_name\
\-------------------\
Bladen County\
Clare County\
Clarendon County\
Glades County\
Langlade County\
Philadelphia County\
Talladega County\
Tulare County

As you can see, the county names include the letters lade or lare between other characters.

The second WHERE clause ➋ uses the tilde-asterisk (\~\*) as well as a negated tilde (!\~) to find county names containing the letters ash but excluding those starting with Wash. This query should return the following:

geo\_name\
\--------------\
Nash County\
Wabash County\
Wabash County\
Wabasha County

All four counties in this output have names that contain the letters ash but don’t start with Wash.

These are fairly simple examples, but you can do more complex matches using regular expressions that you wouldn’t be able to perform with the wildcards available with just LIKE and ILIKE.

_**Additional Regular Expression Functions**_

Let’s look at three more regular expression functions you might find useful when working with text. [Listing 13-13](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list13) shows several regular expression functions that replace and split text:

➊ SELECT regexp\_replace('05/12/2018', '\d{4}', '2017');\
\
➋ SELECT regexp\_split\_to\_table('Four,score,and,seven,years,ago', ',');\
\
➌ SELECT regexp\_split\_to\_array('Phil Mike Tony Steve', ' ');

_Listing 13-13: Regular expression functions to replace and split text_

The regexp\_replace(string, pattern, replacement text) function lets you substitute a matched pattern with replacement text. In the example at ➊, we’re searching the date string 05/12/2018 for any set of four digits in a row using \d{4}. When found, we replace them with the replacement text 2017. The result of that query is 05/12/2017 returned as text.

The regexp\_split\_to\_table(string, pattern) function splits delimited text into rows. [Listing 13-13](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list13) uses this function to split the string 'Four,score,and,seven,years,ago' on commas ➋, resulting in a set of rows that has one word in each row:

regexp\_split\_to\_table\
\---------------------\
Four\
score\
and\
seven\
years\
ago

Keep this function in mind as you tackle the “Try It Yourself” exercises at the end of the chapter.

The regexp\_split\_to\_array(string, pattern) function splits delimited text into an array. The example splits the string Phil Mike Tony Steve on spaces ➌, returning a text array that should look like this in pgAdmin:

regexp\_split\_to\_array\
\----------------------\
{Phil,Mike,Tony,Steve}

The text\[] notation in pgAdmin’s column header along with curly brackets around the results confirms that this is indeed an array type, which provides another means of analysis. For example, you could then use a function such as array\_length() to count the number of words, as shown in [Listing 13-14](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list14).

SELECT array\_length(regexp\_split\_to\_array('Phil Mike Tony Steve', ' '), 1);

_Listing 13-14: Finding an array length_

The query should return 4 because four elements are in this array. You can read more about array\_length() and other array functions at [_https://www.postgresql.org/docs/current/static/functions-array.html_](https://www.postgresql.org/docs/current/static/functions-array.html).

#### Full Text Search in PostgreSQL <a href="#lev229" id="lev229"></a>

PostgreSQL comes with a powerful full text search engine that gives you more options when searching for information in large amounts of text. You’re familiar with Google or other web search engines and similar technology that powers search on news websites or research databases, such as LexisNexis. Although the implementation and capability of full text search demands several chapters, here I’ll walk you through a simple example of setting up a table for text search and functions for searching using PostgreSQL.

For this example, I assembled 35 speeches by former U.S. presidents who served after World War II through the Gerald R. Ford administration. Consisting mostly of State of the Union addresses, these public texts are available through the Internet Archive at [_https://archive.org/_](https://archive.org/) and the University of California’s American Presidency Project at [_http://www.presidency.ucsb.edu/ws/index.php/_](http://www.presidency.ucsb.edu/ws/index.php/). You can find the data in the _sotu-1946-1977.csv_ file along with the book’s resources at [_https://www.nostarch.com/practicalSQL/_](https://www.nostarch.com/practicalSQL/).

Let’s start with the data types unique to full text search.

_**Text Search Data Types**_

PostgreSQL’s implementation of text search includes two data types. The tsvector data type represents the text to be searched and to be stored in an optimized form. The tsquery data type represents the search query terms and operators. Let’s look at the details of both.

**Storing Text as Lexemes with tsvector**

The tsvector data type reduces text to a sorted list of _lexemes_, which are units of meaning in language. Think of lexemes as words without the variations created by suffixes. For example, the tsvector format would store the words _washes_, _washed_, and _washing_ as the lexeme _wash_ while noting each word’s position in the original text. Converting text to tsvector also removes small _stop words_ that usually don’t play a role in search, such as _the_ or _it_.

To see how this data type works, let’s convert a string to tsvector format. [Listing 13-15](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list15) uses the PostgreSQL search function to\_tsvector(), which normalizes the text “I am walking across the sitting room to sit with you” to lexemes:

SELECT to\_tsvector('I am walking across the sitting room to sit with you.');

_Listing 13-15: Converting text to tsvector data_

Execute the code, and it should return the following output in tsvector format:

'across':4 'room':7 'sit':6,9 'walk':3

The to\_tsvector() function reduces the number of words from eleven to four, eliminating words such as _I_, _am_, and _the_, which are not helpful search terms. The function removes suffixes, changing _walking_ to _walk_ and _sitting_ to _sit_. It also orders the words alphabetically, and the number following each colon indicates its position in the original string, taking stop words into account. Note that _sit_ is recognized as being in two positions, one for _sitting_ and one for _sit_.

**Creating the Search Terms with tsquery**

The tsquery data type represents the full text search query, again optimized as lexemes. It also provides operators for controlling the search. Examples of operators include the ampersand (&) for AND, the pipe symbol (|) for OR, and the exclamation point (!) for NOT. A special <-> operator lets you search for adjacent words or words a certain distance apart.

[Listing 13-16](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list16) shows how the to\_tsquery() function converts search terms to the tsquery data type.

SELECT to\_tsquery('walking & sitting');

_Listing 13-16: Converting search terms to tsquery data_

After running the code, you should see that the resulting tsquery data type has normalized the terms into lexemes, which match the format of the data to search:

'walk' & 'sit'

Now you can use terms stored as tsquery to search text optimized as tsvector.

**Using the @@ Match Operator for Searching**

With the text and search terms converted to the full text search data types, you can use the double at sign (@@) match operator to check whether a query matches text. The first query in [Listing 13-17](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list17) uses to\_tsquery() to search for the words _walking_ and _sitting_, which we combine with the & operator. It returns a Boolean value of true because both _walking_ and _sitting_ are present in the text converted by to\_tsvector().

SELECT to\_tsvector('I am walking across the sitting room') @@ to\_tsquery('walking & sitting');\
SELECT to\_tsvector('I am walking across the sitting room') @@ to\_tsquery('walking & running');

_Listing 13-17: Querying a tsvector type with a tsquery_

However, the second query returns false because both _walking_ and _running_ are not present in the text. Now let’s build a table for searching the speeches.

_**Creating a Table for Full Text Search**_

Let’s start by creating a table to hold the speech text. The code in [Listing 13-18](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list18) creates and fills president\_speeches so it contains a column for the original speech text as well as a column of type tsvector. The reason is that we need to convert the original speech text into that tsvector column to optimize it for searching. We can’t easily do that conversion during import, so let’s handle that as a separate step. Be sure to change the file path to match the location of your saved CSV file:

CREATE TABLE president\_speeches (\
&#x20;   sotu\_id serial PRIMARY KEY,\
&#x20;   president varchar(100) NOT NULL,\
&#x20;   title varchar(250) NOT NULL,\
&#x20;   speech\_date date NOT NULL,\
&#x20;   speech\_text text NOT NULL,\
&#x20;   search\_speech\_text tsvector\
);\
\
COPY president\_speeches (president, title, speech\_date, speech\_text)\
FROM '_C:\YourDirectory\\_sotu-1946-1977.csv'\
WITH (FORMAT CSV, DELIMITER '|', HEADER OFF, QUOTE '@');

_Listing 13-18: Creating and filling the president\_speeches table_

After executing the query, run SELECT \* FROM president\_speeches; to see the data. In pgAdmin, hover your mouse over any cell to see extra words not visible in the results grid. You should see a sizeable amount of text in each row of the speech\_text column.

Next, we copy the contents of speech\_text to the tsvector column search\_speech\_text and transform it to that data type at the same time. The UPDATE query in [Listing 13-19](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list19) handles the task:

&#x20; UPDATE president\_speeches\
➊ SET search\_speech\_text = to\_tsvector('english', speech\_text);

_Listing 13-19: Converting speeches to tsvector in the search\_speech\_text column_

The SET clause ➊ fills search\_speech\_text with the output of to\_tsvector(). The first argument in the function specifies the language for parsing the lexemes. We’re using the default of english here, but you can substitute spanish, german, french, or whatever language you want to use (some languages may require you to find and install additional dictionaries). The second argument is the name of the input column. Run the code to fill the column.

Finally, we want to index the search\_speech\_text column to speed up searches. You learned about indexing in [Chapter 7](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07), which focused on PostgreSQL’s default index type, B-Tree. For full text search, the PostgreSQL documentation recommends using the _Generalized Inverted Index_ (_GIN_; see [_https://www.postgresql.org/docs/current/static/textsearch-indexes.html_](https://www.postgresql.org/docs/current/static/textsearch-indexes.html)). You can add a GIN index using CREATE INDEX in [Listing 13-20](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list20):

CREATE INDEX search\_idx ON president\_speeches USING gin(search\_speech\_text);

_Listing 13-20: Creating a GIN index for text search_

The GIN index contains an entry for each lexeme and its location, allowing the database to find matches more quickly.

**NOTE**

_Another way to set up a column for search is to create an index on a text column using the to\_tsvector() function. See_ [https://www.postgresql.org/docs/current/static/textsearch-tables.html](https://www.postgresql.org/docs/current/static/textsearch-tables.html) _for details_.

Now you’re ready to use search functions.

_**Searching Speech Text**_

Thirty-two years’ worth of presidential speeches is fertile ground for exploring history. For example, the query in [Listing 13-21](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list21) lists the speeches in which the president mentioned Vietnam:

&#x20; SELECT president, speech\_date\
&#x20; FROM president\_speeches\
➊ WHERE search\_speech\_text @@ to\_tsquery('Vietnam')\
&#x20; ORDER BY speech\_date;

_Listing 13-21: Finding speeches containing the word_ Vietnam

In the WHERE clause, the query uses the double at sign (@@) match operator ➊ between the search\_speech\_text column (of data type tsvector) and the query term _Vietnam_, which to\_tsquery() transforms into tsquery data. The results should list 10 speeches, showing that the first mention of Vietnam came up in a 1961 special message to Congress by John F. Kennedy and became a recurring topic starting in 1966 as America’s involvement in the Vietnam War escalated.

president            speech\_date\
\-----------------    -----------\
John F. Kennedy      1961-05-25\
Lyndon B. Johnson    1966-01-12\
Lyndon B. Johnson    1967-01-10\
Lyndon B. Johnson    1968-01-17\
Lyndon B. Johnson    1969-01-14\
Richard M. Nixon     1970-01-22\
Richard M. Nixon     1972-01-20\
Richard M. Nixon     1973-02-02\
Gerald R. Ford       1975-01-15\
Gerald R. Ford       1977-01-12

Before we try more searches, let’s add a method for showing the location of our search term in the text.

**Showing Search Result Locations**

To see where our search terms appear in text, we can use the ts\_headline() function. It displays one or more highlighted search terms surrounded by adjacent words. Options for this function give you flexibility in how to format the display. [Listing 13-22](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list22) highlights how to display a search for a specific instance of _Vietnam_ using ts\_headline():

SELECT president,\
&#x20;      speech\_date,\
&#x20;    ➊ ts\_headline(speech\_text, to\_tsquery('Vietnam'),\
&#x20;                ➋ 'StartSel = <,\
&#x20;                   StopSel = >,\
&#x20;                   MinWords=5,\
&#x20;                   MaxWords=7,\
&#x20;                   MaxFragments=1')\
FROM president\_speeches\
WHERE search\_speech\_text @@ to\_tsquery('Vietnam');

_Listing 13-22: Displaying search results with ts\_headline()_

To declare ts\_headline() ➊, we pass the original speech\_text column rather than the tsvector column we used in the search and relevance functions as the first argument. Then, as the second argument, we pass a to\_tsquery() function that specifies the word to highlight. We follow this with a third argument that lists optional formatting parameters ➋ separated by commas. Here, we specify the characters to identify the start and end of the highlighted word (StartSel and StopSel). We also set the minimum and maximum number of words to display (MinWords and MaxWords), plus the maximum number of fragments to show using MaxFragments. These settings are optional, and you can adjust them according to your needs.

The results of this query should show at most seven words per speech, highlighting the word _Vietnam_:

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/prog\_page\_235.jpg)

Using this technique, we can quickly see the context of the term we searched. You might also use this function to provide flexible display options for a search feature on a web application. Let’s continue trying forms of searches.

**Using Multiple Search Terms**

As another example, we could look for speeches in which a president mentioned the word _transportation_ but didn’t discuss _roads_. We might want to do this to find speeches that focused on broader policy rather than a specific roads program. To do this, we use the syntax in [Listing 13-23](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list23):

&#x20; SELECT president,\
&#x20;      speech\_date\
&#x20;    ➊ ts\_headline(speech\_text, to\_tsquery('transportation & !roads'),\
&#x20;                  'StartSel = <,\
&#x20;                   StopSel = >,\
&#x20;                   MinWords=5,\
&#x20;                   MaxWords=7,\
&#x20;                   MaxFragments=1')\
&#x20; FROM president\_speeches\
➋ WHERE search\_speech\_text @@ to\_tsquery('transportation & !roads');

_Listing 13-23: Finding speeches with the word_ transportation _but not_ roads

Again, we use ts\_headline() ➊ to highlight the terms our search finds. In the to\_tsquery() function in the WHERE clause ➋, we pass transportation and roads, combining them with the ampersand (&) operator. We use the exclamation point (!) in front of roads to indicate that we want speeches that do not contain this word. This query should find eight speeches that fit the criteria. Here are the first four rows:

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/prog\_page\_236.jpg)

Notice that the highlighted words in the ts\_headline column include transportation and transport. The reason is that the to\_tsquery() function converted transportation to the lexeme transport for the search term. This database behavior is extremely useful in helping to find relevant related words.

**Searching for Adjacent Words**

Finally, we’ll use the distance (<->) operator, which consists of a hyphen between the less than and greater than signs, to find adjacent words. Alternatively, you can place a number between the signs to find terms that many words apart. For example, [Listing 13-24](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list24) searches for any speeches that include the word _military_ immediately followed by _defense_:

SELECT president,\
&#x20;      speech\_date,\
&#x20;      ts\_headline(speech\_text, to\_tsquery('military <-> defense'),\
&#x20;                  'StartSel = <,\
&#x20;                   StopSel = >,\
&#x20;                   MinWords=5,\
&#x20;                   MaxWords=7,\
&#x20;                   MaxFragments=1')\
FROM president\_speeches\
WHERE search\_speech\_text @@ to\_tsquery('military <-> defense');

_Listing 13-24: Finding speeches where_ defense _follows_ military

This query should find four speeches, and because to\_tsquery() converts the search terms to lexemes, the words identified in the speeches should include plurals, such as _military defenses_. The following shows the four speeches that have the adjacent terms:

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/prog\_page\_237.jpg)

If you changed the query terms to military <2> defense, the database would return matches where the terms are exactly two words apart, as in the phrase “our military and defense commitments.”

_**Ranking Query Matches by Relevance**_

You can also rank search results by relevance using two of PostgreSQL’s full text search functions. These functions are helpful when you’re trying to understand which piece of text, or speech in this case, is most relevant to your particular search terms.

One function, ts\_rank(), generates a rank value (returned as a variable-precision real data type) based on how often the lexemes you’re searching for appear in the text. The other function, ts\_rank\_cd(), considers how close the lexemes searched are to each other. Both functions can take optional arguments to take into account document length and other factors. The rank value they generate is an arbitrary decimal that’s useful for sorting but doesn’t have any inherent meaning. For example, a value of 0.375 generated during one query isn’t directly comparable to the same value generated during a different query.

As an example, [Listing 13-25](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list25) uses ts\_rank() to rank speeches containing all the words _war_, _security_, _threat_, and _enemy_:

&#x20; SELECT president,\
&#x20;        speech\_date,\
&#x20;      ➊ ts\_rank(search\_speech\_text,\
&#x20;                to\_tsquery('war & security & threat & enemy')) AS score\
&#x20; FROM president\_speeches\
➋ WHERE search\_speech\_text @@ to\_tsquery('war & security & threat & enemy')\
&#x20; ORDER BY score DESC/\
&#x20; LIMIT 5;

_Listing 13-25: Scoring relevance with ts\_rank()_

In this query, the ts\_rank() function ➊ takes two arguments: the search\_speech\_text column and the output of a to\_tsquery() function containing the search terms. The output of the function receives the alias score. In the WHERE clause ➋ we filter the results to only those speeches that contain the search terms specified. Then we order the results in score in descending order and return just five of the highest-ranking speeches. The results should be as follows:

president               speech\_date    score\
\--------------------    -----------    ---------\
Harry S. Truman         1946-01-21      0.257522\
Lyndon B. Johnson       1968-01-17      0.186296\
Dwight D. Eisenhower    1957-01-10      0.140851\
Harry S. Truman         1952-01-09     0.0982469\
Richard M. Nixon        1972-01-20     0.0973585

Harry S. Truman’s 1946 State of the Union message, just four months after the end of World War II, contains the words _war_, _security_, _threat_, and _enemy_ more often than the other speeches. However, it also happens to be the longest speech in the table (which you can determine by using char\_length(), as you learned earlier in the chapter). The length of the speeches influences these rankings because ts\_rank() factors in the number of matching terms in a given text. Lyndon B. Johnson’s 1968 State of the Union address, delivered at the height of the Vietnam War, comes in second.

It would be ideal to compare frequencies between speeches of identical lengths to get a more accurate ranking, but this isn’t always possible. However, we can factor in the length of each speech by adding a normalization code as a third parameter of the ts\_rank() function, as shown in [Listing 13-26](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list26):

SELECT president,\
&#x20;      speech\_date,\
&#x20;      ts\_rank(search\_speech\_text,\
&#x20;              to\_tsquery('war & security & threat & enemy'), 2➊)::numeric\
&#x20;              AS score\
FROM president\_speeches\
WHERE search\_speech\_text @@ to\_tsquery('war & security & threat & enemy')\
ORDER BY score DESC\
LIMIT 5;

_Listing 13-26: Normalizing ts\_rank() by speech length_

Adding the optional code 2 ➊ instructs the function to divide the score by the length of the data in the search\_speech\_text column. This quotient then represents a score normalized by the document length, giving an apples-to-apples comparison among the speeches. The PostgreSQL documentation at [_https://www.postgresql.org/docs/current/static/textsearch-controls.html_](https://www.postgresql.org/docs/current/static/textsearch-controls.html) lists all the options available for text search, including using the document length and dividing by the number of unique words.

After running the code in [Listing 13-26](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list26), the rankings should change:

president               speech\_date    score\
\--------------------    -----------    ------------\
Lyndon B. Johnson       1968-01-17     0.0000728288\
Dwight D. Eisenhower    1957-01-10     0.0000633609\
Richard M. Nixon        1972-01-20     0.0000497998\
Harry S. Truman         1952-01-09     0.0000365366\
Dwight D. Eisenhower    1958-01-09     0.0000355315

In contrast to the ranking results in [Listing 13-25](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list25), Johnson’s 1968 speech now tops the rankings, and Truman’s 1946 message falls out of the top five. This might be a more meaningful ranking than the first sample output, because we normalized it by length. But four of the five top-ranked speeches are the same between the two sets, and you can be reasonably certain that each of these four is worthy of closer examination to understand more about wartime presidential speeches.

#### Wrapping Up <a href="#lev234" id="lev234"></a>

Far from being boring, text offers abundant opportunities for data analysis. In this chapter, you’ve learned valuable techniques for turning ordinary text into data you can extract, quantify, search, and rank. In your work or studies, keep an eye out for routine reports that have facts buried inside chunks of text. You can use regular expressions to dig them out, turn them into structured data, and analyze them to find trends. You can also use search functions to analyze the text.

In the next chapter, you’ll learn how PostgreSQL can help you analyze geographic information.

**TRY IT YOURSELF**

Use your new text-wrangling skills to tackle these tasks:

1. The style guide of a publishing company you’re writing for wants you to avoid commas before suffixes in names. But there are several names like Alvarez, Jr. and Williams, Sr. in your database. Which functions can you use to remove the comma? Would a regular expression function help? How would you capture just the suffixes to place them into a separate column?
2. Using any one of the State of the Union addresses, count the number of unique words that are five characters or more. (Hint: You can use regexp\_split\_to\_table() in a subquery to create a table of words to count.) Bonus: Remove commas and periods at the end of each word.
3. Rewrite the query in [Listing 13-25](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch13.xhtml#ch13list25) using the ts\_rank\_cd() function instead of ts\_rank(). According to the PostgreSQL documentation, ts\_rank\_cd() computes cover density, which takes into account how close the lexeme search terms are to each other. Does using the ts\_rank\_cd() function significantly change the results?
