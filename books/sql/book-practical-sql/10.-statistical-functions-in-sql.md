# 10. Statistical Functions In SQL

### **10** **STATISTICAL FUNCTIONS IN SQL** <a href="#ch10" id="ch10"></a>

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/common01.jpg)

A SQL database isn’t usually the first tool a data analyst chooses when performing statistical analysis that requires more than just calculating sums and averages. Typically, the software of choice would be full-featured statistics packages, such as SPSS or SAS, the programming languages R or Python, or even Excel. However, standard ANSI SQL, including PostgreSQL’s implementation, offers a handful of powerful stats functions that reveal a lot about your data without having to export your data set to another program.

In this chapter, we’ll explore these SQL stats functions along with guidelines on when to use them. Statistics is a vast subject worthy of its own book, so we’ll only skim the surface here. Nevertheless, you’ll learn how to apply high-level statistical concepts to help you derive meaning from your data using a new data set from the U.S. Census Bureau. You’ll also learn to use SQL to create comparisons using rankings and rates with FBI crime data as our subject.

#### Creating a Census Stats Table <a href="#lev158" id="lev158"></a>

Let’s return to one of my favorite data sources, the U.S. Census Bureau. In [Chapters 4](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch04.xhtml#ch04) and [5](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch05.xhtml#ch05), you used the 2010 Decennial Census to import data and perform basic math and stats. This time you’ll use county data points compiled from the 2011–2015 American Community Survey (ACS) 5-Year Estimates, a separate survey administered by the Census Bureau.

Use the code in [Listing 10-1](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch10.xhtml#ch10list1) to create the table acs\_2011\_2015\_stats and import the CSV file _acs\_2011\_2015\_stats.csv_. The code and data are available with all the book’s resources at [_https://www.nostarch.com/practicalSQL/_](https://www.nostarch.com/practicalSQL/). Remember to change C:\YourDirectory\ to the location of the CSV file.

&#x20; CREATE TABLE acs\_2011\_2015\_stats (\
&#x20;   ➊ geoid varchar(14) CONSTRAINT geoid\_key PRIMARY KEY,\
&#x20;     county varchar(50) NOT NULL,\
&#x20;     st varchar(20) NOT NULL,\
&#x20;   ➋ pct\_travel\_60\_min numeric(5,3) NOT NULL,\
&#x20;     pct\_bachelors\_higher numeric(5,3) NOT NULL,\
&#x20;     pct\_masters\_higher numeric(5,3) NOT NULL,\
&#x20;     median\_hh\_income integer,\
&#x20;   ➌ CHECK (pct\_masters\_higher <= pct\_bachelors\_higher)\
&#x20; );\
\
&#x20; COPY acs\_2011\_2015\_stats\
&#x20; FROM '_C:\YourDirectory\\_acs\_2011\_2015\_stats.csv'\
&#x20; WITH (FORMAT CSV, HEADER, DELIMITER ',');\
\
➍ SELECT \* FROM acs\_2011\_2015\_stats;

_Listing 10-1: Creating the Census 2011–2015 ACS 5-Year stats table and import data_

The acs\_2011\_2015\_stats table has seven columns. The first three columns ➊ include a unique geoid that serves as the primary key, the name of the county, and the state name st. The next four columns display the following three percentages ➋ I derived for each county from raw data in the ACS release, plus one more economic indicator:

pct\_travel\_60\_min The percentage of workers ages 16 and older who commute more than 60 minutes to work.

**pct\_bachelors\_higher** The percentage of people ages 25 and older whose level of education is a bachelor’s degree or higher. (In the United States, a bachelor’s degree is usually awarded upon completing a four-year college education.)

pct\_masters\_higher The percentage of people ages 25 and older whose level of education is a master’s degree or higher. (In the United States, a master’s degree is the first advanced degree earned after completing a bachelor’s degree.)

median\_hh\_income The county’s median household income in 2015 inflation-adjusted dollars. As you learned in [Chapter 5](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch05.xhtml#ch05), a median value is the midpoint in an ordered set of numbers, where half the values are larger than the midpoint and half are smaller. Because averages can be skewed by a few very large or very small values, government reporting on economic data, such as income, tends to use medians. In this column, we omit the NOT NULL constraint because one county had no data reported.

We include the CHECK constraint ➌ you learned in [Chapter 7](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch07.xhtml#ch07) to check that the figures for the bachelor’s degree are equal to or higher than those for the master’s degree, because in the United States, a bachelor’s degree is earned before or concurrently with a master’s degree. A county showing the opposite could indicate data imported incorrectly or a column mislabeled. Our data checks out: upon import, there are no errors showing a violation of the CHECK constraint.

We use the SELECT statement ➍ to view all 3,142 rows imported, each corresponding to a county surveyed in this Census release.

Next, we’ll use statistics functions in SQL to better understand the relationships among the percentages.

**THE DECENNIAL U.S. CENSUS VS. THE AMERICAN COMMUNITY SURVEY**

Each U.S. Census data product has its own methodology. The Decennial Census is a full count of the U.S. population, conducted every 10 years via a form mailed to every household in the country. One of its primary purposes is to determine the number of seats each state holds in the U.S. House of Representatives. In contrast, the ACS is an ongoing annual survey of about 3.5 million U.S. households. It enquires into details about income, education, employment, ancestry, and housing. Private-sector and public-sector organizations alike use ACS data to track trends and make various decisions.

Currently, the Census Bureau packages ACS data into two releases: a 1-year data set that provides estimates for geographies with populations of 20,000 or more, and a 5-year data set that includes all geographies. Because it’s a survey, ACS results are estimates and have a margin of error, which I’ve omitted for brevity but which you’ll see included in a full ACS data set.

_**Measuring Correlation with corr(Y, X)**_

Researchers often want to understand the relationships between variables, and one such measure of relationships is _correlation_. In this section, we’ll use the corr(Y, X) function to measure correlation and investigate what relationship exists, if any, between the percentage of people in a county who’ve attained a bachelor’s degree and the median household income in that county. We’ll also determine whether, according to our data, a better-educated population typically equates to higher income and how strong the relationship between education level and income is if it does.

First, some background. The _Pearson correlation coefficient_ (generally denoted as _r_) is a measure for quantifying the strength of a _linear relationship_ between two variables. It shows the extent to which an increase or decrease in one variable correlates to a change in another variable. The _r_ values fall between −1 and 1. Either end of the range indicates a perfect correlation, whereas values near zero indicate a random distribution with no correlation. A positive _r_ value indicates a _direct relationship_: as one variable increases, the other does too. When graphed on a scatterplot, the data points representing each pair of values in a direct relationship would slope upward from left to right. A negative _r_ value indicates an _inverse relationship_: as one variable increases, the other decreases. Dots representing an inverse relationship would slope downward from left to right on a scatterplot.

[Table 10-1](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch10.xhtml#ch10tab1) provides general guidelines for interpreting positive and negative _r_ values, although as always with statistics, different statisticians may offer different interpretations.

**Table 10-1:** Interpreting Correlation Coefficients

| **Correlation coefficient (+/−)** | **What it could mean**                |
| --------------------------------- | ------------------------------------- |
| 0                                 | No relationship                       |
| .01 to .29                        | Weak relationship                     |
| .3 to .59                         | Moderate relationship                 |
| .6 to .99                         | Strong to nearly perfect relationship |
| 1                                 | Perfect relationship                  |

In standard ANSI SQL and PostgreSQL, we calculate the Pearson correlation coefficient using corr(Y, X). It’s one of several _binary aggregate functions_ in SQL and is so named because these functions accept two inputs. In binary aggregate functions, the input Y is the _dependent variable_ whose variation depends on the value of another variable, and X is the _independent variable_ whose value doesn’t depend on another variable.

**NOTE**

_Even though SQL specifies the Y and X inputs for the corr() function, correlation calculations don’t distinguish between dependent and independent variables. Switching the order of inputs in corr() produces the same result. However, for convenience and readability, these examples order the input variables according to dependent and independent._

We’ll use the corr(Y, X) function to discover the relationship between education level and income. Enter the code in [Listing 10-2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch10.xhtml#ch10list2) to use corr(Y, X) with the median\_hh\_income and pct\_bachelors\_higher variables as inputs:

SELECT corr(median\_hh\_income, pct\_bachelors\_higher)\
&#x20;   AS bachelors\_income\_r\
FROM acs\_2011\_2015\_stats;

_Listing 10-2: Using corr(Y, X) to measure the relationship between education and income_

Run the query; your result should be an _r_ value of just above .68 given as the floating-point double precision data type:

bachelors\_income\_r\
\------------------\
0.682185675451399

This positive _r_ value indicates that as a county’s educational attainment increases, household income tends to increase. The relationship isn’t perfect, but the _r_ value shows the relationship is fairly strong. We can visualize this pattern by plotting the variables on a scatterplot using Excel, as shown in [Figure 10-1](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch10.xhtml#ch10fig1). Each data point represents one U.S. county; the data point’s position on the x-axis shows the percentage of the population ages 25 and older that have a bachelor’s degree or higher. The data point’s position on the y-axis represents the county’s median household income.

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/f0159-01.jpg)

_Figure 10-1: A scatterplot showing the relationship between education and income_

Notice that although most of the data points are grouped together in the bottom-left corner of the graph, they do generally slope upward from left to right. Also, the points spread out rather than strictly follow a straight line. If they were in a straight line sloping up from left to right, the _r_ value would be 1, indicating a perfect positive linear relationship.

_**Checking Additional Correlations**_

Now let’s calculate the correlation coefficients for the remaining variable pairs using the code in [Listing 10-3](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch10.xhtml#ch10list3):

SELECT\
&#x20; ➊ round(\
&#x20;     corr(median\_hh\_income, pct\_bachelors\_higher)::numeric, 2\
&#x20;     ) AS bachelors\_income\_r,\
&#x20;   round(\
&#x20;     corr(pct\_travel\_60\_min, median\_hh\_income)::numeric, 2\
&#x20;     ) AS income\_travel\_r,\
&#x20;   round(\
&#x20;     corr(pct\_travel\_60\_min, pct\_bachelors\_higher)::numeric, 2\
&#x20;     ) AS bachelors\_travel\_r\
FROM acs\_2011\_2015\_stats;

_Listing 10-3: Using corr(Y, X) on additional variables_

This time we’ll make the output more readable by rounding off the decimal values. We’ll do this by wrapping the corr(Y, X) function inside SQL’s round() function ➊, which takes two inputs: the numeric value to be rounded and an integer value indicating the number of decimal places to round the first value. If the second parameter is omitted, the value is rounded to the nearest whole integer. Because corr(Y, X) returns a floating-point value by default, we’ll change it to the numeric type using the :: notation you learned in [Chapter 3](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch03.xhtml#ch03). Here’s the output:

bachelors\_income\_r    income\_travel\_r    bachelors\_travel\_r\
\------------------    ---------------    ------------------\
&#x20;             0.68               0.05                 -0.14

The bachelors\_income\_r value is 0.68, which is the same as our first run but rounded to two decimal places. Compared to bachelors\_income\_r, the other two correlations are weak.

The income\_travel\_r value shows that the correlation between income and the percentage of those who commute more than an hour to work is practically zero. This indicates that a county’s median household income bears little connection to how long it takes people to get to work.

The bachelors\_travel\_r value shows that the correlation of bachelor’s degrees and commuting is also low at -0.14. The negative value indicates an inverse relationship: as education increases, the percentage of the population that travels more than an hour to work decreases. Although this is interesting, a correlation coefficient that is this close to zero indicates a weak relationship.

When testing for correlation, we need to note some caveats. The first is that even a strong correlation does not imply causality. We can’t say that a change in one variable causes a change in the other, only that the changes move together. The second is that correlations should be subject to testing to determine whether they’re statistically significant. Those tests are beyond the scope of this book but worth studying on your own.

Nevertheless, the SQL corr(Y, X) function is a handy tool for quickly checking correlations between variables.

_**Predicting Values with Regression Analysis**_

Researchers not only want to understand relationships between variables; they also want to predict values using available data. For example, let’s say 30 percent of a county’s population has a bachelor’s degree or higher. Given the trend in our data, what would we expect that county’s median household income to be? Likewise, for each percent increase in education, how much increase, on average, would we expect in income?

We can answer both questions using _linear regression_. Simply put, the regression method finds the best linear equation, or straight line, that describes the relationship between an independent variable (such as education) and a dependent variable (such as income). Standard ANSI SQL and PostgreSQL include functions that perform linear regression.

[Figure 10-2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch10.xhtml#ch10fig2) shows our previous scatterplot with a regression line added.

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/f0161-01.jpg)

_Figure 10-2: Scatterplot with least squares regression line showing the relationship between education and income_

The straight line running through the middle of all the data points is called the _least squares regression line_, which approximates the “best fit” for a straight line that best describes the relationship between the variables. The equation for the regression line is like the _slope-intercept_ formula you might remember from high school math but written using differently named variables: _Y_ = _bX_ + _a_. Here are the formula’s components:

_**Y**_ is the predicted value, which is also the value on the y-axis, or dependent variable.

_**b**_ is the slope of the line, which can be positive or negative. It measures how many units the y-axis value will increase or decrease for each unit of the x-axis value.

_**X**_ represents a value on the x-axis, or independent variable.

_**a**_ is the y-intercept, the value at which the line crosses the y-axis when the _X_ value is zero.

Let’s apply this formula using SQL. Earlier, we questioned what the expected median household income in a county would be if the percentage of people with a bachelor’s degree or higher in that county was 30 percent. In our scatterplot, the percentage with bachelor’s degrees falls along the x-axis, represented by _X_ in the calculation. Let’s plug that value into the regression line formula in place of _X_:

_Y_ = _b_(30) + _a_

To calculate _Y_, which represents the predicted median household income, we need the line’s slope, _b_, and the y-intercept, _a_. To get these values, we’ll use the SQL functions regr\_slope(Y, X) and regr\_intercept(Y, X), as shown in [Listing 10-4](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch10.xhtml#ch10list4):

SELECT\
&#x20;   round(\
&#x20;       regr\_slope(median\_hh\_income, pct\_bachelors\_higher)::numeric, 2\
&#x20;       ) AS slope,\
&#x20;   round(\
&#x20;       regr\_intercept(median\_hh\_income, pct\_bachelors\_higher)::numeric, 2\
&#x20;       ) AS y\_intercept\
FROM acs\_2011\_2015\_stats;

_Listing 10-4: Regression slope and intercept functions_

Using the median\_hh\_income and pct\_bachelors\_higher variables as inputs for both functions, we’ll set the resulting value of the regr\_slope(Y, X) function as slope and the output for the regr\_intercept(Y, X) function as y\_intercept.

Run the query; the result should show the following:

slope     y\_intercept\
\------    -----------\
926.95       27901.15

The slope value shows that for every one-unit increase in bachelor’s degree percentage, we can expect a county’s median household income will increase by 926.95. Slope always refers to change per one unit of _X_. The y\_intercept value shows that when the regression line crosses the y-axis, where the percentage with bachelor’s degrees is at 0, the y-axis value is 27901.15. Now let’s plug both values into the equation to get the Y value:

_Y_ = 926.95(30) + 27901.15

_Y_ = 55709.65

Based on our calculation, in a county in which 30 percent of people age 25 and older have a bachelor’s degree or higher, we can expect a median household income in that county to be about $55,710. Of course, our data includes counties whose median income falls above and below that predicted value, but we expect this to be the case because our data points in the scatterplot don’t line up perfectly along the regression line. Recall that the correlation coefficient we calculated was 0.68, indicating a strong but not perfect relationship between education and income. Other factors probably contributed to variations in income as well.

_**Finding the Effect of an Independent Variable with r-squared**_

Earlier in the chapter, we calculated the correlation coefficient, _r_, to determine the direction and strength of the relationship between two variables. We can also calculate the extent that the variation in the _x_ (independent) variable explains the variation in the _y_ (dependent) variable by squaring the _r_ value to find the _coefficient of determination_, better known as _r-squared_. An _r_-squared value is between zero and one and indicates the percentage of the variation that is explained by the independent variable. For example, if _r_-squared equals .1, we would say that the independent variable explains 10 percent of the variation in the dependent variable, or not much at all.

To find _r_-squared, we use the regr\_r2(Y, X) function in SQL. Let’s apply it to our education and income variables using the code in [Listing 10-5](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch10.xhtml#ch10list5):

SELECT round(\
&#x20;       regr\_r2(median\_hh\_income, pct\_bachelors\_higher)::numeric, 3\
&#x20;       ) AS r\_squared\
FROM acs\_2011\_2015\_stats;

_Listing 10-5: Calculating the coefficient of determination, or_ r-_squared_

This time we’ll round off the output to the nearest thousandth place and set the result to r\_squared. The query should return the following result:

r\_squared\
\---------\
&#x20;   0.465

The _r_-squared value of 0.465 indicates that about 47 percent of the variation in median household income in a county can be explained by the percentage of people with a bachelor’s degree or higher in that county. What explains the other 53 percent of the variation in household income? Any number of factors could explain the rest of the variation, and statisticians will typically test numerous combinations of variables to determine what they are.

But before you use these numbers in a headline or presentation, it’s worth revisiting the following points:

1. Correlation doesn’t prove causality. For verification, do a Google search on “correlation and causality.” Many variables correlate well but have no meaning. (See [_http://www.tylervigen.com/spurious-correlations_](http://www.tylervigen.com/spurious-correlations) for examples of correlations that don’t prove causality, including the correlation between divorce rate in Maine and margarine consumption.) Statisticians usually perform _significance testing_ on the results to make sure values are not simply the result of randomness.
2. Statisticians also apply additional tests to data before accepting the results of a regression analysis, including whether the variables follow the standard bell curve distribution and meet other criteria for a valid result.

Given these factors, SQL’s statistics functions are useful as a preliminary survey of your data before doing more rigorous analysis. If your work involves statistics, a full study on performing regression is worthwhile.

#### Creating Rankings with SQL <a href="#lev163" id="lev163"></a>

Rankings make the news often. You’ll see them used anywhere from weekend box office charts to a sports team’s league standings. You’ve already learned how to order query results based on values in a column, but SQL lets you go further and create numbered rankings. Rankings are useful for data analysis in several ways, such as tracking changes over time if you have several years’ worth of data. You can also simply use a ranking as a fact on its own in a report. Let’s explore how to create rankings using SQL.

_**Ranking with rank() and dense\_rank()**_

Standard ANSI SQL includes several ranking functions, but we’ll just focus on two: rank() and dense\_rank(). Both are _window functions_, which perform calculations across sets of rows we specify using the OVER clause. Unlike aggregate functions, which group rows while calculating results, window functions present results for each row in the table.

The difference between rank() and dense\_rank() is the way they handle the next rank value after a tie: rank() includes a gap in the rank order, but dense\_rank() does not. This concept is easier to understand in action, so let’s look at an example. Consider a Wall Street analyst who covers the highly competitive widget manufacturing market. The analyst wants to rank companies by their annual output. The SQL statements in [Listing 10-6](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch10.xhtml#ch10list6) create and fill a table with this data and then rank the companies by widget output:

CREATE TABLE widget\_companies (\
&#x20;   id bigserial,\
&#x20;   company varchar(30) NOT NULL,\
&#x20;   widget\_output integer NOT NULL\
);\
\
INSERT INTO widget\_companies (company, widget\_output)\
VALUES\
&#x20;   ('Morse Widgets', 125000),\
&#x20;   ('Springfield Widget Masters', 143000),\
&#x20;   ('Best Widgets', 196000),\
&#x20;   ('Acme Inc.', 133000),\
&#x20;   ('District Widget Inc.', 201000),\
&#x20;   ('Clarke Amalgamated', 620000),\
&#x20;   ('Stavesacre Industries', 244000),\
&#x20;   ('Bowers Widget Emporium', 201000);\
\
SELECT\
&#x20;   company,\
&#x20;   widget\_output,\
&#x20; ➊ rank() OVER (ORDER BY widget\_output DESC),\
&#x20; ➋ dense\_rank() OVER (ORDER BY widget\_output DESC)\
FROM widget\_companies;

_Listing 10-6: Using the rank() and dense\_rank() window functions_

Notice the syntax in the SELECT statement that includes rank() ➊ and dense\_rank() ➋. After the function names, we use the OVER clause and in parentheses place an expression that specifies the “window” of rows the function should operate on. In this case, we want both functions to work on all rows of the widget\_output column, sorted in descending order. Here’s the output:

company                       widget\_output    rank    dense\_rank\
\--------------------------    -------------    ----    ----------\
Clarke Amalgamated                   620000       1             1\
Stavesacre Industries                244000       2             2\
Bowers Widget Emporium               201000       3             3\
District Widget Inc.                 201000       3             3\
Best Widgets                         196000       5             4\
Springfield Widget Masters           143000       6             5\
Acme Inc.                            133000       7             6\
Morse Widgets                        125000       8             7

The columns produced by the rank() and dense\_rank() functions show each company’s ranking based on the widget\_output value from highest to lowest, with Clarke Amalgamated at number one. To see how rank() and dense\_rank() differ, check the fifth row listing, Best Widgets.

With rank(), Best Widgets is the fifth highest ranking company, showing there are four companies with more output and there is no company ranking in fourth place, because rank() allows a gap in the order when a tie occurs. In contrast, dense\_rank(), which doesn’t allow a gap in the rank order, reflects the fact that Best Widgets has the fourth highest output number regardless of how many companies produced more. Therefore, Best Widgets ranks in fourth place using dense\_rank().

Both ways of handling ties have merit, but in practice rank() is used most often. It’s also what I recommend using, because it more accurately reflects the total number of companies ranked, shown by the fact that Best Widgets has four companies ahead of it in total output, not three.

Let’s look at a more complex ranking example.

_**Ranking Within Subgroups with PARTITION BY**_

The ranking we just did was a simple overall ranking based on widget output. But sometimes you’ll want to produce ranks within groups of rows in a table. For example, you might want to rank government employees by salary within each department or rank movies by box office earnings within each genre.

To use window functions in this way, we’ll add PARTITION BY to the OVER clause. A PARTITION BY clause divides table rows according to values in a column we specify.

Here’s an example using made-up data about grocery stores. Enter the code in [Listing 10-7](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch10.xhtml#ch10list7) to fill a table called store\_sales:

CREATE TABLE store\_sales (\
&#x20;   store varchar(30),\
&#x20;   category varchar(30) NOT NULL,\
&#x20;   unit\_sales bigint NOT NULL,\
&#x20;   CONSTRAINT store\_category\_key PRIMARY KEY (store, category)\
);\
\
INSERT INTO store\_sales (store, category, unit\_sales)\
VALUES\
&#x20;   ('Broders', 'Cereal', 1104),\
&#x20;   ('Wallace', 'Ice Cream', 1863),\
&#x20;   ('Broders', 'Ice Cream', 2517),\
&#x20;   ('Cramers', 'Ice Cream', 2112),\
&#x20;   ('Broders', 'Beer', 641),\
&#x20;   ('Cramers', 'Cereal', 1003),\
&#x20;   ('Cramers', 'Beer', 640),\
&#x20;   ('Wallace', 'Cereal', 980),\
&#x20;   ('Wallace', 'Beer', 988);\
\
SELECT\
&#x20;   category,\
&#x20;   store,\
&#x20;   unit\_sales,\
&#x20; ➊ rank() OVER (PARTITION BY category ORDER BY unit\_sales DESC)\
FROM store\_sales;

_Listing 10-7: Applying rank() within groups using PARTITION BY_

In the table, each row includes a store’s product category and sales for that category. The final SELECT statement creates a result set showing how each store’s sales ranks within each category. The new element is the addition of PARTITION BY in the OVER clause ➊. In effect, the clause tells the program to create rankings one category at a time, using the store’s unit sales in descending order. Here’s the output:

category     store      unit\_sales    rank\
\---------    -------    ----------    ----\
Beer         Wallace           988       1\
Beer         Broders           641       2\
Beer         Cramers           640       3\
Cereal       Broders          1104       1\
Cereal       Cramers          1003       2\
Cereal       Wallace           980       3\
Ice Cream    Broders          2517       1\
Ice Cream    Cramers          2112       2\
Ice Cream    Wallace          1863       3

Notice that category names are ordered and grouped in the category column as a result of PARTITION BY in the OVER clause. Rows for each category are ordered by category unit sales with the rank column displaying the ranking.

Using this table, we can see at a glance how each store ranks in a food category. For instance, Broders tops sales for cereal and ice cream, but Wallace wins in the beer category. You can apply this concept to many other scenarios: for example, for each auto manufacturer, finding the vehicle with the most consumer complaints; figuring out which month had the most rainfall in each of the last 20 years; finding the team with the most wins against left-handed pitchers; and so on.

SQL offers additional window functions. Check the official PostgreSQL documentation at [_https://www.postgresql.org/docs/current/static/tutorial-window.html_](https://www.postgresql.org/docs/current/static/tutorial-window.html) for an overview of window functions, and check [_https://www.postgresql.org/docs/current/static/functions-window.html_](https://www.postgresql.org/docs/current/static/functions-window.html) for a listing of window functions.

#### Calculating Rates for Meaningful Comparisons <a href="#lev166" id="lev166"></a>

As helpful and interesting as they are, rankings based on raw counts aren’t always meaningful; in fact, they can actually be misleading. Consider this example of crime statistics: according to the U.S. Federal Bureau of Investigation (FBI), in 2015, New York City reported about 130,000 property crimes, which included burglary, larceny, motor vehicle thefts, and arson. Meanwhile, Chicago reported about 80,000 property crimes the same year.

So, you’re more likely to find trouble in New York City, right? Not necessarily. In 2015, New York City had more than 8 million residents, whereas Chicago had 2.7 million. Given that context, just comparing the total numbers of property crimes in the two cities isn’t very meaningful.

A more accurate way to compare these numbers is to turn them into rates. Analysts often calculate a rate per 1,000 people, or some multiple of that number, for apples-to-apples comparisons. For the property crimes in this example, the math is simple: divide the number of offenses by the population and then multiply that quotient by 1,000. For example, if a city has 80 vehicle thefts and a population of 15,000, you can calculate the rate of vehicle thefts per 1,000 people as follows:

(80 / 15,000) × 1,000 = 5.3 vehicle thefts per thousand residents

This is easy math with SQL, so let’s try it using select city-level data I compiled from the FBI’s _2015 Crime in the United States_ report available at [_https://ucr.fbi.gov/crime-in-the-u.s/2015/crime-in-the-u.s.-2015/home_](https://ucr.fbi.gov/crime-in-the-u.s/2015/crime-in-the-u.s.-2015/home). [Listing 10-8](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch10.xhtml#ch10list8) contains the code to create and fill a table. Remember to point the script to the location in which you’ve saved the CSV file, which you can download at [_https://www.nostarch.com/practicalSQL/_](https://www.nostarch.com/practicalSQL/).

CREATE TABLE fbi\_crime\_data\_2015 (\
&#x20;   st varchar(20),\
&#x20;   city varchar(50),\
&#x20;   population integer,\
&#x20;   violent\_crime integer,\
&#x20;   property\_crime integer,\
&#x20;   burglary integer,\
&#x20;   larceny\_theft integer,\
&#x20;   motor\_vehicle\_theft integer,\
&#x20;   CONSTRAINT st\_city\_key PRIMARY KEY (st, city)\
);\
\
COPY fbi\_crime\_data\_2015\
FROM '_C:\YourDirectory\\_fbi\_crime\_data\_2015.csv'\
WITH (FORMAT CSV, HEADER, DELIMITER ',');\
\
SELECT \* FROM fbi\_crime\_data\_2015\
ORDER BY population DESC;

_Listing 10-8: Creating and filling a 2015 FBI crime data table_

The fbi\_crime\_data\_2015 table includes the state, city name, and population for that city. Next is the number of crimes reported by police in categories, including violent crime, vehicle thefts, and property crime. To calculate property crimes per 1,000 people in cities with more than 500,000 people and order them, we’ll use the code in [Listing 10-9](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch10.xhtml#ch10list9):

SELECT\
&#x20;   city,\
&#x20;   st,\
&#x20;   population,\
&#x20;   property\_crime,\
&#x20;   round(\
&#x20;     ➊ (property\_crime::numeric / population) \* 1000, 1\
&#x20;       ) AS pc\_per\_1000\
FROM fbi\_crime\_data\_2015\
WHERE population >= 500000\
ORDER BY (property\_crime::numeric / population) DESC;

_Listing 10-9: Finding property crime rates per thousand in cities with 500,000 or more people_

In [Chapter 5](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch05.xhtml#ch05), you learned that when dividing an integer by an integer, one of the values must be a numeric or decimal for the result to include decimal places. We do that in the rate calculation ➊ with PostgreSQL’s double-colon shorthand. Because we don’t need to see many decimal places, we wrap the statement in the round() function to round off the output to the nearest tenth. Then we give the calculated column an alias of pc\_per\_1000 for easy reference. Here’s a portion of the result set:

![image](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492067580/files/images/prog\_page\_168.jpg)

Tucson, Arizona, has the highest rate of property crimes, followed by San Francisco, California. At the bottom is New York City, with a rate that’s one-fourth of Tucson’s. If we had compared the cities based solely on the raw numbers of property crimes, we’d have a far different result than the one we derived by calculating the rate per thousand.

I’d be remiss not to point out that the FBI website at [_https://ucr.fbi.gov/ucr-statistics-their-proper-use/_](https://ucr.fbi.gov/ucr-statistics-their-proper-use/) discourages creating rankings from its crime data, stating that doing so creates “misleading perceptions which adversely affect geographic entities and their residents.” They point out that variations in crimes and crime rates across the country are often due to a number of factors ranging from population density to economic conditions and even the climate. Also, the FBI’s crime data has well-documented short­comings, including incomplete reporting by police agencies.

That said, asking why a locality has higher or lower crime rates than others is still worth pursuing, and rates do provide some measure of comparison despite certain limitations.

#### Wrapping Up <a href="#lev167" id="lev167"></a>

That wraps up our exploration of statistical functions in SQL, rankings, and rates. Now your SQL analysis toolkit includes ways to find relationships among variables using statistics functions, create rankings from ordered data, and properly compare raw numbers by turning them into rates. That toolkit is starting to look impressive!

Next, we’ll dive deeper into date and time data, using SQL functions to extract the information we need.

**TRY IT YOURSELF**

Test your new skills with the following questions:

1. In [Listing 10-2](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch10.xhtml#ch10list2), the correlation coefficient, or _r_ value, of the variables pct\_bachelors\_higher and median\_hh\_income was about .68. Write a query using the same data set to show the correlation between pct\_masters\_higher and median\_hh\_income. Is the _r_ value higher or lower? What might explain the difference?
2. In the FBI crime data, which cities with a population of 500,000 or more have the highest rates of motor vehicle thefts (column motor\_vehicle\_theft)? Which have the highest violent crime rates (column violent\_crime)?
3. As a bonus challenge, revisit the libraries data in the table pls\_fy2014\_pupld14a in [Chapter 8](https://learning.oreilly.com/library/view/practical-sql/9781492067580/xhtml/ch08.xhtml#ch08). Rank library agencies based on the rate of visits per 1,000 population (column popu\_lsa), and limit the query to agencies serving 250,000 people or more.
