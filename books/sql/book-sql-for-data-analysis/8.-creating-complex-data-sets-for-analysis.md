# 8. Creating Complex Data Sets For Analysis

## Chapter 8. Creating Complex Data Sets for Analysis

In Chapters [3](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#time\_series\_analysis) through [7](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch07.html#experiment\_analysis), we looked at a number of ways in which SQL can be used to perform analysis on data in databases. In addition to these specific use cases, sometimes the goal of a query is to assemble a data set that is specific yet general-purpose enough that it can be used to perform a variety of further analyses. The destination might be a database table, a text file, or a business intelligence tool. The SQL that is needed might be simple, requiring only a few filters or aggregations. Often, however, the code or logic needed to achieve the desired data set can become very complex. Additionally, such code is likely to be updated over time, as stakeholders request additional data points or calculations. The organization, performance, and maintainability of your SQL code become critical in a way that isn’t the case for one-time analyses.

In this chapter, I’ll discuss principles for organizing code so that it’s easier to share and update. Then I’ll discuss when to keep query logic in the SQL and when to consider moving to permanent tables via ETL (extract-transform-load) code. Next, I’ll explain the options for storing intermediate results—subqueries, temp tables, and common table expressions (CTEs)—and considerations for using them in your code. Finally, I’ll wrap up with a look at techniques for reducing data set size and ideas for handling data privacy and removing personally identifiable information (PII).

## When to Use SQL for Complex Data Sets

Almost all data sets prepared for further analysis contain some logic. The logic can range from the relatively simple—such as how tables are _JOIN_ed together and how filters are placed in the _WHERE_ clause—to complex calculations that aggregate, categorize, parse, or perform window functions over partitions of the data. When creating data sets for further analysis, choosing whether to keep the logic within the SQL query or to push it upstream to an ETL job or downstream to another tool is often as much art as science. Convenience, performance, and availability of help from engineers all factor into the decision. There is often no single right answer, but you will develop intuition and confidence the longer you work with SQL.

### Advantages of Using SQL

SQL is a very flexible language. Hopefully I convinced you in the earlier chapters that a wide variety of data preparation and analysis tasks can be accomplished using SQL. This flexibility is the main advantage of using SQL when developing complex data sets.

In the initial stages of working with a data set, you may execute many queries. Work often starts with several profiling queries to understand the data. This is followed by building up the query step-by-step, checking transformations and aggregations along the way to be sure that the results returned are correct. This may be interspersed with more profiling, when actual values turn out to differ from our expectations. Complex data sets may be built up by combining several subqueries that answer specific questions with _JOIN_s or _UNION_s. Running a query and examining the output is fast and allows for rapid iteration.

Aside from relying on the quality and timeliness of the data in the tables, SQL has few dependencies. Queries are run on demand and don’t rely on a data engineer or a release process. Queries can often be embedded into business intelligence (BI) tools or into R or Python code by the analyst or data scientist, without requesting technical support. When a stakeholder needs another attribute or aggregation added to the output, changes can be made quickly.

Keeping logic in the SQL code itself is ideal when working on a new analysis and when you expect the logic and result set to undergo changes frequently. Additionally, when the query is fast and data is returned to stakeholders quickly, there may never be a need to move the logic anywhere else.

### When to Build into ETL Instead

There are times when moving logic into an ETL process is a better choice than keeping all of it in a SQL query, especially when working in an organization that has a data warehouse or data lake. The two main reasons to use ETL are performance and visibility.

Performance of SQL queries depends on the complexity of the logic, the size of the tables queried, and the computational resources of the underlying database. Although many queries run fast, particularly on the newer databases and hardware, you will inevitably end up writing some queries that have complex calculations, involve _JOIN_s of large tables or Cartesian _JOIN_s, or otherwise cause query time to slow down to minutes or longer. An analyst or a data scientist may be willing to wait for a query to return. However, most consumers of data are used to websites’ rapid response times and will get frustrated if they have to wait more than a few seconds for data.

ETL runs behind the scenes at scheduled times and writes the result to a table. Since it is behind the scenes, it can run for 30 seconds, five minutes, or an hour, and end users will not be affected. Schedules are often daily but can be set to shorter intervals. End users can query the resulting table directly, without need for _JOIN_s or other logic, and thus experience fast query times.

A good example of when ETL is often a better choice than keeping all of the logic in a SQL query is the daily snapshot table. In many organizations, keeping a daily snapshot of customers, orders, or other entities is useful for answering analytical questions. For customers, we might want to calculate total orders or visits to date, current status in a sales pipeline, and other attributes that either change or accumulate. We’ve seen how to create daily series, including for days when an entity was not present, in the discussions of time series analysis in [Chapter 3](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#time\_series\_analysis) and cohort analysis in [Chapter 4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch04.html#cohort\_analysis). At the individual entity level, and over long time periods, such queries can become slow. Additionally, attributes such as current status may be overwritten in the source table, so capturing a daily snapshot may be the only way to preserve an accurate picture of history. Developing the ETL and storing daily snapshot results in a table are often worth the effort.

Visibility is a second reason to move logic into ETL. Often SQL queries exist on an individual’s computer or are buried within report code. It can be difficult for others to even find the logic embedded in the query, let alone understand and check it for mistakes. Moving logic into ETL and storing the ETL code in a repository such as GitHub makes it easier for others in an organization to find, check, and iterate on it. Most repositories used by development teams also store change history, an additional benefit that allows you to see when a particular line in a query was added or changed.

There are good reasons to consider putting logic into ETL, but this approach also has its drawbacks. One is that fresh results are not available until the ETL job has run and refreshed the data, even if new data has arrived in the underlying table. This can be overcome by continuing to run SQL against the raw data for very new records but limiting it to a small time window so that the query runs quickly. This can optionally be combined with a query on the ETL table, using subqueries or _UNION_. Another drawback to placing logic in ETL is that it becomes harder to change. Updates or bug fixes often need to be handed to a data engineer and code tested, checked into the repository, and released into the production data warehouse. For this reason, I usually opt to wait until my SQL queries are past the period of rapid iteration, and the resulting data sets have been reviewed and are in use by the organization, before moving them into ETL. Of course, making code harder to change and enforcing code reviews are excellent ways to ensure consistency and data quality.

**VIEWS AS AN ALTERNATIVE TO ETL**

When you want the reusability and code visibility of ETL but without actually storing the results and therefore taking up space permanently, a database _view_ can be a good option. A view is essentially a saved query with a permanent alias that can be referenced just like any other table in the database. The query can be simple or complex and may involve table joins, filters, and any other elements of SQL.

Views can be used to ensure that everyone querying the data uses the same definitions, as defined in the underlying query—for example, by always filtering out test transactions. They can be used to shield users from the complexity of the underlying logic, helpful for more novice or occasional users of a database. Views can also be used to provide an extra layer of security by restricting access to certain rows or columns in a database. For example, a view might be created to exclude PII such as email addresses but allow query writers to view other acceptable customer attributes.

Views have a few drawbacks, however. They are objects in the database and thus require permissions to create and update their definitions. Views do not store the data, so each time a query is run with a view, the database must go back to the underlying table or tables to fetch the data. As a result, they are not a replacement for ETL that creates a precomputed table of data.

Most major databases also have _materialized views_, which are similar to views but do store the returned data in a table. Planning for the creation and refreshing of materialized views is usually best done in consultation with an experienced database administrator, as there are nuanced performance considerations beyond the scope of this book.

### When to Put Logic in Other Tools

SQL code and the query results output in your query editor are frequently only part of an analysis. Results are often embedded in reports, visualized into tables and graphs, or further manipulated in a range of tools, from spreadsheets and BI software to environments in which statistical or machine learning code is applied. In addition to choosing when to move logic upstream into ETL, we also have choices about when to move logic downstream into other tools. Both performance and specific use cases are key factors in the decision.

Each type of tool has performance strengths and limitations. Spreadsheets are very flexible but are not known for being able to handle large numbers of rows or complex calculations across many rows. Databases definitely have a performance advantage, so it’s often best to perform as much of the calculation as possible in the database and pass the smallest data set possible on to the spreadsheet.

BI tools have a range of capabilities, so it’s important to understand both how the software handles calculations and how the data will be used. Some BI tools can _cache_ data (keep a local copy) in an optimized format, speeding up calculations. Others issue a new query each time a field is added to or removed from a report and thus mainly leverage the computational power of the database. Certain calculations such as `count distinct` and `median` require detailed, entity-level data. If it’s not possible to anticipate all the variations on these calculations in advance, it may be necessary to pass a larger, more detailed data set than otherwise might be ideal. Additionally, if the goal is to create a data set that allows exploration and slicing in many different ways, more detail is usually better. Figuring out the best combination of SQL, ETL, and BI tool computation can take some iteration.

When the goal is to perform statistical or machine learning analysis on the data set using a language such as R or Python, detailed data is usually better. Both of these languages can perform tasks that overlap with SQL, such as aggregation and text manipulation. It’s often best to perform as much of the calculation as possible in SQL, to leverage the computational power of the database, but no more. Flexibility to iterate is usually an important part of the modeling process. The choice of whether to perform calculations in SQL or another language may also depend on your familiarity and comfort level with each. Those who are very comfortable in SQL may prefer to do more calculations in the database, while those who are more proficient in R or Python may prefer to do more calculations there.

**TIP**

Although there are few rules to deciding where to put logic, I will encourage you to follow one rule in particular: avoid manual steps. It’s easy enough to open a data set in a spreadsheet or text editor, make a small change, save, and move on. But when you need to iterate, or when new data arrives, it’s easy to forget that manual step or to perform it inconsistently. In my experience, there’s no such thing as a truly “one-off” request. Put logic into code somewhere, if at all possible.

SQL is a great tool and is incredibly flexible. It also sits within the analysis workflow and within an ecosystem of tools. Deciding where to put calculations can take some trial and error as you iterate through what is feasible among SQL, ETL, and downstream tools. The more familiarity and experience you have with all the available options, the better you will be able to estimate trade-offs and continue to improve the performance and flexibility of your work.

## Code Organization

SQL has few formatting rules, which can lead to unruly queries. Query clauses must be in the correct order: _SELECT_ is followed by _FROM_, and _GROUP BY_ cannot precede _WHERE,_ for example. A few keywords such as _SELECT_ and _FROM_ are _reserved_ (i.e., they cannot be used as field names, table names, or aliases). However, unlike in some other languages, newlines, whitespace (other than the spaces that separate words), and capitalization are not important and are ignored by the database. Any of the example queries in this book could have been written on a single line, and with or without capital letters, except in quoted strings. As a result, the burden of code organization is on the person writing the query. Fortunately, we have some formal and informal tools for keeping code organized, from commenting to “cosmetic” formatting such as indentation and storage options for files of SQL code.

### Commenting

Most coding languages have a way to indicate that a block of text should be treated as a comment and ignored during execution. SQL has two options. The first is to use two dash marks, which turns everything on the line that follows into a comment:

```
-- This is a comment
```

The second option is to use the slash (/) and star (\*) characters to start a comment block, which can extend over multiple lines, followed by a star and slash to end the comment block:

```
/*
This is a comment block
with multiple lines
*/
```

Many SQL editors adjust the visual style of code inside comments, by graying them out or otherwise changing the color, to make them easier to spot.

Commenting code is a good practice, but admittedly it’s one that many people struggle to do on a regular basis. SQL is often written quickly, and especially during exploration or profiling exercises, we don’t expect to keep our code for the long term. Overly commented code can be just as difficult to read as code with no comments. And we all suffer from the idea that since we wrote the code, we will always be able to remember _why_ we wrote the code. However, anyone who has inherited a long query written by a colleague or has stepped away from a query for a few months and then come back to update it will know that it can be frustrating and time consuming to decipher the code.

To balance the burden and the benefit of commenting, I try to follow a few rules of thumb. First, add a comment anywhere that a value has a nonobvious meaning. Many source systems will encode values as integers, and their meaning is easy to forget. Leaving a note makes the meaning clear and makes the code easier to change if needed:

```
WHERE status in (1,2) -- 1 is Active, 2 is Pending
```

Second, comment on any other nonobvious calculations or transformations. These can be anything that someone who hasn’t spent the time profiling the data set might not know, from data entry errors to the existence of outliers:

```
case when status = 'Live' then 'Active'
     else status end
     /* we used to call customers Live but in
     2020 we switched it to Active */
```

The third practice I try to follow around commenting is to leave notes when the query contains multiple subqueries. A quick line about what each subquery calculates makes it easy to skip to the relevant piece when coming back later to quality check or edit a longer query:

```
SELECT...
FROM
( -- find the first date for each customer
    SELECT ...
    FROM ...
) a
JOIN 
( -- find all of the products for each customer
    SELECT ...
    FROM ...
) b on a.field = b.field
...
;
```

Commenting well takes practice and some discipline, but it’s worth doing for most queries that are longer than a few lines. Commenting can also be used to add useful information to the overall query, such as purpose, author, date created, and so on. Be kind to your colleagues, and to your future self, by placing helpful comments in your code.&#x20;

### Capitalization, Indentation, Parentheses, and Other Formatting Tricks

Formatting, and consistent formatting especially, is a good way to keep SQL code organized and legible. Databases ignore capitalization and whitespace (spaces, tabs, and newlines) in SQL, so we can use these to our advantage to format code into more legible blocks. Parentheses can both control the order of execution, which we will discuss more later, and also visually group calculation elements.

Capitalized words stand out from the rest, as anyone who has received an email with an all-caps subject line can confirm. I like to use capitalization only for the main clauses: _SELECT_, _FROM_, _JOIN_, _WHERE_, and so on. Particularly in long or complex queries, being able to spot these quickly and thus to understand where the _SELECT_ clause ends and the _FROM_ clause begins saves me a lot of time.

Whitespace is another key way to organize and make parts of the query easier to find, and to understand which parts logically go together. Any SQL query could be written on a single line in the editor, but in most cases this would lead to a lot of scrolling left and right through code. I like to start each clause (_SELECT_, _FROM_, etc.) on a new line, which, along with capitalization, helps me keep track of where each one starts and ends. Additionally, I find that putting aggregations on their own lines, as well as functions that take up some space, helps with organization. For CASE statements with more than two WHEN conditions, separating them onto multiple lines is also a good way to easily see and keep track of what is happening in the code. As an example, we can query the `type` and `mag` (magnitude), parse `place`, and then count the records in the `earthquakes` table, with some filtering in the _WHERE_ clause:

```
SELECT type, mag
,case when place like '%CA%' then 'California'
      when place like '%AK%' then 'Alaska'
      else trim(split_part(place,',',2)) 
      end as place
,count(*)
FROM earthquakes
WHERE date_part('year',time) >= 2019
and mag between 0 and 1
GROUP BY 1,2,3
;

type                mag  place       count
------------------  ---  ----------  -----
chemical explosion  0    California  1
earthquake          0    Alaska      160
earthquake          0    Argentina   1
...                 ...  ...         ...
```

Indentation is another trick for keeping code visually organized. Adding spaces or tabs to line up the WHEN items within a CASE statement is one example. You’ve also seen subqueries indented in examples throughout the book. This makes subqueries visually stand apart, and when a query has multiple levels of nested subqueries, it makes it easier to see and understand the order in which they will be evaluated and which subqueries are peers in terms of level:

```
SELECT... 
FROM 
(
    SELECT...
    FROM
    (
        SELECT...
        FROM...
    ) a
    JOIN
    (
        SELECT...
        FROM
    ) b on...
) a
...
;
```

Any number of other formatting choices can be made, and the query will return the same results. Long-term SQL writers tend to have their own formatting preferences. However, clear and consistent formatting makes creating, maintaining, and sharing SQL code much easier.

Many SQL query editors provide some form of query formatting and coloration. Usually keywords are colored, making them easier to spot within a query. These visual clues make both developing and reviewing SQL queries much easier. If you’ve been writing SQL in a query editor all along, try opening a _.sql_ file in a plain text editor to see the difference coloration makes. [Figure 8-1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch08.html#screenshot\_of\_keyword\_coloration\_in\_the) shows an example of a SQL query editor, and the same code is shown in a plain text editor in [Figure 8-2](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch08.html#the\_same\_code\_as\_plain\_text\_in\_the\_text) (please note that these may appear in grayscale in some versions of this book).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0801.png" alt="" height="150" width="600"><figcaption></figcaption></figure>

**Figure 8-1. Screenshot of keyword coloration in the SQL query editor DBVisualizer**

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0802.png" alt="" height="180" width="600"><figcaption></figcaption></figure>

**Figure 8-2. The same code as plain text in the text editor Atom**

Formatting is optional from the database perspective, but it’s a good practice. Consistent use of spacing, capitalization, and other formatting options goes a long way toward keeping your code readable, therefore making it easier to share and maintain.

### Storing Code

After going to the trouble of commenting and formatting code, it’s a good idea to store it somewhere in case you need to use or reference it later.

Many data analysts and scientists work with a SQL editor, often a desktop piece of software. SQL editors are useful because they usually include tools for browsing the database schema alongside a code window. They save files with a _.sql_ extension, and these text files can be opened and changed in any text editor. Files can be saved in local directories or in cloud-based file storage services.

Since they are text, SQL code files are easy to store in change control repositories such as GitHub. Using a repository provides a nice backup option and makes for easy sharing with others. Repositories also track the change history of files, which is useful when you need to figure out when a particular change was made, or when the change history is required for regulatory reasons. The main drawback of GitHub and other tools is that they are usually not a required step in the analysis workflow. You need to remember to update your code periodically, and as with any manual step, it’s easy to forget to do it.

## Organizing Computations

Two related problems we face when creating complex data sets are getting the logic right and getting good query performance. Logic must be correct, or the results will be meaningless. Query performance for analysis purposes, unlike for transactional systems, usually has a range of “good enough.” Queries that don’t return are problematic, but the difference between waiting 30 seconds and waiting a minute for results may not matter a great deal. With SQL, there is often more than one way to write a query that returns the correct results. We can use this to our advantage to both ensure correct logic and tune performance of long-running queries. There are three main ways to organize the calculation of intermediate results in SQL: the subquery, temp tables, and common table expressions (CTEs). Before we dive into them, we’ll review the order of evaluation in SQL. To wrap up the section, I’ll introduce `grouping sets`, which can replace the need to _UNION_ queries together in certain cases.

### Understanding Order of SQL Clause Evaluation

Databases translate SQL code into a set of operations that will be carried out in order to return the requested data. While understanding exactly how this happens isn’t necessary to be good at writing SQL for analysis, understanding the order in which the database will perform its operations is incredibly useful (and is sometimes necessary to debug unexpected results).

**TIP**

Many modern databases have sophisticated query optimizers that consider various parts of the query to come up with the most efficient plan for execution. Although they may consider parts of the query in a different order from that discussed here and therefore may need less query optimization from humans, they won’t calculate intermediate results in a different order from that discussed here.

The general order of evaluation is shown in [Table 8-1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch08.html#sql\_query\_order\_of\_evaluation). SQL queries usually include only a subset of possible clauses, so actual evaluation includes only the steps relevant to the query.

| 1  | <p>FROM<br>including <em>JOIN</em>s and their <em>ON</em> clauses</p> |
| -- | --------------------------------------------------------------------- |
| 2  | WHERE                                                                 |
| 3  | <p>GROUP BY<br>including aggregations</p>                             |
| 4  | HAVING                                                                |
| 5  | Window functions                                                      |
| 6  | SELECT                                                                |
| 7  | DISTINCT                                                              |
| 8  | UNION                                                                 |
| 9  | ORDER BY                                                              |
| 10 | LIMIT and OFFSET                                                      |

First the tables in the _FROM_ clause are evaluated, along with any _JOIN_s. If the _FROM_ clause includes any subqueries, these are evaluated before proceeding to the rest of the steps. In a _JOIN_, the _ON_ clause specifies how the tables are to be _JOIN_ed, which may also filter the result set.

**TIP**

_FROM_ is always evaluated first, with one exception: when the query doesn’t contain a _FROM_ clause. In most databases, it’s possible to query using only a _SELECT_ clause, as seen in some of the examples in this book. A _SELECT_-only query can return system information such as the date and database version. It can also apply mathematical, date, text, and other functions to constants. While there is admittedly little use for such queries in final analyses, they are handy for testing out functions or iterating over tricky calculations rapidly.

Next, the _WHERE_ clause is evaluated to determine which records should be included in further calculations. Note that _WHERE_ falls early in the order of evaluation and so cannot include the results of calculations that happen in a later step.

_GROUP BY_ is calculated next, including the related aggregations such as `count`, `sum`, and `avg`. As you might expect, _GROUP BY_ will include only the values that exist in the _FROM_ tables after any _JOIN_ing and filtering in the _WHERE_ clause.

_HAVING_ is evaluated next. Since it follows _GROUP BY_, _HAVING_ can perform filtering on aggregated values returned by _GROUP BY_. The only other way to filter by aggregated values is to place the query in a subquery and apply the filters in the main query. For example, we might want to find all the states that have at least one thousand terms in the `legislators_terms` table, and we’ll order by terms in descending order for good measure:

```
SELECT state
,count(*) as terms
FROM legislators_terms
GROUP BY 1
HAVING count(*) >= 1000
ORDER BY 2 desc
;

state  terms
-----  -----
NY     4159
PA     3252
OH     2239
...    ...
```

Window functions, if used, are evaluated next. Interestingly, since aggregates have already been calculated at this point, they can be used in the window function definition. For example, in the `legislators` data set from [Chapter 4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch04.html#cohort\_analysis), we could calculate both the terms served per state and the average terms across all states in a single query:

```
SELECT state
,count(*) as terms
,avg(count(*)) over () as avg_terms
FROM legislators_terms
GROUP BY 1
;

state  terms  avg_terms
-----  -----  ---------
ND     170    746.830
NV     177    746.830
OH     2239   746.830
...    ...    ...
```

Aggregates can also be used in the _OVER_ clause, as in the following query that ranks the states in descending order by the total number of terms:

```
SELECT state
,count(*) as terms
,rank() over (order by count(*) desc)
FROM legislators_terms
GROUP BY 1
;

state  terms  rank
-----  -----  ----
NY     4159   1
PA     3252   2
OH     2239   3
...    ...    ...
```

At this point, the _SELECT_ clause is finally evaluated. This is a little counterintuitive since aggregations and window functions are typed in the _SELECT_ section of the query. However, the database has already taken care of the calculations, and the results are then available for further manipulation or for display as is. For example, an aggregation can be placed within a CASE statement and have mathematical, date, or text functions applied if the result of the aggregation is one of those data types.

**TIP**

The aggregators `sum`, `count`, and `avg` return numeric values. However, `min` and `max` functions return the same data type as the input and use the inherent ordering of that data type. For example, `min` and `max` dates return the earliest and latest calendar dates, while `min` and `max` on text fields use alphabetical order to determine the result.

Following _SELECT_ is _DISTINCT_, if present in the query. This means that all the rows are calculated and then deduplication occurs.

_UNION_ (or _UNION ALL_) is performed next. Up to this point, each query that makes up a _UNION_ is evaluated independently. This stage is when the result sets are assembled together into one. This means that the queries can go about their calculations in very different ways or from different data sets. All _UNION_ looks for is the same number of columns and for those columns to have compatible data types.

_ORDER BY_ is almost the last step in evaluation. This means that it can access any of the prior calculations to sort the result set. The only caveat is that if _DISTINCT_ is used, _ORDER BY_ cannot include any fields that are not returned in the _SELECT_ clause. Otherwise, it is entirely possible to order a result set by a field that doesn’t otherwise appear in the query.

_LIMIT_ and _OFFSET_ are evaluated last in the query execution sequence. This ensures that the subset of results returned will have fully calculated results as specified by any of the other clauses that are in the query. This also means that _LIMIT_ has somewhat limited use in controlling the amount of work the database does before the results are returned to you. This is perhaps most noticeable when a query contains a large _OFFSET_ value. In order to _OFFSET_ by, say, three million records, the database still needs to calculate the entire result set, figure out where the three millionth plus one record is, and then return the records specified by the _LIMIT_. This doesn’t mean _LIMIT_ isn’t useful. Checking a few results can confirm calculations without overwhelming the network or your local machine with data. Also, using _LIMIT_ as early in a query as possible, such as in a subquery, can still dramatically reduce the work required by the database as you develop a more complex query.

Now that we have a good understanding of the order in which databases evaluate queries and perform calculations, we’ll turn to some options for controlling these operations in the context of a larger, complex query: subqueries, temporary tables, and CTEs.

### Subqueries

_Subqueries_ are usually the first way we learn how to control the order of evaluation in SQL, or to accomplish calculations that can’t be achieved in a single main query. They are versatile and can help organize long queries into smaller chunks with discrete purposes.

A subquery is enclosed in parentheses, a notation that should be familiar from mathematics, where parentheses also force evaluation of some part of an equation prior to the rest. Within the parentheses is a standalone query that is evaluated before the main outer query. Assuming the subquery is in the _FROM_ clause, the result set can then be queried by the main code, just like any other table. We’ve already seen many examples with subqueries in this book.

An exception to the standalone nature of a subquery is a special type called a _lateral subquery_, which can access results from previous items in the _FROM_ clause. A comma and the keyword _LATERAL_ are used instead of _JOIN_, and there is no _ON_ clause. Instead, a prior query is used inside the subquery. As an example, imagine we wanted to analyze previous party membership for currently sitting legislators. We could find the first year they were a member of a different party, and check how common that is when grouped by their current party. In the first subquery, we find the currently sitting legislators. In the second, lateral subquery, we use the results from the first subquery to return the earliest `term_start` where the party is different from the current party:

```
SELECT date_part('year',c.first_term) as first_year
,a.party
,count(a.id_bioguide) as legislators
FROM
(
    SELECT distinct id_bioguide, party
    FROM legislators_terms
    WHERE term_end > '2020-06-01'
) a,
LATERAL
(
    SELECT b.id_bioguide
    ,min(term_start) as first_term
    FROM legislators_terms b
    WHERE b.id_bioguide = a.id_bioguide
    and b.party <> a.party
    GROUP BY 1
) c
GROUP BY 1,2
;

first_year  party        legislators
----------  ----------   -----------
1979.0      Republican   1
2011.0      Libertarian  1
2015.0      Democrat     1
```

This turns out to be fairly uncommon. Only three current legislators have switched parties, and no party has had more switchers than other parties. There are other ways to return the same result—for example, by changing the query to a _JOIN_ and moving the criteria in the _WHERE_ clause of the second subquery to the _ON_ clause:

```
SELECT date_part('year',c.first_term) as first_year
,a.party
,count(a.id_bioguide) as legislators
FROM
(
    SELECT distinct id_bioguide, party
    FROM legislators_terms
    WHERE term_end > '2020-06-01'
) a
JOIN
(
    SELECT id_bioguide, party
    ,min(term_start) as first_term
    FROM legislators_terms
    GROUP BY 1,2
) c on c.id_bioguide = a.id_bioguide and c.party <> a.party
GROUP BY 1,2
;
```

If the second table is very large, filtering by a value returned in a previous subquery can speed up execution. In my experience, use of _LATERAL_ is less common, and therefore less well understood, than other syntax, so it’s good to reserve it for use cases that can’t be solved efficiently another way.

Subqueries allow a lot of flexibility and control over the order of calculations. However, a complex series of calculations in the middle of a larger query can become difficult to understand and maintain. At other times, the performance of subqueries is too slow, or the query won’t return results at all. Fortunately, SQL has some additional options that may help in these situations: temporary tables and common table expressions.

### Temporary Tables

A _temporary (temp) table_ is created in a similar way to any other table in the database, but with a key difference: it persists only for the duration of the current session. Temp tables are useful when you are working with only a small part of a very large table, as small tables are much faster to query. They are also useful when you want to use an intermediate result in multiple queries. Since the temp table is a standalone table, it can be queried many times within the same session. Yet another time they are useful is when you are working in certain databases, such as Redshift or Vertica, that partition data across nodes. _INSERT_ing data into a temp table can align the partitioning to other tables that will be _JOIN_ed together in a subsequent query. There are two main drawbacks to temp tables. First, they require database privileges to write data, which may not be allowed for security reasons. Second, some BI tools, such as Tableau and Metabase, allow only a single SQL statement to create a data set,[1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch08.html#ch01fn10) whereas a temp table requires at least two: the statement to _CREATE_ and _INSERT_ data into the temp table and the query using the temp table.

To create a temporary table, use the _CREATE_ command, followed by the keyword TEMPORARY and the name you wish to give it. The table can then be defined and a second statement used to populate it, or you can use _CREATE as SELECT_ to create and populate in one step. For example, you could create a temp table with the distinct states for which there have been legislators:

```
CREATE temporary table temp_states
(
state varchar primary key
)
;

INSERT into temp_states
SELECT distinct state
FROM legislators_terms
;
```

The first statement creates the table, while the second statement populates the temp table with values from a query. Note that by defining the table first, I need to specify the data type for all the columns (in this case `varchar` for the `state` column) and can optionally use other elements of table definition, such as setting a primary key. I like to prefix temp table names with “temp\_” or “tmp\_” to remind myself of the fact that I’m using a temp table in the main query, but this isn’t strictly necessary.

The faster and easier way to generate a temp table is the _CREATE as SELECT_ method:

```
CREATE temporary table temp_states
as
SELECT distinct state
FROM legislators_terms
;
```

In this case, the database automatically decides the data type based on the data returned by the _SELECT_ statement, and no primary key is set. Unless you need fine-grained control for performance reasons, this second method will serve you well.

Since temp tables are written to disk, if you need to repopulate them during a session, you will have to _DROP_ and re-create the table or _TRUNCATE_ the data. Disconnecting and reconnecting to the database also works.

### Common Table Expressions

CTEs are a relative newcomer to the SQL language, having been introduced into many of the major databases only during the early 2000s. I wrote SQL for years without them, making do with subqueries and temp tables. I have to say that since I became aware of them a few years ago, they have steadily grown on me.

You can think of a _common table expression_ as being like a subquery lifted out and placed at the beginning of the query execution. It creates a temporary result set that can then be used anywhere in the subsequent query. A query can have multiple CTEs, and CTEs can use results from previous CTEs to perform additional calculations.

CTEs are particularly useful when the result will be used multiple times in the rest of the query. The alternative, defining the same subquery multiple times, is both slow (since the database needs to execute the same query several times) and error-prone. Forgetting to update the logic in each identical subquery introduces error into the final result. Since CTEs are part of a single query, they don’t require any special database permissions. They can also be a useful way to organize code into discrete chunks and avoid sprawling nested subqueries.

The main drawback of CTEs arises from the fact that they are defined at the beginning, separate from where they are used. This can make a query more difficult to decipher for others when the query is very long, as it’s necessary to scroll to the beginning to check the definition and then back to where the CTE is used to understand what is happening. Good use of comments can help with this. A second challenge is that CTEs make execution of sections of long queries more difficult. To check intermediate results in a longer query, it’s fairly easy to select and run just a subquery in a query development tool. If a CTE is involved, however, all of the surrounding code must be commented out first.

To create a CTE, we use the _WITH_ keyword at the beginning of the overall query, followed by a name for the CTE and then the query that makes it up enclosed in parentheses. For example, we could create a CTE that calculates the first term for each legislator and then use this result in further calculation, such as the cohort calculation introduced in [Chapter 4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch04.html#cohort\_analysis):

```
WITH first_term as
(
    SELECT id_bioguide
    ,min(term_start) as first_term
    FROM legislators_terms 
    GROUP BY 1
) 
SELECT date_part('year',age(b.term_start,a.first_term)) as periods
,count(distinct a.id_bioguide) as cohort_retained
FROM first_term a
JOIN legislators_terms b on a.id_bioguide = b.id_bioguide 
GROUP BY 1
;

periods  cohort_retained
-------  ---------------
0.0      12518
1.0      3600
2.0      3619
...      ...
```

The query result is exactly the same as that returned by the alternate query using subqueries seen in [Chapter 4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch04.html#cohort\_analysis). Multiple CTEs can be used in the same query, separated by commas:

```
WITH first_cte as
(
    SELECT...
),
second_cte as
(
    SELECT...
)
SELECT...
;
```

CTEs are a useful way to control the order of evaluation, improve performance in some instances, and organize your SQL code. They are easy to use once you are familiar with the syntax, and they are available in most major databases. There are often multiple ways to accomplish something in SQL, and although not required, CTEs add useful flexibility to your SQL skills toolbox.

### grouping sets

Although this next topic isn’t strictly about controlling the order of evaluation, it is a handy way to avoid _UNION_s and get the database to do all the work in a single query statement. Within the _GROUP BY_ clause, special syntax is available in many major databases that includes `grouping sets`, `cube`, and `rollup` (though Redshift is an exception, and MySQL only has `rollup`). They are useful when the data set needs to contain subtotals for various combinations of attributes.

For examples in this section, we’ll use a data set of video game sales that is [available on Kaggle](https://oreil.ly/qIxRX). It contains attributes for the name of each game as well as the platform, year, genre, and game publisher. Sales figures are provided for North America, the EU, Japan, Other (the rest of the world), and the global total. The table name is `videogame_sales`. [Figure 8-3](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch08.html#sample\_of\_the\_videogame\_sales\_table) shows a sample of the table.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0803.png" alt="" height="201" width="600"><figcaption></figcaption></figure>

**Figure 8-3. Sample of the `videogame_sales` table**

So in the video game data set, for example, we might want to aggregate `global_sales` by platform, genre, and publisher as standalone aggregations (rather than only the combinations of the three fields that exist in the data) but output the results in one query set. This can be accomplished by _UNION_ing together three queries. Note that each query must contain at least placeholders for all three of the grouping fields:

```
SELECT platform
,null as genre
,null as publisher
,sum(global_sales) as global_sales
FROM videogame_sales
GROUP BY 1,2,3
    UNION
SELECT null as platform
,genre
,null as publisher
,sum(global_sales) as global_sales
FROM videogame_sales
GROUP BY 1,2,3
    UNION
SELECT null as platform
,null as genre
,publisher
,sum(global_sales) as global_sales
FROM videogame_sales
GROUP BY 1,2,3
;

platform  genre      publisher        global_sales
--------  ------     ---------        ------------
2600      (null)     (null)           97.08
3DO       (null)     (null)           0.10
...       ...        ...              ... 
(null)    Action     (null)           1751.18
(null)    Adventure  (null)           239.04
...       ...        ...              ... 
(null)    (null)     10TACLE Studios  0.11
(null)    (null)     1C Company       0.10
...       ...        ...              ...
```

This can be achieved in a more compact query using `grouping sets`. Within the _GROUP BY_ clause, `grouping sets` is followed by the list of groupings to calculate. The previous query can be replaced by:

```
SELECT platform, genre, publisher
,sum(global_sales) as global_sales
FROM videogame_sales
GROUP BY grouping sets (platform, genre, publisher)
;

platform  genre      publisher        global_sales
--------  ------     ---------        ------------
2600      (null)     (null)           97.08
3DO       (null)     (null)           0.10
...       ...        ...              ... 
(null)    Action     (null)           1751.18
(null)    Adventure  (null)           239.04
...       ...        ...              ... 
(null)    (null)     10TACLE Studios  0.11
(null)    (null)     1C Company       0.10
...       ...        ...              ...
```

The items inside the `grouping sets` parentheses can include blanks as well as comma-separated lists of columns. As an example, we can calculate the global sales without any grouping, in addition to the groupings by `platform`, `genre`, and `publisher`, by including a list item that is just a pair of parentheses. We’ll also clean up the output by substituting “All” for the null items using `coalesce`:

```
SELECT coalesce(platform,'All') as platform
,coalesce(genre,'All') as genre
,coalesce(publisher,'All') as publisher
,sum(global_sales) as na_sales
FROM videogame_sales
GROUP BY grouping sets ((), platform, genre, publisher)
ORDER BY 1,2,3
;

platform  genre      publisher        global_sales
--------  ------     ---------        ------------
All       All        All              8920.44
2600      All        All              97.08
3DO       All        All              0.10
...       ...        ...              ... 
All       Action     All              1751.18
All       Adventure  All              239.04
...       ...        ...              ... 
All       All        10TACLE Studios  0.11
All       All        1C Company       0.10
...       ...        ...              ...
```

If we want to calculate all possible combinations of platform, genre, and publisher, such as the individual subtotals just calculated, plus all combinations of platform and genre, platform and publisher, and genre and publisher, we could specify all of these combinations in the `grouping sets`. Or we can use the handy `cube` syntax, which handles all of this for us:

```
SELECT coalesce(platform,'All') as platform
,coalesce(genre,'All') as genre
,coalesce(publisher,'All') as publisher
,sum(global_sales) as global_sales
FROM videogame_sales
GROUP BY cube (platform, genre, publisher)
ORDER BY 1,2,3
;

platform  genre      publisher        global_sales
--------  ------     ---------        ------------
All       All        All              8920.44
PS3       All        All              957.84
PS3       Action     All              307.88
PS3       Action     Atari            0.2
All       Action     All              1751.18
All       Action     Atari            26.65
All       All        Atari            157.22
...       ...        ...              ...
```

A third option is the function `rollup`, which returns a data set that has combinations determined by the ordering of fields in the parentheses, rather than all possible combinations. So the previous query with the following clause:

```
GROUP BY rollup (platform, genre, publisher)
```

returns aggregations for the combinations of:

```
platform, genre, publisher
platform, genre
platform
```

But the query does _not_ return aggregations for the combinations of:

```
platform, publisher
genre,publisher
genre
publisher
```

Although it is possible to create the same output using _UNION_, the `grouping sets`, `cube`, and `rollup` options are big space and time savers when aggregations at multiple levels are needed, because they result in fewer lines of code and fewer scans of the underlying database tables. I once created a query hundreds of lines long using _UNION_s to generate output for a dynamic website graphic that needed to have all possible combinations of filters precalculated. Quality checking it was an enormous chore, and updating it was even worse. Leveraging `grouping sets`, and CTEs for that matter, could have gone a long way toward making the code more compact and easy to write and maintain.

## Managing Data Set Size and Privacy Concerns

After taking care to properly work out the logic in our SQL, organize our code, and make it efficient, we’re often faced with another challenge: the size of the result set. Data storage is ever cheaper, meaning that organizations are storing ever-larger data sets. Computational power is also always increasing, allowing us to crunch this data in the sophisticated ways we’ve seen in previous chapters. However, bottlenecks still occur, either in downstream systems such as BI tools or in the bandwidth available to pass large data sets between systems. Additionally, data privacy is a major concern that impacts how we handle sensitive data. For these reasons, in this section I’ll discuss some ways to limit the size of data sets, as well as considerations for data privacy.

### Sampling with %, mod

One way to reduce the size of a result set is to use a sample of the source data. _Sampling_ means taking only a subset of the data points or observations. This is appropriate when the data set is large enough and a subset is representative of the entire population. You can often sample website traffic and still retain most of the useful insights, for example. There are two choices to make when sampling. The first is the size of the sample that achieves the right balance between reducing the size of the data set and not losing too much critical detail. The sample might include 10%, 1%, or 0.1% of the data points, depending on the starting volume. The second choice is the entity on which to perform the sampling. We might sample 1% of website _visits_, but if the goal of the analysis is to understand how users navigate the website, sampling 1% of website _visitors_ would be a better choice in order to preserve all the data points for the users in the sample.

The most common way to sample is to filter query results in the _WHERE_ clause by applying a function to an entity-level identifier. Many ID fields are stored as integers. If this is the case, taking a modulo is a quick way to achieve the right result. The modulo is the whole number remainder when one number is divided by another. For example, 10 divided by 3 is equal to 3 with a remainder (modulo) of 1. SQL has two equivalent ways to find the modulo—with the % sign and with the `mod` function:

```
SELECT 123456 % 100 as mod_100;

mod_100
-------
56

SELECT mod(123456,100) as mod_100;

mod_100
-------
56
```

Both return the same answer, 56, which is also the last two digits of the input value `123456`. To generate a 1% sample of the data set, place either syntax in the _WHERE_ clause and set it equal to an integer—in this case, 7:

```
SELECT user_id, ...
FROM table
WHERE user_id % 100 = 7
;
```

A mod of 100 creates a 1% sample, while a mod of 1,000 would create a 0.1% sample, and a mod of 10 would create a 10% sample. Although sampling in multiples of 10 is common, it’s not required, and any integer will work.

Sampling from alphanumeric identifiers that include both letters and numbers isn’t as straightforward as sampling purely numeric identifiers. String-parsing functions can be used to isolate just the first or last few characters, and filters can be applied to them. For example, we can sample only identifiers ending in the letter “b” by parsing the last character from a string using the `right` function:

```
SELECT user_id, ...
FROM table
WHERE right(user_id,1) = 'b'
;
```

Assuming that any upper- or lowercase letter or number is a possible value, this will result in a sample of approximately 1.6% (1/62). To return a larger sample, adjust the filter to allow multiple values:

```
SELECT user_id, ...
FROM table
WHERE right(user_id,1) in ('b','f','m')
;
```

To create a smaller sample, include multiple characters:

```
SELECT user_id, ...
FROM table
WHERE right(user_id,2) = 'c3'
;
```

**TIP**

When sampling, it’s worth validating that the function you use to generate a sample does create a random or close-to-random sampling of the data. In one of my previous roles, we discovered that certain types of users were more likely to have certain combinations of the last two digits in their user IDs. In this case, using the `mod` function to generate a 1% sample resulted in noticeable bias in the results. Alphanumeric identifiers in particular often have common patterns at the beginning or end of the string that data profiling can help identify.

Sampling is an easy way to reduce data set size by orders of magnitude. It can both speed up calculations within SQL statements and allow the final result to be more compact, making it faster and easier to transfer to another tool or system. Sometimes the loss of detail from sampling isn’t acceptable, however, and other techniques are needed.

### Reducing Dimensionality

The number of distinct combinations of attributes, or _dimensionality_, greatly impacts the number of records in a data set. To understand this, we can do a simple thought experiment. Imagine we have a field with 10 distinct values, and we `count` the number of records and _GROUP BY_ that field. The query will return 10 results. Now add in a second field, also with 10 distinct values, `count` the number of records, and _GROUP BY_ the two fields. The query will return 100 results. Add in a third field with 10 distinct values, and the query result grows to 1,000 results. Even if not all the combinations of the three fields actually exist in the table queried, it’s clear that adding additional fields into a query can increase the size of the results dramatically.

When performing analysis, we can often control the number of fields and filter the values included in order to end up with a manageable output. However, when preparing data sets for further analysis in other tools, the goal is often to provide flexibility and therefore to include many different attributes and calculations. To retain as much detail as possible while managing the overall size of the data, we can use one or more grouping techniques.

Granularity of dates and times is often an obvious place to look to reduce the size of data. Talk to your stakeholders to determine whether daily data is needed, for example, or whether weekly or monthly aggregations would work just as well. Grouping data by month and day of week might be a solution to aggregating data while still providing visibility into patterns that differ on weekdays versus weekends. Restricting the length of time returned is always an option, but that can restrict exploration of longer-term trends. I have seen data teams provide one data set that aggregates to a monthly level and covers several years, while a companion data set includes the same attributes but with daily or even hourly data for a much shorter time window.

Text fields are another place to check for possible space savings. Differences in spelling or capitalization can result in many more distinct values than are useful. Applying text functions discussed in [Chapter 5](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#text\_analysis), such as `lower`, `trim`, or `initcap`, standardizes values and usually makes data more useful for stakeholders as well. REPLACE or CASE statements can be used to make more nuanced adjustments, such as adjusting spelling or changing a name that has been updated to a new value.

Sometimes only a few values out of a longer list are relevant for analysis, so retaining detail for those while grouping the rest together is effective. I have seen this frequently when working with geographic locations. There are close to two hundred countries in the world, but often only a handful have enough customers or other data points to make reporting on them individually worthwhile. The `legislators` data set used in [Chapter 4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch04.html#cohort\_analysis) contains 59 values for state, which includes the 50 states plus US territories that have representatives. We might want to create a data set with detail for the five states with the largest populations (currently California, Texas, Florida, New York, and Pennsylvania), and then group the rest into an “other” category with a CASE statement:

```
SELECT case when state in ('CA','TX','FL','NY','PA') then state 
            else 'Other' end as state_group
,count(*) as terms
FROM legislators_terms
GROUP BY 1
ORDER BY 2 desc
;


state_group  count
-----------  -----
Other        31980
NY           4159
PA           3252
CA           2121
TX           1692
FL           859
```

The query returns only 6 rows, down from 59, which represents a significant decrease. To make the list more dynamic, we can first rank the values in a subquery, in this case by the distinct `id_bioguide` (legislator ID) values, and then return the `state` value for the top 5 and “Other” for the rest:

```
SELECT case when b.rank <= 5 then a.state 
            else 'Other' end as state_group
,count(distinct id_bioguide) as legislators            
FROM legislators_terms a 
JOIN
(
    SELECT state
    ,count(distinct id_bioguide)
    ,rank() over (order by count(distinct id_bioguide) desc) 
    FROM legislators_terms
    GROUP BY 1
) b on a.state = b.state
GROUP BY 1
ORDER BY 2 desc
;

state_group  legislators
-----------  -----------
Other        8317
NY           1494
PA           1075
OH           694
IL           509
VA           451
```

Several of the states change in this second list. If we continue to update the data set with fresh data points, the dynamic query will ensure that the output always reflects the current top values.

Dimensionality can also be reduced by transforming the data into flag values. Flags are usually binary (i.e., they have only two values). BOOLEAN TRUE and FALSE can be used to encode flags, as can 1 and 0, “Yes” and “No,” or any other pair of meaningful strings. Flags are useful when a threshold value is important, but detail beyond that is less interesting. For example, we might want to know whether or not a website visitor completed a purchase, but detail on the exact number of purchases is less important.

In the `legislators` data set, there are 28 distinct numbers of terms served by the legislators. Instead of the exact value, however, we might want to include in our output only whether a legislator has served at least two terms, which we can do by turning the detailed values into a flag:

```
SELECT case when terms >= 2 then true else false end as two_terms_flag
,count(*) as legislators
FROM
(
    SELECT id_bioguide
    ,count(term_id) as terms
    FROM legislators_terms
    GROUP BY 1
) a
GROUP BY 1
;

two_terms_flag  legislators
--------------  -----------
false           4139
true            8379
```

About twice as many legislators have served at least two terms as compared to those with only one term. When combined with other fields in a data set, this type of transformation can result in much smaller result sets.

Sometimes a simple true/false or presence/absence indicator is not quite enough to capture the needed nuance. In this case, numeric data can be transformed into several levels to maintain some additional detail. This is accomplished with a CASE statement, and the return value can be a number or string.

We might want to include not only whether a legislator served a second term but also another indicator for those who served 10 or more terms:

```
SELECT 
case when terms >= 10 then '10+'
     when terms >= 2 then '2 - 9'
     else '1' end as terms_level
,count(*) as legislators
FROM
(
    SELECT id_bioguide
    ,count(term_id) as terms
    FROM legislators_terms
    GROUP BY 1
) a
GROUP BY 1
;
```

```
terms_level  legislators
-----------  -----------
1            4139
2 - 9        7496
10+          883
```

Here we have reduced 28 distinct values down to 3, while retaining the notion of single-term legislators, those who were reelected, and the ones who are exceptionally good at staying in office. Such groupings or distinctions occur in many domains. As with all the transformations discussed here, it may take some trial and error to find the exact thresholds that are most meaningful for stakeholders. Finding the right balance of detail and aggregation can greatly decrease data set size and therefore often speeds delivery time and performance of the downstream application.

### PII and Data Privacy

Data privacy is one of the most important issues facing data professionals today. Large data sets with many attributes allow for more robust analysis with detailed insights and recommendations. However, when the data set is about individuals, we need to be mindful of both the ethical and the regulatory dimensions of the data collected and used. Regulations around the privacy of patients, students, and financial services customers have existed for many years. Laws regulating the data privacy rights of consumers have also come into force in recent years. The General Data Protection Regulation (GDPR) passed by the EU is probably the most widely known. Other regulations include the California Consumer Privacy Act (CCPA), the Australian Privacy Principles, and Brazil’s General Data Protection Law (LGPD).

These and other regulations cover the handling, storage, and (in some cases) deletion of _personally identifiable information (PII)_. Some categories of PII are obvious: name, address, email, date of birth, and Social Security number. PII also includes health indicators such as heart rate, blood pressure, and medical diagnoses. Location information, such as GPS coordinates, is also considered PII, since a small number of GPS locations can uniquely identify an individual. For example, GPS readings at my house and at my children’s school could uniquely identify someone in my household. A third GPS point at my office could uniquely identify me. As a data practitioner, it’s worthwhile to become familiar with what these regulations cover and to discuss how they affect your work with the privacy lawyers at your organization, who will have the most up-to-date information.

A best practice when analyzing data that includes PII is to avoid including the PII itself in the outputs. This can be accomplished by aggregating data, substituting values, or hashing values.

For most analyses, the goal is to find trends and patterns. Counting customers and averaging their behavior, rather than including individual detail in the output, is often the purpose. Aggregations generally remove PII; however, be aware that a combination of attributes that have a user count of 1 could potentially be tied back to an individual. These can be treated as outliers and removed from the result in order to maintain a higher degree of privacy.

If individual data is needed for some reason—to be able to calculate distinct users in a downstream tool, for instance—we can replace problematic values with random alternate values that maintain uniqueness. The `row_number` window function can be used to assign a new value to each individual in a table:

```
SELECT email
,row_number() over (order by ...) 
FROM users
;
```

The challenge in this case is to find a field to put in the _ORDER BY_ that makes the ordering sufficiently random such that we can consider the resulting user identifier anonymized.

Hashing values is another option. Hashing takes an input value and uses an algorithm to create a new output value. A particular input value will always result in the same output, making this a good option for maintaining uniqueness while obscuring sensitive values. The `md5` function can be used to generate a hashed value:

```
SELECT md5('my info');

md5
--------------------------------
0fb1d3f29f5dd1b7cabbad56cf043d1a
```

**WARNING**

The `md5` function hashes input values but does not encrypt them, and therefore it can be reversed to obtain the original value. For highly sensitive data, you should work with a database administrator to truly encrypt the data.

Avoiding PII in the output of your SQL queries is always the best option if possible, since you avoid proliferating it into other systems or files. Replacing or masking the values is a second-best option. You can also explore secure methods to share data, such as developing a secured data pipeline directly between a database and an email system to avoid writing email addresses out to files, for example. With care and partnership with technical and legal colleagues, it is possible to achieve high-quality analysis while also preserving individuals’ privacy.

## Conclusion

Surrounding every analysis, there are a number of decisions to be made around organizing the code, managing complexity, optimizing query performance, and safeguarding privacy in the output. In this chapter, we’ve discussed a number of options and strategies and special SQL syntax that can help with these tasks. Try not to get overwhelmed by all of these options or to become concerned that, without mastery of these topics, you can’t be an efficient data analyst or data scientist. Not all of the techniques are required in every analysis, and there are often other ways to get the job done. The longer you spend analyzing data with SQL, the more likely you are to come across situations in which one or more of these techniques come in handy.

[1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch08.html#ch01fn10-marker) Though in the case of Tableau, you can get around this with the Initial SQL option.
