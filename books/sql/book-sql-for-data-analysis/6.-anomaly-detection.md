# 6. Anomaly Detection

## Chapter 6. Anomaly Detection

An _anomaly_ is something that is different from other members of the same group. In data, an anomaly is a record, an observation, or a value that differs from the remaining data points in a way that raises concerns or suspicions. Anomalies go by a number of different names, including _outliers_, _novelties_, _noise_, _deviations_, and _exceptions_, to name a few. I’ll use the terms _anomaly_ and _outlier_ interchangeably throughout this chapter, and you may see the other terms used in discussions of this topic as well. Anomaly detection can be the end goal of an analysis or a step within a broader analysis project.

Anomalies typically have one of two sources: real events that are extreme or otherwise unusual, or errors introduced during data collection or processing. While many of the steps used to detect outliers are the same regardless of the source, how we choose to handle a particular anomaly depends on the root cause. As a result, understanding the root cause and distinguishing between the two types of causes is important to the analysis process.

Real events can generate outliers for a variety of reasons. Anomalous data can signal fraud, network intrusion, structural defects in a product, loopholes in policies, or product use that wasn’t intended or envisioned by the developers. Anomaly detection is widely used to root out financial fraud, and cybersecurity also makes use of this type of analysis. Sometimes anomalous data results not because a bad actor is trying to exploit a system but because a customer is using a product in an unexpected way. For example, I knew someone who used a fitness-tracking app, which was intended for running, cycling, walking, and similar activities, to record data from his outings at the auto race track. He hadn’t found a better option and wasn’t thinking about how anomalous the speed and distance values for a car on a track are compared to those recorded for bike rides or running. When anomalies can be tracked to a real process, deciding what to do with them requires a good understanding of the analysis to be done, as well as domain knowledge, terms of use, and sometimes the legal system that governs the product.

Data can also contain anomalies because of errors in collection or processing. Manually entered data is notorious for typos and incorrect data. Changes to forms, fields, or validation rules can introduce unexpected values, including nulls. Behavior tracking of web and mobile applications is common; however, any change to how and when this logging is done can introduce anomalies. I’ve spent enough hours diagnosing changes in metrics that I’ve learned to ask up front whether any logging was recently changed. Data processing can introduce outliers when some values are filtered erroneously, processing steps fail to complete, or data is loaded multiple times, creating duplicates. When anomalies result from data processing, we can generally be more confident in correcting or discarding those values. Of course, fixing the upstream data entry or processing is always a good idea, if possible, to prevent future quality problems.

In this chapter, I’ll first discuss some of the reasons to use SQL for this type of analysis and places in which it falls short. Then I’ll introduce the earthquakes data set that will be used in the examples in the rest of the chapter. After that, I’ll introduce the basic tools that we have at our disposal in SQL for detecting outliers. Then I’ll discuss the various forms of outliers that we can apply the tools to find. Once we’ve detected and understood anomalies, the next step is to decide what to do with them. Anomalies need not always be problematic, as they are in fraud detection, cyberattack detection, and health system monitoring. The techniques in this chapter can also be used to detect unusually good customers or marketing campaigns, or positive shifts in customer behavior. Sometimes the goal of anomaly detection is to pass the anomalies on to other humans or machines to deal with, but often this is a step in a wider analysis, so I’ll wrap up with various options for correcting anomalies.

## Capabilities and Limits of SQL for Anomaly Detection

SQL is a versatile and powerful language for many data analysis tasks, though it can’t do everything. When performing anomaly detection, SQL has a number of strengths, as well as some drawbacks that make other languages or tools better choices for some tasks.

SQL is worth considering when the data set is already in a database, as we previously saw with time series and text analysis in Chapters [3](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#time\_series\_analysis) and [5](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#text\_analysis), respectively. SQL leverages the computational power of the database to perform calculations over many records quickly. Particularly with large tables of data, transferring out of a database and into another tool is time consuming. Working within a database makes even more sense when anomaly detection is a step in a larger analysis that will be done in SQL. Code written in SQL can be examined to understand why particular records were flagged as outliers, and SQL will remain consistent over time even as the data flowing into a database changes.

On the negative side, SQL does not have the statistical sophistication that is available in packages developed for languages like R and Python. SQL has several standard statistical functions, but additional, more complex statistical calculations may be too slow or intense for some databases. For use cases requiring very rapid response, such as fraud or intrusion detection, analyzing data in a database may simply not be appropriate, since there is often lag in loading data, particularly to analytics databases. A common workflow is to use SQL to do the initial analysis and determine typical minimum, maximum, and average values and then develop more real-time monitoring using a streaming service or special real-time data stores. Detecting types of outlier patterns and then implementing in streaming services or special real-time data stores can be an option, however. Finally, SQL code is rule based, as we saw in [Chapter 5](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#text\_analysis). It is very good for handling a known set of conditions or criteria, but SQL will not automatically adjust for the types of changing patterns seen with rapidly changing adversaries. Machine learning approaches, and the languages associated with them, are often a better choice for these applications.

Now that we’ve discussed the advantages of SQL and when to use it instead of another language or tool, let’s take a look at the data we’ll be using for examples in this chapter before moving on to the code itself.

## The Data Set

The data for the examples in this chapter is a set of records for all earthquakes recorded by the US Geological Survey (USGS) from 2010 to 2020. The USGS provides the data in a number of formats, including real-time feeds, at [_https://earthquake.usgs.gov/earthquakes/feed_](https://earthquake.usgs.gov/earthquakes/feed).

The data set contains approximately 1.5 million records. Each record represents a single earthquake event and includes information such as the timestamp, location, magnitude, depth, and source of the information. A sample of the data is shown in [Figure 6-1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#sample\_of\_the\_earthquakes\_data). A full [data dictionary](https://oreil.ly/NjgCt) is available on the USGS site.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0601.png" alt="" height="279" width="600"><figcaption></figcaption></figure>

**Figure 6-1. Sample of the `earthquakes` data**

Earthquakes are caused by sudden slips along faults in the tectonic plates that exist on the outer surface of the earth. Locations on the edges of these plates experience many more, and more dramatic, earthquakes than other places. The so-called Ring of Fire is a region along the rim of the Pacific Ocean in which many earthquakes occur. Various locations within this region, including California, Alaska, Japan, and Indonesia, will appear frequently in our analysis.

_Magnitude_ is a measure of the size of an earthquake at its source, as measured by its seismic waves. Magnitude is recorded on a logarithmic scale, meaning that the amplitude of a magnitude 5 earthquake is 10 times that of a magnitude 4 earthquake. The actual measurement of earthquakes is fascinating but beyond the scope of this book. The [USGS website](https://earthquake.usgs.gov/) is a good place to start if you want to learn more.

## Detecting Outliers

Although the idea of an anomaly or outlier—a data point that is very different from the rest—seems straightforward, actually finding one in any particular data set poses some challenges. The first challenge has to do with knowing when a value or data point is common or rare, and the second is setting a threshold for marking values on either side of this dividing line. As we go through the `earthquakes` data, we’ll profile the depths and magnitudes in order to develop an understanding of which values are normal and which are unusual.

Generally, the larger or more complete the data set, the easier it is to make a judgment on what is truly anomalous. In some instances, we have labeled or “ground truth” values to which we can refer. A label is generally a column in the data set that indicates whether the record is normal or an outlier. Ground truth can be obtained from industry or scientific sources or from past analysis and might tell us, for example, that any earthquake greater than magnitude 7 is an anomaly. In other cases, we must look to the data itself and apply reasonable judgment. For the remainder of the chapter, we’ll assume that we have a large enough data set to do just that, though of course there are outside references we could consult on typical and extreme earthquake magnitudes.

Our tools for detecting outliers using the data set itself fall into a few categories. First, we can sort or _ORDER BY_ the values in the data. This can optionally be combined with various _GROUP BY_ clauses to find outliers by frequency. Second, we can use SQL’s statistical functions to find extreme values at either end of a value range. Finally, we can graph data and inspect it visually.

### Sorting to Find Anomalies

One of the basic tools we have for finding outliers is sorting the data, accomplished with the _ORDER BY_ clause. The default behavior of _ORDER BY_ is to sort ascending (_ASC_). To sort in descending order, add _DESC_ after the column. An _ORDER BY_ clause can include one or more columns, and each column can be sorted ascending or descending, independently of the others. Sorting starts with the first column specified. If a second column is specified, the results of the first sort are then sorted by the second column (retaining the first sort), and so on through all the columns in the clause.

**TIP**

Since ordering happens after the database has calculated the rest of the query, many databases allow you to reference the query columns by number instead of by name. SQL Server is an exception; it requires the full name. I prefer the numbering syntax because it results in more compact code, particularly when query columns include lengthy calculations or function syntax.

For example, we can sort the `earthquakes` table by `mag,` the magnitude:

```
SELECT mag
FROM earthquakes
ORDER BY 1 desc
;

mag
------
(null)
(null)
(null)
...
```

This returns a number of rows of nulls. Let’s make a note that the data set can contain null values for magnitude—a possible outlier in itself. We can exclude the null values:

```
SELECT mag
FROM earthquakes
WHERE mag is not null
ORDER BY 1 desc
;

mag
---
9.1
8.8
8.6
8.3
```

There is only one value greater than 9, and there are only two additional values greater than 8.5. In many contexts, these would not appear to be particularly large values. However, with a little domain knowledge about earthquakes, we can recognize that these values are in fact both very large and unusual. The USGS provides a list of the [20 largest earthquakes in the world](https://oreil.ly/gHUhy). All of them are magnitude 8.4 or larger, while only five are magnitude 9.0 or larger, and three occurred between 2010 and 2020, the time period covered by our data set.

Another way to consider whether values are anomalies within a data set is to calculate their frequency. We can `count` the `id` field and _GROUP BY_ the `mag` to find the number of earthquakes per magnitude. The number of earthquakes per magnitude is then divided by the total number of earthquakes, which can be found using a `sum` window function. All window functions require an _OVER_ clause with a _PARTITION BY_ and/or _ORDER BY_ clause. Since the denominator should count all the records, I have added a _PARTITION BY_ 1, which is a way to force the database to make it a window function but still read from the entire table. Finally, the result set is _ORDER_ed _BY_ the magnitude:

```
SELECT mag
,count(id) as earthquakes
,round(count(id) * 100.0 / sum(count(id)) over (partition by 1),8) 
 as pct_earthquakes
FROM earthquakes
WHERE mag is not null
GROUP BY 1
ORDER BY 1 desc
;

mag  earthquakes  pct_earthquakes
---  -----------  ---------------
9.1  1            0.00006719
8.8  1            0.00006719
8.6  1            0.00006719
8.3  2            0.00013439
...  ...          ... 
6.9  53           0.00356124
6.8  45           0.00302370
6.7  60           0.00403160
...  ...          ...
```

There is only one each of the earthquakes that are over 8.5 in magnitude, but there are two that registered 8.3. By the value 6.9, there are double digits of earthquakes, but those still represent a very small percentage of the data. In our investigation, we should also check the other end of the sorting, the smallest values, by sorting ascending instead of descending:

```
SELECT mag
,count(id) as earthquakes
,round(count(id) * 100.0 / sum(count(id)) over (partition by 1),8) 
 as pct_earthquakes
FROM earthquakes
WHERE mag is not null
GROUP BY 1
ORDER BY 1
;

mag   earthquakes  pct_earthquakes
---    -----------  ---------------
-9.99  258          0.01733587
-9     29           0.00194861
-5     1            0.00006719
-2.6   2            0.00013439
...    ...          ...
```

At the low end of values, –9.99 and –9 occur more frequently than we might expect. Although we can’t take the logarithm of zero or a negative number, a logarithm can be negative when the argument is greater than zero and less than one. For example, log(0.5) is equal to approximately –0.301. The values –9.99 and –9 represent extremely small earthquake magnitudes, and we might question whether such small quakes could really be detected. Given the frequency of these values, I suspect they represent an unknown value rather than a truly tiny earthquake, and thus we may consider them anomalies.

In addition to sorting the overall data, it can be useful to _GROUP BY_ one or more attribute fields to find anomalies within subsets of the data. For example, we might want to check the highest and lowest magnitudes recorded for specific geographies in the `place` field:

```
SELECT place, mag, count(*)
FROM earthquakes
WHERE mag is not null
 and place = 'Northern California'
GROUP BY 1,2
ORDER BY 1,2 desc
;

place                mag   count
-------------------  ----  -----
Northern California  5.61
Northern California  4.73  1
Northern California  4.51  1
...                  ...   ...
Northern California  -1.1  7
Northern California  -1.2  2
Northern California  -1.6  1
```

“Northern California” is the most common `place` in the data set, and inspecting just the subset for it, we can see that the high and low values are not nearly as extreme as those for the data set as a whole. Earthquakes over 5.0 magnitude are not uncommon overall, but they are outliers for “Northern California.”

### Calculating Percentiles and Standard Deviations to Find Anomalies

Sorting and optionally grouping data and then reviewing the results visually is a useful approach for spotting anomalies, particularly when the data has values that are very extreme. Without domain knowledge, however, it might not be obvious that a 9.0 magnitude earthquake is such an anomaly. Quantifying the extremity of data points adds another layer of rigor to the analysis. There are two ways to do this: with percentiles or with standard deviations.

Percentiles represent the proportion of values in a distribution that are less than a particular value. The median of a distribution is the value at which half of the population has a lower value and half has a higher value. The median is so commonly used that it has its own SQL function, `median`, in many but not all databases. Other percentiles can be calculated as well. For example, we can find the 25th percentile, where 25% of the values are lower and 75% are higher, or the 89th percentile, where 89% of values are lower and 11% are higher. Percentiles are often found in academic contexts, such as standardized testing, but they can be applied to any domain.

SQL has a window function, `percent_rank`, that returns the percentile for each row within a partition. As with all window functions, the sorting direction is controlled with an _ORDER BY_ statement. Similar to the `rank` function, `percent_rank` does not take any argument; it operates over all the rows returned by the query. The basic form is:

```
percent_rank() over (partition by ... order by ...)
```

Both the _PARTITION BY_ and the _ORDER BY_ are optional, but the function requires something in the _OVER_ clause, and specifying the ordering is always a good idea. To find the percentile of the magnitudes of each earthquake for each place, we can first calculate the `percent_rank` for each row in the subquery and then count the occurrences of each magnitude in the outer query. Note that it’s important to calculate the `percent_rank` first, before doing any aggregation, so that repeating values are taken into account in the calculation:

```
SELECT place, mag, percentile
,count(*)
FROM
(
    SELECT place, mag
    ,percent_rank() over (partition by place order by mag) as percentile
    FROM earthquakes
    WHERE mag is not null
    and place = 'Northern California'
) a
GROUP BY 1,2,3
ORDER BY 1,2 desc
;

place                mag   percentile             count
-------------------  ----  ---------------------  -----  
Northern California  5.6   1.0                    1
Northern California  4.73  0.9999870597065141     1
Northern California  4.51  0.9999741194130283     1
...                  ...   ...                    ...
Northern California  -1.1  3.8820880457568775E-5  7
Northern California  -1.2  1.2940293485856258E-5  2
Northern California  -1.6  0.0                    1
```

Within Northern California, the magnitude 5.6 earthquake has a percentile of 1, or 100%, indicating that all of the other values are less than this one. The magnitude –1.6 earthquake has a percentile of 0, indicating that no other data points are smaller.

In addition to finding the exact percentile of each row, SQL can carve the data set into a specified number of buckets and return the bucket each row belongs to with a function called `ntile`. For example, we might want to carve the data set up into 100 buckets:

```
SELECT place, mag
,ntile(100) over (partition by place order by mag) as ntile
FROM earthquakes
WHERE mag is not null
and place = 'Central Alaska'
ORDER BY 1,2 desc
;

place           mag  ntile
--------------  ----  -----  
Central Alaska  5.4   100
Central Alaska  5.3   100
Central Alaska  5.2   100
...             ...   ...  
Central Alaska  1.5   79
...             ...   ...    
Central Alaska  -0.5  1
Central Alaska  -0.5  1
Central Alaska  -0.5  1
```

Looking at the results for “Central Alaska,” we see that the three earthquakes greater than 5 are in the 100th percentile, 1.5 falls within the 79th percentile, and the smallest values of –0.5 fall in the first percentile. After calculating these values, we can then find the boundaries of each ntile, using `max` and `min`. For this example, we’ll use four ntiles to keep the display simpler, but any positive integer is allowed in the `ntile` argument:

```
SELECT place, ntile
,max(mag) as maximum
,min(mag) as minimum
FROM
(
    SELECT place, mag
    ,ntile(4) over (partition by place order by mag) as ntile
    FROM earthquakes
    WHERE mag is not null
    and place = 'Central Alaska'
) a
GROUP BY 1,2
ORDER BY 1,2 desc
;

place           ntile  maximum  minimum
--------------  -----  -------  -------
Central Alaska  4      5.4      1.4
Central Alaska  3      1.4      1.1
Central Alaska  2      1.1      0.8
Central Alaska  1      0.8      -0.5
```

The highest ntile, 4, which represents the 75th to 100th percentiles, has the widest range, spanning from 1.4 to 5.4. On the other hand, the middle 50 percent of values, which include ntiles 2 and 3, range only from 0.8 to 1.4.

In addition to finding the percentile or ntile for each row, we can calculate specific percentiles across the entire result set of a query. To do this, we can use the `percentile_cont` function or the `percentile_disc` function. Both are window functions, but with a slightly different syntax than other window functions discussed previously because they require a _WITHIN GROUP_ clause. The form of the functions is:

```
percentile_cont(numeric) within group (order by field_name) over (partition by
field_name)
```

The numeric is a value between 0 and 1 that represents the percentile to return. For example, 0.25 returns the 25th percentile. The _ORDER BY_ clause specifies the field to return the percentile from, as well as the ordering. _ASC_ or _DESC_ can optionally be added, with _ASC_ the default, as in all _ORDER BY_ clauses in SQL. The _OVER_ (_PARTITION BY_...) clause is optional (and confusingly, some databases don’t support it, so check your documentation if you run into errors).

The `percentile_cont` function will return an interpolated (calculated) value that corresponds to the exact percentile but that may not exist in the data set. The `percentile_disc` (discontinuous percentile) function, on the other hand, returns the value in the data set that is closest to the requested percentile. For large data sets, or for ones with fairly continuous values, there is often little practical difference between the output of the two functions, but it’s worth considering which is more appropriate for your analysis. Let’s take a look at an example to see how this looks in practice. We’ll calculate the 25th, 50th (or median), and 75th percentile magnitudes for all nonnull magnitudes in Central Alaska:

```
SELECT 
percentile_cont(0.25) within group (order by mag) as pct_25
,percentile_cont(0.5) within group (order by mag) as pct_50
,percentile_cont(0.75) within group (order by mag) as pct_75
FROM earthquakes
WHERE mag is not null
and place = 'Central Alaska'
;

pct_25  pct_50  pct_75
------  ------  ------
0.8     1.1     1.4
```

The query returns the requested percentiles, summarized across the data set. Notice that the values correspond to the maximum values for ntiles 1, 2, and 3 calculated in the previous example. Percentiles for different fields can be calculated within the same query by changing the field in the _ORDER BY_ clause:

```
SELECT 
percentile_cont(0.25) within group (order by mag) as pct_25_mag
,percentile_cont(0.25) within group (order by depth) as pct_25_depth
FROM earthquakes
WHERE mag is not null
and place = 'Central Alaska'
;

pct_25_mag  pct_25_depth
----------  ------------
0.8         7.1
```

Unlike other window functions, `percentile_cont` and `percentile_disc` require a _GROUP BY_ clause at the query level when other fields are present in the query. For example, if we want to consider two areas within Alaska, and so include the `place` field, the query must also include it in the _GROUP BY_, and the percentiles are calculated per `place`:

```
SELECT place
,percentile_cont(0.25) within group (order by mag) as pct_25_mag
,percentile_cont(0.25) within group (order by depth) as pct_25_depth
FROM earthquakes
WHERE mag is not null
and place in ('Central Alaska', 'Southern Alaska')
GROUP BY place
;

place            pct_25_mag  pct_25_depth
---------------  ----------  ------------
Central Alaska   0.8         7.1
Southern Alaska  1.2         10.1
```

With these functions, we can find any percentile required for analysis. Since the median value is so commonly calculated, a number of databases have implemented a `median` function that has only one argument, the field for which to calculate the median. This is a handy and certainly much simpler syntax, but note that the same can be accomplished with `percentile_cont` if a `median` function is not available.

**TIP**

The `percentile` and `median` functions can be slow and computationally intensive on large data sets. This is because the database must sort and rank all the records, usually in memory. Some database vendors have implemented approximate versions of the functions, such as `approximate_percentile`, that are much faster and return results very close to the function that calculates the entire data set.

Finding the percentiles or ntiles of a data set allows us to add some quantification to anomalies. We’ll see later in the chapter how these values also give us some tools for handling anomalies in data sets. Since percentiles are always scaled between 0 and 100, however, they don’t give a sense of just how unusual certain values are. For that we can turn to additional statistical functions supported by SQL.

To measure how extreme values in a data set are, we can use the _standard deviation_. The standard deviation is a measure of the variation in a set of values. A lower value means less variation, while a higher number means more variation. When data is normally distributed around the mean, about 68% of the values lie within +/– one standard deviation from the mean, and about 95% lie within two standard deviations. The standard deviation is calculated as the square root of the sum of differences from the mean, divided by the number of observations:

∑(��-�)2/�

In this formula, _xi_ is an observation, μ is the average of all the observations, ∑ indicates that all of the values should be summed, and _N_ is the number of observations. Refer to any good statistics text or online resource[1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#ch01fn8) for more information about how the standard deviation is derived.

Most databases have three standard deviation functions. The `stddev_pop` function finds the standard deviation of a population. If the data set represents the entire population, as is often the case with a customer data set, use the `stddev_pop`. The `stddev_samp` finds the standard deviation of a sample and differs from the above formula by dividing by _N_ – 1 instead of _N_. This has the effect of increasing the standard deviation, reflecting the loss of accuracy when only a sample of the entire population is used. The `stddev` function available in many databases is identical to the `stddev_samp` function and may be used simply because it is shorter. If you’re working with data that is a sample, such as from a survey or study from a larger population, use the `stddev_samp` or `stddev`. In practice, when you are working with large data sets, there is usually little difference between the `stddev_pop` and `stddev_samp` results. For example, across the 1.5 million records in the `earthquakes` table, the values diverge only after five decimal places:

```
SELECT stddev_pop(mag) as stddev_pop_mag
,stddev_samp(mag) as stddev_samp_mag
FROM earthquakes
;

stddev_pop_mag        stddev_samp_mag
--------------------  --------------------
1.273605805569390395  1.273606233458381515
```

These differences are small enough that in most practical applications, it doesn’t matter which standard deviation function you use.

With this function, we can now calculate the number of standard deviations from the mean for each value in the data set. This value is known as the _z-score_ and is a way of standardizing data. Values that are above the average have a positive z-score, and those below the average have a negative z-score. [Figure 6-2](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#standard\_deviations\_and\_z\_scores\_for\_a) shows how z-scores and standard deviations relate to the normal distribution.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0602.png" alt="" height="396" width="600"><figcaption></figcaption></figure>

**Figure 6-2. Standard deviations and z-scores for a normal distribution**

To find the z-scores for the earthquakes, first calculate the average and standard deviation for the entire data set in a subquery. Then _JOIN_ this back to the data set using a Cartesian _JOIN_, so that the average and standard deviation values are _JOIN_ed to each earthquake row. This is accomplished with the `1 = 1` syntax, since most databases require that some _JOIN_ condition be specified.

In the outer query, subtract the average magnitude from each individual magnitude and then divide by the standard deviation:

```
SELECT a.place, a.mag
,b.avg_mag, b.std_dev
,(a.mag - b.avg_mag) / b.std_dev as z_score
FROM earthquakes a
JOIN
(
    SELECT avg(mag) as avg_mag
    ,stddev_pop(mag) as std_dev
    FROM earthquakes
    WHERE mag is not null
) b on 1 = 1
WHERE a.mag is not null
ORDER BY 2 desc
;


place                                   mag  avg_mag  std_dev  z_score
--------------------------------------  ---  -------  -------  -------
2011 Great Tohoku Earthquake, Japan     9.1   1.6251   1.2736  5.8691
offshore Bio-Bio, Chile                 8.8   1.6251   1.2736  5.6335
off the west coast of northern Sumatra  8.6   1.6251   1.2736  5.4765
...                                     ...   ...      ...     ...
Nevada                                  -2.5  1.6251   1.2736  -3.2389
Nevada                                  -2.6  1.6251   1.2736  -3.3174
Nevada                                  -2.6  1.6251   1.2736  -3.3174
```

The largest earthquakes have a z-score of almost 6, whereas the smallest (excluding the –9 and –9.99 earthquakes that appear to be data entry anomalies) have z-scores close to 3. We can conclude that the largest earthquakes are more extreme outliers than the ones at the low end.

### Graphing to Find Anomalies Visually

In addition to sorting the data and calculating percentiles and standard deviations to find anomalies, visualizing the data in one of several graph formats can also help in finding anomalies. As we’ve seen in previous chapters, one strength of graphs is their ability to summarize and present many data points in a compact form. By inspecting graphs, we can often spot patterns and outliers that we might otherwise miss if only considering the raw output. Finally, graphs assist in the task of describing the data, and any potential problems with the data related to anomalies, to other people.

In this section, I’ll present three types of graphs that are useful for anomaly detection: bar graphs, scatter plots, and box plots. The SQL needed to generate output for these graphs is straightforward, though you might need to enlist pivoting strategies discussed in previous chapters, depending on the capabilities and limitations of the software used to create the graphs. Any major BI tool or spreadsheet software, or languages such as Python or R, will be able to produce these graph types. The graphs in this section were created using Python with Matplotlib.

The _bar graph_ is used to plot a histogram or distribution of the values in a field and is useful for both characterizing the data and spotting outliers. The full extent of values are plotted along one axis, and the number of occurrences of each value is plotted on the other axis. The extreme high and low values are interesting, as is the shape of the plot. We can quickly determine whether the distribution is approximately normal (symmetric around a peak or average value), has another type of distribution, or has peaks at particular values.

To graph a histogram for the earthquake magnitudes, first create a data set that groups the magnitudes and counts the earthquakes. Then plot the output, as in [Figure 6-3](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#distribution\_of\_earthquake\_magnitudes).

```
SELECT mag
,count(*) as earthquakes
FROM earthquakes
GROUP BY 1
ORDER BY 1
;

mag    earthquakes
-----  -----------
-9.99  258
-9     29
-5     1
...    ...
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0603.png" alt="" height="316" width="600"><figcaption></figcaption></figure>

**Figure 6-3. Distribution of earthquake magnitudes**

The graph extends from –10.0 to +10.0, which makes sense given our previous exploration of the data. It peaks and is roughly symmetric around a value in the range of 1.1 to 1.4 with almost 40,000 earthquakes of each magnitude, but it has a second peak of almost 20,000 earthquakes around the value 4.4. We’ll explore the reason for this second peak in the next section on forms of anomalies. The extreme values are hard to spot in this graph, however, so we might want to zoom in on a subsection of the graph, as in [Figure 6-4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#a\_zoomed\_in\_view\_of\_the\_distribution\_of).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0604.png" alt="" height="319" width="600"><figcaption></figcaption></figure>

**Figure 6-4. A zoomed-in view of the distribution of earthquake magnitudes, focused on the highest magnitudes**

Here the frequencies of these very high-intensity earthquakes are easier to see, as is the decrease in frequency from more than 10 to only 1 as the value goes from the low 7s to over 8. Thankfully these temblors are extremely rare.

A second type of graph that can be used to characterize data and spot outliers is the _scatter plot_. A scatter plot is appropriate when the data set contains at least two numeric values of interest. The x-axis displays the range of values of the first data field, the y-axis displays the range of values of the second data field, and a dot is graphed for every pair of x and y values in the data set. For example, we can graph the magnitude against the depth of earthquakes in the data set. First, query the data to create a data set of each pair of values. Then graph the output, as in [Figure 6-5](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#scatter\_plot\_of\_the\_magnitude\_and\_dept):

```
SELECT mag, depth
,count(*) as earthquakes
FROM earthquakes
GROUP BY 1,2
ORDER BY 1,2
;

mag    depth  earthquakes
-----  -----  -----------
-9.99  -0.59  1
-9.99  -0.35  1
-9.99  -0.11  1
...    ...    ...
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0605.png" alt="" height="599" width="600"><figcaption></figcaption></figure>

**Figure 6-5. Scatter plot of the magnitude and depth of earthquakes**

In this graph, we can see the same range of magnitudes, now plotted against the depths, which range from just below zero to around 700 kilometers. Interestingly, the high depth values, over 300, correspond to magnitudes that are roughly 4 and higher. Perhaps such deep earthquakes can be detected only after they reach a minimum magnitude. Note that, due to the volume of data, I have taken a shortcut and grouped the values by magnitude and depth combination, rather than plotting all 1.5 million data points. The count of earthquakes can be used to size each circle in the scatter, as in [Figure 6-6](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#scatter\_plot\_of\_the\_magnitude\_and\_depth), which is zoomed in to the range of magnitudes from 4.0 to 7.0, and depths from 0 to 50 km.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0606.png" alt="" height="608" width="600"><figcaption></figcaption></figure>

**Figure 6-6. Scatter plot of the magnitude and depth of earthquakes, zoomed in and with circles sized by the number of earthquakes**

A third type of graph useful in finding and analyzing outliers is the _box plot_, also known as the _box-and-whisker plot_. These graphs summarize data in the middle of the range of values while retaining the outliers. The graph type is named for the box, or rectangle, in the middle. The line that forms the bottom of the rectangle is located at the 25th percentile value, the line that forms the top is located at the 75th percentile, and the line through the middle is located at the 50th percentile, or median, value. Percentiles should be familiar from our discussion in the preceding section. The “whiskers” of the box plot are lines that extend out from the box, typically to 1.5 times the _interquartile range_. The interquartile range is simply the difference between the 75th percentile value and the 25th percentile value. Any values beyond the whiskers are plotted on the graph as outliers.

**TIP**

Whichever software or programming language you use for graphing box plots will take care of the calculations of the percentiles and interquartile range. Many also offer options to plot the whiskers based on standard deviations from the mean, or on wider percentiles such as the 10th and 90th. The calculation will always be symmetric around the midpoint (such as one standard deviation above and below the mean), but the length of the upper and lower whiskers can differ based on the data.

Typically, all of the values are plotted in a box plot. Since the data set is so large, for this example we’ll look at the subset of 16,036 earthquakes that include “Japan” in the `place` field. First, create the data set with SQL, which is a simple _SELECT_ of all of the `mag` values that meet the filter criteria:

```
SELECT mag
FROM earthquakes
WHERE place like '%Japan%'
ORDER BY 1
;

mag
---
2.7
3.1
3.2
...
```

Then create a box plot in our graphing software of choice, as shown in [Figure 6-7](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#box\_plot\_showing\_magnitude\_distribution).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0607.png" alt="" height="585" width="600"><figcaption></figcaption></figure>

**Figure 6-7. Box plot showing magnitude distribution of earthquakes in Japan**

Although the graphing software will often provide this information, we can also find the key values for the box plot with SQL:

```
SELECT ntile_25, median, ntile_75
,(ntile_75 - ntile_25) * 1.5 as iqr
,ntile_25 - (ntile_75 - ntile_25) * 1.5 as lower_whisker
,ntile_75 + (ntile_75 - ntile_25) * 1.5 as upper_whisker
FROM
(
    SELECT 
    percentile_cont(0.25) within group (order by mag) as ntile_25
    ,percentile_cont(0.5) within group (order by mag) as median
    ,percentile_cont(0.75) within group (order by mag) as ntile_75
    FROM earthquakes
    WHERE place like '%Japan%'
) a
;

ntile_25  median  ntile_75  iqr  lower_whisker   upper_whisker
--------  ------  --------  ----  -------------  -------------
4.3       4.5     4.7       0.60  3.70           5.30
```

The median Japanese earthquake had a magnitude of 4.5, and the whiskers extend from 3.7 to 5.3. The plotted circles represent outlier earthquakes, both small and large. The Great Tohoku Earthquake of 2011, at 9.1, is an obvious outlier, even among the larger earthquakes Japan experienced.

**WARNING**

In my experience, box plots are one of the more difficult visualizations to explain to those who don’t have a statistics background, or who don’t spend all day making and looking at visualizations. The interquartile range is a particularly confusing concept, though the notion of outliers seems to make sense to most people. If you’re not absolutely sure your audience knows how to interpret a box plot, take the time to explain it in clear but not overly technical terms. I keep a drawing like [Figure 6-8](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#diagram\_of\_parts\_of\_a\_box\_plot\_left\_squ) that explains the parts of a box plot and send it along with my work “just in case” my audience needs a refresher.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0608.png" alt="" height="562" width="600"><figcaption></figcaption></figure>

**Figure 6-8. Diagram of parts of a box plot**

Box plots can also be used to compare across groupings of the data to further identify and diagnose where outliers occur. For example, we can compare earthquakes in Japan in different years. First add the year of the `time` field into the SQL output and then graph, as in [Figure 6-9](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#box\_plots\_by\_year\_of\_magnitudes\_of\_eart):

```
SELECT date_part('year',time)::int as year
,mag
FROM earthquakes
WHERE place like '%Japan%'
ORDER BY 1,2
;
 
year  mag
----  ---
2010  3.6
2010  3.7
2010  3.7
...   ...
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0609.png" alt="" height="334" width="600"><figcaption></figcaption></figure>

**Figure 6-9. Box plot of magnitudes of earthquakes in Japan, by year**

Although the median and the range of the boxes fluctuate a bit from year to year, they are consistently between 4 and 5. Japan experienced large outlier earthquakes every year, with at least one greater than 6.0, and in six of the years it experienced at least one earthquake at or larger than 7.0. Japan is undoubtedly a very seismically active region.

Bar graphs, scatter plots, and box plots are commonly used to detect and characterize outliers in data sets. They allow us to quickly absorb the complexity of large amounts of data and to start to tell the story behind it. Along with sorting, percentiles, and standard deviations, graphs are an important part of the anomaly detection toolkit. With these tools in hand, we’re ready to discuss the various forms that anomalies can take in addition to those we’ve seen so far.

## Forms of Anomalies

Anomalies can come in all shapes and sizes. In this section, I will discuss three general categories of anomalies: values, counts or frequencies, and presence or absence. These are starting points for investigating any data set, either as a profiling exercise or because anomalies are suspected. Outliers and other unusual values are often specific to a particular domain, so in general the more you know about how and why the data was generated, the better. However, these patterns and techniques for spotting anomalies are good starting places for investigation.

### Anomalous Values

Perhaps the most common type of anomaly, and the first thing that comes to mind on this topic, is when single values are either extremely high or low outliers, or when values in the middle of the distribution are otherwise unusual.

In the last section, we looked at several ways to find outliers, through sorting, percentiles and standard deviations, and graphing. We discovered that the earthquakes data set has both unusually large values for the magnitude and some values that appear to be unusually small. The magnitudes also contain varying numbers of _significant digits_, or digits to the right of the decimal point. For example, we can look at a subset of values around 1 and find a pattern that repeats throughout the data set:

```
SELECT mag, count(*)
FROM earthquakes
WHERE mag > 1
GROUP BY 1
ORDER BY 1
limit 100
;

mag         count
----------  -----
...         ... 
1.08        3863
1.08000004  1
1.09        3712
1.1         39728
1.11        3674
1.12        3995
....        ...
```

Every once in a while there is a value with 8 significant digits. Many values have two significant digits, but having only a single significant digit is more common. This is likely due to different levels of precision in the instruments collecting the magnitude data. Additionally, the database does not display a second significant digit when that digit is zero, so “1.10” appears simply as “1.1.” However, the large number of records at “1.1” indicates that this is not just a display issue. Depending on the purpose of the analysis, we may or may not want to adjust the values to all have the same number of significant digits by rounding.

Often in addition to finding anomalous values, understanding why they happened or other attributes that are correlated with anomalies is useful. This is where creativity and data detective work come into play. For example, 1,215 records in the data set have very high depth values of more than 600 kilometers. We might want to know where these outliers occurred or how they were collected. Let’s take a look at the source, which we can find in the `net` (for network) field:

```
SELECT net, count(*)
FROM earthquakes
WHERE depth > 600
GROUP BY 1
;

net  count
---  -----
us   1215
```

The USGS site indicates that this source is the [USGS National Earthquake Information Center, PDE](https://earthquake.usgs.gov/data/comcat/contributor/us). This is not terribly informative, however, so let’s check the `place` values, which contain the earthquake locations:

```
SELECT place, count(*)
FROM earthquakes
WHERE depth > 600
GROUP BY 1
;

place                           count
------------------------------  -----
100km NW of Ndoi Island, Fiji   1
100km SSW of Ndoi Island, Fiji  1
100km SW of Ndoi Island, Fiji   1
...                             ...
```

Visual inspection suggests that many of these very deep earthquakes happen around Ndoi Island in Fiji. However, the place includes a distance and direction component, such as “100km NW of,” that makes summarization more difficult. We can apply some text parsing to focus on the place itself for better insights. For places that contain some values and then “ of ” and some more values, split on the “ of ” string and take the second part:

```
SELECT 
case when place like '% of %' then split_part(place,' of ',2) 
     else place end as place_name
,count(*)
FROM earthquakes
WHERE depth > 600
GROUP BY 1
ORDER BY 2 desc
;

place_name         count
-----------------  -----
Ndoi Island, Fiji  487
Fiji region        186
Lambasa, Fiji      140
...                ...
```

We can now say with more confidence that the majority of the very deep values were recorded for earthquakes somewhere in Fiji, with a particular concentration around the small volcanic island of Ndoi. The analysis could continue to get more complex, for example, by parsing the text to group together all earthquakes recorded in the greater region, which would reveal that after Fiji, other very deep earthquakes have been recorded around Vanuatu and the Philippines.

Anomalies can come in the form of misspellings, variations in capitalization, or other text errors. The ease of finding these depends on the number of distinct values, or _cardinality_, of the field. Differences in capitalization can be detected by counting both the distinct values and the distinct values when a `lower` or `upper` function is applied:

```
SELECT count(distinct type) as distinct_types
,count(distinct lower(type)) as distinct_lower
FROM earthquakes
;

distinct_types  distinct_lower
--------------  --------------
25              24
```

There are 24 distinct values of the `type` field, but 25 different forms. To find the specific types, we can use a calculation to flag those values whose lowercase form doesn’t match the actual value. Including the count of records for each form will help contextualize so that we can later decide how to handle the values:

```
SELECT type
,lower(type)
,type = lower(type) as flag
,count(*) as records
FROM earthquakes
GROUP BY 1,2,3
ORDER BY 2,4 desc
;

type       lower      flag   records
---------  ---------  -----  -------
...        ...        ...    ...
explosion  explosion  true   9887
ice quake  ice quake  true   10136
Ice Quake  ice quake  false  1
...        ...        ...    ...
```

The anomalous value of “Ice quake” is easy to spot, since it is the only value for which the flag calculation returns `false`. Since there is only one record with this value, compared to 10,136 with the lowercase form, we can assume that it can be grouped together with the other records. Other text functions can be applied, such as `trim` if we suspect that the values contain extra leading or trailing spaces, or `replace` if we suspect that certain spellings have multiple forms, such as the number “2” and the word “two.”

Misspellings can be more difficult to discover than other variations. If a known set of correct values and spellings exists, it can be used to validate the data either through an _OUTER JOIN_ to a table containing the values or with a CASE statement combined with an IN list. In either case, the goal is to flag values that are unexpected or invalid. Without such a set of correct values, our options are often either to apply domain knowledge or to make educated guesses. In the `earthquakes` table, we can look at the `type` values with only a few records and then try to determine if there is another, more common value that can be substituted:

```
SELECT type, count(*) as records
FROM earthquakes
GROUP BY 1
ORDER BY 2 desc
;

type                        records
--------------------------  -------
...                         ...
landslide                   15
mine collapse               12
experimental explosion      6
building collapse           5
...                         ...
meteorite                   1
accidental explosion        1
collapse                    1
induced or triggered event  1
Ice Quake                   1
rockslide                   1
```

We looked at “Ice Quake” previously and decided it was likely the same as “ice quake.” There is only one record for “rockslide,” though we might consider this close enough to another of the values, “landslide,” which has 15 records. “Collapse” is more ambiguous, since the data set includes both “mine collapse” and “building collapse.” What we do with these, or whether we do anything at all, depends on the goal of the analysis, as I’ll discuss later in [“Handling Anomalies”](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#handling\_anomalies).

### Anomalous Counts or Frequencies

Sometimes anomalies come not in the form of individual values but in the form of patterns or clusters of activity in the data. For example, a customer spending $100 on an ecommerce site may not be unusual, but that same customer spending $100 every hour over the course of 48 hours would almost certainly be an anomaly.

There are a number of dimensions on which clusters of activity can indicate anomalies, many of them dependent on the context of the data. Time and location are both common across many data sets and are features of the `earthquakes` data set, so I will use them to illustrate the techniques in this section. Keep in mind that these techniques can often be applied to other attributes as well.

Events that happen with unusual frequency over a short time span can indicate anomalous activity. This can be good, such as when a celebrity unexpectedly promotes a product, leading to a burst of sales of that product. They can also be bad, such as when unusual spikes indicate fraudulent credit card use or attempts to bring a website down with a flood of traffic. To understand these types of anomalies and whether there are deviations from the normal trend, we first apply appropriate aggregations and then use the techniques introduced earlier in this chapter, along with time series analysis techniques discussed in [Chapter 3](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#time\_series\_analysis).

In the following examples, I’ll go through a series of steps and queries that will help us understand the normal patterns and hunt for unusual ones. This is an iterative process that uses data profiling, domain knowledge, and insights from previous query results to guide each step. We’ll start our journey by checking the counts of earthquakes by year, which we can do by truncating the `time` field to the year level, and counting the records. For databases that don’t support `date_trunc`, consider `extract` or `trunc` instead:

```
SELECT date_trunc('year',time)::date as earthquake_year
,count(*) as earthquakes
FROM earthquakes
GROUP BY 1
;

earthquake_year  earthquakes
---------------  -----------
2010-01-01       122322
2011-01-01       107397
2012-01-01       105693
2013-01-01       114368
2014-01-01       135247
2015-01-01       122914
2016-01-01       122420
2017-01-01       130622
2018-01-01       179304
2019-01-01       171116
2020-01-01       184523
```

We can see that 2011 and 2012 had low numbers of earthquakes compared to other years. There was also a sharp increase in records in 2018 that was sustained through 2019 and 2020. This seems unusual, and we can hypothesize that the earth became more seismically active suddenly, that there is an error in the data such as duplication of records, or that something changed in the data collection process. Let’s drill down to month level to see if this trend persists at a more granular level of time:

```
SELECT date_trunc('month',time)::date as earthquake_month
,count(*) as earthquakes
FROM earthquakes
GROUP BY 1
;

earthquake_month  earthquakes
----------------  -----------
2010-01-01        9651
2010-02-01        7697
2010-03-01        7750
...               ...
```

The output is displayed in [Figure 6-10](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#number\_of\_earthquakes\_per\_month). We can see that although the number of earthquakes varies from month to month, there does appear to be an overall increase starting in 2017. We can also see that there are three outlier months, in April 2010, July 2018, and July 2019.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0610.png" alt="" height="318" width="600"><figcaption></figcaption></figure>

**Figure 6-10. Number of earthquakes per month**

From here we can continue checking the data at more granular time periods, perhaps optionally filtering the result set by a range of dates to focus in on these anomalous stretches of time. After narrowing in on the specific days or even times of day to pinpoint when the spikes occurred, we might want to break the data down further by other attributes in the data set. This can help explain the anomalies or at least narrow down the conditions in which they occurred. For example, it turns out that the increase in earthquakes starting in 2017 can be at least partially explained by the `status` field. The status indicates whether the event has been reviewed by a human (“reviewed”) or was directly posted by a system without review (“automatic”):

```
SELECT date_trunc('month',time)::date as earthquake_month
,status
,count(*) as earthquakes
FROM earthquakes
GROUP BY 1,2
ORDER BY 1
;

earthquake_month  status     earthquakes
----------------  --------   -----------
2010-01-01        automatic  620
2010-01-01        reviewed   9031
2010-02-01        automatic  695
...               ...        ...
```

The trends of “automatic” and “reviewed” status are plotted in [Figure 6-11](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#number\_of\_earthquakes\_per\_monthcomma\_sp).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0611.png" alt="" height="313" width="600"><figcaption></figcaption></figure>

**Figure 6-11. Number of earthquakes per month, split by status**

In the graph, we can see that the outlier counts in July 2018 and July 2019 are due to large increases in the number of “automatic”-status earthquakes, whereas the spike in April 2010 was in “reviewed”-status earthquakes. A new type of automatic recording equipment may have been added to the data set in 2017, or perhaps there hasn’t been enough time to review all the recordings yet.

Analyzing location in data sets that have that information can be another powerful way to find and understand anomalies. The `earthquakes` table contains information about many thousands of very small earthquakes, potentially obscuring our view of the very large, very noteworthy earthquakes. Let’s look at the locations of the biggest quakes, those of magnitude 6 or larger, and see where they cluster geographically:

```
SELECT place, count(*) as earthquakes
FROM earthquakes
WHERE mag >= 6
GROUP BY 1
ORDER BY 2 desc
;

place                                 earthquakes
------------------------------------  -----------
near the east coast of Honshu, Japan  52
off the east coast of Honshu, Japan   34
Vanuatu                               28
...                                   ...
```

In contrast to time, where we queried at progressively more granular levels, the `place` values are already so granular that it’s a bit difficult to grasp the full picture, although the Honshu, Japan, region clearly stands out. We can apply some of the text analysis techniques from [Chapter 5](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#text\_analysis) to parse and then group the geographic information. In this case, we’ll use `split_part` to remove the direction text (such as “near the coast of” or “100km N of”) that often appears at the beginning of the `place` field:

```
SELECT 
case when place like '% of %' then split_part(place,' of ',2)
     else place
     end as place
,count(*) as earthquakes
FROM earthquakes
WHERE mag >= 6
GROUP BY 1
ORDER BY 2 desc
;

place                  earthquakes
---------------------  -----------
Honshu, Japan          89
Vanuatu                28
Lata, Solomon Islands  28
...                    ...
```

The region around Honshu, Japan, experienced 89 earthquakes, making it not only the location of the largest earthquake in the data set but also an outlier in the number of very large earthquakes recorded. We could continue to parse, clean, and group the `place` values to gain a more refined picture of where major earthquakes occur in the world.

Finding anomalous counts, sums, or frequencies in data is usually an exercise that involves a number of rounds of querying different levels of granularity in succession. It’s common to start broad, then go more granular, zoom out again to compare to baseline trends, and zoom in again on specific splits or dimensions of the data. Fortunately, SQL is a great tool for this sort of rapid iteration. Combining techniques, especially from time series analysis, discussed in [Chapter 3](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#time\_series\_analysis), and text analysis, discussed in [Chapter 5](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#text\_analysis), will bring even more richness to the analysis.

### Anomalies from the Absence of Data

We’ve seen how unusually high frequencies of events can signal anomalies. Keep in mind that the absence of records can also signal anomalies. For example, the heartbeat of a patient undergoing surgery is monitored. The absence of a heartbeat at any time generates an alert, as do irregularities in the heartbeat. In many contexts, however, detecting the absence of data is difficult if you’re not specifically looking for it. Customers don’t always announce they are about to churn. They simply stop using the product or service and quietly drop out of the data set.

One way to ensure that absences in data are noticed is to use techniques from cohort analysis, discussed in [Chapter 4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch04.html#cohort\_analysis). In particular, a _JOIN_ to a date series or data dimension, to ensure that a record exists for every entity whether or not it was present in that time period, makes absences easier to detect.

Another way to detect absence is to query for gaps, or time since last seen. Some regions are more prone to large earthquakes due to the way tectonic plates are arranged around the globe. We’ve also detected some of this in the data in our previous examples. Earthquakes are notoriously hard to predict, even when we have a sense of where they are likely to occur. This doesn’t stop some people from speculating about the next “big one” simply due to the amount of time that has passed since the last one. We can use SQL to find the gaps between large earthquakes and the time since the most recent one:

```
SELECT place
,extract('days' from '2020-12-31 23:59:59' - latest) 
 as days_since_latest
,count(*) as earthquakes
,extract('days' from avg(gap)) as avg_gap
,extract('days' from max(gap)) as max_gap
FROM
(
    SELECT place
    ,time
    ,lead(time) over (partition by place order by time) as next_time
    ,lead(time) over (partition by place order by time) - time as gap
    ,max(time) over (partition by place) as latest
    FROM
    (
        SELECT 
        replace(
          initcap(
          case when place ~ ', [A-Z]' then split_part(place,', ',2)
               when place like '% of %' then split_part(place,' of ',2)
               else place end
        )
        ,'Region','')
        as place
        ,time
        FROM earthquakes
        WHERE mag > 5
    ) a
) a         
GROUP BY 1,2        
;

place             days_since_latest  earthquakes  avg_gap  max_gap
----------------  -----------------  -----------  -------  -------
Greece            62.0               109          36.0     256.0
Nevada            30.0               9            355.0    1234.0
Falkland Islands  2593.0             3            0.0      0.0
...               ...                ...          ...      ...
```

In the innermost subquery, the `place` field is parsed and cleaned, returning larger regions or countries, along with the time of each earthquake, for all earthquakes of magnitude 5 or greater. The second subquery uses a `lead` function to find the `time` of the next earthquake, if any, for each place and time, and the `gap` between each earthquake and the next one. The `max` window function returns the most recent earthquake for each place. The outer query calculates the days since the latest 5+ earthquake in the data set, using the `extract` function to return just the days from the interval that is returned when two dates are subtracted. Since the data set includes records only through the end of 2020, the timestamp “2020-12-31 23:59:59” is used, though `current_timestamp` or an equivalent expression would be appropriate if the data were refreshed on an ongoing basis. Days are extracted in a similar fashion from the average and max of the `gap` value.

The time since the last major earthquake in a location may have little predictive power in practice, but in many domains, gaps and time since last seen metrics have practical applications. Understanding typical gaps between actions sets a baseline against which the current gap can be compared. When the current gap is within range of historical values, we might judge that a customer is retained, but when the current gap is much longer, the risk of churn increases. The result set from a query that returns historical gaps can itself become the subject of an anomaly detection analysis, answering questions such as the longest amount of time that a customer was gone before subsequently returning.

## Handling Anomalies

Anomalies can appear in data sets for a number of reasons and can take a number of forms, as we’ve just seen. After detecting anomalies, the next step is to handle them in some fashion. How this is done depends on both the source of the anomaly—underlying process or data quality issue—and the end goal of the data set or analysis. The options include investigation without changes, removal, replacement, rescaling, and fixing upstream.

### Investigation

Finding, or attempting to find, the cause of an anomaly is usually the first step in deciding what to do with it. This part of the process can be both fun and frustrating—fun in the sense that tracking down and solving a mystery engages our skills and creativity, but frustrating in the sense that we’re often working under time pressure and tracking down anomalies can feel like going down an endless series of rabbit holes, leading us to wonder whether an entire analysis is flawed.

When I’m investigating anomalies, my process usually involves a series of queries that bounce back and forth between searching for patterns and looking at specific examples. A true outlier value is easy to spot. In such cases, I will usually query for the entire row that contains the outlier for clues as to the timing, source, and any other attributes that are available. Next, I’ll check records that share those attributes to see if they have values that seem unusual. For example, I might check to see whether other records on the same day have normal or unusual values. Traffic from a particular website or purchases of a particular product might reveal other anomalies.

After investigating the source and attributes of anomalies when working on data produced internally in my organization, I get in touch with the stakeholders or product owners. Sometimes there is a known bug or flaw, but often enough there is a real issue in a process or system that needs to be addressed, and context information is useful. For external or public data sets, there may not be an opportunity to find the root cause. In these cases, my goal is to gather enough information to decide which of the options discussed next is appropriate.

### Removal

One option for dealing with data anomalies is to simply remove them from the data set. If there is reason to suspect that there was an error in the data collection that might affect the entire record, removal is appropriate. Removal is also a good option when the data set is large enough that dropping a few records is unlikely to affect the conclusions. Another good reason to use removal is when the outliers are so extreme that they would skew the results enough that entirely inappropriate conclusions would be drawn.

We saw previously that the `earthquakes` data set contains a number of records with a magnitude of –9.99 and a few with –9. Since the earthquakes these values would correspond to are extremely small, we might suspect that they are erroneous values or were simply entered when the actual magnitude was unknown. Removing records with these values is straightforward in the _WHERE_ clause:

```
SELECT time, mag, type
FROM earthquakes
WHERE mag not in (-9,-9.99)
limit 100
;

time                 mag   type
-------------------  ----  ----------
2019-08-11 03:29:20  4.3   earthquake
2019-08-11 03:27:19  0.32  earthquake
2019-08-11 03:25:39  1.8   earthquake
```

Before removing the records, however, we might want to determine whether including the outliers actually makes a difference to the output. For example, we might want to know if removing the outliers affects the average magnitude, since averages can easily be skewed by outliers. We can do this by calculating the average across the entire data set, as well as the average excluding the extreme low values, using a CASE statement to exclude them:

```
SELECT avg(mag) as avg_mag
,avg(case when mag > -9 then mag end) as avg_mag_adjusted
FROM earthquakes
;

avg_mag               avg_mag_adjusted
------------------    ------------------
1.6251015161530643    1.6273225642983641
```

The averages are different only at the third significant digit (1.625 versus 1.627), which is a fairly small difference. However, if we filter just to Yellowstone National Park, where many of the –9.99 values occur, the difference is more dramatic:

```
SELECT avg(mag) as avg_mag
,avg(case when mag > -9 then mag end) as avg_mag_adjusted
FROM earthquakes
WHERE place = 'Yellowstone National Park, Wyoming'
;

avg_mag                 avg_mag_adjusted
----------------------  ----------------------
0.40639347873981053095  0.92332793709528214616
```

Although these are still small values, the difference between an average of 0.46 and 0.92 is big enough that we would likely choose to remove the outliers.

Notice that there are two options for doing so: either in the _WHERE_ clause, which removes the outliers from all the results, or in a CASE statement, which removes them only from specific calculations. Which option you choose depends on the context of the analysis, as well as on whether it is important to preserve the rows in order to retain total counts, or useful values in other fields.

### Replacement with Alternate Values

Anomalous values can often be handled by replacing them with other values rather than removing entire records. An alternate value can be a default, a substitute value, the nearest numerical value within a range, or a summary statistic such as the average or median.

We’ve seen previously that null values can be replaced with a default using the `coalesce` function. When values are not necessarily null but are problematic for some other reason, a CASE statement can be used to substitute a default value. For example, rather than report on all the various seismic events, we might want to group the types that are _not_ earthquakes into a single “Other” value:

```
SELECT 
case when type = 'earthquake' then type
     else 'Other'
     end as event_type
,count(*)
FROM earthquakes
GROUP BY 1
;

event_type  count
----------  -------
earthquake  1461750
Other       34176
```

This reduces the amount of detail in the data, of course, but it can also be a way to summarize a data set that has a number of outlier values for `type`, as we saw previously. When you know that outlier values are incorrect, and you know the correct value, replacing them with a CASE statement is also a solution that preserves the row in the overall data set. For example, an extra 0 might have been added to the end of a record, or a value might have been recorded in inches instead of miles.

Another option for handling numeric outliers is to replace the extreme values with the nearest high or low value that is not extreme. This approach maintains much of the range of values but prevents misleading averages that can result from extreme outliers. _Winsorization_ is a specific technique for this, where outliers are set to a specific percentile of the data. For example, values above the 95th percentile are set to the 95th percentile value, while values below the 5th percentile are set to the 5th percentile value. To calculate this in SQL, we first calculate the 5th and 95th percentile values:

```
SELECT percentile_cont(0.95) within group (order by mag) 
 as percentile_95
,percentile_cont(0.05) within group (order by mag) 
 as percentile_05
FROM earthquakes
;

percentile_95  percentile_05
-------------  -------------
4.5            0.12
```

We can put this calculation in a subquery and then use a CASE statement to handle setting values for outliers below the 5th percentile and above the 95th. Note the Cartesian _JOIN_ that allows us to compare the percentile values with each individual magnitude:

```
SELECT a.time, a.place, a.mag
,case when a.mag > b.percentile_95 then b.percentile_95
      when a.mag < b.percentile_05 then b.percentile_05
      else a.mag
      end as mag_winsorized
FROM earthquakes a
JOIN
(
    SELECT percentile_cont(0.95) within group (order by mag) 
     as percentile_95
    ,percentile_cont(0.05) within group (order by mag) 
     as percentile_05
    FROM earthquakes
) b on 1 = 1 
;

time                 place                        mag   mag_winsorize
-------------------  ---------------------------  ----  -------------
2014-01-19 06:31:50  5 km SW of Volcano, Hawaii   -9    0.12
2012-06-11 01:59:01  Nevada                       -2.6  0.12
...                  ...                          ...   ...
2020-01-27 21:59:01  31km WNW of Alamo, Nevada    2     2.0
2013-07-07 08:38:59  54km S of Fredonia, Arizona  3.5   3.5
...                  ...                          ...   ...
2013-09-25 16:42:43  46km SSE of Acari, Peru      7.1   4.5
2015-04-25 06:11:25  36km E of Khudi, Nepal       7.8   4.5
...                  ...                          ...   ...
```

The 5th percentile value is 0.12, while the 95th percentile is 4.5. Values below and above these thresholds are changed to the threshold in the `mag_winsorize` field. Values between these thresholds remain the same. There is no set percentile threshold for winsorizing. The 1st and 99th percentiles or even the 0.01th and 99.9th percentiles can be used depending on the requirements for the analysis and how prevalent and extreme the outliers are.

### Rescaling

Rather than filtering out records or changing the values of outliers, rescaling values provides a path that retains all the values but makes analysis and graphing easier.

We discussed the z-score previously, but it’s worth pointing out that this can be used as a way to rescale values. The z-score is useful because it can be used with both positive and negative values.

Another common transformation is converting to logarithmic (log) scale. The benefit of transforming values into log scale is that they retain the same ordering, but small numbers get spread out more. Log transformations can also be transformed back into the original scale, easing interpretation. A downside is that the log transformation cannot be used on negative numbers. In the `earthquakes` data set, we learned that the magnitude is already expressed in log scale. The magnitude 9.1 Great Tohoku Earthquake is extreme, but the value would appear even more extreme were it not expressed in log scale!

The `depth` field is measured in kilometers. Here we’ll query both the depth and the depth with the `log` function applied and then graph the output in Figures [6-12](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#distribution\_of\_earthquakes\_by\_depthcom) and [6-13](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#distribution\_of\_earthquakes\_by\_depth\_on) in order to demonstrate the difference. The `log` function uses base 10 as a default. To reduce the result set for easier graphing, the depth is also rounded to one significant digit using the `round` function. The table is filtered to exclude values less than 0.05, as these would round to zero or less than zero:

```
SELECT round(depth,1) as depth
,log(round(depth,1)) as log_depth
,count(*) as earthquakes
FROM earthquakes
WHERE depth >= 0.05
GROUP BY 1,2
;

depth  log_depth            earthquakes
-----  -------------------  -----------
0.1    -1.0000000000000000  6994
0.2    -0.6989700043360188  6876
0.3    -0.5228787452803376  7269
...    ...                  ...
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0612.png" alt="" height="318" width="600"><figcaption></figcaption></figure>

**Figure 6-12. Distribution of earthquakes by depth, with unadjusted depths**

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0613.png" alt="" height="318" width="600"><figcaption></figcaption></figure>

**Figure 6-13. Distribution of earthquakes by depth on a log scale**

In [Figure 6-12](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#distribution\_of\_earthquakes\_by\_depthcom), it’s apparent that there are a large number of earthquakes between 0.05 and maybe 20, but beyond that it’s difficult to see the distribution since the x-axis stretches all the way to 700 to capture the range of the data. When the depth is transformed to a log scale in [Figure 6-13](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#distribution\_of\_earthquakes\_by\_depth\_on), however, the distribution of the smaller values is much easier to see. Notably, the spike at 1.0, which corresponds to a depth of 10 kilometers, is apparent.

**TIP**

Other types of scale transformations, while not necessarily appropriate for removing outliers, can be accomplished with SQL. Some common ones include:

* Square root: use the `sqrt` function
* Cube root: use the `cbrt` function
* Reciprocal transformation: 1 / `field_name`

Change the units, such as inches to feet or pounds to kilograms: multiply or divide by the appropriate conversion factor with \* or /.

Rescaling can be done in SQL code, or often alternatively in the software or coding language used for graphing. The log transformation is particularly useful when there is a large spread of positive values and the patterns that are important to detect exist in the lower values.

As with all analysis, deciding how to handle anomalies depends on the purpose and the amount of context or domain knowledge you have about the data set. Removing outliers is the simplest method, but to retain all the records, techniques such as winsorizing and rescaling work well.

## Conclusion

Anomaly detection is a common practice in analysis. The goal may be to detect the outliers, or it may be to manipulate them in order to prepare a data set for further analysis. In either case, the basic tools of sorting, calculating percentiles, and graphing the output of SQL queries can help you find them efficiently. Anomalies come in many varieties, with outlying values, unusual bursts of activity, and unusual absences being most common. Domain knowledge is almost always helpful as you go through the process of finding and gathering information about the causes of anomalies. Options for dealing with anomalies include investigation, removal, replacement with alternate values, and rescaling the data. The choice depends heavily on the goal, but any of these paths can be accomplished with SQL. In the next chapter, we’ll turn our attention to experimentation, where the goal is to figure out whether a whole group of subjects differs from the norm of the control group.

[1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#ch01fn8-marker) [_https://www.mathsisfun.com/data/standard-deviation-formulas.html_](https://www.mathsisfun.com/data/standard-deviation-formulas.html) has a good explanation.
