# 5. Text Analysis

## Chapter 5. Text Analysis

In the last two chapters, we explored applications of dates and numbers with time series analysis and cohort analysis. But data sets are often more than just numeric values and associated timestamps. From qualitative attributes to free text, character fields are often loaded with potentially interesting information. Although databases excel at numeric calculations such as counting, summing, and averaging things, they are also quite good at performing operations on text data.

I’ll begin this chapter by providing an overview of the types of text analysis tasks that SQL is good for, and of those for which another programming language is a better choice. Next, I’ll introduce our data set of UFO sightings. Then we’ll get into coding, covering text characteristics and profiling, parsing data with SQL, making various transformations, constructing new text from parts, and finally finding elements within larger blocks of text, including with regular expressions.

## Why Text Analysis with SQL?

Among the huge volumes of data generated every day, a large portion consists of text: words, sentences, paragraphs, and even longer documents. Text data used for analysis can come from a variety of sources, including descriptors populated by humans or computer applications, log files, support tickets, customer surveys, social media posts, or news feeds. Text in databases ranges from _structured_ (where data is in different table fields with distinct meanings) to _semistructured_ (where the data is in separate columns but may need parsing or cleaning to be useful) or mostly _unstructured_ (where long VARCHAR or BLOB fields hold arbitrary length strings that require extensive structuring before further analysis). Fortunately, SQL has a number of useful functions that can be combined to accomplish a range of text-structuring and analysis tasks.

### What Is Text Analysis?

Text analysis is the process of deriving meaning and insight from text data. There are two broad categories of text analysis, which can be distinguished by whether the output is qualitative or quantitative. _Qualitative analysis_, which may also be called _textual analysis_, seeks to understand and synthesize the meaning from a single text or a set of texts, often applying other knowledge or unique conclusions. This work is often done by journalists, historians, and user experience researchers. _Quantitative analysis_ of text also seeks to synthesize information from text data, but the output is quantitative. Tasks include categorization and data extraction, and analysis is usually in the form of counts or frequencies, often trended over time. SQL is much more suited to quantitative analysis, so that is what the rest of this chapter is concerned with. If you have the opportunity to work with a counterpart who specializes in the first type of text analysis, however, do take advantage of their expertise. Combining the qualitative with the quantitative is a great way to derive new insights and persuade reluctant colleagues.

Text analysis encompasses several goals or strategies. The first is text extraction, where a useful piece of data must be pulled from surrounding text. Another is categorization, where information is extracted or parsed from text data in order to assign tags or categories to rows in a database. Another strategy is sentiment analysis, where the goal is to understand the mood or intent of the writer on a scale from negative to positive.

Although text analysis has been around for a while, interest and research in this area have taken off with the advent of machine learning and the computing resources that are often needed to work with large volumes of text data. _Natural language processing_ (NLP) has made huge advances in recognizing, classifying, and even generating brand-new text data. Human language is incredibly complex, with different languages and dialects, grammars, and slang, not to mention the thousands and thousands of words, some that have overlapping meanings or subtly modify the meaning of other words. As we’ll see, SQL is good at some forms of text analysis, but for other, more advanced tasks, there are languages and tools that are better suited.

### Why SQL Is a Good Choice for Text Analysis

There are a number of good reasons to use SQL for text analysis. One of the most obvious is when the data is already in a database. Modern databases have a lot of computing power that can be leveraged for text tasks in addition to the other tasks we’ve discussed so far. Moving data to a flat file for analysis with another language or tool is time consuming, so doing as much work as possible with SQL within the database has advantages.

If the data is not already in a database, for relatively large data sets, moving the data to a database may be worthwhile. Databases are more powerful than spreadsheets for processing transformations on many records. SQL is less error-prone than spreadsheets, since no copying and pasting is required, and the original data stays intact. Data could potentially be altered with an _UPDATE_ command, but this is hard to do accidentally.

SQL is also a good choice when the end goal is quantification of some sort. Counting how many support tickets contain a key phrase and parsing categories out of larger text that will be used to group records are good examples of when SQL shines. SQL is good at cleaning and structuring text fields. _Cleaning_ includes removing extra characters or whitespace, fixing capitalization, and standardizing spellings. _Structuring_ involves creating new columns from elements extracted or derived from other fields or constructing new fields from parts stored in different places. String functions can be nested or applied to the results of other functions, allowing for almost any manipulations that might be needed.

SQL code for text analysis can be simple or complex, but it is always rule based. In a rule-based system, the computer follows a set of rules or instructions—no more, no less. This can be contrasted with machine learning, in which the computer adapts based on the data. Rules are good because they are easy for humans to understand. They are written down in code form and can be checked to ensure they produce the desired output. The downside of rules is that they can become long and complicated, particularly when there are a lot of different cases to handle. This can also make them difficult to maintain. If the structure or type of data entered into the column changes, the rule set needs to be updated. On more than one occasion, I’ve started with what seemed like a simple CASE statement with 4 or 5 lines, only to have it grow to 50 or 100 lines as the application changed. Rules might still be the right approach, but keeping in sync with the development team on changes is a good idea.

Finally, SQL is a good choice when you know in advance what you are looking for. There are a number of powerful functions, including regular expressions, that allow you to search for, extract, or replace specific pieces of information. “How many reviewers mention ‘short battery life’ in their reviews?” is a question SQL can help you answer. On the other hand, “Why are these customers angry?” is not going to be as easy.

### When SQL Is Not a Good Choice

SQL essentially allows you to harness the power of the database to apply a set of rules, albeit often powerful rules, to a set of text to make it more useful for analysis. SQL is certainly not the only option for text analysis, and there are a number of use cases for which it’s not the best choice. It’s useful to be aware of these.

The first category encompasses use cases for which a human is more appropriate. When the data set is very small or very new, hand labeling can be faster and more informative. Additionally, if the goal is to read all the records and come up with a qualitative summary of key themes, a human is a better choice.

The second category is when there’s a need to search for and retrieve specific records that contain text strings with low latency. Tools like Elasticsearch or Splunk have been developed to index strings for these use cases. Performance will often be an issue with SQL and databases; this is one of the main reasons that we usually try to structure the data into discrete columns that can more easily be searched by the database engine.

The third category comprises tasks in the broader NLP category, where machine learning approaches and the languages that run them, such as Python, are a better choice. Sentiment analysis, used to analyze ranges of positive or negative feelings in texts, can be handled only in a simplistic way with SQL. For example, “love” and “hate” could be extracted and used to categorize records, but given the range of words that can express positive and negative emotions, as well as all the ways to negate those words, it would be nearly impossible to create a rule set with SQL to handle them all. Part-of-speech tagging, where words in a text are labeled as nouns, verbs, and so on, is better handled with libraries available in Python. Language generation, or creating brand-new text based on learnings from example texts, is another example best handled in other tools. We will see how we can create new text by concatenating pieces of data together, but SQL is still bound by rules and won’t automatically learn from and adapt to new examples in the data set.

Now that we’ve discussed the many good reasons to use SQL for text analysis, as well as the types of use cases to avoid, let’s take a look at the data set we’ll be using for the examples before launching into the SQL code itself.&#x20;

## The UFO Sightings Data Set

For the examples in this chapter, we’ll use a data set of UFO sightings compiled by the [National UFO Reporting Center](http://www.nuforc.org/). The data set consists of approximately 95,000 reports posted between 2006 and 2020. Reports come from individuals who can enter information through an online form.

The table we will work with is `ufo`, and it has only two columns. The first is a composite column called `sighting_report` that contains information about when the sighting occurred, when it was reported, and when it was posted. It also contains metadata about the location, shape, and duration of the sighting event. The second column is a text field called `description` that contains the full description of the event. [Figure 5-1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#sample\_of\_the\_ufo\_table) shows a sample of the data.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0501.png" alt="" height="207" width="600"><figcaption></figcaption></figure>

**Figure 5-1. Sample of the `ufo` table**

Through the examples and discussion in this chapter, I will show how to parse the first column into structured dates and descriptors. I will also show how to perform various analyses on the `description` field. If I were working with this data on a continual basis, I might consider creating an ETL pipeline, a job that processes the data in the same way on a regular basis, and storing the resulting structured data in a new table. For the examples in this chapter, however, we’ll stick with the raw table.

Let’s get into the code, starting with SQL to explore and characterize the text from the sightings.

## Text Characteristics

The most flexible data type in a database is VARCHAR, because almost any data can be put in fields of this type. As a result, text data in databases comes in a variety of shapes and sizes. As with other data sets, profiling and characterizing the data is one of the first things we do. From there we can develop a game plan for the kinds of cleaning and parsing that may be necessary for the analysis.

One way we can get to know the text data is to find the number of characters in each value, which can be done with the `length` function (or `len` in some databases). This function takes the string or character field as an argument and is similar to functions found in other languages and spreadsheet programs:

```
SELECT length('Sample string');

length
------
13
```

We can create a distribution of field lengths to get a sense of the typical length and of whether there are any extreme outliers that might need to be handled in special ways:

```
SELECT length(sighting_report), count(*) as records
FROM ufo
GROUP BY 1
ORDER BY 1
;
 
length  records
------  -------
90      1
91      4
92      8
...     ...
```

We can see in [Figure 5-2](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#distribution\_of\_field\_lengths\_in\_the\_fi) that most of the records are between roughly 150 and 180 characters long, and very few are less than 140 or more than 200 characters. The lengths of the `description` field range from 5 to 64,921 characters. We can assume that there is much more variety in this field, even before doing any additional profiling.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0502.png" alt="" height="321" width="600"><figcaption></figcaption></figure>

**Figure 5-2. Distribution of field lengths in the first column of the **_**`ufo`**_** table**

Let’s take a look at a few sample rows of the `sighting_report` column. In a query tool, I might scroll through a hundred or so rows to get familiar with the contents, but these are representative of the values in the column:

```
Occurred : 3/4/2018 19:07 (Entered as : 03/04/18 19:07)Reported: 3/6/2018 7:05:12
PM 19:05Posted: 3/8/2018Location: Colorado Springs, COShape: LightDuration:3
minutes
Occurred : 10/16/2017 21:42 (Entered as : 10/16/2017 21:42)Reported: 3/6/2018
5:09:47 PM 17:09Posted: 3/8/2018Location: North Dayton, OHShape: SphereDuration:~5
minutes
Occurred : 2/15/2018 00:10 (Entered as : 2/15/18 0:10)Reported: 3/6/2018 
6:19:54 PM 18:19Posted: 3/8/2018Location: Grand Forks, NDShape: SphereDuration:
5 seconds
```

This data is what I would call semistructured, or overstuffed. It can’t be used in an analysis as is, but there are clearly distinct pieces of information stored here, and the pattern is similar between rows. For example, each row has the word “Occurred” followed by what looks like a timestamp, “Location” followed by a place, and “Duration” followed by an amount of time.

**NOTE**

Data can end up in overstuffed fields for a variety of reasons, but there are two common ones I see. One is when there aren’t enough fields available in the source system or application to store all the attributes required, so multiple attributes are entered into the same field. Another is when the data is stored in a JSON blob in an application in order to accommodate sparse attributes or frequent additions of new attributes. Although both scenarios are less than ideal from an analysis perspective, as long as there is a consistent structure, we can usually handle these with SQL.

Our next step is to make this field more usable by parsing it into several new fields, each of which contains a single piece of information. The steps in this process are:

* Plan the field(s) desired as output
* Apply parsing functions
* Apply transformations, including data type conversions
* Check the results when applied to the entire data set, since there will often be some records that don’t conform to the pattern
* Repeat these steps until the data is in the desired columns and formats

The new columns we will parse out of `sighting_report` are `occurred`, `entered_as`, `reported`, `posted`, `location`, `shape`, and `duration`. Next, we will learn about parsing functions and work on structuring the `ufo` data set.

## Text Parsing

Parsing data with SQL is the process of extracting pieces of a text value to make them more useful for analysis. Parsing splits the data into the part that we want and “everything else,” though typically our code returns only the part we want.

The simplest parsing functions return a fixed number of characters from either the beginning or the end of a string. The `left` function returns characters from the left side or beginning of the string, while the `right` function returns characters from the right side or end of the string. They otherwise work in the same way, taking the value to parse as the first argument and the number of characters as the second. Either argument can be a database field or calculation, allowing for dynamic results:

```
SELECT left('The data is about UFOs',3) as left_digits
,right('The data is about UFOs',4) as right_digits
;

left_digits  right_digits
-----------  -----
The          UFOs
```

In the `ufo` data set, we can parse out the first word, “Occurred,” using the `left` function:

```
SELECT left(sighting_report,8) as left_digits
,count(*)
FROM ufo
GROUP BY 1
;

left_digits  count
-----------  -----
Occurred     95463
```

We can confirm that all records start with this word, which is good news because it means at least this part of the pattern is consistent. However, what we really want is the values for what occurred, not the word itself, so let’s try again. In the first example record, the end of the occurred timestamp is at character 25. In order to remove “Occurred” and retain only the actual timestamp, we can return the rightmost 14 characters using the `right` function. Note that the `right` and `left` functions are nested—the first argument of the `right` function is the result of the `left` function:

```
SELECT right(left(sighting_report,25),14) as occurred
FROM ufo
;

occurred
--------------
3/4/2018 19:07
10/16/2017 21:
2/15/2018 00:1
...
```

Although this returns the correct result for the first record, it unfortunately can’t handle the records that have two-digit month or day values. We could increase the number of characters returned by the `left` and `right` functions, but the result would then include too many characters for the first record.

The `left` and `right` functions are useful for extracting fixed-length parts of a string, as in our extraction of the word “Occurred,” but for more complex patterns, a function called `split_part` is more useful. The idea behind this function is to split a string into parts based on a delimiter and then allow you to select a specific part. A _delimiter_ is one or more characters that are used to specify the boundary between regions of text or other data. The comma delimiter and tab delimiter are probably the most common, as these are used in text files (with extensions such as _.csv_, _.tsv_, or _.txt_) to indicate where columns start and end. However, any sequence of characters can be used, which will come in handy for our parsing task. The form of the function is:

```
split_part(string or field name, delimiter, index)
```

The index is the position of the text to be returned, relative to the delimiter. So index = 1 returns all of the text to the left of the first instance of the delimiter, index = 2 returns the text between the first and second instance of the delimiter (or all of the text to the right of the delimiter if the delimiter appears only once), and so on. There is no zero index, and the values must be positive integers:

```
SELECT split_part('This is an example of an example string'
                  ,'an example'
                  ,1);

split_part
----------
This is 

SELECT split_part('This is an example of an example string'
                  ,'an example'
                  ,2);

split_part
----------
 of
```

**TIP**

MySQL has a `substring_index` function instead of `split_part`. SQL Server does not have a `split_part` function at all

Note that spaces in the text will be retained unless specified as part of the delimiter. Let’s take a look at how we can parse the elements of the `sighting_report` column. As a reminder, a sample value looks like this:

```
Occurred : 6/3/2014 23:00 (Entered as : 06/03/14 11:00)Reported: 6/3/2014 10:33:24
PM 22:33Posted: 6/4/2014Location: Bethesda, MDShape: LightDuration:15 minutes
```

The value we want our query to return is the text between “Occurred : ” and “ (Entered”. That is, we want the string “6/3/2014 23:00”. Checking the sample text, “Occurred :” and “(Entered” both appear only once. A colon (:) appears several times, both to separate the label from the value and in the middle of timestamps. This might make parsing using the colon tricky. The open parenthesis character appears only once. We have some choices as to what to specify as the delimiter, choosing either longer strings or only the fewest characters required to split the string accurately. I tend to be a little more verbose to ensure that I get exactly the piece that I want, but it really depends on the situation.

First, split `sighting_report` on “Occurred : ” and check the result:

```
SELECT split_part(sighting_report,'Occurred : ',2) as split_1
FROM ufo
;

split_1
--------------------------------------------------------------
6/3/2014 23:00 (Entered as : 06/03/14 11:00)Reported: 6/3/2014 10:33:24 PM
22:33Posted: 6/4/2014Location: Bethesda, MDShape: LightDuration:15 minutes
```

We have successfully removed the label, but we still have a lot of extra text remaining. Let’s check the result when we split on “ (Entered”:

```
SELECT split_part(sighting_report,' (Entered',1) as split_2
FROM ufo
;

split_2
-------------------------
Occurred : 6/3/2014 23:00
```

This is closer, but it still has the label in the result. Fortunately, nesting `split_part` functions will return only the desired date and time value:

```
SELECT split_part(
         split_part(sighting_report,' (Entered',1)
         ,'Occurred : ',2) as occurred
FROM ufo
;

occurred
---------------
6/3/2014 23:00
4/25/2014 21:15
5/25/2014
```

Now the result includes the desired values. Reviewing a few additional lines shows that two-digit day and month values are handled appropriately, as are dates that do not have a time value. It turns out that some records omit the “Entered as” value, so one additional split is required to handle records where the “Reported” label marks the end of the desired string:

```
SELECT 
split_part(
  split_part(
    split_part(sighting_report,' (Entered',1)
    ,'Occurred : ',2)
    ,'Reported',1) as occurred
FROM ufo
;

occurred
---------------
6/24/1980 14:00
4/6/2006 02:05
9/11/2001 09:00
...
```

The most common `occurred` values parsed out with the SQL code are graphed in [Figure 5-3](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#top\_onezero\_most\_common\_occurred\_values).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0503.png" alt="" height="362" width="600"><figcaption></figcaption></figure>

**Figure 5-3. Top 10 most common `occurred` values for UFO sightings**

**TIP**

Finding a set of functions that works for all values in the data set is one of the hardest parts of text parsing. It often takes several rounds of trial and error and profiling the results along the way to get it right.

The next step is to apply similar parsing rules to extract the other desired fields, using beginning and ending delimiters to isolate just the relevant part of the string. The final query uses `split_part` several times, with different arguments for each value:

```
SELECT 
  split_part(
    split_part(
      split_part(sighting_report,' (Entered',1)
      ,'Occurred : ',2)
    ,'Reported',1) as occurred
,split_part(
  split_part(sighting_report,')',1)
    ,'Entered as : ',2) as entered_as
,split_part(
  split_part(
    split_part(
      split_part(sighting_report,'Post',1)
      ,'Reported: ',2)
    ,' AM',1)
  ,' PM',1) as reported
,split_part(split_part(sighting_report,'Location',1),'Posted: ',2) 
  as posted
,split_part(split_part(sighting_report,'Shape',1),'Location: ',2) 
  as location
,split_part(split_part(sighting_report,'Duration',1),'Shape: ',2) 
  as shape
,split_part(sighting_report,'Duration:',2) as duration
FROM ufo
;
 
occurred   entered_as   reported  posted   location     shape       duration
--------   ----------   --------  -------  -----------  ---------   -----------
7/4/2...   07/04/2...   7/5...    7/5/...  Columbus...  Formation   15 minutes
7/4/2...   07/04/2...   7/5...    7/5/...  St. John...  Circle      2-3 minutes
7/4/2...   07/7/1...    7/5...    7/5/...  Royal Pa...  Circle      3 minutes
...        ...          ...       ...      ...          ...         ...
```

With this SQL parsing, the data is now in a much more structured and usable format. Before we finish, however, there are a few transformations that will clean up the data a little further. We’ll take a look at these string transformation functions next.

## Text Transformations

Transformations change string values in some way. We saw a number of date and timestamp transformation functions in [Chapter 3](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#time\_series\_analysis). There is a set of functions in SQL that specifically work on string values. These are useful for working with parsed data, but also for any text data in a database that needs to be adjusted or cleaned for analysis.

Among the most common transformations are the ones that change capitalization. The `upper` function converts all letters to their uppercase form, while the `lower` function converts all letters to their lowercase form. For example:

```
SELECT upper('Some sample text');

upper
----------------
SOME SAMPLE TEXT

SELECT lower('Some sample text');

lower
----------------
some sample text
```

These are useful for standardizing values that may have been entered in different ways. For example, any human will recognize that “California,” “caLiforNia,” and “CALIFORNIA” all refer to the same state, but a database will treat them as distinct values. If we were to count UFO sightings by states with these values, we would end up with three records for California, resulting in incorrect analysis conclusions. Converting them to all uppercase or all lowercase letters would solve this problem. Some databases, including Postgres, have an `initcap` function that capitalizes the first letter of each word in a string. This is useful for proper nouns, such as state names:

```
SELECT initcap('caLiforNia'), initcap('golden gate bridge');

initcap     initcap
----------  ------------------
California  Golden Gate Bridge
```

The `shape` field in the data set we parsed contains one value that is in all capitals, “TRIANGULAR.” To clean this and standardize it with the other values, which all have only their first letter capitalized, apply the `initcap` function:

```
SELECT distinct shape, initcap(shape) as shape_clean
FROM
(
    SELECT split_part(
             split_part(sighting_report,'Duration',1)
             ,'Shape: ',2) as shape
    FROM ufo
) a
;

shape       shape_clean
----------  -----------
...         ... 
Sphere      Sphere
TRIANGULAR  Triangular
Teardrop    Teardrop
...         ...
```

The number of sightings for each shape is shown in [Figure 5-4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#frequency\_of\_shapes\_in\_ufo\_sightings). Light is by far the most common shape, followed by circle and triangle. Some sightings do not report a shape, so a count for null value appears in the graph as well.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0504.png" alt="" height="487" width="600"><figcaption></figcaption></figure>

**Figure 5-4. Frequency of shapes in UFO sightings**

Another useful transformation function is one called `trim` that removes blank spaces at the beginning and end of a string. Extra whitespace characters are a common problem when parsing values out of longer strings, or when data is created by human entry or by copying data from one application to another. As an example, we can remove the leading spaces before “California” in the following string by using the `trim` function:

```
SELECT trim('  California  ');

trim
----------
California
```

The `trim` function has a few optional parameters that make it flexible for a variety of data-cleaning challenges. First, it can remove characters from the start of a string or from the end of a string, or both. Trimming from both ends is the default, but the other options can be specified with `leading` or `trailing`. Additionally, `trim` can remove any character, not just whitespace. So, for example, if an application placed a dollar sign ($) at the beginning of each state name for some reason, we could remove this with `trim`:

```
SELECT trim(leading '$' from '$California');
```

A few of the values in the `duration` field have leading spaces, so applying `trim` will result in a cleaner output:

```
SELECT duration, trim(duration) as duration_clean
FROM
(
    SELECT split_part(sighting_report,'Duration:',2) as duration
    FROM ufo
) a
;

duration               duration_clean
---------------------  --------------------
 ~2 seconds            ~2 seconds
 15 minutes            15 minutes
 20 minutes (ongoing)  20 minutes (ongoing)
```

The number of sightings for the most common durations are graphed in [Figure 5-5](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#top\_onezero\_most\_common\_durations\_of\_uf). Sightings lasting between 1 and 10 minutes are common. Some sightings do not report a duration, so a count for null value appears in the graph.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0505.png" alt="" height="390" width="600"><figcaption></figcaption></figure>

**Figure 5-5. Top 10 most common durations of UFO sightings**

The next type of transformation is a data type conversion. This type of transformation, discussed in [Chapter 2](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch02.html#preparing\_data\_for\_analysis), will be useful for ensuring that the results of our parsing have the intended data type. In our case, there are two fields that should be treated as timestamps—the `occurred` and `reported` columns—and the `posted` column should be a date type. The data types can be changed with casting, using either the double colon (::) operator or the `CAST field as type` syntax. We’ll leave the `entered_as`, `location`, `shape`, and `duration` values as VARCHAR:

```
SELECT occurred::timestamp
,reported::timestamp as reported
,posted::date as posted
FROM
(
    SELECT 
    split_part(
      split_part(
        split_part(sighting_report,' (Entered',1)
        ,'Occurred : ',2)
      ,'Reported',1) 
      as occurred   
    ,split_part(
      split_part(
        split_part(
          split_part(sighting_report,'Post',1)
          ,'Reported: ',2)
        ,' AM',1),' PM',1) 
      as reported
    ,split_part(
      split_part(sighting_report,'Location',1)
      ,'Posted: ',2) 
      as posted
    FROM ufo
) a
;

occurred             reported             posted
-------------------  -------------------  ----------
2015-05-24 19:30:00  2015-05-25 10:07:21  2015-05-29
2015-05-24 22:40:00  2015-05-25 09:09:09  2015-05-29
2015-05-24 22:30:00  2015-05-24 10:49:43  2015-05-29
...                  ...                  ...
```

A sample of the data converts to the new formats. Notice that the database adds the seconds to the timestamp, even though there were no seconds in the original value, and correctly recognizes dates that were in month/day/year (mm/dd/yyyy) format.[1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#ch01fn7) There is a problem when applying these transformations to the entire data set, however. A few records do not have values at all, appearing as an empty string, and some have the time value but no date associated with them. Although an empty string and null seem to contain the same information—nothing—databases treat them differently. An empty string is still a string and can’t be converted to another data type. Setting all the nonconforming records to null with a CASE statement allows the type conversion to work properly. Since we know that dates must contain at least eight characters (four digits for year, one or two digits each for month and day, and two “-” or “/” characters), one way to accomplish this is by setting any record with LENGTH less than 8 equal to null with a CASE statement:

```
SELECT 
case when occurred = '' then null 
     when length(occurred) < 8 then null
     else occurred::timestamp 
     end as occurred
,case when length(reported) < 8 then null
      else reported::timestamp 
      end as reported
,case when posted = '' then null
      else posted::date  
      end as posted
FROM
(
    SELECT 
    split_part(
      split_part(
        split_part(sighting_report,'(Entered',1)
        ,'Occurred : ',2)
      ,'Reported',1) as occurred 
    ,split_part(
      split_part(
        split_part(
          split_part(sighting_report,'Post',1)
          ,'Reported: ',2)
        ,' AM',1)
      ,' PM',1) as reported
    ,split_part(
       split_part(sighting_report,'Location',1)
       ,'Posted: ',2) as posted
    FROM ufo
) a
;

occurred             reported             posted
-------------------  -------------------  ----------
1991-10-01 14:00:00  2018-03-06 08:54:22  2018-03-08
2018-03-04 19:07:00  2018-03-06 07:05:12  2018-03-08
2017-10-16 21:42:00  2018-03-06 05:09:47  2018-03-08
...                  ...                  ...
```

The final transformation I’ll discuss in this section is the `replace` function. Sometimes there is a word, phrase, or other string within a field that we would like to change to another string or remove entirely. The `replace` function comes in handy for this task. It takes three arguments—the original text, the string to find, and the string to substitute in its place:

```
replace(string or field, string to find, string to substitute)
```

So, for example, if we want to change references of “unidentified flying objects” to “UFOs,” we can use the `replace` function:

```
SELECT replace('Some unidentified flying objects were noticed
above...','unidentified flying objects','UFOs');

replace
-------------------------------
Some UFOs were noticed above...
```

This function will find and replace every instance of the string in the second argument, regardless of where it appears. An empty string can be used as the third argument, which is a good way to remove parts of a string that are not wanted. Like other string functions, `replace` can be nested, with the output from one `replace` becoming the input for another.

In the parsed UFO-sighting data set we’ve been working with, some of the `location` values include qualifiers indicating that the sighting took place “near,” “close to,” or “outside of” a city or town. We can use `replace` to standardize these to “near”:

```
SELECT location
,replace(replace(location,'close to','near')
         ,'outside of','near') as location_clean
FROM
(
    SELECT split_part(split_part(sighting_report,'Shape',1)
                      ,'Location: ',2) as location
    FROM ufo
) a
;

location                     location_clean
---------------------------  ---------------------
Tombstone (outside of), AZ   Tombstone (near), AZ
Terrell (close to), TX       Terrell (near), TX
Tehachapie (outside of), CA  Tehachapie (near), CA
...                          ...
```

The top 10 sighting locations are graphed in [Figure 5-6](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#most\_common\_locations\_of\_ufo\_sightings).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0506.png" alt="" height="285" width="600"><figcaption></figcaption></figure>

**Figure 5-6. Most common locations of UFO sightings**

Now we have parsed and cleaned all the elements of the `sighting_report` field into distinct, appropriately typed columns. The final code looks like this:

```
SELECT 
case when occurred = '' then null 
     when length(occurred) < 8 then null
     else occurred::timestamp 
     end as occurred
,entered_as
,case when length(reported) < 8 then null
      else reported::timestamp 
      end as reported
,case when posted = '' then null
      else posted::date  
      end as posted
,replace(replace(location,'close to','near'),'outside of','near') 
 as location
,initcap(shape) as shape
,trim(duration) as duration
FROM
(
    SELECT 
    split_part(
        split_part(split_part(sighting_report,' (Entered',1)
          ,'Occurred : ',2)
          ,'Reported',1) as occurred
    ,split_part(
      split_part(sighting_report,')',1)
        ,'Entered as : ',2) as entered_as   
    ,split_part(
      split_part(
        split_part(
          split_part(sighting_report,'Post',1)
          ,'Reported: ',2)
        ,' AM',1)
      ,' PM',1) as reported
    ,split_part(
       split_part(sighting_report,'Location',1)
       ,'Posted: ',2) as posted
    ,split_part(
       split_part(sighting_report,'Shape',1)
       ,'Location: ',2) as location
    ,split_part(
       split_part(sighting_report,'Duration',1)
       ,'Shape: ',2) as shape
    ,split_part(sighting_report,'Duration:',2) as duration
    FROM ufo
) a
;
 
occurred   entered_as  reported  posted   location     shape      duration
--------   ----------  --------  -------  ----------   --------   ----------
1988-...   8-8-198...  2018-...  2018...  Amity, ...   Unknown    4 minutes
2018-...   07/41/1...  2018-...  2018...  Bakersf...   Triangle   15 minutes
2018-...   08/01/1...  2018-...  2018...  Naples,...   Light      10 seconds
...        ...         ...       ...      ...          ...        ...
```

This piece of SQL code can be reused in other queries, or it can be used to copy the raw UFO data into a new, cleaned-up table. Alternatively, it could be turned into a view or put into a common table expression for reuse. [Chapter 8](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch08.html#creating\_complex\_data\_sets\_for\_analysis) will discuss these strategies in more detail.

We’ve seen how to apply parsing and transformation functions to clean and improve the analysis value of text data that has some amount of structure in it. Next, we’ll look at the other field in the UFO sightings data set, the free text `description` field, and learn how to use SQL functions to search for specific elements.

## Finding Elements Within Larger Blocks of Text

Parsing and transformations are common operations applied to text data to prepare it for analysis. Another common operation with text data is finding strings within larger blocks of text. This can be done to filter results, categorize records, or replace the searched-for strings with alternate values.

### Wildcard Matches: LIKE, ILIKE

SQL has several functions for matching patterns within strings. The LIKE operator matches the specified pattern within the string. In order to allow it to match a pattern and not just find an exact match, wildcard symbols can be added before, after, or in the middle of the pattern. The “%” wildcard matches zero or more characters, while the “\_” wildcard matches exactly one character. If the goal is to match the “%” or “\_” itself, place the backslash escape symbol (“\”) in front of that character:

```
SELECT 'this is an example string' like '%example%';

true

SELECT 'this is an example string' like '%abc%';

false

SELECT 'this is an example string' like '%this_is%';

true
```

The LIKE operator can be used in a number of clauses within the SQL statement. It can be used to filter records in the _WHERE_ clause. For example, some reporters mention that they were with a spouse at the time, and so we might want to find out how many reports mention the word “wife.” Since we want to find the string anywhere in the description text, we’ll place the “%” wildcard before and after “wife”:

```
SELECT count(*)
FROM ufo
WHERE description like '%wife%'
;


count
-----
6231
```

We can see that more than six thousand reports mention “wife.” However, this will return only matches on the lowercase string. What if some reporters mention “Wife,” or they left Caps Lock on and typed in “WIFE” instead? There are two options for making the search case insensitive. One option is to transform the field to be searched using either the `upper` or `lower` function discussed in the previous section, which has the effect of making the search case insensitive since characters are all either uppercase or lowercase:

```
SELECT count(*)
FROM ufo
WHERE lower(description) like '%wife%'
;

count
-----
6439
```

Another way to accomplish this is with the ILIKE operator, which is effectively a case-insensitive LIKE operator. The drawback is that it is not available in every database; notably, MySQL and SQL Server do not support it. However, it’s a nice, compact syntax option if you are working in a database that does support it:

```
SELECT count(*)
FROM ufo
WHERE description ilike '%wife%'
;

count
-----
6439
```

Any of these variations of LIKE and ILIKE can be negated with NOT. So, for example, to find the records that do not mention “wife,” we can use NOT LIKE:

```
SELECT count(*)
FROM ufo
WHERE lower(description) not like '%wife%'
;

count
-----
89024
```

Filtering on multiple strings is possible with AND and OR operators:

```
SELECT count(*)
FROM ufo
WHERE lower(description) like '%wife%'
or lower(description) like '%husband%'
;

count
-----
10571
```

Be careful to use parentheses to control the order of operations when using OR in conjunction with AND operators, or you might get unexpected results. For example, these _WHERE_ clauses do not return the same result since OR is evaluated before AND:

```
SELECT count(*)
FROM ufo
WHERE lower(description) like '%wife%'
or lower(description) like '%husband%'
and lower(description) like '%mother%'
;

count
-----
6610

SELECT count(*)
FROM ufo
WHERE (lower(description) like '%wife%'
       or lower(description) like '%husband%'
       )
and lower(description) like '%mother%'
;

count
-----
382
```

In addition to filtering in _WHERE_ or _JOIN_..._ON_ clauses, LIKE can be used in the _SELECT_ clause to categorize or aggregate certain records. Let’s start with categorization. The LIKE operator can be used within a CASE statement to label and group records. Some of the descriptions mention an activity the observer was doing during or prior to the sighting, such as driving or walking. We can find out how many descriptions contain such terms by using a CASE statement with LIKE:

```
SELECT 
case when lower(description) like '%driving%' then 'driving'
     when lower(description) like '%walking%' then 'walking'
     when lower(description) like '%running%' then 'running'
     when lower(description) like '%cycling%' then 'cycling'
     when lower(description) like '%swimming%' then 'swimming'
     else 'none' end as activity
,count(*)
FROM ufo
GROUP BY 1
ORDER BY 2 desc
;

activity  count
--------  -----
none      77728
driving   11675
walking   4516
running   1306
swimming  196
cycling   42
```

The most common activity was driving, whereas not many people report sightings while swimming or cycling. This is perhaps not surprising, since these activities are simply less common than driving.

**TIP**

Although values derived through text-parsing transformation functions can be used in _JOIN_ criteria, database performance is often a problem. Consider parsing and/or transforming in a subquery and then joining the result with an exact match in the _JOIN_ clause.

Note that this CASE statement labels each description with only one of the activities and evaluates whether each record matches the pattern in the order in which the statement is written. A description that contains both “driving” and “walking” will be labeled as “driving.” This is appropriate in many cases, but particularly when analyzing longer text such as from reviews, survey comments, or support tickets, the ability to label records with multiple categories is important. For this type of use case, a series of binary or BOOLEAN flag columns is called for.

We saw earlier that LIKE can be used to generate a BOOLEAN response of TRUE or FALSE, and we can use this to label rows. In the data set, a number of descriptions mention the direction in which the object was detected, such as north or south, and some mention more than one direction. We might want to label each record with a field indicating whether the description mentions each direction:

```
SELECT description ilike '%south%' as south
,description ilike '%north%' as north
,description ilike '%east%' as east
,description ilike '%west%' as west
,count(*)
FROM ufo
GROUP BY 1,2,3,4
ORDER BY 1,2,3,4
;
```

```
south  north  east   west   count
-----  -----  ----   -----  -----
false  false  false  false  43757
false  false  false  true   3963
false  false  true   false  5724
false  false  true   true   4202
false  true   false  false  4048
false  true   false  true   2607
false  true   true   false  3299
false  true   true   true   2592
true   false  false  false  3687
true   false  false  true   2571
true   false  true   false  3041
true   false  true   true   2491
true   true   false  false  3440
true   true   false  true   2064
true   true   true   false  2684
true   true   true   true   5293
```

The result is a matrix of BOOLEANs that can be used to find the frequency of various combinations of directions, or to find when a direction is used without any of the other directions in the same description.

All of the combinations are useful in some contexts, particularly in building data sets that will be used by others to explore the data, or in a BI or visualization tool. However, sometimes it is more useful to summarize the data further and perform an aggregation on the records that contain a string pattern. Here we will count the records, but other aggregations such as `sum` and `average` can be used if the data set contains other numerical fields, such as sales figures:

```
SELECT 
count(case when description ilike '%south%' then 1 end) as south
,count(case when description ilike '%north%' then 1 end) as north
,count(case when description ilike '%west%' then 1 end) as west
,count(case when description ilike '%east%' then 1 end) as east
FROM ufo
;

south  north  west   east
-----  -----  -----  -----
25271  26027  25783  29326
```

We now have a much more compact summary of the frequency of direction terms in the description field, and we can see that “east” is mentioned more often than other directions. The results are graphed in [Figure 5-7](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#frequency\_of\_compass\_directions\_mention).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0507.png" alt="" height="296" width="600"><figcaption></figcaption></figure>

**Figure 5-7. Frequency of compass directions mentioned in UFO sighting reports**

In the preceding query, we still allow a record that contains more than one direction to be counted more than once. However, there is no longer visibility into which specific combinations exist. Complexity can be added into the query as needed to handle such cases, with a statement such as:

```
count(case when description ilike '%east%' 
and description ilike '%north%' then 1 end) as east
```

Pattern matching with LIKE, NOT LIKE, and ILIKE is flexible and can be used in various places in a SQL query to filter, categorize, and aggregate data for a variety of output needs. These operators can be used in combination with the text-parsing and transformation functions we discussed earlier for even more flexibility. Next, I’ll discuss handling multiple elements when the matches are exact before returning to more patterns in a discussion of regular expressions.

### Exact Matches: IN, NOT IN

Before we move on to more complex pattern matching with regular expressions, it’s worth looking at a couple of additional operators that are useful in text analysis. Although not strictly about pattern matching, they are often useful in combination with LIKE and its relatives in order to come up with a rule set that includes exactly the right set of results. The operators are IN and its negation, NOT IN. These allow you to specify a list of matches, resulting in more compact code.

Let’s imagine we are interested in categorizing the sightings based on the first word of the `description`. We can find the first word using the `split_part` function, with a space character as the delimiter. Many reports start with a color as the first word. We might want to filter the records in order to take a look at reports that start by naming a color. This can be done by listing each color with an OR construction:

```
SELECT first_word, description
FROM
(
    SELECT split_part(description,' ',1) as first_word
    ,description
    FROM ufo
) a
WHERE first_word = 'Red'
or first_word = 'Orange'
or first_word = 'Yellow'
or first_word = 'Green'
or first_word = 'Blue'
or first_word = 'Purple'
or first_word = 'White'
;

first_word  description
----------  ----------------------------------------------------
Blue        Blue Floating LightSaw blue light hovering...
White       White dot of light traveled across the sky, very...
Blue        Blue Beam project known seen from the high desert... 
...         ...
```

Using an IN list is more compact and often less error-prone, particularly when there are other elements in the _WHERE_ clause. IN takes a comma-separated list of items to match. The data type of elements should match the data type of the column. If the data type is numeric, the elements should be numbers; if the data type is text, the elements should be quoted as text (even if the element is a number):

```
SELECT first_word, description
FROM
(
    SELECT split_part(description,' ',1) as first_word
    ,description
    FROM ufo
) a
WHERE first_word in ('Red','Orange','Yellow','Green','Blue','Purple','White')
;

first_word  description
----------  ----------------------------------------------------
Red         Red sphere with yellow light in middleMy Grandson... 
Blue        Blue light fireball shape shifted into several...
Orange      Orange lights.Strange orange-yellow hovering not...
...         ...
```

The two forms are identical in their results, and the frequencies are shown in [Figure 5-8](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#frequency\_of\_select\_colors\_used\_as\_the).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0508.png" alt="" height="337" width="600"><figcaption></figcaption></figure>

**Figure 5-8. Frequency of select colors used as the first word in UFO sighting descriptions**

The main benefit of IN and NOT IN is that they make code more compact and readable. This can come in handy when creating more complex categorizations in the _SELECT_ clause. For example, imagine we wanted to categorize and count the records by the first word into colors, shapes, movements, or other possible words. We might come up with something like the following that combines elements of parsing, transformations, pattern matching, and IN lists:

```
SELECT 
case when lower(first_word) in ('red','orange','yellow','green', 
'blue','purple','white') then 'Color'
when lower(first_word) in ('round','circular','oval','cigar') 
then 'Shape'
when first_word ilike 'triang%' then 'Shape'
when first_word ilike 'flash%' then 'Motion'
when first_word ilike 'hover%' then 'Motion'
when first_word ilike 'pulsat%' then 'Motion'
else 'Other' 
end as first_word_type
,count(*)
FROM
(
    SELECT split_part(description,' ',1) as first_word
    ,description
    FROM ufo
) a
GROUP BY 1
ORDER BY 2 desc
;


first_word_type  count
---------------  -----
Other            85268
Color            6196
Shape            2951
Motion           1048
```

Of course, given the nature of this data set, it would likely take many more lines of code and rules to accurately categorize the reports by first word. SQL allows you to create a variety of complex and nuanced expressions to deal with text data. Next, we’ll turn to some even more sophisticated ways to work with text data in SQL, using regular expressions.

### Regular Expressions

There are a number of ways to match patterns in SQL. One of the most powerful methods, though it is also confusing, is the use of regular expressions (regex). I will admit to finding regular expressions intimidating, and I avoided using them for a long time in my data analysis career. In a pinch, I was lucky enough to have colleagues who were willing to share code snippets and get my work unstuck. It was only when I ended up with a big text analysis project that I decided it was finally time to learn about them.

_Regular expressions_ are sequences of characters, many with special meanings, that define search patterns. The main challenge in learning regex, and in using and maintaining code that contains it, is that the syntax is not particularly intuitive. Code snippets don’t read anything like a human language, or even like computer languages such as SQL or Python. With a working knowledge of the special characters, however, the code can be written and deciphered. Like the code for all our queries, it’s a good idea to start simple, build in complexity as needed, and check results as you go. And leave comments liberally, both for other analysts and for future you.

Regex is a language, but it’s one that is used only within other languages. For example, regular expressions can be called within Java, Python, and SQL, but there is no independent way to program with them. All major databases have some implementation of regex. The syntax isn’t always exactly the same, but as with other functions, once you have a sense of the possibilities, adjusting syntax to your environment should be possible.

A full explanation, and all of the syntax and ways to use regex, is beyond the scope of this book, but I’ll show you enough for you to get started and accomplish a number of common tasks in SQL. For a more thorough introduction, [_Learning Regular Expressions_](https://oreil.ly/5aYkb) by Ben Forta (O’Reilly) is a good choice. Here I’ll start by introducing the ways to indicate to the database that you are using a regex, and then I’ll introduce the syntax, before moving on to some examples of how regex can be useful in the UFO sighting reports analysis.

Regex can be used in SQL statements in a couple of ways. The first is with POSIX comparators, and the second is with regex functions. POSIX stands for Portable Operating System Interface and refers to a set of IEEE standards, but you don’t need to know any more than that to use POSIX comparators in your SQL code. The first comparator is the \~ (tilde) symbol, which compares two statements and returns TRUE if one string is contained in the other. As a simple example, we can check to see whether the string “The data is about UFOs” contains the string “data”:

```
SELECT 'The data is about UFOs' ~ 'data' as comparison;

comparison
----------
true
```

The return value is a BOOLEAN, TRUE or FALSE. Note that, although it doesn’t contain any special syntax, “data” is a regex. Regular expressions can also contain normal text strings. This example is similar to what could be accomplished with a LIKE operator. The \~ comparator is case sensitive. To make it case insensitive, similar to ILIKE, use \~\* (the tilde followed by an asterisk):

```
SELECT 'The data is about UFOs' ~* 'DATA' as comparison;

comparison
----------
true
```

To negate the comparator, place an ! (exclamation point) before the tilde or tilde-asterisk combination:

```
SELECT 'The data is about UFOs' !~ 'alligators' as comparison;

comparison
----------
true
```

[Table 5-1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#posix\_comparators) summarizes the four POSIX comparators.

| Syntax | What it does                                                               | Case sensitive? |
| ------ | -------------------------------------------------------------------------- | --------------- |
| \~     | Compares two statements and returns TRUE if one is contained in the other  | Yes             |
| \~\*   | Compares two statements and returns TRUE if one is contained in the other  | No              |
| !\~    | Compares two statements and returns FALSE if one is contained in the other | Yes             |
| !\~\*  | Compares two statements and returns FALSE if one is contained in the other | No              |

Now that we have a way to introduce regex into our SQL, let’s get familiar with some of the special pattern-matching syntax it offers. The first special character to know is the . (period) symbol, a wildcard that is used to match any single character:

```
SELECT 
'The data is about UFOs' ~ '. data' as comparison_1
,'The data is about UFOs' ~ '.The' as comparison_2
;

comparison_1  comparison_2
------------  ------------
true          false
```

Let’s break this down in order to understand what’s going on and develop our intuition about how regex works. In the first comparison, the pattern tries to match any character, indicated by the period, a space, and then the word “data.” This pattern matches the string “e data” in the example sentence, so TRUE is returned. If this seems counterintuitive, since there are additional characters before the letter “e” and after the word “data,” remember that the comparator is only looking for this pattern somewhere within the string, similar to a LIKE operator. In the second comparison, the pattern tries to match any character followed by “The.” Since in the example sentence “The” is the start of the string and there are no characters before it, the value FALSE is returned.

To match multiple characters, use the \* (asterisk) symbol. This will match zero or more characters, similar to using the % (percent) symbol in a LIKE statement.  This use of the asterisk is different from placing it immediately after the tilde (\~\*), which makes the match case insensitive. Notice, however, that in this case “%” is not a wildcard and is instead treated as a literal character to be matched:

```
SELECT 'The data is about UFOs' ~ 'data *' as comparison_1
,'The data is about UFOs' ~ 'data %' as comparison_2
;

comparison_1  comparison_2
------------  ------------
true          false
```

The next special characters to know are \[ and ] (left and right brackets). These are used to enclose a set of characters, any one of which must match. The brackets match a single character even though multiple characters can be between them, though we’ll see shortly how to match more than one time. One use for the brackets is to make part of a pattern case insensitive by enclosing the uppercase and lowercase letters within the brackets (do not use a comma, as that would match the comma character itself):

```
SELECT 'The data is about UFOs' ~ '[Tt]he' as comparison;

comparison
----------
true
```

In this example, the pattern will match either “the” or “The”; since this string starts the example sentence, the statement returns the value TRUE. This isn’t quite the same thing as the case-insensitive match \~\*, because in this case variations such as “tHe” and “THE” do not match the pattern:

```
SELECT 'The data is about UFOs' ~ '[Tt]he' as comparison_1
,'the data is about UFOs' ~ '[Tt]he' as comparison_2
,'tHe data is about UFOs' ~ '[Tt]he' as comparison_3
,'THE data is about UFOs' ~ '[Tt]he' as comparison_4
;

comparison_1  comparison_2  comparison_3  comparison_4
------------  ------------  ------------  ------------
true          true          false         false
```

Another use of the bracket set match is to match a pattern that includes a number, allowing for any number. For example, imagine we wanted to match any description that mentions “7 minutes,” “8 minutes,” or “9 minutes.” This could be accomplished with a CASE statement with several LIKE operators, but with regex the pattern syntax is more compact:

```
SELECT 'sighting lasted 8 minutes' ~ '[789] minutes' as comparison;

comparison
----------
true
```

To match any number, we could enclose all the digits between the brackets:

```
[0123456789]
```

However, regex allows a range of characters to be entered with a - (dash) separator. All of the numbers can be indicated by \[0-9]. Any smaller range of numbers can be used as well, such as \[0-3] or \[4-9]. This pattern, with a range, is equivalent to the last example that listed out each number:

```
SELECT 'sighting lasted 8 minutes' ~ '[7-9] minutes' as comparison;

comparison
----------
true
```

Ranges of letters can be matched in a similar way. [Table 5-2](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#regex\_range\_patterns) summarizes the range patterns that are most useful in SQL analysis. Nonnumber and nonletter values can also be placed between brackets, as in \[$%@].

| Range pattern | Purpose                                                                                        |
| ------------- | ---------------------------------------------------------------------------------------------- |
| \[0-9]        | Match any number                                                                               |
| \[a-z]        | Match any lowercase letter                                                                     |
| \[A-Z]        | Match any uppercase letter                                                                     |
| \[A-Za-z0-9]  | Match any lower- or uppercase letter, or any number                                            |
| \[A-z]        | Match any ASCII character; generally not used because it matches everything, including symbols |

If the desired pattern match contains more than one instance of a particular value or type of value, one option is to include as many ranges as needed, one after the other. For example, we can match a three-digit number by repeating the number range notation three times:

```
SELECT 'driving on 495 south' ~ 'on [0-9][0-9][0-9]' as comparison;

comparison
----------
true
```

Another option is to use one of the optional special syntaxes for repeating a pattern multiple times. This can be useful when you don’t know exactly how many times the pattern will repeat, but be careful to check the results to make sure you don’t accidentally return more matches than intended. To match one or more times, place the + (plus) symbol after the pattern:

```
SELECT 
'driving on 495 south' ~ 'on [0-9]+' as comparison_1
,'driving on 1 south' ~ 'on [0-9]+' as comparison_2
,'driving on 38east' ~ 'on [0-9]+' as comparison_3
,'driving on route one' ~ 'on [0-9]+' as comparison_4
;

comparison_1  comparison_2  comparison_3  comparison_4
------------  ------------  ------------  ------------
true          true          true          false
```

[Table 5-3](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#table\_five\_threedotregex\_patterns\_for\_m) summarizes the other options for indicating the number of times to repeat a pattern.

| Symbol | Purpose                                                                                                                                                                           |
| ------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| +      | Match the character set one or more times                                                                                                                                         |
| \*     | Match the character set zero or more times                                                                                                                                        |
| ?      | Match the character set zero or one time                                                                                                                                          |
| { }    | Match the character set the number of times specified between the curly braces; for example, {3} matches exactly three times                                                      |
| { , }  | Match the character set any number of times in a range specified by the comma-separated numbers between the curly braces; for example, {3,5} matches between three and five times |

Sometimes rather than matching a pattern, we want to find items that do _not_ match a pattern. This can be done by placing the ^ (caret) symbol before the pattern, which serves to negate the pattern:

```
SELECT 
'driving on 495 south' ~ 'on [0-9]+' as comparison_1
,'driving on 495 south' ~ 'on ^[0-9]+' as comparison_2
,'driving on 495 south' ~ '^on [0-9]+' as comparison_3
;

comparison_1  comparison_2  comparison_3
------------  ------------  ------------ 
true          false         false
```

We might want to match a pattern that includes one of the special characters, so we need a way to tell the database to check for that literal character and not treat it as special. To do this, we need an escape character, which is the \ (backslash) symbol in regex:

```
SELECT 
'"Is there a report?" she asked' ~ '\?' as comparison_1
,'it was filed under ^51.' ~ '^[0-9]+' as comparison_2
,'it was filed under ^51.' ~ '\^[0-9]+' as comparison_3
;

comparison_1  comparison_2  comparison_3
------------  ------------  ------------ 
true          false         true
```

In the first line, omitting the backslash before the question mark causes the database to return an “invalid regular expression” error (the exact wording of the error may be different depending on the database type). In the second line, even though ^ is followed by one or more digits (\[0-9]+), the database interprets the ^ in the comparison `'^[0-9]+'` to be a negation and will evaluate whether the string does not include the specified digits. The third line escapes the caret with a backslash, and the database now interprets this as the literal ^ character.

Text data usually includes whitespace characters. These range from the space, which our eyes notice, to the subtle and sometimes unprinted tab and newline characters. We will see later how to replace these with regex, but for now let’s stick to how to match them in a regex. Tabs are matched with \t. Newlines are matched with \r for a carriage return or \n for a line feed, and depending on the operating system, sometimes both are required: \r\n. Experiment with your environment by running a few simple queries to see what returns the desired result. To match any whitespace character, use \s, but note that this also matches the space character:

```
SELECT 
'spinning
flashing
and whirling' ~ '\n' as comparison_1
,'spinning
flashing
and whirling' ~ '\s' as comparison_2
,'spinning flashing' ~ '\s' as comparison_3
,'spinning' ~ '\s' as comparison_4
;  

comparison_1  comparison_2  comparison_3  comparison_4
------------  ------------  ------------  ------------
true          true          true          false
```

**TIP**

SQL query tools or SQL query parsers may have trouble interpreting new lines typed directly into them and thus may return an error. If this is the case, try copying and pasting the text from the source rather than typing it in. All SQL query tools should be able to work with newlines that exist in a database table, however.

Similar to mathematical expressions, parentheses can be used to enclose expressions that should be treated together. For example, we might want to match a somewhat complex pattern that repeats several times:

```
SELECT 
'valid codes have the form 12a34b56c' ~ '([0-9]{2}[a-z]){3}' 
  as comparison_1
,'the first code entered was 123a456c' ~ '([0-9]{2}[a-z]){3}' 
  as comparison_2
,'the second code entered was 99x66y33z' ~ '([0-9]{2}[a-z]){3}' 
  as comparison_3
;

comparison_1  comparison_2  comparison_3
------------  ------------  ------------
true          false          true
```

All three lines use the same regex pattern, `'([0-9]{2}[a-z]){3}'`, for matching. The pattern inside the parentheses, `[0-9]{2}[a-z]`, looks for two digits followed by a lowercase letter. Outside of the parentheses, `{3}` indicates that the whole pattern should be repeated three times. The first line follows this pattern, since it contains the string `12a34b56c`. The second line does not match the pattern; it does have two digits followed by a lowercase letter (`23a`) and then two more digits (`23a45`), but this second repetition is followed by a third digit rather than by another lowercase letter (`23a456`), so there is no match. The third line has a matching pattern, `99x66y33z`.

As we’ve just seen, regex can be used in any number of combinations with other expressions, both regex and normal text, to create pattern-matching code. In addition to specifying _what_ to match, regex can be used to specify _where_ to match. Use the special character \y to match a pattern starting at the beginning or end of a word (in some databases, this might be \b instead). As an example, imagine we were interested in finding the word “car” in the UFO sighting reports. We could write an expression like this:

```
SELECT 
'I was in my car going south toward my home' ~ 'car' as comparison;

comparison
----------
true
```

It finds “car” in the string and returns TRUE as expected. However, let’s look at a few more strings from the data set, looking for the same expression:

```
SELECT 
'I was in my car going south toward my home' ~ 'car' 
  as comparison_1
,'UFO scares cows and starts stampede breaking' ~ 'car' 
  as comparison_2
,'I''m a carpenter and married father of 2.5 kids' ~ 'car' 
  as comparison_3
,'It looked like a brown boxcar way up into the sky' ~ 'car' 
  as comparison_4
;

comparison_1  comparison_2  comparison_3  comparison_4
------------  ------------  ------------  ------------
true          true          true          true
```

All of these strings match the pattern “car” as well, even though “scares,” “carpenter,” and “boxcar” aren’t exactly what was intended when we went looking for mentions of cars. To fix this, we can add \y to the beginning and end of the “car” pattern in our expression:

```
SELECT 
'I was in my car going south toward my home' ~ '\ycar\y' 
  as comparison_1
,'UFO scares cows and starts stampede breaking' ~ '\ycar\y' 
  as comparison_2
,'I''m a carpenter and married father of 2.5 kids' ~ '\ycar\y' 
  as comparison_3
,'It looked like a brown boxcar way up into the sky' ~ '\ycar\y' 
  as comparison_4
;

comparison_1  comparison_2  comparison_3  comparison_4
------------  ------------  ------------  ------------
true          false         false         false
```

Of course, in this simple example, we could have simply added spaces before and after the word “car” with the same result. The benefit of the pattern is that it will also pick up cases in which the pattern is at the beginning of a string and thus does not have a leading space:

```
SELECT 'Car lights in the sky passing over the highway' ~* '\ycar\y' 
 as comparison_1
,'Car lights in the sky passing over the highway' ~* ' car ' 
 as comparison_2
;

comparison_1  comparison_2
------------  ------------
true          false
```

The pattern `'\ycar\y'` makes a case-insensitive match when “Car” is the first word, but the pattern `' car '` does not. To match thebeginning of an entire string, use the \A special character, and to match the end of a string, use \Z:

```
SELECT 
'Car lights in the sky passing over the highway' ~* '\Acar\y' 
  as comparison_1
,'I was in my car going south toward my home' ~* '\Acar\y' 
  as comparison_2
,'An object is sighted hovering in place over my car' ~* '\ycar\Z' 
  as comparison_3
,'I was in my car going south toward my home' ~* '\ycar\Z' 
  as comparison_4
;

comparison_1  comparison_2  comparison_3  comparison_4
------------  ------------  ------------  ------------
true          false         true          false
```

In the first line, the pattern matches “Car” at the beginning of the string. The second line starts with “I,” so the pattern does not match. In the third line, the pattern is looking for “car” at the end of the string and does match it. Finally, in the fourth line, the last word is “home,” so the pattern does not match.

If this is your first time working with regular expressions, it may take a few read-throughs and some experimentation in your SQL editor to get the hang of them. There’s nothing like working with real examples to help solidify learning, so next I’ll go through some applications to our UFO sightings analysis, and I’ll also introduce a couple of specific regex SQL functions.

**NOTE**

Regular expression implementations vary widely by database vendor. The POSIX operators in this section work in Postgres and in databases derived from Postgres such as Amazon Redshift, but not necessarily in others.

An alternative to the \~ operator is the `rlike` or `regexp_like` function (depending on the database). These have the following format:

```
regexp_like(string, pattern, optional_parameters)
```

The first example in this section would be written as:

```
SELECT regexp_like('The data is about UFOs','data') 
 as comparison;
```

The optional parameters control matching type, such as whether the match is case insensitive.

Many of these databases have additional functions not covered here, such as `regexp_substr` to find matching substrings, and `regexp_count` to find the number of times a pattern is matched. Postgres supports POSIX but unfortunately does not support these other functions. Organizations that expect to do a lot of text analysis will do well to choose a database type with a robust set of regular expression functions.

#### Finding and replacing with regex

In the previous section, we discussed regular expressions and how to construct patterns with regex to match parts of strings in our data sets. Let’s apply this technique to the UFO sightings data set to see how it works in practice. Along the way, I’ll also introduce some additional regex SQL functions.

The sighting reports contain a variety of details, such as what the reporter was doing at the time of the sighting and when and where they were doing it. Another detail commonly mentioned is seeing some number of lights. As a first example, let’s find the descriptions that contain a number and the word “light” or “lights.” For the sake of display in this book, I’ll just check the first 100 characters, but this code can also work across the entire description field:

```
SELECT left(description,50)
FROM ufo
WHERE left(description,50) ~ '[0-9]+ light[s ,.]'
;

left
--------------------------------------------------
Was walking outside saw 5 lights in a line changed
2 lights about 5 mins apart, goin from west to eas
Black triangular aircraft with 3 lights hovering a
...
```

The regular expression pattern matches any number of digits (\[0-9]+), followed by a space, then the string “light”, and finally either a letter “s,” a space, a comma, or a period. In addition to finding the relevant records, we might want to split out just the part that refers to the number and the word “lights.” To do this, we’ll use the regex function `regexp_matches`.

**TIP**

Regex function support varies widely by database vendor and sometimes by database software version. SQL Server does not support the functions, while MySQL has minimal support for them. Analytic databases such as Redshift, Snowflake, and Vertica support a variety of useful functions. Postgres has only match and replace functions. Explore the documentation for your database for specific function availability.

The `regexp_matches` function takes two arguments: a string to search and a regex match pattern. It returns an array of the string(s) that matched the pattern. If there are no matches, a null value is returned. Since the return value is an array, we’ll use an index of \[1] to return just a single value as a VARCHAR, which will allow for additional string manipulation as needed. If you are working in another type of database, the `regexp_substr` function is similar to `regexp_matches`, but it returns a VARCHAR value, so there is no need to add the \[1] index.

**TIP**

An _array_ is a collection of objects stored together in the computer’s memory. In databases, arrays are enclosed in { } (curly braces), and this is a good way to spot that something in the database is not one of the regular data types we’ve been working with so far. Arrays have some advantages when storing and retrieving data, but they are not as easy to work with in SQL since they require special syntax. Elements in an array are accessed using \[ ] (square brackets) notation. For our purposes here, it’s enough to know that the first element is found with \[1], the second with \[2], and so on.

Building on our example, we can parse the desired value, the number, and the word “light(s)” from the description field and then _GROUP BY_ this value and the most common variations:

```
SELECT (regexp_matches(description,'[0-9]+ light[s ,.]'))[1]
,count(*)
FROM ufo
WHERE description ~ '[0-9]+ light[s ,.]'
GROUP BY 1
ORDER BY 2 desc
; 

regexp_matches  count
--------------  -----
3 lights        1263
2 lights        565
4 lights        549
...             ...
```

The top 10 results are graphed in [Figure 5-9](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#number\_of\_lights\_mentioned\_at\_the\_begin).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0509.png" alt="" height="353" width="600"><figcaption></figcaption></figure>

**Figure 5-9. Number of lights mentioned at the beginning of UFO sighting descriptions**

Reports mentioning three lights are more than twice as common as the second most often mentioned number of lights, and from two to six lights are most commonly seen. To find the full range of the number of lights, we can parse the matched text and then find the `min` and `max` values:

```
SELECT min(split_part(matched_text,' ',1)::int) as min_lights
,max(split_part(matched_text,' ',1)::int) as max_lights
FROM
(
    SELECT (regexp_matches(description
                           ,'[0-9]+ light[s ,.]')
                           )[1] as matched_text
    ,count(*)
    FROM ufo
    WHERE description ~ '[0-9]+ light[s ,.]'
    GROUP BY 1
) a
; 

min_lights  max_lights
----------  -----
0           2000
```

At least one report mentions two thousand lights, and the minimum value of zero lights is also mentioned. We might want to review these reports further to see if there is anything else interesting or unusual about these extreme values.

In addition to finding matches, we might want to replace the matched text with some alternate text. This is particularly useful when trying to clean a data set of text that has multiple spellings for the same underlying thing. The `regexp_replace` function can accomplish this. It is similar to the `replace` function discussed earlier in the chapter, but it can take a regular expression argument as the pattern to match. The syntax is similar to the `replace` function:

```
regexp_replace(field or string, pattern, replacement value)
```

Let’s put this to work to try to clean up the `duration` field that we parsed out of the `sighting_report` column earlier. This appears to be a free text entry field, and there are more than eight thousand different values. However, inspection reveals that there are common themes—most refer to some combination of seconds, minutes, and hours:

```
SELECT split_part(sighting_report,'Duration:',2) as duration
,count(*) as reports
FROM ufo
GROUP BY 1
;

duration    reports
--------    -------
10 minutes  4571
1 hour      1599
10 min      333
10 mins     150
>1 hour     113
...         ...
```

Within this sample, the durations of “10 minutes,” “10 min,” and “10 mins” all represent the same amount of time, but the database doesn’t know to combine them because the spellings are slightly different. We could use a series of nested `replace` functions to convert all these different spellings. However, we would also have to take into account other variations, such as capitalizations. Regex is handy in this situation, allowing us to create more compact code. The first step is to develop a pattern that matches the desired string, which we can do with the `regexp_matches` function. It’s a good idea to review this intermediate step to make sure we’re matching the correct text:

```
SELECT duration
,(regexp_matches(duration
                 ,'\m[Mm][Ii][Nn][A-Za-z]*\y')
                 )[1] as matched_minutes
FROM
(
    SELECT split_part(sighting_report,'Duration:',2) as duration
    ,count(*) as reports
    FROM ufo
    GROUP BY 1
) a
;

duration      matched_minutes
------------  ---------------
10 min.       min
10 minutes+   minutes
10 min        min
10 minutes +  minutes
10 minutes?   minutes
10 minutes    minutes
10 mins       mins
...           ...
```

Let’s break this down. In the subquery, the `duration` value is split out of the `sighting_report` field. Then the `regexp_matches` function looks for strings that match the pattern:

```
'\m[Mm][Ii][Nn][A-Za-z]*\y'
```

This pattern starts at the beginning of a word (\m) and then looks for any sequence of the letters “m,” “i,” and “n,” regardless of capitalization (\[Mm] and so on). Next, it looks for zero or more instances of any other lowercase or uppercase letter (\[A-Za-z]\*), and then it finally checks for the end of a word (\y) so that only the word that includes the variation of “minutes” is included and not the rest of the string. Notice that the “+” and “?” characters are not matched. With this pattern, we can now replace all these variations with the standard value “min”:

```
SELECT duration
,(regexp_matches(duration
                 ,'\m[Mm][Ii][Nn][A-Za-z]*\y')
                 )[1] as matched_minutes
,regexp_replace(duration
                 ,'\m[Mm][Ii][Nn][A-Za-z]*\y'
                 ,'min') as replaced_text
FROM
(
    SELECT split_part(sighting_report,'Duration:',2) as duration
    ,count(*) as reports
    FROM ufo
    GROUP BY 1
) a
;

duration      matched_minutes  replaced_text
-----------   ---------------  -------------
10 min.       min              10 min.
10 minutes+   minutes          10 min+
10 min        min              10 min
10 minutes +  minutes          10 min +
10 minutes?   minutes          10 min?
10 minutes    minutes          10 min
10 mins       mins             10 min
...           ...              ...
```

The values in the `replaced_text` column are much more standardized now. The period, plus, and question mark characters could also be replaced by enhancing the regex. From an analytical standpoint, however, we might want to consider how to represent the uncertainty that the plus and question mark represent. The `regexp_replace` functions can be nested in order to achieve replacement of different parts or types of strings. For example, we can standardize both the minutes and the hours:

```
SELECT duration
,(regexp_matches(duration
                 ,'\m[Hh][Oo][Uu][Rr][A-Za-z]*\y')
                 )[1] as matched_hour
,(regexp_matches(duration
                 ,'\m[Mm][Ii][Nn][A-Za-z]*\y')
                 )[1] as matched_minutes
,regexp_replace(
        regexp_replace(duration
                       ,'\m[Mm][Ii][Nn][A-Za-z]*\y'
                       ,'min') 
        ,'\m[Hh][Oo][Uu][Rr][A-Za-z]*\y'
        ,'hr') as replaced_text
FROM
(
    SELECT split_part(sighting_report,'Duration:',2) as duration
    ,count(*) as reports
    FROM ufo
    GROUP BY 1
) a
;

duration             matched_hour  matched_minutes  replaced_text
-------------------  ------------  ---------------  -------------
1 Hour 15 min        Hour          min              1 hr 15 min
1 hour & 41 minutes  hour          minutes          1 hr & 41 min
1 hour 10 mins       hour          mins             1 hr 10 min
1 hour 10 minutes    hour          minutes          1 hr 10 min
...                  ...           ...              ...
```

The regex for hours is similar to the one for minutes, looking for case-insensitive matches of “hour” at the beginning of a word, followed by zero or more other letter characters before the end of the word. The intermediate hour and minutes matches may not be needed in the final result, but I find them helpful to review as I’m developing my SQL code to prevent errors later. A full cleaning of the `duration` column would likely involve many more lines of code, and it’s all too easy to lose track and introduce a typo.

The `regexp_replace` function can be nested any number of times, or it can be combined with the basic `replace` function. Another use for `regexp_replace` is in CASE statements, for targeted replacement when conditions in the statement are met. Regex is a powerful and flexible tool within SQL that, as we’ve seen, can be used in a number of ways within an overall SQL query.

In this section, I’ve introduced a number of ways to search for, find, and replace specific elements within longer texts, from wildcard matches with LIKE to IN lists and more complex pattern matching with regex. All of these, along with the text-parsing and transformation functions introduced earlier, allow us to create customized rule sets with as much complexity as needed to handle the data sets in hand. It’s worth keeping in mind the balance between complexity and maintenance burden, however. For one-time analysis of a data set, it can be worth the trouble to create complex rule sets that perfectly clean the data. For ongoing reporting and monitoring, it’s usually worthwhile to explore options for receiving cleaner data from data sources. Next, we’ll turn to several ways to construct new text strings with SQL: using constants, existing strings, and parsed strings.

## Constructing and Reshaping Text

We’ve seen how to parse, transform, find, and replace elements of strings in order to perform a variety of cleaning and analysis tasks with SQL. In addition to these, SQL can be used to generate new combinations of text. In this section, I’ll first discuss _concatenation_, which allows different fields and types of data to be consolidated into a single field. Then I’ll discuss changing text shape with functions that combine multiple columns into a single row, as well as the opposite: breaking up a single string into multiple rows.

### Concatenation

New text can be created with SQL with concatenation. Any combination of constant or hardcoded text, database fields, and calculations on those fields can be joined together. There are a few ways to concatenate. Most databases support the `concat` function, which takes as arguments the fields or values to be concatenated:

```
concat(value1, value2)
concat(value1, value2, value3...)
```

Some databases support the `concat_ws` (concatenate with separator) function, which takes a separator value as the first argument, followed by the list of values to concatenate. This is useful when there are multiple values that you want to put together, using a comma, dash, or similar element to separate them:

```
concat_ws(separator, value1, value2...)
```

Finally, || (double pipe) can be used in  many databases to concatenate strings (SQL Server uses + instead):

```
value1 || value2
```

**TIP**

If any of the values in a concatenation are null, the database will return null. Be sure to use `coalesce` or CASE to replace null values with a default if you suspect they can occur.

Concatenation can bring together a field and a constant string. For example, imagine we wanted to label the shapes as such and add the word “reports” to the count of reports for each shape. The subquery parses the name of the shape from the `sighting_report` field and `count`s the number of records. The outer query concatenates the shapes with the string `' (shape)'` and the `reports` with the string `' reports'`:

```
SELECT concat(shape, ' (shape)') as shape
,concat(reports, ' reports') as reports
FROM
(
    SELECT split_part(
                 split_part(sighting_report,'Duration',1)
                 ,'Shape: ',2) as shape
    ,count(*) as reports
    FROM ufo
    GROUP BY 1
) a
;

Shape             reports
----------------  ------------
Changing (shape)  2295 reports
Chevron (shape)   1021 reports
Cigar (shape)     2119 reports
...               ...
```

We can also combine two fields together, optionally with a string separator. For example, we could unite the shape and location values into a single field:

```
SELECT concat(shape,' - ',location) as shape_location
,reports
FROM
(
    SELECT 
    split_part(split_part(sighting_report,'Shape',1)
      ,'Location: ',2) as location
    ,split_part(split_part(sighting_report,'Duration',1)
       ,'Shape: ',2) as shape
    ,count(*) as reports
    FROM ufo
    GROUP BY 1,2
) a
;

shape_location           reports
-----------------------  -------
Light - Albuquerque, NM  58
Circle - Albany, OR      11
Fireball - Akron, OH     8
...                      ...
```

The top 10 combinations are graphed in [Figure 5-10](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#top\_combinations\_of\_shape\_and\_location).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0510.png" alt="" height="319" width="600"><figcaption></figcaption></figure>

**Figure 5-10. Top combinations of shape and location in UFO sightings**

We saw earlier that “light” is the most common shape, so it’s not surprising that it appears in each of the top results. Phoenix is the most common location, while Las Vegas is the second most common overall.

In this case, since we went to so much trouble to parse out the different fields, it might not make as much sense to concatenate them back together. However, it can be useful to rearrange text or combine values into a single field for display in another tool. By combining various fields and text, we can also generate sentences that can function as summaries of the data, for use in emails or automated reports. In this example, subquery `a` parses the `occurred` and `shape` fields, as we’ve seen previously, and `count`s the records. Then in subquery `aa`, the `min` and `max` of `occurred` are calculated, along with the total number of `reports`, and the results are _GROUP_ed _BY_ `shape`. Rows with `occurred` fields shorter than eight characters are excluded, to remove ones that don’t have properly formed dates and avoid errors in the `min` and `max` calculations. Finally, in the outer query, the final text is assembled with the `concat` function. The format of the dates is changed to read as long dates (April 9, 1957) for the earliest and latest dates:

```
SELECT 
concat('There were '
       ,reports
       ,' reports of '
       ,lower(shape)
       ,' objects. The earliest sighting was '
       ,trim(to_char(earliest,'Month'))
       , ' '
       , date_part('day',earliest)
       , ', '
       , date_part('year',earliest)
       ,' and the most recent was '
       ,trim(to_char(latest,'Month'))
       , ' '
       , date_part('day',latest)
       , ', '
       , date_part('year',latest)
       ,'.'
       )
FROM
(
    SELECT shape
    ,min(occurred::date) as earliest
    ,max(occurred::date) as latest
    ,sum(reports) as reports
    FROM
    (
        SELECT split_part(
                     split_part(
                           split_part(sighting_report,' (Entered',1)
                           ,'Occurred : ',2)
                     ,'Reported',1) as occurred
        ,split_part(
               split_part(sighting_report,'Duration',1)
               ,'Shape: ',2) as shape
        ,count(*) as reports
        FROM ufo
        GROUP BY 1,2
    ) a
    WHERE length(occurred) >= 8
    GROUP BY 1
) aa    
;

concat
---------------------------------------------------------------------
There were 820 reports of teardrop objects. The earliest sighting was 
April 9, 1957 and the most recent was October 3, 2020.
There were 7331 reports of fireball objects. The earliest sighting was 
June 30, 1790 and the most recent was October 5, 2020.
There were 1020 reports of chevron objects. The earliest sighting was 
July 15, 1954 and the most recent was October 3, 2020.
```

We could get even more creative with formatting the number of reports or adding `coalesce` or CASE statements to handle blank shape names, for example. Although these sentences are repetitive and are therefore no match for human (or AI) writers, they will be dynamic if the data source is frequently updated and thus can be useful in reporting applications.

Along with functions and operators for creating new text with concatenation, SQL has some special functions for reshaping text, which we’ll turn to next.

### Reshaping Text

As we saw in [Chapter 2](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch02.html#preparing\_data\_for\_analysis), changing the shape of the data—either pivoting from rows to columns or the reverse, changing the data from columns to rows—is sometimes useful. We saw how to do that with _GROUP BY_ and aggregations, or with _UNION_ statements. In SQL there are some special functions for reshaping text, however.

One use case for reshaping text is when there are multiple rows with different text values for an entity and we would like to combine them into a single value. Combining values can make them more difficult to analyze, of course, but sometimes the use case requires a single record per entity in the output. Combining the individual values into a single field allows us to retain the detail. The `string_agg` function takes two arguments, a field or an expression, and a separator, which is commonly a comma but can be any separator character desired. The function aggregates only values that are not null, and the order can be controlled with an _ORDER BY_ clause within the function as needed:

```
SELECT location
,string_agg(shape,', ' order by shape asc) as shapes
FROM
(
    SELECT 
    case when split_part(
                    split_part(sighting_report,'Duration',1)
                    ,'Shape: ',2) = '' then 'Unknown'
         when split_part(
                    split_part(sighting_report,'Duration',1)
                    ,'Shape: ',2) = 'TRIANGULAR' then 'Triangle'
         else split_part(
                    split_part(sighting_report,'Duration',1),'Shape: ',2)  
         end as shape
    ,split_part(
            split_part(sighting_report,'Shape',1)
            ,'Location: ',2) as location
    ,count(*) as reports
    FROM ufo
    GROUP BY 1,2
) a
GROUP BY 1
;

location        shapes
--------------  -----------------------------------
Macungie, PA    Fireball, Formation, Light, Unknown
Kingsford, MI   Circle, Light, Triangle
Olivehurst, CA  Changing, Fireball, Formation, Oval
...             ...
```

Since `string_agg` is an aggregate function, it requires a _GROUP BY_ clause on the other fields in the query. In MySQL, an equivalent function is `group_concat`, and analytic databases such as Redshift and Snowflake have a similar function called `listagg`.

Another use case is to do just the opposite of `string_agg` and instead split out a single field into multiple rows. There is a lot of inconsistency in how this is implemented in different databases, and even whether a function exists for this at all. Postgres has a function called `regexp_split_to_table`, while certain other databases have a `split_to_table` function that operates similarly (check documentation for availability and syntax in your database). The `regexp_split_to_table` function takes two arguments, a string value and a delimiter. The delimiter can be a regular expression, but keep in mind that a regex can also be a simple string such as a comma or space character. The function then splits the values into rows:

```
SELECT 
regexp_split_to_table('Red, Orange, Yellow, Green, Blue, Purple'
                      ,', '); 

regexp_split_to_table
---------------------
Red
Orange
Yellow
Green
Blue
Purple
```

The string to be split can include anything and doesn’t necessarily need to be a list. We can use the function to split up any string, including sentences. We can then use this to find the most common words used in text fields, a potentially useful tool for text analysis work. Let’s take a look at the most common words used in UFO sighting report descriptions:

```
SELECT word, count(*) as frequency
FROM
(
    SELECT regexp_split_to_table(lower(description),'\s+') as word
    FROM ufo
) a
GROUP BY 1
ORDER BY 2 desc
;

word  frequency
----  ---------
the   882810
and   477287
a     450223
```

The subquery first transforms the `description` into lowercase, since case variations are not interesting for this example. Next, the string is split using the regex `'\s+'`, which splits on any one or more whitespace characters.

The most commonly used words are not surprising; however, they are not particularly useful since they are just commonly used words in general. To find a more meaningful list, we can remove what are called _stop words_. These are simply the most commonly used words in a language. Some databases have built-in lists in what are called dictionaries, but the implementations are not standard. There is also no single agreed-upon correct list of stop words, and it is common to adjust the particular list for the desired application; however, there are a number of lists of common stop words on the internet. For this example, I loaded a list of 421 common words into a table called `stop_words`, available on the book’s [GitHub site](https://oreil.ly/94jIS). The stop words are removed from the result set with a _LEFT JOIN_ to the `stop_words` table, filtered to results that are not in that table:

```
SELECT word, count(*) as frequency
FROM
(
    SELECT regexp_split_to_table(lower(description),'\s+') as word
    FROM ufo
) a
LEFT JOIN stop_words b on a.word = b.stop_word
WHERE b.stop_word is null
GROUP BY 1
ORDER BY 2 desc
;

word    frequency
------  ---------
light   97071
lights  89537
object  80785
...     ...
```

The top 10 most common words are graphed in [Figure 5-11](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#most\_common\_words\_in\_ufo\_sighting\_descr).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0511.png" alt="" height="309" width="600"><figcaption></figcaption></figure>

**Figure 5-11. Most common words in UFO sighting descriptions, excluding stop words**

We could continue to get more sophisticated by adding additional common words to the `stop_words` table or by _JOIN_ing the results with the descriptions to tag them with the interesting words they contain. Note that `regexp_split_to_table` and similar functions in other databases can be slow, depending on the length and number of records analyzed.

Constructing and reshaping text with SQL can be done in as simple or complex a way or ways as needed. Concatenation, string aggregation, and string-splitting functions can be used alone, in combination with each other, and with other SQL functions and operators to achieve the desired data output.

## Conclusion

Although SQL isn’t always the first tool mentioned when it comes to text analysis, it has many powerful functions and operators for accomplishing a variety of tasks. From parsing and transformations, to finding and replacing, to constructing and reshaping text, SQL can be used to both clean and prepare text data as well as perform analysis.

In the next chapter, we’ll turn to using SQL for anomaly detection, another topic in which SQL isn’t always the first tool mentioned but for which it has surprising capabilities.

[1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#ch01fn7-marker) Since the data set was created in the United States, it is in mm/dd/yyyy format. Many other parts of the world use the dd/mm/yyyy format instead. It’s always worth checking your source and adjusting your code as needed.
