# 9. Conclusion

## Chapter 9. Conclusion

Throughout the book, we’ve seen how SQL is a flexible and powerful language for a range of data analysis tasks. From data profiling to time series, text analysis, and anomaly detection, SQL can tackle a number of common requirements. Techniques and functions can also be combined in any given SQL statement to perform experiment analysis and build complex data sets. While SQL can’t accomplish all analysis goals, it fits well into the ecosystem of analysis tools.

In this final chapter, I’ll discuss a few additional types of analysis and point out how various SQL techniques covered in the book can be combined to accomplish them. Then I’ll wrap up with some resources that you can use to continue your journey of mastering data analysis or to dig deeper into specific topics.

## Funnel Analysis

A funnel consists of a series of steps that must be completed to reach a defined goal. The goal might be registering for a service, completing a purchase, or obtaining a course completion certificate. Steps in a website purchase funnel, for example, might include clicking the “Add to Cart” button, filling out shipping information, entering a credit card, and finally clicking the “Place Order” button.

Funnel analysis combines elements of time series analysis, discussed in [Chapter 3](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#time\_series\_analysis), and cohort analysis, discussed in [Chapter 4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch04.html#cohort\_analysis). The data for funnel analysis comes from a time series of events, although in this case those events correspond to distinct real-world actions rather than being repetitions of the same event. Measuring retention from step to step is a key goal of funnel analysis, although in this context we often use the term _conversion_. Typically, entities drop out along the steps of the process, and a graph of their number at each stage ends up looking like a household funnel—hence the name.

This type of analysis is used to identify areas of friction, difficulty, or confusion. Steps at which large numbers of users drop out, or that many fail to complete, provide insight into opportunities for optimization. For example, a checkout process that asks for credit card information before showing the total amount including shipping might turn off some would-be purchasers. Showing the total before this step may encourage more purchase completions. Such changes are often subjects of experiments, discussed in [Chapter 7](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch07.html#experiment\_analysis). Funnels can also be monitored in order to detect unexpected external events. For example, changes in completion rates might correspond to good (or bad) PR or to a change in the pricing or tactics of a competitor.

The first step in a funnel analysis is to figure out the base population of all users, customers, or other entities that were eligible to enter the process. Next, assemble the data set of completion for each step of interest, including the final goal. Often this includes one or more _LEFT JOIN_s in order to include all of the base population, along with those who completed each step. Then `count` the users in each step and divide these step-wise counts by the total `count`. There are two ways to set up the queries, depending on whether all steps are required.

When all steps in the funnel are required—or if you only want to include users who have completed all steps—_LEFT JOIN_ each table to the previous table:

```
SELECT count(a.user_id) as all_users
,count(b.user_id) as step_one_users
,count(b.user_id) / count(a.user_id) as pct_step_one
,count(c.user_id) as step_two_users
,count(c.user_id) / count(b.user_id) as pct_one_to_two
FROM users a
LEFT JOIN step_one b on a.user_id = b.user_id
LEFT JOIN step_two c on b.user_id = c.user_id
;
```

When users can skip a step, or if you want to allow for this possibility, _LEFT JOIN_ each table to the one containing the full population and calculate the share of that starting group:

```
SELECT count(a.user_id) as all_users
,count(b.user_id) as step_one_users
,count(b.user_id) / count(a.user_id) as pct_step_one
,count(c.user_id) as step_two_users
,count(c.user_id) / count(b.user_id) as pct_step_two
FROM users a
LEFT JOIN step_one b on a.user_id = b.user_id
LEFT JOIN step_two c on a.user_id = c.user_id
;
```

It’s a subtle difference, but it’s one worth paying attention to and tailoring to the specific context. Consider including time boxes, to only include users who complete an action within a specific time frame, if users can reenter the funnel after a lengthy absence. Funnel analyses can also include additional dimensions, such as cohort or other entity attributes, to facilitate comparisons and generate additional hypotheses about why a funnel is or is not performing well.

## Churn, Lapse, and Other Definitions of Departure

The topic of churn came up in [Chapter 4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch04.html#cohort\_analysis), since churn is  essentially the opposite of retention. Often organizations want or need to come up with a specific definition of churn in order to measure it directly. In some cases, there is a contractually defined end date, such as with B2B software. But often churn is a fuzzier concept, and a time-based definition is more appropriate. Even when there is a contractual end date, measuring when a customer stops using a product can be an early warning sign of an imminent contract cancellation. Churn definitions can also be applied to certain products or features, even when the customer doesn’t churn from the organization entirely.

A time-based churn metric counts customers as churned when they haven’t purchased or interacted with a product for a period of time, usually ranging from 30 days to as much as a year. The exact length depends a lot on the type of business and on typical usage patterns. To arrive at a good churn definition, you can use gap analysis to find typical periods between purchases or usage. To do gap analysis, you will need a time series of actions or events, the `lag` window function, and some date math.

As an example, we can calculate the typical gaps between representatives’ terms, using the legislators data set introduced in [Chapter 4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch04.html#cohort\_analysis). We’ll ignore the fact that politicians are often voted out of office rather than choosing to leave, since otherwise this data set has the right structure for this type of analysis. First we’ll find the average gap. To do this, we create a subquery that calculates the gap between the `start_date` and the previous `start_date` for each legislator for each term, and then we find the average value in the outer query. The previous `start_date` can be found using the `lag` function, and the gap as a time interval is calculated with the `age` function:

```
SELECT avg(gap_interval) as avg_gap
FROM
(
    SELECT id_bioguide, term_start
    ,lag(term_start) over (partition by id_bioguide 
                           order by term_start) 
                           as prev
    ,age(term_start,
         lag(term_start) over (partition by id_bioguide 
                               order by term_start)
         ) as gap_interval
    FROM legislators_terms
    WHERE term_type = 'rep'
) a
WHERE gap_interval is not null
;

avg_gap
-------------------------------------
2 years 2 mons 17 days 15:41:54.83805
```

As we might expect, the average is close to two years, which makes sense since the term length for this office is two years. We can also create a distribution of gaps in order to pick a realistic churn threshold. In this case, we’ll transform the gap to months:

```
SELECT gap_months, count(*) as instances
FROM
(
    SELECT id_bioguide, term_start
    ,lag(term_start) over (partition by id_bioguide 
                           order by term_start) 
                           as prev
    ,age(term_start,
         lag(term_start) over (partition by id_bioguide 
                               order by term_start)
         ) as gap_interval
    ,date_part('year',
               age(term_start,
                   lag(term_start) over (partition by id_bioguide
                                         order by term_start)
                   )
              ) * 12
     + 
     date_part('month',
               age(term_start,
                   lag(term_start) over (partition by id_bioguide 
                                         order by term_start)
                   )
              ) as gap_months
    FROM legislators_terms
    WHERE term_type = 'rep'
) a
GROUP BY 1
;

gap_months  instances
----------  ---------
1.0         25
2.0         4
3.0         2
...         ...
```

If `date_part` is not supported in your database, `extract` can be used as an alternative. (Refer to [Chapter 3](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#time\_series\_analysis) for an explanation and examples.) The output can be plotted, as in [Figure 9-1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch09.html#distribution\_of\_length\_of\_gap\_between\_r). Since there is a long tail of months, this plot is zoomed in to show the range in which most gaps fall. The most common gap is 24 months, but there are also several hundred instances per month out to 32 months. There is another small bump to over 100 at 47 and 48 months. With the average and distribution in hand, I would likely set a threshold of either 36 or 48 months and say that any representative who hasn’t been reelected within this window has “churned.”

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0901.png" alt="" height="314" width="600"><figcaption></figcaption></figure>

**Figure 9-1. Distribution of length of gap between representative term start dates, showing range from 10 to 59 months**

Once you have a defined threshold for churn, you can monitor the customer base with a “time since last” analysis. This can refer to last purchase, last payment, last time an app was opened, or whatever time-based metric is relevant for the organization. For this calculation, you need a data set that has the most recent date or timestamp for each customer. If starting with a time series, first find the most recent timestamp for each customer in a subquery. Then apply date math to find the time elapsed between that date and the current date, or the latest date in the data set if some time has elapsed since the data was assembled.

For example, we could find the distribution of years since the last election from the `legislators_terms` table. In the subquery, calculate the latest starting date using the `max` function and then find the time elapsed since then using the `age` function. In this case, the maximum data in the data set, May 5, 2019, is used. In a data set with up-to-date data, substitute `current_date` or an equivalent expression. The outer query finds the years from the interval using `date_part` and counts the number of legislators:

```
SELECT date_part('year',interval_since_last) as years_since_last
,count(*) as reps
FROM
(
    SELECT id_bioguide
    ,max(term_start) as max_date
    ,age('2020-05-19',max(term_start)) as interval_since_last
    FROM legislators_terms
    WHERE term_type = 'rep'
    GROUP BY 1
) a
GROUP BY 1
;

years_since_last  reps
----------------  -----
0.0               6
1.0               440
2.0               1
...               ...
```

A related concept is “lapsed,” which is often used as an intermediate stage between fully active customers and churned customers and might alternatively be called “dormant.” A lapsed customer may be at higher risk of churning because we haven’t seen them for a while but still have a decent likelihood of returning based on our past experience. In consumer services, I’ve seen “lapsed” cover periods from 7 to 30 days, with “churned” being defined as a customer not using the service for more than 30 days. Companies often experiment with reactivating lapsed users, using tactics ranging from email to support team outreach. Customers in each state can be defined by first finding their “time since last” as above and then tagging them with a CASE statement using the appropriate number of days or months. For example, we can group the representatives according to how long ago they were elected:

```
SELECT 
case when months_since_last <= 23 then 'Current'
     when months_since_last <= 48 then 'Lapsed'
     else 'Churned' 
     end as status
,sum(reps) as total_reps     
FROM
(
    SELECT 
    date_part('year',interval_since_last) * 12 
      + date_part('month',interval_since_last)
      as months_since_last
    ,count(*) as reps
    FROM
    (
        SELECT id_bioguide
        ,max(term_start) as max_date
        ,age('2020-05-19',max(term_start)) as interval_since_last
        FROM legislators_terms
        WHERE term_type = 'rep'
        GROUP BY 1
    ) a
    GROUP BY 1
) a
GROUP BY 1
;

status    total_reps
-------   ----------
Churned   10685
Current   446
Lapsed    105
```

This data set contains more than two hundred years of legislator terms, so of course many of the people included have died, and some are still living but are retired. In the context of a business, we would hope that our churned customers didn’t outnumber our current customers by such a wide margin, and we would want to know more about the lapsed customers.

Most organizations are very concerned about churn, since customers are generally more expensive to acquire than to retain. To learn more about the customers in any status or about the range of time since last seen, these analyses can be further sliced by any of the customer attributes available in the data set.

## Basket Analysis

I have three kids, and when I go to the grocery store, my basket (or more often my shopping cart) fills up quickly with grocery items to feed them for the week. Milk, eggs, and bread are usually in there, but other items might change depending on what produce is in season, whether the kids are in school or on break, or if we’re planning to cook a special meal. Basket analysis takes its name from the practice of analyzing the products consumers buy together to find patterns that can be used for marketing, store placement, or other strategic decisions. The goal of a basket analysis may be to find groups of items purchased together. It can also be framed around a particular product: when someone buys ice cream, what else do they buy?

Although basket analysis was originally framed around items purchased together in a single transaction, the concept can be extended in several ways. A retailer or an ecommerce store might be interested in the basket of items a customer purchases across their lifetime. Services and product feature usage can also be analyzed in this fashion. Services that are commonly purchased together might be bundled into a new offering, such as when travel sites offer deals if a flight, hotel, and rental car are booked together. Product features that are used together might be placed in the same navigation window or used to make suggestions for where to go next in an application. Basket analysis can also be used to identify stakeholder personas, or segments, which are then used in other types of analysis.

To find the most common baskets, using all items in a basket, we can use the `string_agg` function (or an analogous one, depending on the type of database—see [Chapter 5](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#text\_analysis)). For example, imagine we have a `purchases` table that has one row for each `product` bought by a `customer_id`. First, use the `string_agg` function to find the list of products purchased by each customer in a subquery. Then _GROUP BY_ this list and `count` the number of customers:

```
SELECT products
,count(customer_id) as customers
FROM
(
    SELECT customer_id
    ,string_agg(product,', ') as products
    FROM purchases
    GROUP BY 1
) a
GROUP BY 1
ORDER BY 2 desc
;
```

This technique works well when there is a relatively small number of possible items. Another option is to find pairs of products purchased together. To do this, self-_JOIN_ the `purchases` table to itself, _JOIN_ing on the `customer_id`. The second _JOIN_ condition solves the problem of duplicate entries that differ only in their order. For example, imagine a customer who purchased apples and bananas—without this clause, the result set would include “apples, bananas” and “bananas, apples.” The clause `b.product > a.product` ensures only one of these variations is included and also filters out results in which a product is matched with itself:

```
SELECT product1, product2
,count(customer_id) as customers
FROM
(
    SELECT a.customer_id
    ,a.product as product1
    ,b.product as product2
    FROM purchases a
    JOIN purchases b on a.customer_id = b.customer_id 
    and b.product > a.product
) a
GROUP BY 1,2
ORDER BY 3 desc
;
```

This can be extended to include three or more products by adding additional _JOIN_s. To include baskets that contain only one item, change the _JOIN_ to a _LEFT JOIN_.

There are a few common challenges when running a basket analysis. The first is performance, particularly when there is a large catalog of products, services, or features. The resultant calculations can become slow on the database, particularly when the goal is to find groups of three or more items, and thus the SQL contains three or more self-_JOIN_s. Consider filtering the tables with _WHERE_ clauses to remove infrequently purchased items before performing the _JOIN_s. Another challenge occurs when a few items are so common that they swamp all other combinations. For example, milk is so frequently purchased that groups with it and any other item top the list of combinations. The query results, while accurate, may still be meaningless in a practical sense. In this case, consider removing the most common items entirely, again with a _WHERE_ clause, before performing the _JOIN_s. This should have the added benefit of improving query performance by making the data set smaller.

A final challenge with basket analysis is the self-fulfilling prophecy. Items that show up together in a basket analysis may then be marketed together, increasing the frequency with which they are purchased together. This may strengthen the case to market them together further, leading to more copurchasing, and so on. Products that are even better matches may never have a chance, simply because they didn’t appear in the original analysis and become candidates for promotion. The famous [beer and diapers correlation](https://oreil.ly/4d5PF) is only one example of this. Various machine learning techniques and large online companies have tried to tackle this problem, and there are plenty of interesting directions for analysis in this area still to be developed.

## Resources

Data analysis as a profession (or even as a hobby!) requires a mix of technical proficiency, domain knowledge, curiosity, and communication skills. I thought I would share some of my favorite resources so that you might draw on them as you continue your journey, both to learn more and to practice your new skills on real data sets.

### Books and Blogs

Although this book assumes a working knowledge of SQL, good resources for the basics or for a refresher are:

* Forta, Ben. Sams _Teach Yourself SQL in 10 Minutes a Day_. 5th ed. Hoboken, NJ: Sams, 2020.
* The software company Mode offers a [SQL tutorial](https://mode.com/sql-tutorial) with an interactive query interface, useful for practicing your skills.

There is no single universally accepted SQL style, but you may find the [SQL Style Guide](https://www.sqlstyle.guide/) and the [Modern SQL Style Guide](https://oreil.ly/rsxBh) useful. Note that their styles don’t exactly match those used in this book, or each other. I believe that using a style that is both consistent with itself and readable is the most important consideration.

Your approach to analysis and to communicating the results can often matter just as much as the code you write. Two good books for sharpening both aspects are:

* Hubbard, Douglas W. _How to Measure Anything: Finding the Value of “Intangibles” in Business_. 2nd ed. Hoboken, NJ: Wiley, 2010.
* Kahneman, Daniel. _Thinking, Fast and Slow_. New York: Farrar, Straus and Giroux, 2011.

The [Towards Data Science blog](https://towardsdatascience.com/) is a great source for articles about many analysis topics. Although many of the posts there focus on Python as a programming language, approaches and techniques can often be adapted to SQL.

For an amusing take on correlation versus causation, see [Tyler Vigen’s Spurious Correlations](http://tylervigen.com/spurious-correlations).

Regular expressions can be tricky. If you’re looking to increase your understanding or to solve complex cases not covered in this book, a good resource is:

* Forta, Ben. _Learning Regular Expressions_. Boston: Addison-Wesley, 2018.

Randomized testing has a long history and touches many fields across the natural and social sciences. Compared to statistics, however, analysis of online experiments is still relatively new. Many classic statistics texts give a good introduction but discuss problems in which the sample size is very small, so they fail to address many of the unique opportunities and challenges of online testing. A couple of good books that discuss online experiments are:

* Georgiev, Georgi Z. _Statistical Methods in Online A/B Testing_. Sofia, Bulgaria: self-published, 2019.
* Kohavi, Ron, Diane Tang, and Ya Xu. _Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing_. Cambridge, UK: Cambridge University Press, 2020.

[Evan Miller’s Awesome A/B Tools](https://www.evanmiller.org/ab-testing) has calculators for both binary and continuous outcome experiments, as well as several other tests that may be useful for experiment designs beyond the scope of this book.

### Data Sets

The best way to learn and improve your SQL skills is to put them to use on real data. If you are employed and have access to a database within your organization, that’s a good place to start since you probably already have context on how the data is produced and what it means. There are plenty of interesting public data sets that you can analyze instead, however, and these range across a wide variety of topics. Listed below are a few good places to start when looking for interesting data sets:

* [Data Is Plural](https://www.data-is-plural.com/) is a newsletter of new and interesting data sets, and the [Data Is Plural archive](https://dataset-finder.netlify.app/) is a searchable treasure trove of data sets.
* [FiveThirtyEight](https://fivethirtyeight.com/) is a journalism site that covers politics, sports, and science through a data lens. The data sets behind the stories are on the [FiveThirtyEight GitHub site](https://github.com/fivethirtyeight/data).
* [Gapminder](https://www.gapminder.org/data) is a Swedish foundation that publishes yearly data for many human and economic development indicators, including many sourced from the World Bank.
* The United Nations publishes a number of statistics. The UN’s Department of Economic and Social Affairs produces data on [population dynamics](https://population.un.org/wpp/Download/Standard/Population) in a relatively easy-to-use format.
* Kaggle hosts data analysis competitions and has a [library of data sets](https://www.kaggle.com/datasets) that can be downloaded and analyzed even outside of the formal competitions.
* Many governments at all levels, from national to local, have adopted the open data movement and publish various statistics. [Data.gov](https://www.data.gov/open-gov) maintains a list of sites both in the United States and around the world that is a good starting point.

## Final Thoughts

I hope you’ve found the techniques and code in this book useful. I believe that it’s important to have a good foundation in the tools you’re using, and there are many useful SQL functions and expressions that can make your analyses faster and more accurate. Developing great analysis skills isn’t just about learning the latest fancy techniques or language, however. Great analysis comes from asking good questions; taking the time to understand the data and the domain; applying appropriate analysis techniques to come up with high-quality, reliable answers; and finally, communicating the results to your audience in a way that is relevant and supports decision making. Even after almost 20 years of working with SQL, I still get excited about finding new ways to apply it, new data sets to apply it to, and all of the insights in the world patiently waiting to be discovered.
