# 3. Time Series Analysis

## Chapter 3. Time Series Analysis

Now that I’ve covered SQL and databases and the key steps in preparing data for analysis, it’s time to turn to specific types of analysis that can be done with SQL. There are a seemingly unending number of data sets in the world, and correspondingly infinite ways in which they could be analyzed. In this and the following chapters, I have organized types of analysis into themes that I hope will be helpful as you build your analysis and SQL skills. Many of the techniques to be discussed build on those shown in [Chapter 2](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch02.html#preparing\_data\_for\_analysis) and then on the preceding chapters as the book progresses. Time series of data are so prevalent and so important that I’ll start the series of analysis themes here.

Time series analysis is one of the most common types of analysis done with SQL. A _time series_ is a sequence of measurements or data points recorded in time order, often at regularly spaced intervals. There are many examples of time series data in daily life, such as the daily high temperature, the closing value of the S\&P 500 stock index, or the number of daily steps recorded by your fitness tracker. Time series analysis is used in a wide variety of industries and disciplines, from statistics and engineering to weather forecasting and business planning. Time series analysis is a way to understand and quantify how things change over time.

Forecasting is a common goal of time series analysis. Since time only marches forward, future values can be expressed as a function of past values, while the reverse is not true. However, it’s important to note that the past doesn’t perfectly predict the future. Any number of changes to wider market conditions, popular trends, product introductions, or other large changes make forecasting difficult. Still, looking at historical data can lead to insights, and developing a range of plausible outcomes is useful for planning. As I’m writing this, the world is in the midst of a global COVID-19 pandemic, the likes of which haven’t been seen in 100 years—predating all but the most long-lived organizations’ histories. Thus many current organizations haven’t seen this specific event before, but they have existed through other economic crises, such as those following the dot-com burst and the 9/11 attacks in 2001, as well as the global financial crisis of 2007–2008. With careful analysis and understanding of context, we can often extract useful insights.

In this chapter, we’ll first cover the SQL building blocks of time series analysis: syntax and functions for working with dates, timestamps, and time. Next, I’ll introduce the retail sales data set used for examples throughout the rest of the chapter. A discussion of methods for trending analysis follows, and then I’ll cover calculating rolling time windows. Next are period-over-period calculations to analyze data with seasonality components. Finally, we’ll wrap up with some additional techniques that are useful for time series analysis.

## Date, Datetime, and Time Manipulations

Dates and times come in a wide variety of formats, depending on the data source. We often need or want to transform the raw data format for our output, or to perform calculations to arrive at new dates or parts of dates. For example, the data set might contain transaction timestamps, but the goal of the analysis is to trend monthly sales. At other times, we might want to know how many days or months have elapsed since a particular event. Fortunately, SQL has powerful functions and formatting capabilities that can transform just about any raw input to almost any output we might need for analysis.

In this section, I’ll show you how to convert between time zones, and then I’ll go into depth on formatting dates and datetimes. Next, I’ll explore date math and time manipulations, including those that make use of intervals. An interval is a data type that holds a span of time, such as a number of months, days, or hours. Although data can be stored in a database table as an interval type, in practice I rarely see this done, so I will talk about intervals alongside the date and time functions that you can use them with. Last, I’ll discuss some special considerations when joining or otherwise combining data from different sources.

### Time Zone Conversions

Understanding the standard time zone used in a data set can prevent misunderstandings and mistakes further into the analysis process. Time zones split the world into north-south regions that observe the same time. Time zones allow different parts of the world to have similar clock times for daytime and nighttime—so, for example, the sun is overhead at 12 p.m. wherever you are in the world. The zones follow irregular boundaries that are as much political as geographic ones. Most are one hour apart, but some are offset only 30 or 45 minutes, and so there are more than 30 time zones spanning the globe. Many countries that are distant from the equator observe daylight savings time for parts of the year as well, but there are exceptions, such as in the United States and Australia, where some states observe daylight savings time and others do not. Each time zone has a standard abbreviation, such as PST for Pacific Standard Time and PDT for Pacific Daylight Time.

Many databases are set to _Coordinated Universal Time_ (UTC), the global standard used to regulate clocks, and record events in this time zone. It replaced _Greenwich Mean Time_ (GMT), which you might still see if your data comes from an older database. UTC does not have daylight savings time, so it stays consistent all year long. This turns out to be quite useful for analysis. I remember one time a panicked product manager asked me to figure out why sales on a particular Sunday dropped so much compared to the prior Sunday. I spent hours writing queries and investigating possible causes before eventually figuring out that our data was recorded in Pacific Time (PT). Daylight savings started early Sunday morning, the database clock moved ahead 1 hour, and the day had only 23 hours instead of 24, and thus sales appeared to drop. Half a year later we had a corresponding 25-hour day, when sales appeared unusually high.

**NOTE**

Often timestamps in the database are not encoded with the time zone, and you will need to consult with the source or developer to figure out how your data was stored. UTC has become most common in the data sets I see, but that is certainly not universal.

One drawback to UTC, or really to any logging of machine time, is that we lose information about the local time for the human doing the actions that generated the event recorded in the database. I might want to know whether people tend to use my mobile app more during the workday or during nights and weekends. If my audience is clustered in one time zone, it’s not hard to figure this out. But if the audience spans multiple time zones or is international, then it becomes a calculation task of converting each recorded time to its local time zone.

All local time zones have a UTC offset. For example, the offset for PDT is UTC – 7 hours, while the offset for PST is UTC – 8 hours. Timestamps in databases are stored in the format YYYY-MM-DD hh:mi:ss (for years-months-days hours:minutes:seconds). Timestamps with the time zone have an additional piece of information for the UTC offset, expressed as a positive or negative number. Converting from one time zone to another can be accomplished with `at time zone` followed by the destination time zone’s abbreviation. For example, we can convert a timestamp in UTC (offset – 0) to PST:

```
SELECT '2020-09-01 00:00:00 -0' at time zone 'pst';

timezone
-------------------
2020-08-31 16:00:00
```

The destination time zone name can be a constant, or a database field, allowing this conversion to be dynamic to the data set. Some databases have a `convert_timezone` or `convert_tz` function that works similarly. One argument is the time zone of the result, and the other argument is the time zone from which to convert:

```
SELECT convert_timezone('pst','2020-09-01 00:00:00 -0');

timezone
-------------------
2020-08-31 16:00:00
```

Check your database’s documentation for the exact name and ordering of the target time zone and the source timestamp arguments. Many databases contain a list of time zones and their abbreviations in a system table. Some common ones are seen in [Table 3-1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#time\_zone\_information\_system\_tables\_in). These can be queried with _SELECT \* FROM_ the table name. Wikipedia also has a useful list of [standard time zone abbreviations and their UTC offsets](https://oreil.ly/im0wi).

| Postgres   | `pg_timezone_names`     |
| ---------- | ----------------------- |
| MySQL      | `mysql.time_zone_names` |
| SQL Server | `sys.time_zone_info`    |
| Redshift   | `pg_timezone_names`     |

Time zones are an innate part of working with timestamps. With time zone conversion functions, moving between the time zone in which the data was recorded and any other world time zone is possible. Next, I’ll show you a variety of techniques for manipulating dates and timestamps with SQL.

### Date and Timestamp Format Conversions

Dates and timestamps are key to time series analysis. Due to the wide variety of ways in which dates and times can be represented in source data, it is almost inevitable that you will need to convert date formats at some point. In this section, I’ll cover several of the most common conversions and how to accomplish them with SQL: changing the data type, extracting parts of a date or timestamp, and creating a date or timestamp from parts. I’ll begin by introducing some handy functions that return the current date and/or time.

Returning the current date or time is a common analysis task—for example, to include a timestamp for the result set or to use in date math, covered in the next section. The current date and time are referred to as _system time_, and while returning them is easy to do with SQL, there are some syntax differences between databases.

To return the current date, some databases have a `current_date` function, with no parentheses:

```
SELECT current_date;
```

There is a wider variety of functions to return the current date and time. Check your database’s documentation or just experiment by typing into a SQL window to see whether a function returns a value or an error. The functions with parentheses do not take arguments, but it is important to include the parentheses:

```
current_timestamp
localtimestamp
get_date()
now()
```

Finally, there are functions to return only the timestamp portion of the current system time. Again, consult documentation or experiment to figure out which function(s) to use with your database:

```
current_time
localtime
timeofday()
```

SQL has a number of functions for changing the format of dates and times. To reduce the granularity of a timestamp, use the `date_trunc` function. The first argument is a text value indicating the time period level to which to truncate the timestamp in the second argument. The result is a timestamp value:

```
date_trunc (text, timestamp)

SELECT date_trunc('month','2020-10-04 12:33:35'::timestamp);

date_trunc
-------------------
2020-10-01 00:00:00
```

Standard arguments that can be used are listed in [Table 3-2](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#standard\_time\_period\_arguments). They range all the way from microseconds to millennia, providing plenty of flexibility. Databases that don’t support `date_trunc`, such as MySQL, have an alternate function called `date_format` that can be used in a similar way:

```
SELECT date_format('2020-10-04 12:33:35','%Y-%m-01') as date_trunc;

date_trunc
-------------------
2020-10-01 00:00:00
```

| Time period arguments |
| --------------------- |
| microsecond           |
| millisecond           |
| second                |
| minute                |
| hour                  |
| day                   |
| week                  |
| month                 |
| quarter               |
| year                  |
| decade                |
| century               |
| millennium            |

Rather than returning dates or timestamps, sometimes our analysis calls for parts of dates or times. For example, we might want to group sales by month, day of the week, or hour of the day.

SQL provides a few functions for returning just the part of the date or timestamp required. Dates and timestamps are usually interchangeable, except when the request is to return a time part. In those cases, time is of course required.

The `date_part` function takes a text value for the part to be returned and a date or timestamp value. The returned value is a FLOAT, which is a numeric value with a decimal part; depending on your needs, you may want to cast the value to an integer data type:

```
SELECT date_part('day',current_timestamp);
SELECT date_part('month',current_timestamp);
SELECT date_part('hour',current_timestamp);
```

Another function that works similarly is `extract`, which takes a part name and a date or timestamp value and returns a FLOAT value:

```
SELECT extract('day' from current_timestamp);

date_part
---------
27.0

SELECT extract('month' from current_timestamp);


date_part
---------
5.0

SELECT extract('hour' from current_timestamp);

date_part
---------
14.0
```

The functions `date_part` and `extract` can be used with intervals, but note that the requested part must match the units of the interval. So, for example, requesting days from an interval stated in days returns the expected value of 30:

```
SELECT date_part('day',interval '30 days');

SELECT extract('day' from interval '30 days');

date_part
---------
30.0
```

However, requesting days from an interval stated in months returns a value of 0.0:

```
SELECT extract('day' from interval '3 months');

date_part
---------
0.0
```

**NOTE**

A full list of date parts can be found in your database’s documentation or by searching online, but some of the most common are “day,” “month,” and “year” for dates, and “second,” “minute,” and “hour” for timestamps.

To return text values of the date parts, use the `to_char` function, which takes the input value and the output format as arguments:

```
SELECT to_char(current_timestamp,'Day');
SELECT to_char(current_timestamp,'Month');
```

**TIP**

If you ever encounter timestamps stored as Unix epochs (the number of seconds that have elapsed since January 1, 1970, at 00:00:00 UTC), you can convert them to timestamps using the `to_timestamp` function.

Sometimes analysis calls for creating a date from parts from different sources. This can occur when the year, month, and day values are stored in different columns in the database. It can also be necessary when the parts have been parsed out of text, a topic I’ll cover in more depth in [Chapter 5](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch05.html#text\_analysis).

A simple way to create a timestamp from separate date and time components is to concatenate them together with a plus sign (+):

```
SELECT date '2020-09-01' + time '03:00:00' as timestamp;

timestamp 
-------------------
2020-09-01 03:00:00
```

A date can be assembled using the `make_date`, `makedate`, `date_from_parts`, or `datefromparts` function. These are equivalent, but different databases name the functions differently. The function takes arguments for the year, month, and day parts and returns a value with a date format:

```
SELECT make_date(2020,09,01);

make_date
----------
2020-09-01
```

The arguments can be constants or reference field names and must be integers. Yet another way to assemble a date or timestamp is to concatenate the values together and then cast the result to a date format using one of the casting syntaxes or the `to_date` function:

```
SELECT to_date(concat(2020,'-',09,'-',01), 'yyyy-mm-dd');

to_date
----------
2020-09-01

SELECT cast(concat(2020,'-',09,'-',01) as date);

to_date
----------
2020-09-01
```

SQL has a number of ways to format and convert dates and timestamps and retrieve system dates and times. In the next section, I will start putting them to use in date math.

### Date Math

SQL allows us to do various mathematical operations on dates. This might be surprising since, strictly speaking, dates are not numeric data types, but the concept should be familiar if you’ve ever tried to figure out what day it will be four weeks from now. Date math is useful for a variety of analytics tasks. For example, we can use it to find the age or tenure of a customer, how much time elapsed between two events, and how many things occurred within a window of time.

Date math involves two types of data: the dates themselves and intervals. We need the concept of intervals because date and time components don’t behave like integers. One-tenth of 100 is 10; one-tenth of a year is 36.5 days. Half of 100 is 50; half of a day is 12 hours. Intervals allow us to move smoothly between units of time. Intervals come in two types: year-month intervals and day-time ones. We’ll start with a few operations that return integer values and then move on to functions that work with or return intervals.

First, let’s find the days elapsed between two dates. There are several ways to do this in SQL. The first way is by using a mathematical operator, the minus sign (–):

```
SELECT date('2020-06-30') - date('2020-05-31') as days;

days
----
30
```

This returns the number of days between these two dates. Note that the answer is 30 days and not 31. The number of days is inclusive of only one of the endpoints. Subtracting the dates in the reverse also works and returns an interval of –30 days:

```
SELECT date('2020-05-31') - date('2020-06-30') as days;

days
----
-30
```

Finding the difference between two dates can also be accomplished with the `datediff` function. Postgres does not support it, but many other popular databases do, including SQL Server, Redshift, and Snowflake, and it’s quite handy, particularly when the goal is to return an interval other than the number of days. The function takes three arguments—the time period units you want to return, a starting timestamp or date, and an ending timestamp or date:

```
datediff(interval_name, start_timestamp, end_timestamp)
```

So our previous example would look like this:

```
SELECT datediff('day',date('2020-05-31'), date('2020-06-30')) as days;

days
----
30
```

We can also find the number of months between two dates, and the database will do the correct math even though month lengths differ throughout the year:

```
SELECT datediff('month'
                ,date('2020-01-01')
                ,date('2020-06-30')
                ) as months;

months
------
5
```

In Postgres, this can be accomplished using the `age` function, which calculates the interval between two dates:

```
SELECT age(date('2020-06-30'),date('2020-01-01'));

age
--------------
5 mons 29 days
```

We can then find the number of months component of the interval with the `date_part()` function:

```
SELECT date_part('month',age('2020-06-30','2020-01-01')) as months;

months
------
5.0
```

Subtracting dates to find the time elapsed between them is quite powerful. Adding dates does not work in the same way. To do addition with dates, we need to leverage intervals or special functions.

For example, we can add seven days to a date by adding the interval `'7 days'`:

```
SELECT date('2020-06-01') + interval '7 days' as new_date;

new_date
-------------------
2020-06-08 00:00:00
```

Some databases don’t require the interval syntax and instead automatically convert the provided number to days, although it’s generally good practice to use the interval notation, both for cross-database compatibility and to make your code easier to read:

```
SELECT date('2020-06-01') + 7 as new_date;

new_date
-------------------
2020-06-08 00:00:00
```

If you want to add a different unit of time, use the interval notation with months, years, hours, or another date or time period. Note that this can also be used to subtract intervals from dates by using a “-” instead of a “+.” Many but not all databases have a `date_add` or `dateadd` function that takes the desired interval, a value, and the starting date and does the math:

```
SELECT date_add('month',1,'2020-06-01') as new_date;

new_date
----------
2020-07-01
```

**TIP**

Consult your database’s documentation, or just experiment with queries, to figure out the syntax and functions that are available and appropriate for your project.

Any of these formulations can be used in the _WHERE_ clause in addition to the _SELECT_ clause. For example, we can filter to records that occurred at least three months ago:

```
WHERE event_date < current_date - interval '3 months'
```

They can also be used in _JOIN_ conditions, but note that database performance will usually be slower when the _JOIN_ condition contains a calculation rather than an equality or inequality between dates.

Using date math is common in analysis with SQL, both to find the time elapsed between dates or timestamps and to calculate new dates based on an interval from a known date. There are several ways to find the elapsed time between two dates, add intervals to dates, and subtract intervals from dates. Next, we’ll turn to time manipulations, which are similar.

### Time Math

Time math is less common in many areas of analysis, but it can be useful in some situations. For example, we might want to know how long it takes for a support representative to answer a phone call in a call center or respond to an email requesting assistance. Whenever the elapsed time between two events is less than a day, or when rounding the result to a number of days doesn’t provide enough information, time manipulation comes into play. Time math works similarly to date math, by leveraging intervals. We can add time intervals to times:

```
SELECT time '05:00' + interval '3 hours' as new_time;

new_time
--------
08:00:00
```

We can subtract intervals from times:

```
SELECT time '05:00' - interval '3 hours' as new_time;

new_time
--------
02:00:00
```

We can also subtract times, resulting in an interval:

```
SELECT time '05:00' - time '03:00' as time_diff;

time_diff
---------
02:00:00
```

Times, unlike dates, can be multiplied:

```
SELECT time '05:00' * 2 as time_multiplied;

time_multiplied
---------------
10:00:00
```

Intervals can also be multiplied, resulting in a time value:

```
SELECT interval '1 second' * 2000 as interval_multiplied;

interval_multiplied
-------------------
00:33:20


SELECT interval '1 day' * 45 as interval_multiplied;

interval_multiplied
-------------------
45 days
```

These examples use constant values, but you can include database field names or calculations in the SQL query as well to make the calculations dynamic. Next, I’ll discuss special date considerations to keep in mind when combining data sets from different source systems.

### Joining Data from Different Sources

Combining data from different sources is one of the most compelling use cases for a data warehouse. However, different source systems can record dates and times in different formats or different time zones or even just be off slightly due to issues with the internal clock time of the server. Even tables from the same data source can have differences, though this is less common. Reconciling and standardizing dates and timestamps is an important step before moving further in the analysis.

Dates and timestamps that are in different formats can be standardized with SQL. _JOIN_ing on dates or including date fields in _UNION_s generally requires that the dates or timestamps be in the same format. Earlier in the chapter, I showed techniques for formatting dates and timestamps that will serve well with these problems. Take care with time zones when combining data from different sources. For example, an internal database may use UTC time, but data from a third party could be in a local time zone. I have seen data sourced from software as a service (SaaS) that was recorded in a variety of local times. Note that the timestamp values themselves won’t necessarily have the time zone embedded. You may need to consult the vendor’s documentation and convert the data to UTC if the rest of your data is stored that way. Another option is to store the time zone in a field so that the timestamp value can be converted as needed.

Another thing to look out for when working with data from different sources is timestamps that are slightly out of sync. This can happen when timestamps are recorded from client devices—for example, from a laptop or mobile phone in one data source and a server in the other data source. I once saw a series of experiment results be miscalculated because the client mobile device that recorded a user’s action was offset by a few minutes from the server that recorded the treatment group to which the user was assigned. Data from the mobile clients appeared to arrive before the treatment group timestamp, so some events were inadvertently excluded. A fix for something like this is relatively straightforward: rather than filter for action timestamps greater than the treatment group timestamp, allow events within a short interval or window of time prior to the treatment timestamp to be included in the results. This can be accomplished with a _BETWEEN_ clause and date math, as seen in the last section.

When working with data from mobile apps, pay particular attention to whether the timestamps represent when the action happened on the device _or_ when the event arrived in the database. The difference can range from negligible all the way up to days, depending on whether the mobile app allows offline usage and on how it handles sending data during periods of low signal strength. Data from mobile apps can be late-arriving or may make its way into the database days after it occurred on the device. Dates and timestamps can also become corrupted en route, and you may see ones that are impossibly distant in the past or future as a result.

Now that I’ve shown how to manipulate dates, datetimes, and time by changing the formats, converting time zones, performing date math, and working across data sets from different sources, we’re ready to get into some time series examples. First, I’ll introduce the data set for examples in the rest of the chapter.

## The Retail Sales Data Set

The examples in the rest of this chapter use a data set of monthly US retail sales from the [Monthly Retail Trade Report: Retail and Food Services Sales: Excel (1992–present)](https://www.census.gov/retail/index.html#mrts), available on the [Census.gov website](http://census.gov/). The data in this report is used as an economic indicator to understand trends in US consumer spending patterns. While gross domestic product (GDP) figures are published quarterly, this retail sales data is published monthly, so it is also used to help predict GDP. For both of these reasons, the latest figures are usually covered in the business press when they are released.

The data spans from 1992 to 2020 and includes both total sales as well as details for subcategories of retail sales. It contains both unadjusted and seasonally adjusted numbers. This chapter will use the unadjusted numbers, since one of the goals is analyzing seasonality. Sales figures are in millions of US dollars. The original file format is an Excel file, with a tab for each year and with months as columns. The [GitHub site for this book](https://oreil.ly/LMiHw) has the data in a format that’s easier to import into a database, along with code specifically for importing into Postgres. [Figure 3-1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#preview\_of\_the\_us\_retail\_sales\_data\_set) shows a sample of the `retail_sales` table.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0301.png" alt="" height="319" width="600"><figcaption></figcaption></figure>

**Figure 3-1. Preview of the US retail sales data set**

## Trending the Data

With time series data, we often want to look for trends in the data. A trend is simply the direction in which the data is moving. It may be moving up or increasing over time, or it may be moving down or decreasing over time. It can remain more or less flat, or there could be so much noise, or movement up and down, that it’s hard to determine a trend at all. This section will cover several techniques for trending time series data, from simple trends for graphing to comparing components of a trend, using percent of total calculations to compare parts to the whole, and finally indexing to see the percent change from a reference time period.

### Simple Trends

Creating a trend may be a step in profiling and understanding data, or it may be the final output. The result set is a series of dates or timestamps and a numerical value. When graphing a time series, the dates or timestamps will become the x-axis, and the numerical value will be the y-axis. For example, we can check the trend of total retail and food services sales in the US:

```
SELECT sales_month
,sales
FROM retail_sales
WHERE kind_of_business = 'Retail and food services sales, total'
;

sales_month  sales
-----------  ------
1992-01-01   146376
1992-02-01   147079
1992-03-01   159336
...          ...
```

The results are graphed in [Figure 3-2](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#trend\_of\_monthly\_retail\_and\_food\_servic).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0302.png" alt="" height="433" width="600"><figcaption></figcaption></figure>

**Figure 3-2. Trend of monthly retail and food services sales**

This data clearly has some patterns, but it also has some noise. Transforming the data and aggregating at the yearly level can help us gain a better understanding. First, we’ll use the `date_part` function to return just the year from the `sales_month` field and then `sum` the `sales`. The results are filtered to the “Retail and food services sales, total” `kind_of_business` in the _WHERE_ clause:

```
SELECT date_part('year',sales_month) as sales_year   
,sum(sales) as sales
FROM retail_sales
WHERE kind_of_business = 'Retail and food services sales, total'
GROUP BY 1
;

sales_year  sales
----------  -------
1992.0      2014102
1993.0      2153095
1994.0      2330235
...         ...
```

After graphing this data, as in [Figure 3-3](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#trend\_of\_yearly\_total\_retail\_and\_food\_s), we now have a smoother time series that is generally increasing over time, as might be expected, since the sales values are not adjusted for inflation. Sales for all retail and food services fell in 2009, during the global financial crisis. After growing every year throughout the 2010s, sales were flat in 2020 compared to 2019, due to the impact of the COVID-19 pandemic.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0303.png" alt="" height="429" width="600"><figcaption></figcaption></figure>

**Figure 3-3. Trend of yearly total retail and food services sales**

Graphing time series data at different levels of aggregation, such as weekly, monthly, or yearly, is a good way to understand trends. This step can be used to simply profile the data, but it can also be the final output, depending on the goals of the analysis. Next, we’ll turn to using SQL to compare components of a time series.

### Comparing Components

Often data sets contain not just a single time series but multiple slices or components of a total across the same time range. Comparing these slices often reveals interesting patterns. In the retail sales data set, there are values for total sales but also a number of subcategories. Let’s compare the yearly sales trend for a few categories that are associated with leisure activities: book stores, sporting goods stores, and hobby stores. This query adds `kind_of_business` in the _SELECT_ clause and, since it is another attribute rather than an aggregation, adds it to the _GROUP BY_ clause  as well:

```
SELECT date_part('year',sales_month) as sales_year
,kind_of_business
,sum(sales) as sales
FROM retail_sales
WHERE kind_of_business in ('Book stores'
 ,'Sporting goods stores','Hobby, toy, and game stores')
GROUP BY 1,2
;

sales_year  kind_of_business             sales
----------  ---------------------------  -----
1992.0      Book stores                  8327
1992.0      Hobby, toy, and game stores  11251
1992.0      Sporting goods stores        15583
...         ...                          ...
```

The results are graphed in [Figure 3-4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#trend\_of\_yearly\_retail\_sales\_for\_sporti). Sales at sporting goods retailers started the highest among the three categories and grew much faster during the time period, and by the end of the time series, those sales were substantially higher. Sales at sporting goods stores started declining in 2017 but had a big rebound in 2020. Sales at hobby, toy, and game stores were relatively flat over this time span, with a slight dip in the mid-2000s and another slight decline prior to a rebound in 2020. Sales at book stores grew until the mid-2000s and have been on the decline since then. All of these categories have been impacted by the growth of online retailers, but the timing and magnitude seem to differ.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0304.png" alt="" height="404" width="600"><figcaption></figcaption></figure>

**Figure 3-4. Trend of yearly retail sales for sporting goods stores; hobby, toy, and game stores; and book stores**

In addition to looking at simple trends, we might want to perform more complex comparisons between parts of the time series. For the next few examples, we’ll look at sales at women’s clothing stores and at men’s clothing stores. Note that since the names contain apostrophes, the character otherwise used to indicate the beginning and end of strings, we need to escape them with an extra apostrophe. This lets the database know that the apostrophe is part of the string rather than the end. Although we might consider adding a step in a data-loading pipeline that removes extra apostrophes in names, I’ve left them in here as a demonstration of the types of code adjustments that are often needed in the real world. First, we’ll trend the data for each type of store by month:

```
SELECT sales_month
,kind_of_business
,sales
FROM retail_sales
WHERE kind_of_business in ('Men''s clothing stores'
 ,'Women''s clothing stores')
;

sales_month  kind_of_business         sales
-----------  -----------------------  -----
1992-01-01   Men's clothing stores    701
1992-01-01   Women's clothing stores  1873
1992-02-01   Women's clothing stores  1991
...          ...                      ...
```

The results are graphed in [Figure 3-5](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#monthly\_trend\_of\_sales\_at\_womenapostrop). Sales at women’s clothing retailers are much higher than those at men’s clothing retailers. Both types of stores exhibit seasonality, a topic I’ll cover in depth in [“Analyzing with Seasonality”](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#analyzing\_with\_seasonality). Both experienced significant drops in 2020 due to store closures and a reduction in shopping because of the COVID-19 pandemic.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0305.png" alt="" height="407" width="600"><figcaption></figcaption></figure>

**Figure 3-5. Monthly trend of sales at women’s and men’s clothing stores**

The monthly data has intriguing patterns but is noisy, so we’ll use yearly aggregates for the next few examples. We’ve seen this query format previously when rolling up total sales and sales for leisure categories:

```
SELECT date_part('year',sales_month) as sales_year
,kind_of_business
,sum(sales) as sales
FROM retail_sales
WHERE kind_of_business in ('Men''s clothing stores'
 ,'Women''s clothing stores')
GROUP BY 1,2
;
```

Are sales at women’s clothing stores uniformly higher than those at men’s clothing stores? In the yearly trend shown in [Figure 3-6](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#yearly\_trend\_of\_sales\_at\_womenapostroph), the gap between men’s and women’s sales does not appear constant but rather was increasing during the early to mid-2000s. Women’s clothing sales in particular dipped during the global financial crisis of 2008–2009, and sales in both categories dropped a lot during the pandemic in 2020.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0306.png" alt="" height="404" width="600"><figcaption></figcaption></figure>

**Figure 3-6. Yearly trend of sales at women’s and men’s clothing stores**

We don’t need to rely on visual estimation, however. For more precision on this gap, we can calculate the gap between the two categories, the ratio, and the percent difference between them. To do this, the first step is to arrange the data so that there is a single row for each month, with a column for each category. Pivoting the data with aggregate functions combined with CASE statements accomplishes this:

```
SELECT date_part('year',sales_month) as sales_year
,sum(case when kind_of_business = 'Women''s clothing stores' 
          then sales 
          end) as womens_sales
,sum(case when kind_of_business = 'Men''s clothing stores' 
          then sales 
          end) as mens_sales
FROM retail_sales
WHERE kind_of_business in ('Men''s clothing stores'
 ,'Women''s clothing stores')
GROUP BY 1
;

sales_year  womens_sales  mens_sales
----------  ------------  ----------
1992.0      31815         10179
1993.0      32350         9962
1994.0      30585         10032
...         ...           ...
```

With this building block calculation, we can find the difference, ratio, and percent difference between time series in the data set. The difference can be calculated by subtracting one value from the other using the mathematical “–” operator. Depending on the goals of the analysis, either finding the difference from men’s sales or finding the difference from women’s sales might be appropriate. Both are shown here and are equivalent except for the sign:

```
SELECT sales_year
,womens_sales - mens_sales as womens_minus_mens
,mens_sales - womens_sales as mens_minus_womens
FROM
(
    SELECT date_part('year',sales_month) as sales_year
    ,sum(case when kind_of_business = 'Women''s clothing stores' 
              then sales 
              end) as womens_sales
    ,sum(case when kind_of_business = 'Men''s clothing stores' 
              then sales 
              end) as mens_sales
    FROM retail_sales
    WHERE kind_of_business in ('Men''s clothing stores'
     ,'Women''s clothing stores')
    and sales_month <= '2019-12-01'
    GROUP BY 1
) a
;

sales_year  womens_minus_mens  mens_minus_womens
----------  -----------------  -----------------
1992.0      21636              -21636
1993.0      22388              -22388
1994.0      20553              -20553
...         ...                ...
```

The subquery is not required from a query execution standpoint, since aggregations can be added to or subtracted from each other. A subquery is often more legible but does add more lines to the code. Depending on how long or complex the rest of your SQL query is, you might prefer to place the intermediate calculation in a subquery, or just calculate it in the main query. Here is an example without the subquery, subtracting men’s sales from women’s sales, with an added _WHERE_ clause filter to remove 2020, since a few months have null values:[1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#ch01fn6)

```
SELECT date_part('year',sales_month) as sales_year
,sum(case when kind_of_business = 'Women''s clothing stores' 
          then sales end) 
 - 
 sum(case when kind_of_business = 'Men''s clothing stores' 
          then sales end)
 as womens_minus_mens
FROM retail_sales
WHERE kind_of_business in ('Men''s clothing stores'
 ,'Women''s clothing stores')
and sales_month <= '2019-12-01'
GROUP BY 1
;

sales_year  womens_minus_mens
----------  -----------------
1992.0      21636
1993.0      22388
1994.0      20553
...         ...
```

[Figure 3-7](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#yearly\_difference\_between\_sales\_at\_wome) shows that the gap decreased between 1992 and about 1997, began a long increase through about 2011 (with a brief dip in 2007), and then was more or less flat through 2019.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0307.png" alt="" height="435" width="600"><figcaption></figcaption></figure>

**Figure 3-7. Yearly difference between sales at women’s and men’s clothing stores**

Let’s continue our investigation and look at the ratio of these categories. We’ll use men’s sales as the baseline or denominator, but note that we could just as easily use women’s store sales instead:

```
SELECT sales_year
,womens_sales / mens_sales as womens_times_of_mens
FROM
(
    SELECT date_part('year',sales_month) as sales_year
    ,sum(case when kind_of_business = 'Women''s clothing stores' 
              then sales 
              end) as womens_sales
    ,sum(case when kind_of_business = 'Men''s clothing stores' 
              then sales 
              end) as mens_sales
    FROM retail_sales
    WHERE kind_of_business in ('Men''s clothing stores'
     ,'Women''s clothing stores')
    and sales_month <= '2019-12-01'
    GROUP BY 1
) a
;

sales_year  womens_times_of_mens
----------  --------------------
1992.0      3.1255526083112290
1993.0      3.2473398915880345
1994.0      3.0487440191387560
...         ...
```

**TIP**

SQL returns a lot of decimal digits when performing division. You should generally consider rounding the result before presenting the analysis. Use the level of precision (number of decimal places) that tells the story.

Plotting the result, shown in [Figure 3-8](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#yearly\_ratio\_of\_womenapostrophes\_to\_men), reveals that the trend is similar to the difference trend, but while there was a drop in the difference in 2009, the ratio actually increased.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0308.png" alt="" height="456" width="600"><figcaption></figcaption></figure>

**Figure 3-8. Yearly ratio of women’s to men’s clothing sales**

Next, we can calculate the percent difference between sales at women’s and men’s clothing stores:

```
SELECT sales_year
,(womens_sales / mens_sales - 1) * 100 as womens_pct_of_mens
FROM
(
    SELECT date_part('year',sales_month) as sales_year
    ,sum(case when kind_of_business = 'Women''s clothing stores' 
              then sales 
              end) as womens_sales
    ,sum(case when kind_of_business = 'Men''s clothing stores' 
              then sales 
              end) as mens_sales
    FROM retail_sales
    WHERE kind_of_business in ('Men''s clothing stores'
     ,'Women''s clothing stores')
    and sales_month <= '2019-12-01'
    GROUP BY 1
) a
;


sales_year  womens_pct_of_mens
----------  --------------------
1992.0      212.5552608311229000
1993.0      224.7339891588034500
1994.0      204.8744019138756000
...         ...
```

Although the units for this output are different from those in the previous example, the shape of this graph is the same as that of the ratio graph. The choice of which to use depends on your audience and the norms in your domain. All of these statements are accurate: in 2009, sales at women’s clothing stores were $28.7 billion higher than sales at men’s stores; in 2009, sales at women’s clothing stores were 4.9 times the sales at men’s stores; in 2009, sales at women’s stores were 390% higher than sales at men’s stores. Which version to select depends on the story you want to tell with the analysis.

The transformations we’ve seen in this section allow us to analyze time series by comparing related parts. The next section will continue the theme of comparing time series by showing ways to analyze series that represent parts of a whole.

### Percent of Total Calculations

When working with time series data that has multiple parts or attributes that constitute a whole, it’s often useful to analyze each part’s contribution to the whole and whether that has changed over time. Unless the data already contains a time series of the total values, we’ll need to calculate the overall total in order to calculate the percent of total for each row. This can be accomplished with a self-_JOIN_, or a window function, which as we saw in [Chapter 2](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch02.html#preparing\_data\_for\_analysis) is a special kind of SQL function that can reference any row within a specified partition of the table.

First I’ll show the self-_JOIN_ method. A self-_JOIN_ is any time a table is joined to itself. As long as each instance of the table in the query is given a different alias, the database will treat them all as distinct tables. For example, to find the percent of combined men’s and women’s clothing sales that each series represents, we can _JOIN_ `retail_sales`, aliased as `a`, to `retail_sales`, aliased as `b`, on the `sales_month` field. We then _SELECT_ the individual series name (`kind_of_business`) and `sales` values from alias `a`. Then, from alias `b` we `sum` the `sales` for both categories and call the result `total_sales`. Note that the _JOIN_ between the tables on the `sales_month` field creates a partial Cartesian _JOIN_, which results in two rows from alias `b` for each row in alias `a`. Grouping by `a.sales_month`, `a.kind_of_business`, and `a.sales` and aggregating `b.sales` returns exactly the results needed, however. In the outer query, the percent of total for each row is calculated by dividing `sales` by `total_sales`:

```
SELECT sales_month
,kind_of_business
,sales * 100 / total_sales as pct_total_sales
FROM
(
    SELECT a.sales_month, a.kind_of_business, a.sales
    ,sum(b.sales) as total_sales
    FROM retail_sales a
    JOIN retail_sales b on a.sales_month = b.sales_month
    and b.kind_of_business in ('Men''s clothing stores'
     ,'Women''s clothing stores')
    WHERE a.kind_of_business in ('Men''s clothing stores'
     ,'Women''s clothing stores')
    GROUP BY 1,2,3
) aa
;

sales_month  kind_of_business         pct_total_sales
-----------  -----------------------  -------------------
1992-01-01   Men's clothing stores    27.2338772338772339
1992-01-01   Women's clothing stores  72.7661227661227661
1992-02-01   Men's clothing stores    24.8395620989052473
...          ...                      ...
```

The subquery isn’t required here, as the same result could be obtained without it, but it makes the code a little easier to follow. A second way to calculate the percent of total sales for each category is to use the `sum` window function and _PARTITION BY_ the `sales_month`. Recall that the _PARTITION BY_ clause indicates the section of the table within which the function should calculate. The _ORDER BY_ clause is not required in this `sum` window function, because the order of calculation doesn’t matter. Additionally, the query does not need a _GROUP BY_ clause, because window functions look across multiple rows, but they do not reduce the number of rows in the result set:

```
SELECT sales_month, kind_of_business, sales
,sum(sales) over (partition by sales_month) as total_sales
,sales * 100 / sum(sales) over (partition by sales_month) as pct_total
FROM retail_sales 
WHERE kind_of_business in ('Men''s clothing stores'
 ,'Women''s clothing stores')
;

sales_month  kind_of_business         sales  total_sales  pct_total
-----------  -----------------------  -----  -----------  ---------
1992-01-01   Men's clothing stores    701    2574         27.233877
1992-01-01   Women's clothing stores  1873   2574         72.766122
1992-02-01   Women's clothing stores  1991   2649         75.160437
...          ...                      ...    ...          ...
```

Graphing this data, as in [Figure 3-9](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#menapostrophes\_and\_womenapostrophes\_cl), reveals some interesting trends. First, starting in the late 1990s, women’s clothing store sales became an increasing percentage of the total.  Second, early in the series a seasonal pattern is evident, where men’s sales spike as a percent of total sales in December and January. In the first decade of the 21st century, two seasonal peaks appear, in the summer and the winter, but by the late 2010s, the seasonal patterns are dampened almost to the point of randomness. We’ll take a look at analyzing seasonality in greater depth later in this chapter.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0309.png" alt="" height="474" width="600"><figcaption></figcaption></figure>

**Figure 3-9. Men’s and women’s clothing store sales as percent of monthly total**

Another percent of total we might want to find is the percent of sales within a longer time period, such as the percent of yearly sales each month represents. Again, either a self-_JOIN_ or a window function will do the job. In this example, we’ll use a self-_JOIN_ in the subquery:

```
SELECT sales_month
,kind_of_business
,sales * 100 / yearly_sales as pct_yearly
FROM
(
    SELECT a.sales_month, a.kind_of_business, a.sales
    ,sum(b.sales) as yearly_sales
    FROM retail_sales a
    JOIN retail_sales b on 
     date_part('year',a.sales_month) = date_part('year',b.sales_month)
     and a.kind_of_business = b.kind_of_business
     and b.kind_of_business in ('Men''s clothing stores'
      ,'Women''s clothing stores')
    WHERE a.kind_of_business in ('Men''s clothing stores'
     ,'Women''s clothing stores')
    GROUP BY 1,2,3
) aa
;

sales_month  kind_of_business       pct_yearly
-----------  ---------------------  ------------------
1992-01-01   Men's clothing stores  6.8867275763827488
1992-02-01   Men's clothing stores  6.4642892229099126
1992-03-01   Men's clothing stores  7.1814520090382159
...          ...                    ...
```

Alternatively, the window function method can be used:

```
SELECT sales_month, kind_of_business, sales
,sum(sales) over (partition by date_part('year',sales_month)
                               ,kind_of_business
                               ) as yearly_sales
,sales * 100 / 
 sum(sales) over (partition by date_part('year',sales_month)
                               ,kind_of_business
                               ) as pct_yearly
FROM retail_sales 
WHERE kind_of_business in ('Men''s clothing stores'
 ,'Women''s clothing stores')
;

sales_month  kind_of_business       pct_yearly
-----------  ---------------------  ------------------
1992-01-01   Men's clothing stores  6.8867275763827488
1992-02-01   Men's clothing stores  6.4642892229099126
1992-03-01   Men's clothing stores  7.1814520090382159
...          ...                    ...
```

The results, zoomed in to 2019, are shown in [Figure 3-10](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#percent\_of\_yearly\_sales\_for\_twozeroonen). The two time series track fairly closely, but men’s stores had a greater percentage of their sales in January than did women’s stores. Men’s stores had a summer dip in July, while the corresponding dip in women’s store sales wasn’t until September.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0310.png" alt="" height="409" width="600"><figcaption></figcaption></figure>

**Figure 3-10. Percent of yearly sales for 2019 for women’s and men’s clothing sales**

Now that I’ve shown how to use SQL for percent of total calculations and the types of analysis that can be accomplished, I’ll turn to indexing and calculating percent change over time.

### Indexing to See Percent Change over Time

The values in time series usually fluctuate over time. Sales increase with growing popularity and availability of a product, while web page response time decreases with engineers’ efforts to optimize code. Indexing data is a way to understand the changes in a time series relative to a base period (starting point). Indices are widely used in economics as well as business settings. One of the most famous indices is the Consumer Price Index (CPI), which tracks the change in the prices of items that a typical consumer purchases and is used to track inflation, to decide salary increases, and for many other applications. The CPI is a complex statistical measure using various weights and data inputs, but the basic premise is straightforward. Pick a base period and compute the percent change in value from that base period for each subsequent period.

Indexing time series data with SQL can be done with a combination of aggregations and window functions, or self-_JOIN_s. As an example, we index women’s clothing store sales to the first year in the series, 1992. The first step is to aggregate the `sales` by `sales_year` in a subquery, as we’ve done previously. In the outer query, the `first_value` window function finds the value associated with the first row in the _PARTITION BY_ clause, according to the sort in the _ORDER BY_ clause. In this example, we can omit the _PARTITION BY_ clause, because we want to return the sales `value` for the first row in the entire data set returned by the subquery:

```
SELECT sales_year, sales
,first_value(sales) over (order by sales_year) as index_sales
FROM
(
    SELECT date_part('year',sales_month) as sales_year
    ,sum(sales) as sales
    FROM retail_sales
    WHERE kind_of_business = 'Women''s clothing stores'
    GROUP BY 1
) a
;

sales_year  sales  index_sales
----------  -----  -----------
1992.0      31815  31815
1993.0      32350  31815
1994.0      30585  31815
...         ...    ...
```

With this sample of data, we can visually verify that the index value is correctly set at the value for 1992. Next, find the percent change from this base year for each row:

```
SELECT sales_year, sales
,(sales / first_value(sales) over (order by sales_year) - 1) * 100 
 as pct_from_index
FROM
(
    SELECT date_part('year',sales_month) as sales_year
    ,sum(sales) as sales
    FROM retail_sales
    WHERE kind_of_business = 'Women''s clothing stores'
    GROUP BY 1
) a
;

sales_year  sales  pct_from_index
----------  -----  --------------
1992.0      31815  0
1993.0      32350  1.681596731101
1994.0      30585  -3.86610089580
...         ...    ...
```

The percent change can be either positive or negative, and we’ll see that does in fact occur in this time series. The `last_value` window function could be substituted for `first_value` in this query. Indexing from the last value in a series is much less common, however, since analysis questions more often relate to change from a starting point rather than looking back from an arbitrary ending point; still, the option is there. Additionally, the sort order can be used to achieve indexing from the first or last value by switching between _ASC_ and _DESC_:

```
first_value(sales) over (order by sales_year desc)
```

Window functions provide a lot of flexibility. Indexing can be accomplished without them through a series of self-_JOIN_s, though more lines of code are required:

```
SELECT sales_year, sales
,(sales / index_sales - 1) * 100 as pct_from_index
FROM
(
    SELECT date_part('year',aa.sales_month) as sales_year
    ,bb.index_sales
    ,sum(aa.sales) as sales
    FROM retail_sales aa
    JOIN 
    (
        SELECT first_year, sum(a.sales) as index_sales
        FROM retail_sales a
        JOIN 
        (
            SELECT min(date_part('year',sales_month)) as first_year
            FROM retail_sales
            WHERE kind_of_business = 'Women''s clothing stores'
        ) b on date_part('year',a.sales_month) = b.first_year 
        WHERE a.kind_of_business = 'Women''s clothing stores'
        GROUP BY 1
    ) bb on 1 = 1
    WHERE aa.kind_of_business = 'Women''s clothing stores'
    GROUP BY 1,2
) aaa
;

sales_year  sales  pct_from_index
----------  -----  --------------
1992.0      31815  0
1993.0      32350  1.681596731101
1994.0      30585  -3.86610089580
...         ...    ...
```

Notice the unusual _JOIN_ clause `on 1 = 1` between alias `aa` and subquery `bb`. Since we want the `index_sales` value to populate for every row in the result set, we can’t _JOIN_ on the year or any other value, which would restrict the results. However, the database will return an error if no _JOIN_ clause is specified. We can fool the database by using any expression that evaluates to TRUE in order to create the desired Cartesian _JOIN_. Any other TRUE statement, such as `on 2 = 2` or `on 'apples' = 'apples'`, could be used instead.

**WARNING**

Beware of zeros in the denominator of division operations such as `sales / index_sales` in the last example. Databases return an error when they encounter division by zero, which can be frustrating. Even when you think a zero in the denominator field is unlikely, it’s good practice to prevent this by telling the database to return an alternate default value when it encounters a zero. This can be done with a CASE statement. The examples in this section do not have zeros in the denominator, so I will omit this extra code for the sake of legibility.

To wrap up this section, let’s look at a graph of the indexed time series for men’s and women’s clothing stores, shown in [Figure 3-11](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#menapostrophes\_and\_womenapostrophes\_clo). The SQL code looks like:

```
SELECT sales_year, kind_of_business, sales
,(sales / first_value(sales) over (partition by kind_of_business 
                                   order by sales_year)
 - 1) * 100 as pct_from_index
FROM
(
    SELECT date_part('year',sales_month) as sales_year
    ,kind_of_business
    ,sum(sales) as sales
    FROM retail_sales
    WHERE kind_of_business in ('Men''s clothing stores'
     ,'Women''s clothing stores')
    and sales_month <= '2019-12-31'
    GROUP BY 1,2
) a
;
```

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0311.png" alt="" height="408" width="600"><figcaption></figcaption></figure>

**Figure 3-11. Men’s and women’s clothing store sales, indexed to 1992 sales**

It’s apparent from this graph that 1992 was something of a high-water mark for sales at men’s clothing stores. After 1992 sales dropped, then returned briefly to the same level in 1998, and have been declining ever since. This is striking since the data set is not adjusted for inflation, the tendency for prices to rise over time. Sales at women’s clothing stores decreased from 1992 levels initially, but they returned to the 1992 level by 2003. They have increased since, with the exception of the drop during the financial crisis that decreased sales in 2009 and 2010. One explanation for these trends is that men simply decreased spending on clothes over time, perhaps becoming less fashion conscious relative to women. Perhaps men’s clothing simply became less expensive as global supply chains decreased costs. Yet another explanation might be that men shifted their clothing purchases from retailers categorized as “men’s clothing stores” to other types of retailers, such as sporting goods stores or online retailers.

Indexing time series data is a powerful analysis technique, allowing us to find a range of insights in the data. SQL is well suited to this task, and I’ve shown how to construct indexed time series with and without window functions. Next, I’ll show you how to analyze data by using rolling time windows to find patterns in noisy time series.

## Rolling Time Windows

Time series data is often noisy, a challenge for one of our primary goals of finding patterns. We’ve seen how aggregating data, such as from monthly to yearly, can smooth out the results and make them easier to interpret. Another technique for smoothing data is _rolling time windows_, also known as moving calculations, that take into account multiple periods. Moving averages are probably the most common, but with the power of SQL, any aggregate function is available for analysis. Rolling time windows are used in a wide variety of analysis areas, including stock markets, macroeconomic trends, and audience measurement. Some calculations are so commonly used that they have their own acronyms: last twelve months (LTM), trailing twelve months (TTM), and year-to-date (YTD).

[Figure 3-12](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#example\_of\_ltm\_and\_ytd\_rolling\_sum\_of\_s) shows an example of a rolling time window and a cumulative calculation, relative to the month of October in the time series.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0312.png" alt="" height="800" width="565"><figcaption></figcaption></figure>

**Figure 3-12. Example of LTM and YTD rolling sum of sales**

There are several important pieces of any rolling time series calculation. First is the size of the window, which is the number of periods to include in the calculation. Larger windows with more time periods have a greater smoothing effect, but at the risk of losing sensitivity to important short-term changes in the data. Shorter windows with fewer time periods do less smoothing and thus are more sensitive to short-term changes, but at the risk of too little noise reduction.

The second piece of time series calculations is the aggregate function used. As noted previously, moving averages are probably the most common. Moving sums, counts, minimums, and maximums can also be calculated with SQL. Moving counts are useful in user population metrics (see the following sidebar). Moving minimums and maximums can help in understanding the extremes of the data, useful for planning analyses.

The third piece of time series calculations is choosing the partitioning, or grouping, of the data that is included in the window. The analysis might call for resetting the window every year. Or the analysis might need a different moving series for each component or user group. [Chapter 4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch04.html#cohort\_analysis) will go into more detail on cohort analysis of user groups, where we will consider how retention and cumulative values such as spend differ between populations over time. Partitioning will be controlled through grouping as well as the _PARTITION BY_ statement of window functions.

With these three pieces in mind, we’ll move into the SQL code and calculations for moving time periods, continuing with the US retail sales data set for examples.

**MEASURING “ACTIVE USERS”: DAU, WAU, AND MAU**

Many consumer and some B2B SaaS applications use active user calculations such as daily active users (DAU), weekly active users (WAU), and monthly active users (MAU) to estimate their audience size. Since each of these are rolling windows, they can be calculated on a daily basis. I’ve often been asked what is the right or best metric to use, and my answer is always “it depends.”

DAU helps companies with capacity planning, such as estimating how much load to expect on servers. Depending on the service, however, even more detailed data might be needed, such as peak hourly or even minute-by-minute concurrent user information.

MAU is commonly used to estimate relative sizes of applications or services. It is useful for measuring fairly stable or growing user populations that have regular usage patterns that aren’t necessarily daily, such as higher use on the weekend for leisure products, or higher weekday use for work- or school-related products. MAU is not as well suited to detecting changes in underlying churn from users who stop using an application. Since it takes a user 30 days, the most common window, to pass through MAU, a user can have been absent from the product for 29 days before they trigger a drop in MAU.

WAU, calculated over 7 days, can be a happy medium between DAU and MAU. WAU is more sensitive to short-term fluctuations, alerting teams to changes in churn more quickly than MAU while smoothing over day of week fluctuations that are tracked by DAU. A drawback to WAU is that it is still sensitive to short-term fluctuations driven by events such as holidays.

### Calculating Rolling Time Windows

Now that we know what rolling time windows are, how they’re useful, and their key components, let’s get into calculating them using the US retail sales data set. We’ll start with the simpler case, when the data set contains a record for each period that should be in the window, and then in the next section we’ll look at what to do when this is not the case.

There are two main methods for calculating a rolling time window: a self-_JOIN_, which can be used in any database, and a window function, which as we’ve seen isn’t available in some databases. In both cases we need the same result: a date and a number of data points that corresponds to the size of the window to which we will apply an average or another aggregate function.

For this example, we’ll use a window of 12 months to get rolling annual sales, since the data is at a monthly level of granularity. We’ll then apply an average to get a 12-month moving average of retail sales. First, let’s develop the intuition for what will go into the calculation. In this query, alias `a` of the table is our “anchor” table, the one from which we gather the dates. To start, we’ll look at a single month, December 2019. From alias `b`, the query gathers the 12 individual months of sales that will go into the moving average. This is accomplished with the _JOIN_ clause `b.sales_month between a.sales_month - interval '11 months' and a.sales_month`, which creates an intentional Cartesian _JOIN_:

```
SELECT a.sales_month
,a.sales
,b.sales_month as rolling_sales_month
,b.sales as rolling_sales
FROM retail_sales a
JOIN retail_sales b on a.kind_of_business = b.kind_of_business 
 and b.sales_month between a.sales_month - interval '11 months' 
 and a.sales_month
 and b.kind_of_business = 'Women''s clothing stores'
WHERE a.kind_of_business = 'Women''s clothing stores'
and a.sales_month = '2019-12-01'
;

sales_month  sales  rolling_sales_month  rolling_sales
-----------  -----  -------------------  -------------
2019-12-01   4496   2019-01-01           2511
2019-12-01   4496   2019-02-01           2680
2019-12-01   4496   2019-03-01           3585
2019-12-01   4496   2019-04-01           3604
2019-12-01   4496   2019-05-01           3807
2019-12-01   4496   2019-06-01           3272
2019-12-01   4496   2019-07-01           3261
2019-12-01   4496   2019-08-01           3325
2019-12-01   4496   2019-09-01           3080
2019-12-01   4496   2019-10-01           3390
2019-12-01   4496   2019-11-01           3850
2019-12-01   4496   2019-12-01           4496
```

Notice that the `sales_month` and `sales` figures from alias `a` are repeated for each row of the 12 months in the window.

**WARNING**

Remember that the dates in a _BETWEEN_ clause are inclusive (both will be returned in the result set). It’s a common mistake to use 12 instead of 11 in the preceding query. When in doubt, check the intermediate query results as I’ve done here to make sure the intended number of periods ends up in the window calculation.

The next step is to apply the aggregation—in this case, `avg`, since we want a rolling average. The `count` of records returned from alias `b` is included to confirm that each row averages 12 data points, a useful data quality check. Alias `a` also has a filter on `sales_month`. Since this data set starts in 1992, months in that year, except for December, have fewer than 12 historical records:

```
SELECT a.sales_month
,a.sales
,avg(b.sales) as moving_avg
,count(b.sales) as records_count
FROM retail_sales a
JOIN retail_sales b on a.kind_of_business = b.kind_of_business 
 and b.sales_month between a.sales_month - interval '11 months' 
 and a.sales_month
 and b.kind_of_business = 'Women''s clothing stores'
WHERE a.kind_of_business = 'Women''s clothing stores'
and a.sales_month >= '1993-01-01'
GROUP BY 1,2
;


sales_month  sales  moving_avg  records_count
-----------  -----  ----------  -------------
1993-01-01   2123   2672.08     12
1993-02-01   2005   2673.25     12
1993-03-01   2442   2676.50     12
...          ...    ...         ...
```

The results are graphed in [Figure 3-13](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#monthly\_sales\_and\_onetwo\_month\_moving\_a). While the monthly trend is noisy, the smoothed moving average trend makes detecting changes such as the increase from 2003 to 2007 and the subsequent dip through 2011 easier to spot. Notice that the extreme drop in early 2020 pulls the moving average down even after sales start to rebound later in the year.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0313.png" alt="" height="474" width="600"><figcaption></figcaption></figure>

**Figure 3-13. Monthly sales and 12-month moving average sales for women’s clothing stores**

**TIP**

Adding the filter `kind_of_business = 'Women''s clothing stores'` to each alias isn’t strictly necessary. Since the query uses an _INNER JOIN_, filtering on one table will automatically filter on the other. However, filtering on both tables often makes queries run faster, particularly when the tables are large.

Window functions are another way to calculate rolling time windows. To make a rolling window, we need to use another optional part of a window calculation: the _frame clause_. The frame clause allows you to specify which records to include in the window. By default, all records in the partition are included, and for many cases this works just fine. However, controlling the included records at a more fine-grained level is useful for cases like moving window calculations. The syntax is simple and yet can be confusing when encountering it for the first time. The frame clause can be specified as:

```
{ RANGE | ROWS | GROUPS } BETWEEN frame_start AND frame_end
```

Within the curly braces are three options for the frame type: range, rows, and groups. These are the ways you can specify which records to include in the result, relative to the current row. Records are always chosen from the current partition and follow the _ORDER BY_ specified. The default sorting is ascending (_ASC_), but it can be changed to descending (_DESC_). _Rows_ is the most straightforward and will allow you to specify the exact number of rows that should be returned. _Range_ includes records that are within some boundary of values relative to the current row. _Groups_ can be used when there are multiple records with the same _ORDER BY_ value, such as when a data set includes multiple lines per sales month, one for each customer.

The _frame\_start_ and _frame\_end_ can be any of the following:

```
UNBOUNDED PRECEDING
offset PRECEDING
CURRENT ROW
offset FOLLOWING
UNBOUNDED FOLLOWING
```

_Preceding_ means to include rows before the current row, according to the _ORDER BY_ sorting. _Current row_ is just that, and _following_ means to include rows that occur after the current row according to the _ORDER BY_ sorting. The _UNBOUNDED_ keyword means to include all records in the partition before or after the current row. The _offset_ is the number of records, often just an integer constant, though a field or an expression that returns an integer could also be used. Frame clauses also have an optional _frame\_exclusion_ option, which is beyond the scope of the discussion here. [Figure 3-14](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#window\_frame\_clauses\_and\_the\_rows\_they) shows an example of the rows that each of the window frame options will pick up.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0314.png" alt="" height="612" width="600"><figcaption></figcaption></figure>

**Figure 3-14. Window frame clauses and the rows they include**

From partition to ordering to window frames, window functions have a variety of options that control the calculations, making them incredibly powerful and well suited to tackling complex calculations with relatively simple syntax. Returning to our retail sales example, the moving average that we calculated using a self-_JOIN_ can be accomplished with window functions in fewer lines of code:

```
SELECT sales_month
,avg(sales) over (order by sales_month 
                  rows between 11 preceding and current row
                  ) as moving_avg
,count(sales) over (order by sales_month 
                  rows between 11 preceding and current row
                  ) as records_count
FROM retail_sales
WHERE kind_of_business = 'Women''s clothing stores'
;

sales_month  moving_avg  records_count
-----------  ----------  -------------
1992-01-01   1873.00     1
1992-02-01   1932.00     2
1992-03-01   2089.00     3
...          ...         ...
1993-01-01   2672.08     12
1993-02-01   2673.25     12
1993-03-01   2676.50     12
...          ...         ...
```

In this query, the window orders the sales by month (ascending) to ensure that the window records are in chronological order.  The frame clause is `rows between 11 preceding and current row`, since I know that I have one record for each month and I want the 11 prior months and the month from the current row included in the average and count calculations. The query returns all months, including those that don’t have 11 prior months, and we might want to filter these out by placing this query in a subquery and filtering by month or number of records in the outer query.

**TIP**

While calculating moving averages from prior time periods is common in many business contexts, SQL window functions are flexible enough to include future time periods as well. They can also be used in any scenario in which the data has some ordering, not just in time series analysis.

Calculating rolling averages or other moving aggregations can be accomplished with self-_JOIN_s or window functions when records exist in the data set for each time period in the window. There may be performance differences between the two methods, depending on the type of database and the size of the data set. Unfortunately, it’s difficult to predict which one will be performant or to give general advice on which to use. It’s worth trying both methods and paying attention to how long it takes to return your query results; then make whichever one seems to run faster your default choice. Now that we’ve seen how to calculate rolling time windows, I’ll show how to calculate rolling windows with sparse data sets.

### Rolling Time Windows with Sparse Data

Data sets in the real world may not contain a record for every time period that falls within the window. The measurement of interest might be seasonal or intermittent by nature. For example, customers might return to purchase from a website at irregular intervals, or a particular product might go in and out of stock. This results in sparse data.

In the last section, I showed how to calculate a rolling window with a self-_JOIN_ and a date interval in the _JOIN_ clause. You might be thinking that this will pick up any records within the 12-month time window, whether all were in the data set or not, and you’d be correct. The problem with this approach comes when there is no record for the month (or day or year) itself. For example, imagine I want to calculate the rolling 12-month sales for each model of shoe my store stocks as of December 2019. Some of the shoes went out of stock prior to December, however, and so don’t have sales records in that month. Using a self-_JOIN_ or window function will return a data set of rolling sales for all the shoes that sold in December, but the data will be missing the shoes that went out of stock. Fortunately, we have a way to solve this problem: by using a date dimension.

The _date dimension_, a static table that contains a row for each calendar date, was introduced in [Chapter 2](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch02.html#preparing\_data\_for\_analysis). With such a table we can ensure that a query returns a result for every date of interest, whether or not there was a data point for that date in the underlying data set. Since the `retail_sales` data does include rows for all months, I’ve simulated a sparse data set by adding a subquery to filter the table to only `sales_month`s from January and July (1 and 7). Let’s look at the results when _JOIN_ed to the `date_dim`, but before aggregation, to develop intuition about the data before applying calculations:

```
SELECT a.date, b.sales_month, b.sales
FROM date_dim a
JOIN 
(
    SELECT sales_month, sales
    FROM retail_sales 
    WHERE kind_of_business = 'Women''s clothing stores'
     and date_part('month',sales_month) in (1,7)
) b on b.sales_month between a.date - interval '11 months' and a.date
WHERE a.date = a.first_day_of_month
 and a.date between '1993-01-01' and '2020-12-01'
;

date        sales_month  sales
----------  -----------  -----
1993-01-01  1992-07-01   2373
1993-01-01  1993-01-01   2123
1993-02-01  1992-07-01   2373
1993-02-01  1993-01-01   2123
1993-03-01  1992-07-01   2373
...         ...          ...
```

Notice that the query returns results for February and March `date`s in addition to January, even though there are no sales for these months in the subquery results. This is possible because the date dimension contains records for all months. The filter `a.date = a.first_day_of_month` restricts the result set to one value per month, instead of the 28 to 31 rows per month that would result from joining to every date. The construction of this query is otherwise very similar to the self-_JOIN_ query in the last section, with the _JOIN_ clause `on b.sales_month between a.date - interval '11 months' and a.date` of the same form as the _JOIN_ clause in the self-_JOIN_. Now that we have developed an understanding of what the query will return, we can go ahead and apply the `avg` aggregation to get the moving average:

```
SELECT a.date
,avg(b.sales) as moving_avg
,count(b.sales) as records
FROM date_dim a
JOIN 
(
    SELECT sales_month, sales
    FROM retail_sales 
    WHERE kind_of_business = 'Women''s clothing stores'
     and date_part('month',sales_month) in (1,7)
) b on b.sales_month between a.date - interval '11 months' and a.date
WHERE a.date = a.first_day_of_month
 and a.date between '1993-01-01' and '2020-12-01'
GROUP BY 1
;

date        moving_avg  records
----------  ----------  -------
1993-01-01  2248.00     2
1993-02-01  2248.00     2
1993-03-01  2248.00     2
...         ...         ...
```

As we saw above, the result set includes a row for every month; however, the moving average stays constant until a new data point (in this case, a January or July) is added. Each moving average consists of two underlying data points. In a real use case, the number of underlying data points is likely to vary. To return the current month’s value when using a data dimension, an aggregation with a CASE statement can be used—for example:

```
,max(case when a.date = b.sales_month then b.sales end) 
 as sales_in_month
```

The conditions inside the CASE statement can be changed to return any of the underlying records that the analysis requires through use of equality, inequality, or offsets with date math. If a date dimension is not available in your database, then another technique can be used to simulate one. In a subquery, _SELECT_ the _DISTINCT_ dates needed and _JOIN_ them to your table in the same way as in the preceding examples:

```
SELECT a.sales_month, avg(b.sales) as moving_avg
FROM
(
    SELECT distinct sales_month
    FROM retail_sales
    WHERE sales_month between '1993-01-01' and '2020-12-01'
) a
JOIN retail_sales b on b.sales_month between 
 a.sales_month - interval '11 months' and a.sales_month
 and b.kind_of_business = 'Women''s clothing stores' 
GROUP BY 1
;

sales_month  moving_avg
-----------  ----------
1993-01-01   2672.08
1993-02-01   2673.25
1993-03-01   2676.50
...          ...
```

In this example, I used the same underlying table because I know it contains all the months. However, in practice any database table that contains the needed dates can be used, whether or not it is related to the table from which you want to calculate the rolling aggregation.

Calculating rolling time windows with sparse or missing data can be done in SQL with controlled application of Cartesian _JOIN_s. Next, we’ll look at how to calculate cumulative values that are often used in analysis.

### Calculating Cumulative Values

Rolling window calculations, such as moving averages, typically use fixed-size windows, such as 12 months, as we saw in the last section. Another commonly used type of calculation is the _cumulative value_, such as YTD, quarter-to-date (QTD), and month-to-date (MTD). Rather than a fixed-length window, these rely on a common starting point, with the window size growing with each row.

The simplest way to calculate cumulative values is with a window function. In this example, `sum` is used to find total sales YTD as of each month. Other analyses might call for a monthly average YTD or a monthly maximum YTD, which can be accomplished by swapping `sum` for `avg` or `max`. The window resets according to the _PARTITION BY_ clause, in this case the year of the sales month. The _ORDER BY_ clause typically includes a date field in time series analysis. Omitting the _ORDER BY_ can lead to incorrect results due to the way the data is sorted in the underlying table, so it’s a good idea to include it even if you think the data is already sorted by date:

```
SELECT sales_month, sales
,sum(sales) over (partition by date_part('year',sales_month) 
                  order by sales_month
                  ) as sales_ytd
FROM retail_sales
WHERE kind_of_business = 'Women''s clothing stores'
;

sales_month  sales  sales_ytd
-----------  -----  ---------
1992-01-01   1873   1873
1992-02-01   1991   3864
1992-03-01   2403   6267
...          ...    ...
1992-12-01   4416   31815
1993-01-01   2123   2123
1993-02-01   2005   4128
...          ...    ...
```

The query returns a record for each `sales_month`, the `sales` for that month, and the running total `sales_ytd`. The series starts in 1992 and then resets in January 1993, as it will for every year in the data set. The results for years 2016 through 2020 are graphed in [Figure 3-15](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#monthly\_sales\_and\_cumulative\_annual\_sal). The first four years show similar patterns through the year, but of course 2020 looks very different.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0315.png" alt="" height="485" width="600"><figcaption></figcaption></figure>

**Figure 3-15. Monthly sales and cumulative annual sales for women’s clothing stores**

The same results can be achieved without window functions, by using a self-_JOIN_ that leverages a Cartesian _JOIN_. In this example, the two table aliases are _JOIN_ed on the year of the `sales_month` to ensure that the aggregated values are for the same year, resetting each year. The _JOIN_ clause also specifies that the results should include `sales_month`s from alias `b` that are less than or equal to the `sales_month` in alias `a`. In January 1992, only the January 1992 row from alias `b` meets this criterion; in February 1992, both January and February 1992 do; and so on:

```
SELECT a.sales_month, a.sales
,sum(b.sales) as sales_ytd
FROM retail_sales a
JOIN retail_sales b on 
 date_part('year',a.sales_month) = date_part('year',b.sales_month)
 and b.sales_month <= a.sales_month
 and b.kind_of_business = 'Women''s clothing stores'
WHERE a.kind_of_business = 'Women''s clothing stores'
GROUP BY 1,2
;


sales_month  sales  sales_ytd
-----------  -----  ---------
1992-01-01   1873   1873
1992-02-01   1991   3864
1992-03-01   2403   6267
...          ...    ...
1992-12-01   4416   31815
1993-01-01   2123   2123
1993-02-01   2005   4128
...          ...    ...
```

Window functions require fewer characters of code, and it’s usually easier to keep track of exactly what they are calculating once you are familiar with the syntax. There’s often more than one way to approach a problem in SQL, and rolling time windows are a good example of that. I find it useful to know multiple approaches, because every once in a while I run into a tricky problem that is actually better solved with an approach that seems less efficient in other contexts. Now that we’ve covered rolling time windows, we’ll move on to our final topic in time series analysis with SQL: seasonality.

## Analyzing with Seasonality

_Seasonality_ is any pattern that repeats over regular intervals. Unlike other noise in the data, seasonality can be predicted. The word _seasonality_ brings to mind the four seasons of the year—spring, summer, fall, winter—and some data sets include these patterns. Shopping patterns change with the seasons, from the clothes and food people buy to the money spent on leisure and travel. The winter holiday shopping season can be make-or-break for many retailers. Seasonality can also exist at other time scales, from years down to minutes. Presidential elections in the United States happen every four years, leading to distinct patterns in media coverage.  Day of week cyclicality is common, as work and school dominate Monday to Friday, while chores and leisure activities dominate the weekend. Time of day is another type of seasonality that restaurants experience, with rushes around lunch and dinner time and slower sales in between.

To understand whether seasonality exists in a time series, and at what scale, it’s useful to graph it and then visually inspect for patterns. Try aggregating at different levels, from hourly to daily, weekly, and monthly. You should also incorporate knowledge about the data set. Are there patterns that you can guess based on what you know about the entity or process it represents? Consult subject matter experts, if available.

Let’s take a look at some seasonal patterns in the retail sales data set, shown in [Figure 3-16](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#examples\_of\_seasonality\_patterns\_in\_boo). Jewelry stores have a highly seasonal pattern, with annual peaks in December related to holiday gift giving. Book stores have two peaks each year: one peak is in August, corresponding with back-to-school time in the United States; the other peak starts in December and lasts through January, including both the holiday gift period and back-to-school time for the spring semester. A third example is grocery stores, which have much less monthly seasonality than the other two time series (although they likely have seasonality at the day of week and time of day level). This isn’t surprising: people need to eat year-round. Grocery store sales increase a bit in December for the holidays, and they decline in February, since that month simply has fewer days.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0316.png" alt="" height="476" width="600"><figcaption></figcaption></figure>

**Figure 3-16. Examples of seasonality patterns in book store, grocery store, and jewelry store sales**

Seasonality can take many forms, though there are some common approaches to analyzing it regardless. One way to deal with seasonality is to smooth it out, either by aggregating the data to a less granular time period or by using rolling windows, as we saw previously. Another way to work with seasonal data is to benchmark against similar time periods and analyze the difference. I’ll show several ways to accomplish this next.

### Period-over-Period Comparisons: YoY and MoM

Period-over-period comparisons can take multiple forms. The first one is to compare a time period to the previous value in the series, a practice so common in analysis that there are acronyms for the most often-used comparisons. Depending on the level of aggregation the comparison might be year-over-year (YoY), month-over-month (MoM), day-over-day (DoD), and so on.

For these calculations we’ll use the `lag` function, another one of the window functions. The `lag` function returns a previous or lagging value from a series. The `lag` function has the following form:

```
lag(return_value [,offset [,default]])
```

The _`return_value`_ is any field from the data set and thus can be any data type. The optional _OFFSET_ indicates how many rows back in the partition to take the _`return_value`_ from. The default is 1, but any integer value can be used. You can also optionally specify a _`default`_ value to use if there is no lagging record to retrieve a value from. Like other window functions, `lag` is also calculated over a partition, with sorting determined by the _ORDER BY_ clause. If no _PARTITION BY_ clause is specified, `lag` looks back over the whole data set, and likewise if no _ORDER BY_ clause is specified, the database order is used. It’s usually a good idea to at least include an _ORDER BY_ clause in a `lag` window function to control the output.

**TIP**

The `lead` window function works in the same way as the `lag` function, except that it returns a subsequent value as determined by the offset. Changing the _ORDER BY_ from ascending (_ASC_) to descending (_DESC_) in a time series has the effect of turning a `lag` statement into the equivalent of a `lead` statement. Alternatively, a negative integer can be used as the _OFFSET_ value to return a value from a subsequent row.

Let’s apply this to our retail sales data set to calculate MoM and YoY growth. In this section, we’ll focus on book store sales, since I’m a real book store nerd. First, we’ll develop our intuition about what is returned by the `lag` function by returning both the lagging month and the lagging sales values:

```
SELECT kind_of_business, sales_month, sales
,lag(sales_month) over (partition by kind_of_business 
                        order by sales_month
                        ) as prev_month
,lag(sales) over (partition by kind_of_business 
                  order by sales_month
                  ) as prev_month_sales
FROM retail_sales
WHERE kind_of_business = 'Book stores'
;

kind_of_business  sales_month  sales  prev_month  prev_month_sales
----------------  -----------  -----  ----------  ----------------
Book stores       1992-01-01   790    (null)      (null)
Book stores       1992-02-01   539    1992-01-01  790
Book stores       1992-03-01   535    1992-02-01  539
...               ...          ...    ...         ...
```

For each row, the previous `sales_month` is returned, as well as the `sales` for that month, and we can confirm this by inspecting the first few lines of the result set. The first row has null for `prev_month` and `prev_month_sales` since there is no earlier record in this data set. With an understanding of the values returned by the `lag` function, we can calculate the percent change from the previous value:

```
SELECT kind_of_business, sales_month, sales
,(sales / lag(sales) over (partition by kind_of_business 
                           order by sales_month)
 - 1) * 100 as pct_growth_from_previous
FROM retail_sales
WHERE kind_of_business = 'Book stores'
;

kind_of_business  sales_month  sales  pct_growth_from_previous
----------------  -----------  -----  ------------------------
Book stores       1992-01-01   790    (null)
Book stores       1992-02-01   539    -31.77
Book stores       1992-03-01   535    -0.74
...               ...          ...    ...
```

Sales dropped 31.8% from January to February, due at least in part to the seasonal decline after the holidays and the return to school for the spring semester. Sales were down only 0.7% from February to March.

The calculation for the YoY comparison is similar, but first we need to aggregate sales to the yearly level. Since we’re looking at only one `kind_of_business`, I’ll drop that field from the rest of the examples to simplify the code:

```
SELECT sales_year, yearly_sales
,lag(yearly_sales) over (order by sales_year) as prev_year_sales
,(yearly_sales / lag(yearly_sales) over (order by sales_year)
 -1) * 100 as pct_growth_from_previous
FROM
(
    SELECT date_part('year',sales_month) as sales_year
    ,sum(sales) as yearly_sales
    FROM retail_sales
    WHERE kind_of_business = 'Book stores'
    GROUP BY 1
) a
;

sales_year  yearly_sales  prev_year_sales  pct_growth_from_previous
----------  ------------  ---------------  ------------------------
1992.0      8327          (null)           (null)
1993.0      9108          8327             9.37
1994.0      10107         9108             10.96
...         ...           ...              ...
```

Sales grew more than 9.3% from 1992 to 1993, and almost 11% from 1993 to 1994. These period-over-period calculations are useful, but they don’t quite allow us to analyze the seasonality in the data set. For example, in [Figure 3-17](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#percent\_growth\_from\_previous\_month\_for) the MoM percent growth values are plotted, and they contain just as much seasonality as the original time series.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0317.png" alt="" height="474" width="600"><figcaption></figcaption></figure>

**Figure 3-17. Percent growth from previous month for US retail book store sales**

To tackle this, the next section will demonstrate how to use SQL to compare current values to the values for the same month in the previous year.

### Period-over-Period Comparisons: Same Month Versus Last Year

Comparing data for one time period to data for a similar previous time period can be a useful way to control for seasonality. The previous time period may be the same day of the week in the previous week, the same month in the previous year, or another variation that makes sense for the data set.

To accomplish this comparison, we can use the `lag` function along with some clever partitioning: the unit of time with which we want to compare the current value. In this case, we will compare monthly `sales` to the `sales` for the same month in the previous year. For example, January `sales` will be compared to prior year January `sales`, February `sales` will be compared to prior year February `sales`, and so on.

First, recall that the `date_part` function returns a numeric value when used with the “month” argument:

```
SELECT sales_month
,date_part('month',sales_month)
FROM retail_sales
WHERE kind_of_business = 'Book stores'
;

sales_month  date_part
-----------  ---------
1992-01-01   1.0
1992-02-01   2.0
1992-03-01   3.0
...          ...
```

Next, we include the `date_part` in the _PARTITION BY_ clause so that the window function looks up the value for the matching month number from the prior year.

This is an example of how window function clauses can include calculations in addition to database fields, giving them even more versatility. I find it useful to check intermediate results to build intuition about what the final query will return, so first we’ll confirm that the `lag` function with `partition by date_part('mon⁠th',​sales_month)` returns the intended values:

```
SELECT sales_month, sales
,lag(sales_month) over (partition by date_part('month',sales_month) 
                        order by sales_month
                        ) as prev_year_month
,lag(sales) over (partition by date_part('month',sales_month) 
                  order by sales_month
                  ) as prev_year_sales
FROM retail_sales
WHERE kind_of_business = 'Book stores'
;


sales_month  sales  prev_year_month  prev_year_sales
-----------  -----  ---------------  ---------------
1992-01-01   790    (null)           (null)
1993-01-01   998    1992-01-01       790
1994-01-01   1053   1993-01-01       998
...          ...    ...              ...
1992-02-01   539    (null)           (null)
1993-02-01   568    1992-02-01       539
1994-02-01   635    1993-02-01       568
...          ...     ...             ...
```

The first `lag` function returns the same month for the prior year, which we can verify by looking at the `prev_year_month` value. The row for the 1993-01-01 `sales_month` returns 1992-01-01 for the `prev_year_month` as intended, and the `prev_year_sales` of 790 match the `sales` we can see in the 1992-01-01 row. Notice that the `prev_year_month` and `prev_year_sales` are null for 1992 since there are no prior records in the data set.

Now that we’re confident the `lag` function as written returns the correct values, we can calculate comparison metrics such as absolute difference and percent change from previous:

```
SELECT sales_month, sales
,sales - lag(sales) over (partition by date_part('month',sales_month) 
                          order by sales_month
                          ) as absolute_diff
,(sales / lag(sales) over (partition by date_part('month',sales_month) 
                          order by sales_month)
 - 1) * 100 as pct_diff
FROM retail_sales
WHERE kind_of_business = 'Book stores'
;

sales_month  sales  absolute_diff  pct_diff
-----------  -----  -------------  --------
1992-01-01   790    (null)         (null)
1993-01-01   998    208            26.32
1994-01-01   1053   55             5.51
...          ...    ...            ...
```

We can now graph the results in [Figure 3-18](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#book\_store\_salescomma\_yoy\_absolute\_diff) and more easily see the months where growth was unusually high, such as January 2002, or unusually low, such as December 2001.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0318.png" alt="" height="476" width="600"><figcaption></figcaption></figure>

**Figure 3-18. Book store sales, YoY absolute difference in sales, and YoY percent growth**

Another useful analysis tool is to create a graph that lines up the same time period—in this case, months—with a line for each time series—in this case, years. To do this, we’ll create a result set that has a row for each month number or name, and a column for each of the years we want to consider. To get the month, we can use either the `date_part` or the `to_char` function, depending on whether we want numeric or text values for the months. Then we’ll pivot the data using an aggregate function.

This example uses the `max` aggregate, but depending on the analysis, a `sum`, `count`, or other aggregation might be appropriate. We’ll zoom in on 1992 through 1994 for this example:

```
SELECT date_part('month',sales_month) as month_number
,to_char(sales_month,'Month') as month_name
,max(case when date_part('year',sales_month) = 1992 then sales end) 
 as sales_1992
,max(case when date_part('year',sales_month) = 1993 then sales end) 
 as sales_1993
,max(case when date_part('year',sales_month) = 1994 then sales end) 
 as sales_1994
FROM retail_sales
WHERE kind_of_business = 'Book stores'
 and sales_month between '1992-01-01' and '1994-12-01'
GROUP BY 1,2
;

month_number  month_name  sales_1992  sales_1993  sales_1994
------------  ----------  ----------  ----------  ----------
1.0           January     790         998         1053
2.0           February    539         568         635
3.0           March       535         602         634
4.0           April       523         583         610
5.0           May         552         612         684
6.0           June        589         618         724
7.0           July        592         607         678
8.0           August      894         983         1154
9.0           September   861         903         1022
10.0          October     645         669         732
11.0          November    642         692         772
12.0          December    1165        1273        1409
```

By lining the data up in this way, we can see some trends immediately. December sales are the highest monthly sales of the year. Sales in 1994 were higher every month than sales in 1992 and 1993. The August-to-September sales bump is visible, and particularly easy to spot in 1994.

With a graph of the data, as in [Figure 3-19](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#book\_store\_sales\_for\_onenineninetwoen\_d), the trends are much easier to identify. Sales increased year to year in every month, though the increases were larger in some months than others. With this data and graph in hand, we can start to construct a story about book store sales that might help with inventory planning or scheduling of marketing promotions or might serve as a piece of evidence in a wider story about US retail sales.

With SQL there are a number of techniques for cutting through the noise of seasonality to compare data in time series. In this section, we’ve seen how to compare current values to prior comparable periods using `lag` functions and how to pivot the data with `date_part`, `to_char`, and aggregate functions. Next, I’ll show some techniques for comparing multiple prior periods in order to further control for noisy time series data.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0319.png" alt="" height="407" width="600"><figcaption></figcaption></figure>

**Figure 3-19. Book store sales for 1992–1994, aligned by month**

### Comparing to Multiple Prior Periods

Comparing data to prior comparable periods is a useful way to reduce the noise that arises from seasonality. Sometimes comparing to a single prior period is insufficient, particularly if that prior period was impacted by unusual events. Comparing a Monday to the previous Monday is difficult if one of them was a holiday. The month in the prior year might be unusual due to economic events, severe weather, or a site outage that changed typical behavior. Comparing current values to an aggregate of multiple prior periods can help smooth out these fluctuations. These techniques also combine what we’ve learned about using SQL to calculate rolling time periods and comparable prior period results.

The first technique uses the `lag` function, as in the last section, but here we’ll take advantage of the optional offset value. Recall that when no offset is provided to `lag`, the function returns the immediate prior value according to the _PARTITION BY_ and _ORDER BY_ clauses. An offset value of 2 skips over the immediate prior value and returns the value prior to that, an offset value of 3 returns the value from 3 rows back, and so on.

For this example, we’ll compare the current month’s sales to the same month’s sales over three prior years. As usual, first we’ll inspect the returned values to confirm the SQL is working as expected:

```
SELECT sales_month, sales
,lag(sales,1) over (partition by date_part('month',sales_month) 
                    order by sales_month
                    ) as prev_sales_1
,lag(sales,2) over (partition by date_part('month',sales_month) 
                    order by sales_month
                    ) as prev_sales_2
,lag(sales,3) over (partition by date_part('month',sales_month) 
                    order by sales_month
                    ) as prev_sales_3
FROM retail_sales
WHERE kind_of_business = 'Book stores'
;

sales_month  sales  prev_sales_1  prev_sales_2  prev_sales_3
-----------  -----  ------------  ------------  ------------
1992-01-01   790    (null)        (null)        (null)
1993-01-01   998    790           (null)        (null)
1994-01-01   1053   998           790           (null)
1995-01-01   1308   1053          998           790
1996-01-01   1373   1308          1053          998
...          ...    ...           ...           ...
```

Null is returned where no prior record exists, and we can confirm that the correct same month, prior year value appears. From here we can calculate whatever comparison metric the analysis calls for—in this case, the percent of the rolling average of three prior periods:

```
SELECT sales_month, sales
,sales / ((prev_sales_1 + prev_sales_2 + prev_sales_3) / 3) 
 as pct_of_3_prev
FROM
(
    SELECT sales_month, sales
    ,lag(sales,1) over (partition by date_part('month',sales_month) 
                        order by sales_month
                        ) as prev_sales_1
    ,lag(sales,2) over (partition by date_part('month',sales_month) 
                        order by sales_month
                        ) as prev_sales_2
    ,lag(sales,3) over (partition by date_part('month',sales_month) 
                        order by sales_month
                        ) as prev_sales_3
    FROM retail_sales
    WHERE kind_of_business = 'Book stores'
) a
;

sales_month  sales  pct_of_3_prev
-----------  -----  -------------
1995-01-01   1308   138.12
1996-01-01   1373   122.69
1997-01-01   1558   125.24
...          ...    ...   
2017-01-01   1386   94.67
2018-01-01   1217   84.98
2019-01-01   1004   74.75
...          ...    ...
```

We can see from the result that book sales grew from the prior three-year rolling average in the mid-1990s, but the picture was different in the late 2010s, when sales were a shrinking percentage of that three-year rolling average each year.

You might have noticed that this problem resembles one we saw earlier when calculating rolling time windows. As an alternative to the last example, we can use an `avg` window function with a frame clause. To accomplish this, the _PARTITION BY_ will use the same `date_part` function, and the _ORDER BY_ is the same. A frame clause is added to include `rows between 3 preceding and 1 preceding`.This includes the values in the 1, 2, and 3 rows prior but excludes the value in the current row:

```
SELECT sales_month, sales
,sales / avg(sales) over (partition by date_part('month',sales_month)
                          order by sales_month
                          rows between 3 preceding and 1 preceding
                          ) as pct_of_prev_3
FROM retail_sales
WHERE kind_of_business = 'Book stores'
;

sales_month  sales  pct_of_prev_3
-----------  -----  -------------
1995-01-01   1308   138.12
1996-01-01   1373   122.62
1997-01-01   1558   125.17
...          ...    ... 
2017-01-01   1386   94.62
2018-01-01   1217   84.94
2019-01-01   1004   74.73
...          ...    ...
```

The results match those of the previous example, confirming that the alternative code is equivalent.

**TIP**

If you look closely, you’ll notice that the decimal place values are slightly different in the result using the three `lag` windows and the result using the `avg` window function. This is due to how the database handles decimal rounding in intermediate calculations. For many analyses, the difference won’t matter, but pay careful attention if you’re working with financial or other highly regulated data.

Analyzing data with seasonality often involves trying to reduce noise in order to make clear conclusions about the underlying trends in the data. Comparing data points against multiple prior time periods can give us an even smoother trend to compare to and determine what is actually happening in the current time period. This does require that the data include enough history to make these comparisons, but when we have a long enough time series, it can be insightful.

## Conclusion

Time series analysis is a powerful way to analyze data sets. We’ve seen how to set up our data for analysis with date and time manipulations. We talked about date dimensions and saw how to apply them to calculating rolling time windows. We looked at period-over-period calculations and how to analyze data with seasonality patterns. In the next chapter, we’ll delve deep into a related topic that extends on time series analysis: cohort analysis.

[1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch03.html#ch01fn6-marker) October and November 2020 data points were suppressed by the publisher of the data, due to concerns about the data quality. Collecting the data likely became more difficult due to store closures during the 2020 pandemic.
