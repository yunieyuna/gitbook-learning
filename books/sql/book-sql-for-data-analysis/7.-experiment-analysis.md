# 7. Experiment Analysis

## Chapter 7. Experiment Analysis

_Experimentation_, also known as _A/B testing_ or _split testing_, is considered the gold standard for establishing causality. Much data analysis work involves establishing correlations: one thing is more likely to happen when another thing also happens, whether that be an action, an attribute, or a seasonal pattern. You’ve probably heard the saying “correlation does not imply causation,” however, and it is exactly this problem in data analysis that experimentation attempts to solve.

All experiments begin with a _hypothesis_: a guess about behavioral change that will result from some alteration to a product, process, or message. The change might be to a user interface, a new user onboarding flow, an algorithm that powers recommendations, marketing messaging or timing, or any number of other areas. If the organization built it or has control over it, it can be experimented on, at least in theory. Hypotheses are often driven by other data analysis work. For example, we might find that a high percentage of people drop out of the checkout flow, and we could hypothesize that more people might complete the checkout process if the number of steps were reduced.

The second element necessary for any experiment is a _success metric_. The behavioral change we hypothesize might be related to form completion, purchase conversion, click-through, retention, engagement, or any other behavior that is important to the organization’s mission. The success metric should quantify this behavior, be reasonably easy to measure, and be sensitive enough to detect a change. Click-through, checkout completion, and time to complete a process are often good success metrics. Retention and customer satisfaction are often less suitable success metrics, despite being very important, because they are frequently influenced by many factors beyond what is being tested in any individual experiment and thus are less sensitive to the changes we’d like to test. Good success metrics are often ones that you already track as part of understanding company or organizational health.

**TIP**

You may wonder whether an experiment can have multiple success metrics. Certainly with SQL, it is usually possible to generate many different calculations and metrics. You should be aware of the multiple comparisons problem, however. I won’t go into a full explanation here, but the gist is that the more places you look for a significant change, the more likely you are to find one. Check one metric, and you may or may not find a significant change in one of the experiment variants. Check 20 metrics, however, and there’s a pretty good chance that at least one will show significance, whether or not the experiment had anything to do with that metric in the first place. As a rule of thumb, there should be one or maybe two primary success metrics. One to five additional metrics may be used for downside protection. These are sometimes called _guardrail metrics_. For example, you may want to ensure that an experiment doesn’t hurt page-loading time, even though it’s not the goal of the experiment to improve it.

The third element of experimentation is a system that randomly assigns entities to a control or experiment variant group and alters the experience accordingly. This type of system is also sometimes called a _cohorting system_. A number of software vendors offer experiment-cohorting tools, though some organizations choose to build them internally in order to achieve more flexibility. Either way, to perform experiment analysis with SQL, the entity-level assignment data must flow into a table in the database that also contains behavioral data.

**TIP**

The discussions of experiments in this chapter specifically refer to online experiments, in which variant assignment happens through a computer system and behavior is tracked digitally. There are certainly many types of experiments performed across science and social science disciplines. A key difference is that the success metrics and behaviors that are examined in online experiments are usually already tracked for other purposes, whereas in many scientific studies, the resulting behavior is tracked specifically for the experiment and only during the period of the experiment. With online experiments, we sometimes need to be creative about finding metrics that are good proxies for an impact when a direct measurement isn’t possible.

With a hypothesis, a success metric, and a variant cohorting system in place, you can run experiments, collect the data, and analyze the outcomes using SQL.

## Strengths and Limits of Experiment Analysis with SQL

SQL is useful for analyzing experiments. In many cases with experiment analysis, the experiment cohort data and behavioral data are already flowing into a database, making SQL a natural choice. Success metrics are often already part of an organization’s reporting and analysis vocabulary, with SQL queries already developed. Joining variant assignment data to existing query logic is often relatively straightforward.

SQL is a good choice for automating experiment result reporting. The same query can be run for each experiment, substituting the name or identifier of the experiment in the _WHERE_ clause. Many organizations with high volumes of experiments have created standardized reports to speed up readouts and simplify the interpretation process.

While SQL is useful for many of the steps involved with experiment analysis, it does have one major shortcoming: SQL is not able to calculate statistical significance. Many databases allow developers to extend SQL functionality with _user-defined functions_ (UDFs). UDFs may be able to leverage statistical tests from languages such as Python, but they are beyond the scope of this book. A good option is to calculate summary statistics in SQL and then use an online calculator such as the one provided at [Evanmiller.org](https://oreil.ly/3uspA) to determine whether the experiment result is statistically significant.

**WHY CORRELATION IS NOT CAUSATION: HOW VALUES CAN RELATE TO EACH OTHER**

It’s easier to prove that two values are correlated (they rise or fall together, or one exists primarily in the presence of the other) than it is to prove that one _causes_ the other. Why is this the case? Although our brains are wired to detect causality, there are actually five ways in which two values, X and Y, can relate to each other:

1. _X causes Y:_ This is of course what we’re all trying to find. Through some mechanism, Y is the result of X.
2. _Y causes X:_ The relationship is there, but the direction of causality is reversed. For example, umbrellas don’t cause rain, but rather the presence of rain causes people to use umbrellas.
3. _X and Y have a common cause:_ The values are related because there is some third variable that explains both of them. Sales of ice cream and use of air conditioners both rise in the summer, but neither causes the other. Higher temperatures cause both increases.
4. _A feedback loop exists between X and Y:_ When Y increases, X increases to compensate, which leads to Y increasing in turn, and so on. This can happen when a customer is in the process of churning from a service. Fewer interactions lead to fewer items to suggest or remind, which leads to fewer interactions, and so on. Did the lack of recommendations cause less engagement, or is it the other way around?
5. _There is no relationship; it’s just random:_ Search long enough and you will find metrics that are correlated even though there is no actual relationship between them.&#x20;

## The Data Set

For this chapter, we will use a data set for a mobile game from the fictional Tanimura Studios. There are four tables. The `game_users` table contains records for people who downloaded the mobile game, along with the date and country. A sample of the data is shown in [Figure 7-1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch07.html#sample\_of\_the\_game\_users\_table).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0701.png" alt="" height="397" width="543"><figcaption></figcaption></figure>

**Figure 7-1. Sample of the `game_users` table**

The `game_actions` table contains records for things the users did in the game. A sample of the data is shown in [Figure 7-2](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch07.html#sample\_of\_the\_game\_actions\_table).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0702.png" alt="" height="398" width="590"><figcaption></figcaption></figure>

**Figure 7-2. Sample of the `game_actions` table**

The `game_purchases` table tracks purchases of in-game currency in US dollars. A sample of the data is shown in [Figure 7-3](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch07.html#sample\_of\_the\_game\_purchases\_table).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0703.png" alt="" height="396" width="592"><figcaption></figcaption></figure>

**Figure 7-3. Sample of the `game_purchases` table**

Finally, the `exp_assignment` table contains records of which variant users were assigned to for a particular experiment. A sample of the data is shown in [Figure 7-4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch07.html#sample\_of\_the\_exp\_assignment\_table).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0704.png" alt="" height="376" width="600"><figcaption></figcaption></figure>

**Figure 7-4. Sample of the `exp_assignment` table**

All of the data in these tables is fictional, created with random number generators, though the structure is similar to what you might see in the database of a real digital gaming company.&#x20;

## Types of Experiments

There is a wide range of experiments. If you can change something that a user, customer, constituent, or other entity experiences, you can in theory test that change. From an analysis standpoint, there are two main types of experiments: those with binary outcomes and those with continuous outcomes.

### Experiments with Binary Outcomes: The Chi-Squared Test

As you might expect, a binary outcome experiment has only two outcomes: either an action is taken or it isn’t. Either a user completes a registration flow or they don’t. A consumer clicks on a website ad or they don’t. A student graduates or they don’t. For these types of experiments, we calculate the proportion of each variant that completes the action. The numerator is the number of completers, while the denominator is all units that were exposed. This metric is also described as a rate: completion rate, click-through rate, graduation rate, and so on.

To determine whether the rates in the variants are statistically different, we can use the _chi-squared test_, which is a statistical test for categorical variables.[1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch07.html#ch01fn9) Data for a chi-squared test is often shown in the form of a _contingency table_, which shows the frequency of observations at the intersection of two attributes. This looks just like a pivot table to those who are familiar with that type of table.

Let’s take a look at an example, using our mobile game data set. A product manager has introduced a new version of the onboarding flow, a series of screens that teach a new player how the game works. The product manager hopes that the new version will increase the number of players who complete the onboarding and start their first game session. The new version was introduced in an experiment called “Onboarding” that assigned users to either control or variant 1, as tracked in the `exp_assignment` table. An event called “onboarding complete” in the `game_actions` table indicates whether a user completed the onboarding flow.

The contingency table shows the frequency at the intersection of the variant assignment (control or variant 1) and whether or not onboarding was completed. We can use a query to find the values for the table. Here we `count` the number of users with and without an “onboarding complete” action and _GROUP BY_ the `variant`:

```
SELECT a.variant
,count(case when b.user_id is not null then a.user_id end) as completed
,count(case when b.user_id is null then a.user_id end) as not_completed
FROM exp_assignment a
LEFT JOIN game_actions b on a.user_id = b.user_id
 and b.action = 'onboarding complete'
WHERE a.exp_name = 'Onboarding'
GROUP BY 1
;

variant    completed  not_completed
---------  ---------  -------------
control    36268      13629
variant 1  38280      11995
```

Adding totals for each row and column turns this output into a contingency table, as in [Figure 7-5](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch07.html#contingency\_table\_for\_onboarding\_comple).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492088776/files/assets/sfda_0705.png" alt="" height="277" width="600"><figcaption></figcaption></figure>

**Figure 7-5. Contingency table for onboarding completions**

To make use of one of the online significance calculators, we will need the number of successes, or times when the action was taken, and the total number cohorted for each variant. The SQL to find the required data points is straightforward. The assigned variant and the `count` of users assigned to that variant are queried from the `exp_assignment` table. We then _LEFT JOIN_ the `game_actions` table to find the `count` of users who completed onboarding. The _LEFT JOIN_ is required since we expect that not all users completed the relevant action. Finally, we find the percent completed in each variant by dividing the number of users who completed by the total number cohorted:

```
SELECT a.variant
,count(a.user_id) as total_cohorted
,count(b.user_id) as completions
,count(b.user_id) / count(a.user_id) as pct_completed
FROM exp_assignment a
LEFT JOIN game_actions b on a.user_id = b.user_id
 and b.action = 'onboarding complete'
WHERE a.exp_name = 'Onboarding'
GROUP BY 1
;

variant    total_cohorted  completions  pct_completed
---------  --------------  -----------  -------------
control    49897           36268        0.7269
variant 1  50275           38280        0.7614
```

We can see that variant 1 did indeed have more completions than the control experience, with 76.14% completing compared to 72.69%. But is this difference statistically significant, allowing us to reject the hypothesis that there is no difference? For this, we plug our results into an online calculator and confirm that the completion rate for variant 1 was significantly higher at a 95% confidence level than the completion rate for the control. Variant 1 can be declared the winner.

**TIP**

A 95% confidence level is commonly used, although this is not the only option. There are many online articles and discussions about the meaning of confidence levels, which level to use, and adjustments in scenarios in which you are comparing multiple variants to a control.

Binary outcome experiments follow this basic pattern. Calculate the successes or completions as well as the total members in each variant. The SQL used to derive the success events may be more complicated depending on the tables and how actions are stored in the database, but the output is consistent. Next, we’ll turn to experiments with continuous outcomes.

### Experiments with Continuous Outcomes: The t-Test

Many experiments seek to improve _continuous metrics_, rather than the binary outcomes discussed in the last section. Continuous metrics can take on a range of values. Examples include amount spent by customers, time spent on page, and days an app is used. Ecommerce sites often want to increase sales, and so they might experiment on product pages or checkout flows. Content sites may test layout, navigation, and headlines to try to increase the number of stories read. A company running an app might run a remarketing campaign to remind users to come back to the app.

For these and other experiments with continuous success metrics, the goal is to figure out whether the average values in each variant differ from each other in a statistically significant way. The relevant statistical test is the _two-sample t-test_, which determines whether we can reject the null hypothesis that the averages are equal with a defined confidence interval, usually 95%. The statistical test has three inputs, all of which are straightforward to calculate with SQL: the mean, the standard deviation, and the count of observations.

Let’s take a look at an example using our game data. In the last section, we looked at whether a new onboarding flow increased the completion rate. Now we will consider whether that new flow increased user spending on in-game currency. The success metric is the amount spent, so we need to calculate the mean and standard deviation of this value for each variant. First we need to calculate the amount per user, since users can make multiple purchases. Retrieve the cohort assignment from the `exp_assignment` table and `count` the users. Next, _LEFT JOIN_ to the `game_purchases` table to gather the amount data. The _LEFT JOIN_ is required since not all users make a purchase, but we still need to include them in the mean and standard deviation calculations. For users without purchases, the amount is set to a default of 0 with `coalesce`. Since the `avg` and `stddev` functions ignore nulls, the 0 default is required to ensure that these records are included. The outer query summarizes the output values by variant:

```
SELECT variant
,count(user_id) as total_cohorted
,avg(amount) as mean_amount
,stddev(amount) as stddev_amount
FROM
(
    SELECT a.variant
    ,a.user_id
    ,sum(coalesce(b.amount,0)) as amount
    FROM exp_assignment a
    LEFT JOIN game_purchases b on a.user_id = b.user_id
    WHERE a.exp_name = 'Onboarding'
    GROUP BY 1,2
) a
GROUP BY 1
;

variant    total_cohorted  mean_amount  stddev_amount
---------  --------------  -----------  -------------
control    49897           3.781        18.940
variant 1  50275           3.688        19.220
```

Next, we plug these values into an online calculator and find that there is no significant difference between the control and variant groups at a 95% confidence interval. The “variant 1” group appears to have increased onboarding completion rates but not the `amount` spent.

Another question we might consider is whether variant 1 affected spending among those users who completed the onboarding. Those who don’t complete the onboarding never make it into the game and therefore don’t even have the opportunity to make a purchase. To answer this question, we can use a query similar to the previous one, but we’ll add an _INNER JOIN_ to the `game_actions` table to restrict the users counted to only those who have an action of “onboarding complete”:

```
SELECT variant
,count(user_id) as total_cohorted
,avg(amount) as mean_amount
,stddev(amount) as stddev_amount
FROM
(
    SELECT a.variant
    ,a.user_id
    ,sum(coalesce(b.amount,0)) as amount
    FROM exp_assignment a
    LEFT JOIN game_purchases b on a.user_id = b.user_id
    JOIN game_actions c on a.user_id = c.user_id
     and c.action = 'onboarding complete'
    WHERE a.exp_name = 'Onboarding'
    GROUP BY 1,2
) a
GROUP BY 1
;

variant    total_cohorted  mean_amount  stddev_amount
---------  --------------  -----------  -------------
control    36268           5.202        22.049
variant 1  38280           4.843        21.899
```

Plugging these values into the calculator reveals that the average for the control group is statistically significantly higher than that for variant 1 at a 95% confidence interval. This result may seem perplexing, but it illustrates why it is so important to agree on the success metric for an experiment up front. The experiment variant 1 had a positive effect on onboarding completion and so can be judged a success. It did not have an effect on the overall spending level. This could be due to a mix shift: the additional users who made it through onboarding in variant 1 were less likely to pay. If the underlying hypothesis was that increasing onboarding completion rates would increase revenue, then the experiment should not be judged a success, and the product managers should come up with some new ideas to test.

## Challenges with Experiments and Options for Rescuing Flawed Experiments

Although experimentation is the gold standard for understanding causality, there are a number of ways experiments can go wrong. If the entire premise is flawed, there won’t be much that SQL can do to save the day. If the flaw is more technical in nature, we may be able to query the data in such a way as to adjust or exclude problematic data points and still interpret some results. Running experiments has a cost in terms of the time spent by engineers, designers, or marketers who create variants. It also has _opportunity cost_, or the missed benefit that could have been gained by sending customers down an optimal conversion path or product experience. On a practical level, using SQL to help the organization at least learn something from an experiment is often time well spent.

### Variant Assignment

Random assignment of experiment units (which can be users, sessions, or other entities) to control and variant groups is one of the key elements of experimentation. However, sometimes errors in the assignment process happen, whether because of a flaw in the experiment specification, a technical failure, or a limitation in the cohorting software. As a result, the control and variant groups may be of unequal sizes, fewer overall entities may have been cohorted than expected, or the assignment may not have actually been random.

SQL can sometimes help salvage an experiment in which too many units were cohorted. I have seen this happen when an experiment is meant to target only new users but all users are cohorted instead. Another way this can happen is when an experiment tests something that only a subset of users will see, either because it is a few clicks into the experience or because certain conditions must be met, such as previous purchase. Due to technical limitations, all users get cohorted, even though a chunk of them would never see the experiment treatment even if they are in that group. The solution is to add a _JOIN_ into the SQL that restricts the users or entities considered to only those that were intended to be eligible. For example, in the case of a new user experiment, we can add an _INNER JOIN_ to a table or subquery that contains user registration date and set a _WHERE_ condition to exclude users who registered too far prior to the cohort event to be considered new. The same strategy can be used when a certain condition must be met to even see the experiment. Restrict the entities included by excluding those that shouldn’t be eligible via _JOIN_s and _WHERE_ conditions. After doing this, you should check to make sure that the resulting population is a large enough sample to produce significant results.

If too few users or entities are cohorted, it’s important to check whether the sample is large enough to produce significant results. If not, run the experiment again. If the sample size is large enough, a second consideration is whether there is bias in who or what was cohorted. As an example, I have seen cases in which users on certain browsers or older app versions were not cohorted due to technical limitations. If populations that were excluded aren’t random and represent differences in location, technical savvy, or socioeconomic status, it’s important to consider both how large this population is relative to the rest and whether any adjustments should be made to include them in the final analysis.

Another possibility is that the variant assignment system is flawed and entities are not assigned randomly. This is fairly unusual with most modern experiment tools, but if it happens, it invalidates the whole experiment. Results that are “too good to be true” might signal a variant assignment problem. I have seen, for example, cases in which highly engaged users are accidentally assigned to both treatment and control due to a change in experiment configuration. Careful data profiling can check whether entities have been assigned to multiple variants or whether users with high or low engagement prior to the experiment are clustered in a particular variant.

**TIP**

Running an A/A test can help uncover flaws in the variant assignment software. In this type of test, entities are cohorted and success metrics are compared, just like in any other experiment. However, no changes are made to the experience, and both cohorts receive the control experience. Since the groups receive the same experience, we should expect no significant differences in the success metric. If it turns out there is a difference, further investigation should be done to uncover and fix the problem.

### Outliers

Statistical tests for analyzing continuous success metrics rely on averages. As a result, they are sensitive to unusually high or low outlier values. I have seen experiments in which the presence of one or two particularly high-spending customers in a variant gives that variant a statistically significant edge over others. Without those few high spenders, the result may be neutral or even the reverse. In most cases, we are more interested in whether a treatment has an effect across a range of individuals, and thus adjusting for these outliers can make an experiment result more meaningful.

We discussed anomaly detection in [Chapter 6](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#anomaly\_detection), and experiment analysis is another place in which those techniques can be applied. Outlier values can be determined either by analyzing the experiment results or by finding the base rate prior to the experiment. The outliers may be removed via a technique such as winsorizing (also discussed in [Chapter 6](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch06.html#anomaly\_detection)), which removes values beyond a threshold, such as the 95th or 99th percentile. This can be done in SQL before moving on to the rest of the experiment analysis.

Another option for dealing with outliers in continuous success metrics is to turn the success metric into a binary outcome. For example, instead of comparing the average spend across the control and variant groups, which may be distorted due to a few very high spenders, compare the purchase rate between the two groups and then follow the procedure discussed in the section on experiments with binary outcomes. We could consider the conversion rate to purchaser among users who completed onboarding in the control and variant 1 groups from the “Onboarding” experiment:

```
SELECT a.variant
,count(distinct a.user_id) as total_cohorted
,count(distinct b.user_id) as purchasers
,count(distinct b.user_id) / count(distinct a.user_id) 
 as pct_purchased
FROM exp_assignment a
LEFT JOIN game_purchases b on a.user_id = b.user_id
JOIN game_actions c on a.user_id = c.user_id
 and c.action = 'onboarding complete'
WHERE a.exp_name = 'Onboarding'
GROUP BY 1
;

variant    total_cohorted  purchasers  pct_purchased
---------  --------------  ----------  -------------
control    36268           4988        0.1000
variant 1  38280           4981        0.0991
```

We can look at the numbers and observe that even though there are more users in variant 1, there are fewer purchasers. The percentage of users who purchased in the control group is 10%, compared to 9.91% for variant 1. Next, we plug the data points into an online calculator. The conversion rate is statistically significantly higher for the control group. In this case, even though the rate of purchasing was higher for the control group, on a practical level we may be willing to accept this small decline if we believe that more users completing the onboarding process has other benefits. More players might boost rankings, for example, and players who enjoy the game may spread it to their friends via word of mouth, both of which can help growth and may then lead to attracting other new players who will become purchasers.

The success metric can also be set to a threshold, and the share of entities meeting that threshold compared. For example, the success metric could be reading at least three stories or using an app at least two times a week. An infinite number of metrics could be constructed in this way, so it’s important to understand what is both important and meaningful to the organization.

### Time Boxing

Experiments are often run over the course of several weeks. This means that individuals who enter the experiment earlier have a longer window in which to complete actions associated with the success metric. To control for this, we can apply _time boxing_—imposing a fixed length of time relative to the experiment entry date and considering actions only during that window. This concept was also covered in [Chapter 4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch04.html#cohort\_analysis).

For experiments, the appropriate size of the time box depends on what you are measuring. The window could be as short as one hour when measuring an action that typically has an immediate response, such as clicking on an ad. For purchase conversion, experimenters often allow a window of 1 to 7 days. Shorter windows allow experiments to be analyzed sooner, since all cohorted entities need to be allowed the full time to complete actions. The best windows balance the need to obtain results with the actual dynamics of the organization. If customers typically convert in a few days, consider a 7-day window; if customers often take 20 or more days, consider a 30-day window.

As an example, we can revise our first example from experiments with continuous outcomes by only including purchases within 7 days of the cohorting event. Note that it is important to use the time the entity was assigned to a variant as the starting point of the time box. An additional _ON_ clause is added, restricting the results to purchases that occurred within the interval “7 days”:

```
SELECT variant
,count(user_id) as total_cohorted
,avg(amount) as mean_amount
,stddev(amount) as stddev_amount
FROM
(
    SELECT a.variant
    ,a.user_id
    ,sum(coalesce(b.amount,0)) as amount
    FROM exp_assignment a
    LEFT JOIN game_purchases b on a.user_id = b.user_id 
     and b.purch_date <= a.exp_date + interval '7 days'
    WHERE a.exp_name = 'Onboarding'
    GROUP BY 1,2
) a
GROUP BY 1
;

variant    total_cohorted  mean_amount  stddev_amount
---------  --------------  -----------  -------------
control    49897           1.369        5.766
variant 1  50275           1.352        5.613
```

The means are similar, and in fact statistically they are not significantly different from each other. In this example, the time-boxed conclusion agrees with the conclusion when there was no time box.

In this case, purchase events are relatively rare. For metrics that measure common events and those that accumulate quickly, such as pageviews, clicks, likes, and articles read, using a time box can prevent the earliest cohorted users from looking substantially “better” than those cohorted later.

### Repeated Exposure Experiments

In discussions of online experimentation, most examples are of what I like to call “one-and-done” experiences: the user encounters a treatment once, reacts to it, and does not pass that way again. User registration is a classic example: a consumer signs up for a particular service only once, and therefore any changes to the sign-up process affect only new users. Analyzing tests on these experiences is relatively straightforward.

There is another type of experience that I call “repeated exposure,” in which an individual comes into contact with the change many times during the course of using a product or service. In any experiment involving these changes, we can expect individuals to encounter them more than once. Changes to an app’s user interface, such as color, text, and placement of important information and links, are experienced by users throughout their app usage. Email marketing programs that send customers reminders or promotions on a regular basis also have this repeated exposure quality. Emails are experienced many times as subject lines in the inbox, and as content if opened.

Measuring repeated exposure experiments is trickier than measuring one-and-done experiments due to novelty effects and regression to the mean. A _novelty effect_ is the tendency for behavior to change just because something is new, not because it is necessarily better. _Regression to the mean_ is the tendency for phenomena to return to an average level over time. As an example, changing any part of a user interface tends to increase the number of people who interact with it, whether it is a new button color, logo, or placement of functionality. Initially the metrics look good, because the click-through rate or engagement goes up. This is the novelty effect. But over time, users get used to the change, and they tend to click or use the functionality at rates that return closer to the baseline. This is the regression to the mean. The important question to answer when running this kind of experiment is whether the new baseline is higher (or lower) than the previous one. One solution is to allow passage of a long enough time period, in which you might expect regression to happen, before evaluating the results. In some cases, this will be a few days; in others, it might be a few weeks or months.

When there are many changes, or the experiment comes in a series such as email or physical mail campaigns, figuring out whether the entire program makes a difference can be a challenge. It’s easy to claim success when customers receiving a certain email variant purchase a product, but how do we know if they would have made that purchase anyway? One option is to set up a _long-term holdout_. This is a group that is set up to not receive any marketing messages or changes to the product experience. Note that this is different from simply comparing to users who have opted out of marketing messages, since there is usually some bias in who opts out and who doesn’t. Setting up long-term holdouts can be complicated, but there are few better ways to truly measure the cumulative effects of campaigns and product changes.

Another option is to perform cohort analysis (discussed in [Chapter 4](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch04.html#cohort\_analysis)) on the variants. The groups can be followed for a longer time period, from weeks to months. Retention or cumulative metrics can be calculated and tested to see whether effects differ between variants over the long term.

Even with the various challenges that can be encountered with experiments, they are still the best way to test and prove the causality around changes made to experiences ranging from marketing messaging and creative to in-product experience. We often encounter less than ideal situations in data analysis, however, so next we’ll turn to some analysis options for when A/B testing isn’t possible.

## When Controlled Experiments Aren’t Possible: Alternative Analyses

Randomized experiments are the gold standard for going beyond correlation to establish causality. However, there are a number of reasons why a randomized experiment may not be possible. It may be unethical to give different treatments to different groups, particularly in medical or educational settings. Regulatory requirements may prevent experiments in other settings, such as financial services. There may be practical reasons, such as the difficulty of restricting access to a variant treatment to only a randomized group. It’s always worth considering whether there are pieces that are worth testing or are testable within ethical, regulatory, and practical boundaries. Wording, placement, and other design elements are some examples.

A second situation in which experimentation isn’t possible is when a change happened in the past and the data has already been collected. Aside from reverting the change, going back and running an experiment isn’t an option. Sometimes a data analyst or data scientist wasn’t available to advise on experimentation. More than once, I have joined an organization and been asked to untangle the results of changes that would have been much more straightforward to understand had there been a holdout group. At other times, the change is unintended. Examples include site outages that affect some or all customers, errors in forms, and natural disasters such as storms, earthquakes, and wildfires.

Although causal conclusions aren’t as strong in situations in which there was no experiment, there are a few quasi-experimental analysis methods that can be used to draw insights from the data. These rely on constructing groups from the available data that represent “control” and “treatment” conditions as closely as possible.

### Pre-/Post-Analysis

A pre-/post-analysis compares either the same or similar populations before and after a change. The measurement of the population before the change is used as the control, while the measurement after the change is used as the variant or treatment.

Pre-/post-analysis works best when there is a clearly defined change that happened on a well-known date, so that the before and after groups can be cleanly divided. In this type of analysis, you will need to choose how long to measure before and after the change, but the periods should be equal or close to equal. For example, if two weeks have elapsed since a change, compare that period to the two weeks prior to the change. Consider comparing multiple periods, such as one week, two weeks, three weeks, and four weeks before and after the change. If the results agree across all these windows, you can have more confidence in the result than you would if the results differed.

Let’s walk through an example. Imagine that the onboarding flow for our mobile game includes a step in which the user can check a box to indicate whether they want to receive emails with game news. This had always been checked by default, but a new regulation requires that it now be unchecked by default. On January 27, 2020, the change was released into the game, and we would like to find out if it had a negative effect on email opt-in rates. To do this, we will compare the two weeks before the change to the two weeks after the change and see whether the opt-in rate is statistically significantly different. We could use one-week or three-week periods, but two weeks is chosen because it is long enough to allow for some day-of-week variability and also short enough to restrict the number of other factors that could otherwise affect users’ willingness to opt in.

The variants are assigned in the SQL query via a CASE statement: the users who were created in the time range prior to the change are labeled “pre,” while those created after the change are labeled “post.” Next, we `count` the number of users in each group from the `game_users` table. Then we `count` the number of users who opted in, which is accomplished with a _LEFT JOIN_ to the `game_actions` table, restricting to the records with the “email\_optin” action. Then we divide the values to find the percent who opted in. I like to include the count of days in each variant as a quality check, though it is not necessary to perform the rest of the analysis:

```
SELECT 
case when a.created between '2020-01-13' and '2020-01-26' then 'pre'
     when a.created between '2020-01-27' and '2020-02-09' then 'post'
     end as variant
,count(distinct a.user_id) as cohorted
,count(distinct b.user_id) as opted_in
,count(distinct b.user_id) / count(distinct a.user_id) as pct_optin
,count(distinct a.created) as days
FROM game_users a
LEFT JOIN game_actions b on a.user_id = b.user_id 
 and b.action = 'email_optin'
WHERE a.created between '2020-01-13' and '2020-02-09'
GROUP BY 1
;

variant  cohorted  opted_in  pct_optin  days
-------  --------  --------  ---------  ----
pre      24662     14489     0.5875     14
post     27617     11220     0.4063     14
```

**TIP**

Many databases will recognize dates entered as strings, as in `'2020-01-13'`. If your database does not, cast the string to a date using one of these options:

```
cast('2020-01-13' as date)

date('2020-01-13')

'2020-01-13'::date
```

In this case, we can see that the users who went through the onboarding flow before the change had a much higher email opt-in rate—58.75%, compared to 40.63% afterward. Plugging the values into an online calculator results in confirmation that the rate for the “pre” group is statistically significantly higher than the rate for the “post” group. In this example, there may not be much the game company can do, since the change is due to a regulation. Further tests could determine whether providing sample content or other information about the email program might encourage more new players to opt in, if this is a business goal.

When performing a pre-/post-analysis, keep in mind that other factors beyond the change that you’re trying to learn about may cause an increase or a decrease in the metric. External events, seasonality, marketing promotions, and so on can drastically change the environment and customers’ mindsets even within the span of a few weeks. As a result, this type of analysis is not as good as a true randomized experiment for proving causality. However, sometimes this is one of the few analysis options available, and it can generate working hypotheses that can be tested and refined in future controlled experiments.

### Natural Experiment Analysis

A _natural experiment_ occurs when entities end up with different experiences through some process that approximates randomness. One group receives the normal or control experience, and another receives some variation that may have a positive or negative effect. Usually these are unintentional, such as when a software bug is introduced, or when an event happens in one location but not in other locations. For this type of analysis to have validity, we must be able to clearly determine which entities were exposed. Additionally, a control group that is as similar as possible to the exposed group is needed.

SQL can be used to construct the variants and to calculate the size of cohorts and success events in the case of a binary outcome event (or the mean, standard deviation, and population sizes in the case of a continuous outcome event). The results can be plugged into an online calculator just like with any other experiment.

As an example in the video game data set, imagine that, during the time period of our data, users in Canada were accidentally given a different offer on the virtual currency purchase page the first time they looked at it: an extra zero was added to the number of virtual coins in each package. So, for example, instead of 10 coins the user would receive 100 game coins, or instead of 100 coins they would receive 1,000 game coins, and so on. The question we would like to answer is whether Canadians converted to buyers at a higher rate than other users. Rather than compare to the entire user base, we will compare only to users in the United States. The countries are close geographically, and most users in the two countries speak the same language—and for the sake of the example, we’ll assume that we’ve done other analysis showing that their behavior is similar, while the behavior of users in other countries differs enough to exclude them.

To perform the analysis, we create the “variants” from whatever the distinguishing characteristic is—in this case, the `country` field from the `game_users` table—but note that sometimes more complex SQL will be required, depending on the data set. The counts of users cohorted, and those who purchased, are calculated in the same way we saw previously:

```
SELECT a.country
,count(distinct a.user_id) as total_cohorted
,count(distinct b.user_id) as purchasers
,count(distinct b.user_id) / count(distinct a.user_id) 
 as pct_purchased
FROM game_users a
LEFT JOIN game_purchases b on a.user_id = b.user_id
WHERE a.country in ('United States','Canada')
GROUP BY 1
;

country        total_cohorted  purchasers  pct_purchased
-------------  --------------  ----------  -------------  
Canada         20179           5011        0.2483
United States  45012           4958        0.1101
```

The share of users in Canada who purchased is in fact higher—24.83%, compared to 11.01% of those in the United States. Plugging these values into an online calculator confirms that the conversion rate in Canada is statistically significantly higher at a 95% confidence interval.

The hardest part of analyzing a natural experiment tends to be finding a comparable population and showing that the two populations are similar enough to support the conclusions from the statistical test. Although it is virtually impossible to prove that there are no confounding factors, careful comparison of population demographics and behaviors lends credibility to the results. Since a natural experiment is not a true random experiment, the evidence for causality is weaker, and this should be noted in the presentation of analysis of this type.

### Analysis of Populations Around a Threshold

In some cases, there is a threshold value that results in some people or other subject units getting a treatment, while others do not. For example, a certain grade point average might qualify students for a scholarship, a certain income level might qualify households for subsidized health care, or a high churn risk score might trigger a sales rep to follow up with a customer. In such cases, we can leverage the idea that subjects on either side of the threshold value are likely quite similar to each other. So instead of comparing the entire populations that did and did not receive the reward or intervention, we can compare only those that were close to the threshold both on the positive and the negative side. The formal name for this is _regression discontinuity design_ (RDD).

To perform this type of analysis, we can construct “variants” by splitting the data around the threshold value, similar to what we did in the pre-/post-analysis. Unfortunately, there is no hard-and-fast rule about how wide to make the bands of values on either side of the threshold. The “variants” should be similar in size, and they should be large enough to allow for significance in the results analysis. One option is to perform the analysis several times with a few different ranges. For example, you might analyze the differences between the “treated” group and the control when each group contains subjects that fall within 5%, 7.5%, and 10% of the threshold. If the conclusions from these analyses agree, there is more support for the conclusions. If they do not agree, however, the data may be considered inconclusive.

As with other types of nonexperimental analysis, results from RDD should be taken as proving causality less conclusively. Potential confounding factors should receive careful attention as well. For example, if customers with high churn risk receive interventions from multiple teams, or a special discount to encourage them to retain, in addition to a call from a sales rep, the data can potentially be tainted by those other changes.

## Conclusion

Experiment analysis is a rich field that often incorporates different types of analysis seen in other parts of this book, from anomaly detection to cohort analysis. Data profiling can be useful in tracking down issues that occur. When randomized experiments are not possible, a variety of other techniques are available, and SQL can be used to create synthetic control and variant groups.

In the next chapter, we’ll turn to constructing complex data sets for analysis, an area that brings together various topics that we’ve discussed in the book so far.

[1](https://learning.oreilly.com/library/view/sql-for-data/9781492088776/ch07.html#ch01fn9-marker) See [_https://www.mathsisfun.com/data/chi-square-test.html_](https://www.mathsisfun.com/data/chi-square-test.html) for a good explanation of this test.
