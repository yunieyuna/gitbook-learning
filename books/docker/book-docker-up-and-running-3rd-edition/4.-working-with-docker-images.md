# 4. Working With Docker Images

## Chapter 4. Working with Docker Images

Every Linux container is based on an image. Images are the underlying definition of what gets reconstituted into a running container, much like a virtual disk becomes a VM when you start it up. Docker or [Open Container Initiative (OCI)](https://opencontainers.org/) images provide the basis for everything that you will ever deploy and run with Docker. To launch a container, you must either download a public image or create your own. You can think of the image as a single asset that primarily represents the filesystem for the container. However, in reality, every image consists of one or more linked filesystem layers that generally have a direct one-to-one mapping to each build step used to create that image.

Because images are built up from individual layers, they put special demands on the Linux kernel, which must provide the drivers that Docker needs to run the storage backend. For image management, Docker relies heavily on this storage backend, which communicates with the underlying Linux filesystem to build and manage the multiple layers that combine into a single usable image. The primary storage backends that are supported include the following:

* [Overlay2](https://oreil.ly/r4JHY)[1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#idm46803155693712)
* [B-Tree File System (Btrfs)](https://btrfs.wiki.kernel.org/index.php/Main\_Page)
* [Device Mapper](https://www.sourceware.org/dm)

Each storage backend provides a fast copy-on-write (CoW) system for image management. We discuss the specifics of various backends in [Chapter 11](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch11.html#advanced\_topics). For now, we’ll use the default backend and explore how images work, since they make up the basis for almost everything else that you will do with Docker, including the following:

* Building images
* Uploading (pushing) images to an image registry
* Downloading (pulling) images from an image registry
* Creating and running containers from an image

## Anatomy of a Dockerfile

To create a custom Docker image with the default tools, you will need to become familiar with the _Dockerfile_. This file describes all the steps that are required to create an image and is usually contained within the root directory of the source code repository for your application.

A typical _Dockerfile_ might look something like the one shown here, which creates a container for a Node.js-based application:

```
FROM node:18.13.0

ARG email="anna@example.com"
LABEL "maintainer"=$email
LABEL "rating"="Five Stars" "class"="First Class"

USER root

ENV AP /data/app
ENV SCPATH /etc/supervisor/conf.d

RUN apt-get -y update

# The daemons
RUN apt-get -y install supervisor
RUN mkdir -p /var/log/supervisor

# Supervisor Configuration
COPY ./supervisord/conf.d/* $SCPATH/

# Application Code
COPY *.js* $AP/

WORKDIR $AP

RUN npm install

CMD ["supervisord", "-n"]
```

Dissecting this _Dockerfile_ will provide some initial exposure to a number of the possible instructions for controlling how an image is assembled. Each line in a _Dockerfile_ creates a new image layer that is stored by Docker. This layer contains all of the changes that are a result of that command being issued. This means that when you build new images, Docker will only need to build layers that deviate from previous builds: you can reuse all the layers that haven’t changed.

Although you could build a Node instance from a plain, base Linux image, you can also explore [Docker Hub](https://registry.hub.docker.com/) for official images for Node. The Node.js community maintains a series of [Docker images](https://registry.hub.docker.com/\_/node) and tags that allow you to quickly determine what versions are available. If you want to lock the image to a specific point release of Node, you could point it at something like `node:18.13.0`. The following base image will provide you with an Ubuntu Linux image running Node 11.11.x:

```
FROM docker.io/node:18.13.0
```

The `ARG` parameter provides a way for you to set variables and their default values, which are only available during the image build process:

```
ARG email="anna@example.com"
```

Applying labels to images and containers allows you to add metadata via key/value pairs that can later be used to search for and identify Docker images and containers. You can see the labels applied to any image using the `docker image inspect` command. For the maintainer label, we are leveraging the value of the `email` build argument that was defined in the previous line of the _Dockerfile_. This means that this label can be changed anytime we build this image:

```
LABEL "maintainer"=$email
LABEL "rating"="Five Stars" "class"="First Class"
```

By default, Docker runs all processes as `root` within the container, but you can use the `USER` instruction to change this:

```
USER root
```

**CAUTION**

Even though containers provide some isolation from the underlying operating system, they still run on the host kernel. Due to potential security risks, production containers should almost always be run in the context of an unprivileged user.

Unlike the `ARG` instruction, the `ENV` instruction allows you to set shell variables that can be used by your running application for configuration, in addition to being available during the build process. The `ENV` and `ARG` instructions can be used to simplify the _Dockerfile_ and help keep it DRYer (Don’t Repeat Yourself):

```
ENV AP /data/app
ENV SCPATH /etc/supervisor/conf.d
```

In the following code, you’ll use a collection of `RUN` instructions to start and create the required file structure that you need, and install some required software dependencies:

```
RUN apt-get -y update

# The daemons
RUN apt-get -y install supervisor
RUN mkdir -p /var/log/supervisor
```

**WARNING**

While we’re demonstrating it here for simplicity, it is not recommended that you run commands like `apt-get -y update` or `dnf -y update` in your application’s _Dockerfile_. This is because it requires crawling the repository index each time you run a build, which means that your build is not guaranteed to be repeatable since package versions might change between builds. Instead, consider basing your application image on another image that already has these updates applied to it and where the versions are in a known state. It will be faster and more repeatable.

The `COPY` instruction is used to copy files from the local filesystem into your image. Most often this will include your application code and any required support files. Because `COPY` copies the files into the image, you no longer need access to the local filesystem to access them once the image is built. You’ll also start to use the build variables you defined in the previous section to save you a bit of work and help protect you from typos:

```
# Supervisor Configuration
COPY ./supervisord/conf.d/* $SCPATH/

# Application Code
COPY *.js* $AP/
```

**TIP**

Remember that every instruction creates a new Docker image layer, so it often makes sense to combine a few logically grouped commands onto a single line. It is even possible to use the `COPY` instruction in combination with the `RUN` instruction to copy a complex script to your image and then execute that script with only two commands in the _Dockerfile_.

With the `WORKDIR` instruction, you change the working directory in the image for the remaining build instructions and the default process that launches with any resulting containers:

```
WORKDIR $AP

RUN npm install
```

**CAUTION**

The order of commands in a _Dockerfile_ can have a very significant impact on ongoing build times. You should try to order commands so that things that change between every single build are closer to the bottom. This means that adding your code and similar steps should be held off until the end. When you rebuild an image, every single layer after the first introduced change will need to be rebuilt.

And finally, you end with the `CMD` instruction, which defines the command that launches the process that you want to run within the container:

```
CMD ["supervisord", "-n"]
```

**NOTE**

Though not a hard-and-fast rule, it is generally considered a best practice to try to run only a single process within a container. The core idea is that a container should provide a single function so that it remains easy to horizontally scale individual functions within your architecture. In the example, you are using `supervisord` as a process manager to help improve the resiliency of the node application within the container and ensure that it stays running. This can also be useful for troubleshooting your application during development so that you can restart your service without restarting the whole container.

You could also achieve a similar effect by using the `--init` command-line argument to `docker container run`, which we discuss in [“Controlling Processes”](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch07.html#controlling\_processes).

## Building an Image

To build your first image, go ahead and clone a Git repo that contains an example application called _docker-node-hello_, as shown here:[2](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#idm46803155211200)

```
$ git clone https://github.com/spkane/docker-node-hello.git \
    --config core.autocrlf=input
Cloning into 'docker-node-hello'…
remote: Counting objects: 41, done.
remote: Total 41 (delta 0), reused 0 (delta 0), pack-reused 41
Unpacking objects: 100% (41/41), done.

$ cd docker-node-hello
```

**NOTE**

Git is frequently installed on Linux and macOS systems, but if you do not already have Git available, you can download a simple installer from [_git-scm.com_](https://git-scm.com/downloads).

The `--config core.autocrlf=input` option we use helps ensure that the line endings are not accidentally altered from the Linux standard that is expected.

This will download a working _Dockerfile_ and related source code files into a directory called _docker-node-hello_. If you look at the contents while ignoring the Git repo directory, you should see the following:

```
$ tree -a -I .git
.
├── .dockerignore
├── .gitignore
├── Dockerfile
├── index.js
├── package.json
└── supervisord
    └── conf.d
        ├── node.conf
        └── supervisord.conf
```

Let’s review the most relevant files in the repo.

The _Dockerfile_ should be the same as the one you just reviewed.

The _.dockerignore_ file allows you to define files and directories that you do not want to upload to the Docker host when you are building the image. In this instance, the _.dockerignore_ file contains the following line:

```
.git
```

This instructs `docker image build` to exclude the _.git_ directory, which contains the whole source code repository, from the build. The rest of the files reflect the current state of your source code on the checked-out branch. You don’t need the contents of the _.git_ directory to build the Docker image, and since it can grow quite large over time, you don’t want to waste time copying it every time you do a build. _package.json_ defines the Node.js application and lists any dependencies that it relies on. _index.js_ is the main source code for the application.

The _supervisord_ directory contains the configuration files for `supervisord` that you will use to start and monitor the application.

**NOTE**

Using [`supervisord`](http://supervisord.org/) in this example to monitor the application is overkill, but it is intended to provide a bit of insight into some of the techniques you can use in a container to provide more control over your application and its running state.

As we discussed in [Chapter 3](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch03.html#installing\_docker), you will need to have your Docker server running and your client properly set up to communicate with it before you can build a Docker image. Assuming that this is all working, you should be able to initiate a new build by running the upcoming command, which will build and tag an image based on the files in the current directory.

Each step identified in the following output maps directly to a line in the _Dockerfile_, and each step creates a new image layer based on the previous step. The first build that you run will take a few minutes because you have to download the base node image. Subsequent builds should be much faster unless a new version of our base image tag has been released.

**NOTE**

The output that follows is from the new BuildKit included in Docker. If you see significantly different output, then you are likely still using the older image building code.

You can enable BuildKit in your environment by setting the `DOCKER_BUILDKIT` environment variable to `1`.

You can find more details on the [Docker website](https://docs.docker.com/build/buildkit).

At the end of the `build` command, you will notice a period. This refers to the build context, which tells Docker what files it should upload to the server so that it can build our image. In many cases, you will simply see a `.` at the end of a `build` command, since a single period represents the current directory. This build context is what the _.dockerignore_ file is filtering so that we don’t upload more than we need.

**TIP**

Docker assumes that the _Dockerfile_ is in the current directory, but if it is not, you can point directly to it using the `-f` argument.

Let’s run the build:

```
$ docker image build -t example/docker-node-hello:latest .

 => [internal] load build definition from Dockerfile
 => => transferring dockerfile: 37B
 => [internal] load .dockerignore
 => => transferring context: 34B
 => [internal] load metadata for docker.io/library/node:18.13.0
 => CACHED [1/8] FROM docker.io/library/node:18.13.0@19a9713dbaf3a3899ad…
 => [internal] load build context
 => => transferring context: 233B
 => [2/8] RUN apt-get -y update
 => [3/8] RUN apt-get -y install supervisor
 => [4/8] RUN mkdir -p /var/log/supervisor
 => [5/8] COPY ./supervisord/conf.d/* /etc/supervisor/conf.d/
 => [6/8] COPY *.js* /data/app/
 => [7/8] WORKDIR /data/app
 => [8/8] RUN npm install
 => exporting to image
 => => exporting layers
 => => writing image sha256:991844271ca5b984939ab49d81b24d4d53137f04a1bd…
 => => naming to docker.io/example/docker-node-hello:latest
```

**TIP**

To improve the speed of builds, Docker will use a local cache when it thinks it is safe. This can sometimes lead to unexpected issues because it doesn’t always notice that something changed in a lower layer. In the preceding output, you will notice lines like `⇒ [2/8] RUN apt-get -y update`. If instead you see `⇒ CACHED [2/8] RUN apt-get -y update`, you know that Docker decided to use the cache. You can disable the cache for a build by using the `--no-cache` argument to the `docker image build` command.

If you are building your Docker images on a system that is used for other simultaneous processes, you can limit the resources available to your builds by using many of the same cgroup methods that we will discuss in [Chapter 5](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch05.html#docker\_containers). You can find detailed documentation on the `docker image build` arguments in the [official documentation](https://docs.docker.com/engine/reference/commandline/image\_build).

**TIP**

Using `docker image build` is functionally the same as using `docker build`.

If you have any issues getting a build to work correctly, you may want to skip ahead and read the sections [“Multistage builds”](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#multi\_stage) and [“Troubleshooting Broken Builds”](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#broken\_builds) in this chapter.

## Running Your Image

Once you have successfully built the image, you can run it on your Docker host with the following command:

```
$ docker container run --rm -d -p 8080:8080 example/docker-node-hello:latest
```

This command tells Docker to create a running container in the background from the image with the `example/docker-node-hello:latest` tag, and then map port 8080 in the container to port 8080 on the Docker host. If everything goes as expected, the new Node.js application should be running in a container on the host. You can verify this by running `docker container ls`. To see the running application in action, you will need to open up a web browser and point it at port 8080 on the Docker host. You can usually determine the Docker host IP address by examining the entry from `docker context list` that is marked with an asterisk or checking the value of the `DOCKER_HOST` environment variable if it happens to be set. If the `DOCKER ENDPOINT` is set to a Unix socket, then the IP address is most likely `127.0.0.1`:

```
$ docker context list
NAME      TYPE … DOCKER ENDPOINT             …
default * moby … unix:///var/run/docker.sock …
…
```

Get the IP address and enter something like [_http://127.0.0.1:8080/_](http://127.0.0.1:8080/) (or your remote Docker address if it’s different than that) into your web browser address bar, or use a command-line tool like `curl`. You should see the following text:

```
Hello World. Wish you were here.
```

### Build Arguments

If you inspect the image that we built, you will be able to see that the maintainer label was set to `anna@example.com`:

```
$ docker image inspect \
  example/docker-node-hello:latest | grep maintainer
            "maintainer": "anna@example.com",
```

If we wanted to change the `maintainer` label, we could simply rerun the build and provide a new value for the `email` `ARG` via the `--build-arg` command-line argument, like so:

```
$ docker image build --build-arg email=me@example.com \
    -t example/docker-node-hello:latest .

…
 => => naming to docker.io/example/docker-node-hello:latest
```

After the build has finished, we can check the results by reinspecting the new image:

```
$ docker image inspect \
  example/docker-node-hello:latest | grep maintainer
            "maintainer": "me@example.com",
```

The `ARG` and `ENV` instructions can help make _Dockerfile_s very flexible while also avoiding a lot of repeated values that can be hard to keep up to date.

### Environment Variables as Configuration

If you read the _index.js_ file, you will notice that part of the file refers to the variable `$WHO`, which the application uses to determine who the application is going to say Hello to:

```
var DEFAULT_WHO = "World";
var WHO = process.env.WHO || DEFAULT_WHO;

app.get('/', function (req, res) {
  res.send('Hello ' + WHO + '. Wish you were here.\n');
});
```

Let’s quickly cover how you can configure this application by passing in environment variables when you start it. First, you need to stop the existing container using two commands. The first command will provide you with the container ID, which you will need to use in the second command:

```
$ docker container ls
CONTAINER ID  IMAGE                             STATUS       …
b7145e06083f  example/centos-node-hello:latest  Up 4 minutes …
```

**NOTE**

You can format the output of `docker container ls` by using a [Go template](https://developer.hashicorp.com/nomad/tutorials/templates/go-template-syntax) so that you see only the information that you care about. In the preceding example, you might decide to run something like `docker container ls --format "table {{.ID}}\t{{.Image}}\t{{.Status}}"` to limit the output to the three fields you care about. Additionally, running `docker container ls --quiet` with no format options will limit the output to only the container ID.

And then, using the container ID from the previous output, you can stop the running container by typing the following:

```
$ docker container stop b7145e06083f
b7145e06083f
```

**TIP**

Using `docker container ls` is functionally equivalent to using `docker container list`, `docker container ps`, or `docker ps`.

Using `docker container stop` is also functionally equivalent to using `docker stop`.

You can then restart the container after adding a single instance of the `--env` argument to the previous `docker container run` command:

```
$ docker container run --rm -d \
    --publish mode=ingress,published=8080,target=8080 \
    --env WHO="Sean and Karl" \
    example/docker-node-hello:latest
```

If you reload your web browser, you should see that the text on the web page now reads as follows:

```
Hello Sean and Karl. Wish you were here.
```

**NOTE**

You could shorten the preceding `docker` command to the following if you wanted:

```
$ docker container run --rm -d -p 8080:8080 \
    -e WHO="Sean and Karl" \
    example/docker-node-hello:latest
```

You can go ahead and stop this container now, by using `docker container stop` and passing in the correct container ID.

## Custom Base Images

Base images are the lowest-level images that other Docker images will build upon. Most often, these are based on minimal installs of Linux distributions like Ubuntu, Fedora, or Alpine Linux, but they can also be much smaller, containing a single statically compiled binary. For most people, using the official base images for their favorite distribution or tool is a great option.

However, there are times when it is preferable to build your own base images rather than use an image created by someone else. One reason to do this is to maintain a consistent OS image across all your deployment methods for hardware, VMs, and containers. Another is to get the image size down substantially. There is no need to ship around an entire Ubuntu distribution, for example, if your application is a statically built C or Go application. You might find that you only need the tools you regularly use for debugging, and some other shell commands and binaries. Making the effort to build such an image could pay off in better deployment times and easier application distribution.

A common middle ground between these two approaches is to build images using Alpine Linux, which is designed to be very small and is popular as a basis for Docker images. To keep the distribution size very small, Alpine Linux is based on the modern, lightweight [musl standard library](https://musl.libc.org/), instead of the more traditional [GNU C Library (glibc)](https://www.gnu.org/software/libc). In general, this is not a big issue, since many packages support _musl_, but it is something to be aware of. It has the largest impact on Java-based applications and DNS resolution. It’s widely used in production, however, because of its diminutive image size. Alpine Linux is highly optimized for space, which is the reason that it ships with _/bin/sh_ instead of _/bin/bash_, by default. However, you can also install _glibc and bash_ in Alpine Linux if you need it, and this is often done in the case of JVM containers.

In the official Docker documentation, there is some good information about how you can build base images on the various [Linux distributions](https://dockr.ly/2N1FZcU).

## Storing Images

Now that you have created a Docker image that you’re happy with, you’ll want to store it somewhere so that it can be easily accessed by any Docker host that you want to deploy it to. This is also the normal hand-off point between building images and storing them somewhere for future deployment. You don’t normally build the images on a production server and then run them. This process was described when we talked about handoff between teams for application deployment. Ordinarily, deployment is the process of pulling an image from a repository and running it on one or more Linux servers. There are a few ways you can go about storing your images into a central repository for easy retrieval.

### Public Registries

Docker provides an [image registry](https://registry.hub.docker.com/) for public images that the community wants to share. These include official images for Linux distributions, ready-to-go WordPress containers, and much more.

If you have images that can be published on the internet, the best place for them is a public registry, like [Docker Hub](https://hub.docker.com/). However, there are other options. When the core Docker tools were first gaining popularity, Docker Hub did not exist. To fill this obvious void in the community, [Quay.io](https://quay.io/) was created. Since then, Quay.io has gone through a few acquisitions and is now owned by Red Hat. Cloud vendors like Google and SaaS companies like GitHub also have their own registry offerings. Here we’ll talk about just the two of them.

Both Docker Hub and Quay.io provide centralized Docker image registries that can be accessed from anywhere on the internet, and provide a method to store private images in addition to public ones. Both have nice user interfaces and the ability to separate team access permissions and manage users. Both also offer reasonable commercial options for private SaaS hosting of your images, much in the same way that GitHub sells private registries on its systems. This is probably the right first step if you’re getting serious about Docker but are not yet shipping enough code to need an internally hosted solution.

For companies that use Docker heavily, one of the biggest downsides to these registries is that they are not local to the network on which the application is being deployed. This means that every layer of every deployment might need to be dragged across the internet to deploy an application. Internet latencies have a very real impact on software deployments, and outages that affect these registries could have a very detrimental impact on a company’s ability to deploy smoothly and on schedule. This is mitigated by good image design, where you make thin layers that are easy to move around the internet.

### Private Registries

The other option that many companies consider is to host some type of Docker image registry internally, which can interact with the Docker client to support pushing, pulling, and searching images. The open source [Distribution](https://github.com/distribution/distribution) project provides the basic functionality that most other registries build upon.

Other strong contenders in the private registry space include [Harbor](https://goharbor.io/) and [Red Hat Quay](https://www.redhat.com/en/technologies/cloud-computing/quay). In addition to the basic Docker registry functionality, these products have solid GUI interfaces and many additional features, like image verification.

### Authenticating to a Registry

Communicating with a registry that stores container images is a part of daily life with Docker. For many registries, this means you’ll need to authenticate to gain access to images. But Docker also tries to make it easy to automate things so it can store your login information and use it on your behalf when you request things like pulling down a private image. By default, Docker assumes the registry will be Docker Hub, the public repository hosted by Docker, Inc.

**TIP**

Although a bit more advanced, it is worth noting that you can also configure the Docker daemon to use a [custom registry mirror](https://oreil.ly/16Kns)[3](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#idm46803154385056) or a [pull-through image cache](https://oreil.ly/2Am1f).[4](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#idm46803154382192)

#### Creating a Docker Hub account

For these examples, you will create an account on Docker Hub. You don’t need an account to download publicly shared images, but you will need to be logged in to avoid rate limits and upload any containers that you build.

To create your account, use a web browser of your choice to navigate to [Docker Hub](https://hub.docker.com/).

From there, you can log in via an existing account or create a new login based on your email address. When you create your account, Docker Hub sends a verification email to the address that you provided during sign-up. You should immediately log in to your email account and click the verification link inside the email to finish the validation process.

At this point, you have created a public registry to which you can upload new images. The [Account Settings](https://hub.docker.com/settings/default-privacy) option under your profile picture has a `Default Privacy` section that allows you to change your registry default visibility to `private` if that is what you need.

**WARNING**

For much better security, you should create and log in to Docker Hub with a [limited-privilege personal access token](https://docs.docker.com/go/access-tokens).

#### Logging in to a registry

Now let’s log in to the Docker Hub registry using our account:

```
$ docker login
Login with your Docker ID to push and pull images from Docker Hub. If you
don't have a Docker ID, head over to https://hub.docker.com to create one.
Username: <hub_username>
Password: <hub_password/token>
Login Succeeded
```

**NOTE**

The command `docker login` is functionally the same command as `docker login docker.io`.

When you get `Login Succeeded` back from the server, you know you’re ready to pull images from the registry. But what happened behind the scenes? It turns out that Docker has written a dotfile for you in your home directory to cache this information. The permissions are set to 0600 as a security precaution against other users reading your credentials. You can inspect the file with something like this:

```
$ ls -la ${HOME}/.docker/config.json
-rw-------@ 1 …  158 Dec 24 10:37 /Users/someuser/.docker/config.json

$ cat ${HOME}/.docker/config.json
```

On Linux you will see something like this:

```
{
    "auths": {
    "https://index.docker.io/v1/": {
      "auth":"cmVsaEXamPL3hElRmFCOUE=",
      "email":"someuser@example.com"
    }
  }
}
```

**NOTE**

Docker is constantly evolving and has added support for many OS native secret management systems like the macOS Keychain or Windows Credential Manager. So, your _config.json_ file might look significantly different than the example. There is also a set of [credentials managers](https://github.com/docker/docker-credential-helpers) for different platforms that can make your life easier here.

**WARNING**

The `auth` value in the Docker client config file is only base64 encoded. It is _not_ encrypted. This is typically only a significant issue on multiuser Linux systems, because there isn’t a default system-wide credential manager that just works, and other privileged users on the system can likely read your Docker client config file and access those secrets. It is possible to configure `gpg` pr `pass` to encrypt these files on Linux.

Here you can see that the _${HOME}/.docker/config.json_ file contains `docker.io` credentials for the user `someuser@example.com` in JSON. This configuration file supports storing credentials for multiple registries. In this case, you just have one entry, for Docker Hub, but you could have more if you needed it. From now on, when the registry needs authentication, Docker will look in _${HOME}/.docker/config.json_ to see if you have credentials stored for this hostname. If so, it will supply them. You will notice that one value is completely lacking here: a timestamp. These credentials are cached forever or until you tell Docker to remove them, whichever comes first.

As with logging in, you can also log out of a registry if you no longer want to cache the credentials:

```
$ docker logout
Removing login credentials for https://index.docker.io/v1/
$ cat ${HOME}/.docker/config.json
```

```
{
  "auths": {
  }
}
```

Here you have removed the cached credentials and they are no longer stored by Docker. Some versions of Docker may even remove this file if it is empty. If you were trying to log in to something other than the Docker Hub registry, you could supply the hostname on the command line:

```
$ docker login someregistry.example.com
```

This would then add another auth entry into your _${HOME}/.docker/config.json_ file.

#### Pushing images into a repository

The first step required to push your image is to ensure that you are logged in to the Docker repository you intend to use. For this example, we will focus on Docker Hub, so ensure that you are logged in to Docker Hub with your preferred credentials:

```
$ docker login
Login with your Docker ID to push and pull images from Docker Hub. If you
don't have a Docker ID, head over to https://hub.docker.com to create one.
Username: <hub_username>
Password: <hub_password/token>
Login Succeeded

Logging in with your password grants your terminal complete access to
your account.
```

Once you are logged in, you can upload an image. Earlier, you used the command `docker image build -t example/docker-node-hello:latest .` to build the `docker-node-hello` image.

In reality, the Docker client, and for compatibility reasons, many other container tools, actually interpret `example/docker-node-hello:latest` as `docker.io/example/docker-node-hello:latest`. Here, `docker.io` signifies the image registry hostname, and `example/docker-node-hello` is the repository inside the registry that contains the images in question.

When you are building an image locally, the registry and repository name can be anything that you want. However, when you are going to upload your image to a real registry, you need that to match the login.

You can easily edit the tags on the image that you already created by running the following command and replacing `${<myuser>}` with your Docker Hub username:

```
$ docker image tag example/docker-node-hello:latest \
    docker.io/${<myuser>}/docker-node-hello:latest
```

If you need to rebuild the image with the new naming convention or simply want to give it a try, you can accomplish this by running the following command in the _docker-node-hello_ working directory that was generated when you performed the Git checkout earlier in the chapter.

**NOTE**

For the following examples, you will need to replace `${<myuser>}` in all the examples with the user that you created in Docker Hub. If you are using a different registry, you will also need to replace `docker.io` with the hostname of the registry you are using.

```
$ docker image build -t docker.io/${<myuser>}/docker-node-hello:latest .
…
```

On the first build, this will take a little time. If you rebuild the image, you may find that it is very fast. This is because most, if not all, of the layers already exist on your Docker server from the previous build. We can quickly verify that our image is indeed on the server by running `docker image ls ${<myuser>}/docker-node-hello`:

```
$ docker image ls ${<myuser>}/docker-node-hello
REPOSITORY                 TAG      IMAGE ID       CREATED             SIZE
myuser/docker-node-hello   latest   f683df27f02d   About an hour ago   649MB
```

**TIP**

It is possible to format the output of `docker image ls` to make it more concise by using the `--format` argument, like this: `docker image ls --format="table {{.ID}}\t{{.Repository}}"`.

At this point you can upload the image to the Docker repository by using the `docker image push` command:

```
$ docker image push ${<myuser>}/docker-node-hello:latest
Using default tag: latest
The push refers to repository [docker.io/myuser/docker-node-hello]
5f3ee7afc69c: Pushed
…
5bb0785f2eee: Mounted from library/node
latest: digest: sha256:f5ceb032aec36fcacab71e468eaf0ba8a832cfc8244fbc784d0…
```

If this image was uploaded to a public repository, anyone in the world can now easily download it by running the `docker image pull` command.

**TIP**

If you uploaded the image to a private repository, then users must log in with credentials that have access to those repositories using the `docker login` command before they will be able to pull the image down to their local system.

```
$ docker image pull ${<myuser>}/docker-node-hello:latest
Using default tag: latest
latest: Pulling from myuser/docker-node-hello
Digest: sha256:f5ceb032aec36fcacab71e468eaf0ba8a832cfc8244fbc784d040872be041cd5
Status: Image is up to date for myuser/docker-node-hello:latest
docker.io/myuser/docker-node-hello:latest
```

#### Exploring images in Docker Hub

In addition to simply using the [Docker Hub website](https://hub.docker.com/) to explore what images are available, you can also use the `docker search` command to find images that might be useful.

Running `docker search node` will return a list of images that contain the word `node` in either the image name or the description:

```
$ docker search node
NAME                     DESCRIPTION                 STARS OFFICIAL AUTOMATED
node                     Node.js is a JavaScript-ba… 12267 [OK]
mongo-express            Web-based MongoDB admin in… 1274  [OK]
nodered/node-red         Low-code programming for e… 544
nodered/node-red-docker  Deprecated - older Node-RE… 356            [OK]
circleci/node            Node.js is a JavaScript-ba… 130
kindest/node             sigs.k8s.io/kind node imag… 78
bitnami/node             Bitnami Node.js Docker Ima… 69             [OK]
cimg/node                The CircleCI Node.js Docke… 14
opendronemap/nodeodm     Automated build for NodeOD… 10             [OK]
bitnami/node-exporter    Bitnami Node Exporter Dock… 9              [OK]
appdynamics/nodejs-agent Agent for monitoring Node.… 5
wallarm/node             Wallarm: end-to-end API se… 5              [OK]
…
```

The `OFFICIAL` header tells you that the image is one of the [official curated images](https://docs.docker.com/docker-hub/official\_images) on Docker Hub. This typically means that the image is maintained by the company or official development community that oversees that application. `AUTOMATED` denotes that the image is automatically built and uploaded by a CI/CD process triggered via commits to the underlying source code repository. Official images are always automated.

### Running a Private Registry

In keeping with the spirit of the open source community, Docker encourages the community to share Docker images via Docker Hub by default. There are times, however, when this is not a viable option due to commercial, legal, image retention, or reliability concerns.

In these cases, it makes sense to host an internal private registry. Setting up a basic registry is not difficult, but for production use, you should take the time to familiarize yourself with all the available configuration options for [the open source Docker Registry (Distribution)](https://docs.docker.com/registry).

For this example, we are going to create a very simple secure registry using SSL and HTTP basic auth.

First, let’s create a few directories and files on our Docker server. If you are using a VM or cloud instance to run your Docker server, then you will need to SSH to that server for the next few commands. If you are using Docker Desktop or Community Edition, then you should be able to run these on your local system.

**TIP**

Windows users may need to download additional tools, like `htppaswd`, or alter the non-Docker commands to accomplish the same tasks on your local system.

First let’s clone a Git repository that contains the basic files required to set up a simple, authenticated Docker registry:

```
$ git clone https://github.com/spkane/basic-registry \
  --config core.autocrlf=input
Cloning into 'basic-registry'…
remote: Counting objects: 10, done.
remote: Compressing objects: 100% (8/8), done.
remote: Total 10 (delta 0), reused 10 (delta 0), pack-reused 0
Unpacking objects: 100% (10/10), done.
```

Once you have the files locally, you can change directories and examine the files that you just downloaded:

```
$ cd basic-registry
$ ls
Dockerfile          config.yaml.sample  registry.crt.sample
README.md           htpasswd.sample     registry.key.sample
```

The _Dockerfile_ simply takes the upstream registry image from Docker Hub and copies some local configuration and support files into a new image.

For testing, you can use some of the included sample files, but _do not_ use these in production.

If your Docker server is available via `localhost` (127.0.0.1), then you can use these files unmodified by simply copying each of them like this:

```
$ cp config.yaml.sample config.yaml
$ cp registry.key.sample registry.key
$ cp registry.crt.sample registry.crt
$ cp htpasswd.sample htpasswd
```

If, however, your Docker server is on a remote IP address, then you will need to do a little additional work.

First, copy _config.yaml.sample_ to _config.yaml_:

```
$ cp config.yaml.sample config.yaml
```

Then edit _config.yaml_ and replace `127.0.0.1` with the IP address of your Docker server so that:

```
http:
  host: https://127.0.0.1:5000
```

becomes something like this:

```
http:
  host: https://172.17.42.10:5000
```

**NOTE**

It is easy to create a registry using a fully qualified domain name (FQDN), like `my-registry.example.com`, but for this example, working with IP addresses is easier because no DNS is required.

Next, you need to create an SSL keypair for your registry’s IP address.

One way to do this is with the following OpenSSL command. Note that you will need to set the IP address in this portion of the command, `/CN=172.17.42.10`, to match your Docker server’s IP address:

```
$ openssl req -x509 -nodes -sha256 -newkey rsa:4096 \
  -keyout registry.key -out registry.crt \
  -days 14 -subj '{/CN=172.17.42.10}'
```

Finally, you can either use the example `htpasswd` file by copying it:

```
$ cp htpasswd.sample htpasswd
```

or you can create your own username and password pair for authentication by using a command like the following, replacing `${<username>}` and `${<password>}` with your preferred values:

```
$ docker container run --rm --entrypoint htpasswd g \
  -Bbn ${<username>} ${<password>} > htpasswd
```

If you look at the directory listing again, it should now look like this:

```
$ ls
Dockerfile          config.yaml.sample  registry.crt        registry.key.sample
README.md           htpasswd            registry.crt.sample
config.yaml         htpasswd.sample     registry.key
```

If any of these files are missing, review the previous steps to ensure that you did not miss one, before moving on.

If everything looks correct, then you should be ready to build and run the registry:

```
$ docker image build -t my-registry .
$ docker container run --rm -d -p 5000:5000 --name registry my-registry
$ docker container logs registry
```

**TIP**

If you see errors like “docker: Error response from daemon: Conflict. The container name “/registry” is already in use,” then you need to either change the preceding container name or remove the existing container with that name. You can remove the container by running `docker container rm registry`.

#### Testing the private registry

Now that the registry is running, you can test it. The very first thing that you need to do is authenticate against it. You will need to make sure that the IP address in the `docker login` matches the IP address of your Docker server that is running the registry.

**NOTE**

`myuser` is the default username, and `myuser-pw!` is the default password. If you generated your own `htpasswd`, then these will be whatever you choose.

```
$ docker login 127.0.0.1:5000
Username: <registry_username>
Password: <registry_password>
Login Succeeded
```

**WARNING**

This registry container has an embedded SSL key and is not using any external storage, which means that it contains a secret, and when you delete the running container, all your images will also be deleted. This is by design.

In production, you will want to have your containers pull secrets from a secrets management system and use some type of redundant external storage, like an object store. If you want to keep your development registry images between containers, you could add something like `--mount type=bind,source=/tmp/registry-data,target=/var/lib/registry` to your `docker container run` command to store the registry data on the Docker server.

Now, let’s see if you can push the image you just built into your local private registry.

**TIP**

In all of these commands, ensure that you use the correct IP address for your registry.

```
$ docker image tag my-registry 127.0.0.1:5000/my-registry
$ docker image push 127.0.0.1:5000/my-registry
Using default tag: latest
The push refers to repository [127.0.0.1:5000/my-registry]
f09a0346302c: Pushed
…
4fc242d58285: Pushed
latest: digest: sha256:c374b0a721a12c41d5b298930d11e658fbd37f22dc2a0fac7d6a2…
```

You can then try to pull the same image from your repository:

```
$ docker image pull 127.0.0.1:5000/my-registry
Using default tag: latest
latest: Pulling from my-registry
Digest: sha256:c374b0a721a12c41d5b298930d11e658fbd37f22dc2a0fac7d6a2ecdc0ba5490
Status: Image is up to date for 127.0.0.1:5000/my-registry:latest
127.0.0.1:5000/my-registry:latest
```

**TIP**

It’s worth keeping in mind that both Docker Hub and Docker Distribution expose an API endpoint that you can query for useful information. You can find out more information about the API via the [official documentation](https://github.com/distribution/distribution/blob/main/docs/spec/api.md).

If you have not encountered any errors, then you have a working registry for development and can build on this foundation to create a production registry. At this point, you may want to stop the registry for the time being. You can easily accomplish this by running the following:

```
$ docker container stop registry
```

**TIP**

As you become comfortable with Docker Distribution, you may also want to consider exploring the Cloud Native Computing Foundation (CNCF) open source project, called [Harbor](https://goharbor.io/), which extends the Docker Distribution with a lot of security and reliability-focused features.

## Optimizing Images

After you have spent a little bit of time working with Docker, you will quickly notice that keeping your image sizes small and your build times fast can be very beneficial in decreasing the time required to build and deploy new versions of your software into production. In this section, we will talk a bit about some of the considerations you should always keep in mind when designing your images and a few techniques that can help you achieve these goals.

### Keeping Images Small

In most modern businesses, downloading a single 1 GB file from a remote location on the internet is not something that people often worry about. It is so easy to find software on the internet that people will often rely on simply re-downloading it if they need it again, instead of keeping a local copy for the future. This may often be acceptable when you truly need a single copy of this software on a single server, but it can quickly become a scaling problem when you need the same software on 100+ nodes and you deploy new releases multiple times a day. Downloading these large files can quickly cause network congestion and slower deployment cycles that have a real impact on the production environment.

For convenience, a large number of Linux containers inherit from a base image that contains a minimal Linux distribution. Although this is an easy starting place, it isn’t required. Containers only need to contain the files that are required to run the application on the host kernel, and nothing else. The best way to explain this is to explore a very minimal container.

Go is a compiled programming language that can easily generate statically compiled binary files. For this example, we are going to use a very small web application written in Go that can be found on [GitHub](https://github.com/spkane/scratch-helloworld).

Let’s go ahead and try out the application so that you can see what it does. Run the following command, and then open up a web browser and point it to your Docker host on port 8080 (e.g., _http://127.0.0.1:8080_ for Docker Desktop and Community Edition):

```
$ docker container run --rm -d -p 8080:8080 spkane/scratch-helloworld
```

If all goes well, you should see the following message in your web browser: “Hello World from Go in minimal Linux container.” Now let’s take a look at what files this container comprises. It would be fair to assume that at a minimum it will include a working Linux environment and all the files required to compile Go programs, but you will soon see that this is not the case.

While the container is still running, execute the following command to determine what the container ID is. The following command returns the information for the last container that you created:

```
$ docker container ls -l
CONTAINER ID IMAGE                     COMMAND       CREATED           …
ddc3f61f311b spkane/scratch-helloworld "/helloworld" 4 minutes ago     …
```

You can then use the container ID that you obtained from running the previous command to export the files in the container into a tarball, which can be easily examined:

```
$ docker container export ddc3f61f311b -o web-app.tar
```

Using the `tar` command, you can now examine the contents of your container at the time of the export:

```
$ tar -tvf web-app.tar
-rwxr-xr-x  0 0      0           0 Jan  7 15:54 .dockerenv
drwxr-xr-x  0 0      0           0 Jan  7 15:54 dev/
-rwxr-xr-x  0 0      0           0 Jan  7 15:54 dev/console
drwxr-xr-x  0 0      0           0 Jan  7 15:54 dev/pts/
drwxr-xr-x  0 0      0           0 Jan  7 15:54 dev/shm/
drwxr-xr-x  0 0      0           0 Jan  7 15:54 etc/
-rwxr-xr-x  0 0      0           0 Jan  7 15:54 etc/hostname
-rwxr-xr-x  0 0      0           0 Jan  7 15:54 etc/hosts
lrwxrwxrwx  0 0      0           0 Jan  7 15:54 etc/mtab -> /proc/mounts
-rwxr-xr-x  0 0      0           0 Jan  7 15:54 etc/resolv.conf
-rwxr-xr-x  0 0      0     3604416 Jul  2  2014 helloworld
drwxr-xr-x  0 0      0           0 Jan  7 15:54 proc/
drwxr-xr-x  0 0      0           0 Jan  7 15:54 sys/
```

The first thing you might notice here is that there are almost no files in this container, and almost all of them are zero bytes in length. All of the files that have a zero length are required to exist in every Linux container and are automatically [bind-mounted](https://unix.stackexchange.com/questions/198590/what-is-a-bind-mount) from the host into the container when it is first created. All of these files, except for _.dockerenv_, are critical files that the kernel needs to do its job properly. The only file in this container that has any actual size and is related to our application is the statically compiled `helloworld` binary.

The takeaway from this exercise is that your containers are only required to contain exactly what they need to run on the underlying kernel. Everything else is unnecessary. Because it is often useful for troubleshooting to have access to a working shell in your container, people will often compromise and build their images from a very lightweight Linux distribution like Alpine Linux.

**TIP**

If you find yourself exploring image files a lot, you might want to take a look at the tool [dive](https://github.com/wagoodman/dive), which provides a nice CLI interface for understanding what an image contains.

To dive into this a little deeper, let’s look at that same container again so that we can dig into the underlying filesystem and compare it with the popular `alpine` base image.

Although we could easily poke around in the `alpine` image by simply running `docker container run -ti alpine:latest /bin/sh`, we cannot do this with the `spkane/scratch-helloworld` image, because it does not contain a shell or SSH. This means that we can’t use `ssh`, `nsenter`, or `docker container exec` to examine it, though there is an advanced trick discussed in [“Debugging Shell-less Containers”](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch11.html#shellless). Earlier, we took advantage of the `docker container export` command to create a _.tar_ file that contained a copy of all the files in the container, but this time around we are going to examine the container’s filesystem by connecting directly to the Docker server and then looking into the container’s filesystem itself. To do this, we need to find out where the image files reside on the server’s disk.

To determine where on the server our files are actually being stored, run `docker image inspect` on the `alpine:latest` image:

```
$ docker image inspect alpine:latest
```

```
[
    {
        "Id": "sha256:3fd…353",
        "RepoTags": [
            "alpine:latest"
        ],
        "RepoDigests": [
            "alpine@sha256:7b8…f8b"
        ],
…
        "GraphDriver": {
            "Data": {
                "MergedDir":
                "/var/lib/docker/overlay2/ea8…13a/merged",
                "UpperDir":
                "/var/lib/docker/overlay2/ea8…13a/diff",
                "WorkDir":
                "/var/lib/docker/overlay2/ea8…13a/work"
            },
            "Name": "overlay2"
…
        }
    }
…
]
```

And then on the `spkane/scratch-helloworld:latest` image:

```
$ docker image inspect spkane/scratch-helloworld:latest
```

```
[
    {
        "Id": "sha256:4fa…06d",
        "RepoTags": [
            "spkane/scratch-helloworld:latest"
        ],
        "RepoDigests": [
            "spkane/scratch-helloworld@sha256:46d…a1d"
        ],
…
        "GraphDriver": {
            "Data": {
                "LowerDir":
                "/var/lib/docker/overlay2/37a…84d/diff:
                /var/lib/docker/overlay2/28d…ef4/diff",
                "MergedDir":
                "/var/lib/docker/overlay2/fc9…c91/merged",
                "UpperDir":
                "/var/lib/docker/overlay2/fc9…c91/diff",
                "WorkDir":
                "/var/lib/docker/overlay2/fc9…c91/work"
            },
            "Name": "overlay2"
…
        }
    }
…
]
```

**NOTE**

In this particular example, we are going to use Docker Desktop running on macOS, but this general approach will work on most Docker servers. However, you can access your Docker server via whatever method is easiest.

Since we are using Docker Desktop, we need to use our `nsenter` trick to enter the SSH-less VM and explore the filesystem:

```
$ docker container run --rm -it --privileged --pid=host debian \
  nsenter -t 1 -m -u -n -i sh

/ #
```

Inside the VM, we should now be able to explore the various directories listed in the `GraphDriver` section of the `docker image inspect` commands.

In this example, if we look at the first entry for the `alpine` image, we will see that it is labeled `MergedDir` and lists the folder _/var/lib/docker/overlay2/ea86408b2b15d33ee27d78ff44f82104705286221f055ba1331b58673f4b313a/merged_. If we list that directory, we will get an error, but from listing the parent directory, we quickly discover that we actually want to look at the _diff_ directory:

```
/ # ls -lFa /var/lib/docker/overlay2/ea…3a/merged

ls: /var/lib/docker/overlay2/ea..3a/merged: No such file or directory

/ # ls -lF /var/lib/docker/overlay2/ea…3a/

total 8
drwxr-xr-x   18 root     root          4096 Mar 15 19:27 diff/
-rw-r--r--    1 root     root            26 Mar 15 19:27 link

/ # ls -lF /var/lib/docker/overlay2/ea…3a/diff

total 64
drwxr-xr-x    2 root     root          4096 Jan  9 19:37 bin/
drwxr-xr-x    2 root     root          4096 Jan  9 19:37 dev/
drwxr-xr-x   15 root     root          4096 Jan  9 19:37 etc/
drwxr-xr-x    2 root     root          4096 Jan  9 19:37 home/
drwxr-xr-x    5 root     root          4096 Jan  9 19:37 lib/
drwxr-xr-x    5 root     root          4096 Jan  9 19:37 media/
drwxr-xr-x    2 root     root          4096 Jan  9 19:37 mnt/
dr-xr-xr-x    2 root     root          4096 Jan  9 19:37 proc/
drwx------    2 root     root          4096 Jan  9 19:37 root/
drwxr-xr-x    2 root     root          4096 Jan  9 19:37 run/
drwxr-xr-x    2 root     root          4096 Jan  9 19:37 sbin/
drwxr-xr-x    2 root     root          4096 Jan  9 19:37 srv/
drwxr-xr-x    2 root     root          4096 Jan  9 19:37 sys/
drwxrwxrwt    2 root     root          4096 Jan  9 19:37 tmp/
drwxr-xr-x    7 root     root          4096 Jan  9 19:37 usr/
drwxr-xr-x   11 root     root          4096 Jan  9 19:37 var/

/ # du -sh  /var/lib/docker/overlay2/ea…3a/diff
4.5M    /var/lib/docker/overlay2/ea…3a/diff
```

Now, `alpine` happens to be a very small base image, weighing in at only 4.5 MB, and it is ideal for building containers on top of it. However, we can see that there is still a lot of stuff in this container before we have started to build anything from it.

Now, let’s take a look at the files in the `spkane/scratch-helloworld` image. In this case, we want to look at the first directory from the `LowerDir` entry of the `docker image inspect` output, which you’ll notice also ends in a directory called _diff_:

```
/ # ls -lFh /var/lib/docker/overlay2/37…4d/diff

total 3520
-rwxr-xr-x    1 root     root        3.4M Jul  2  2014 helloworld*

/ # exit
```

You’ll notice that there is only a single file in this directory, and it is 3.4 MB. This `helloworld` binary is the only file shipped in this container and is smaller than the starting size of the `alpine` image before any application files have been added to it.

**NOTE**

It is possible to run the `helloworld` application from that directory on your Docker server because it does not require any other files. You really don’t want to do this on anything but a development box, but it can help drive the point home about how useful these types of statically compiled applications can be.

#### Multistage builds

There is a way you can constrain containers to an even smaller size in many cases: multistage builds. This is how we recommend that you build most production containers. You don’t have to worry as much about bringing in extra resources to build your application, and you can still run a lean production container. Multistage containers also encourage doing builds inside Docker, which is a great pattern for repeatability in your build system.

As the original author of [the `scratch-helloworld` application has written](https://medium.com/@adriaandejonge/simplify-the-smallest-possible-docker-image-62c0e0d342ef), the release of multistage build support in Docker itself has made the process of creating small containers much easier than it used to be. In the past, to do the same thing that multistage delivers for nearly free, you were required to build one image that compiled your code, extract the resulting binary, and then build a second image without all the build dependencies that you would then inject that binary into. This was often difficult to set up and did not always work out of the box with standard deployment pipelines.

Today, you can now achieve similar results using a _Dockerfile_ as simple as this one:

```
# Build container
FROM docker.io/golang:alpine as builder
RUN apk update && \
    apk add git && \
    CGO_ENABLED=0 go install -a -ldflags '-s' \
    github.com/spkane/scratch-helloworld@latest

# Production container
FROM scratch
COPY --from=builder /go/bin/scratch-helloworld /helloworld
EXPOSE 8080
CMD ["/helloworld"]
```

The first thing you’ll notice about this _Dockerfile_ is that it looks a lot like two _Dockerfile_s that have been combined into one. Indeed this is the case, but there is more to it. The `FROM` command has been extended so that you can name the image during the build phase. In this example, the first line, which reads `FROM docker.io/golang as builder`, means that you want to base your build on the `golang` image and will be referring to this build image/stage as `builder`.

On the fourth line, you’ll see another `FROM` line, which was not allowed before the introduction of multistage builds. This `FROM` line uses a special image name, called `scratch`, that tells Docker to start from an empty image, which includes no additional files. The next line, which reads `COPY --from=builder /go/bin/scratch-helloworld /helloworld`, allows you to copy the binary that you built in the _builder_ image directly into the current image. This will ensure that you end up with the smallest container possible.

The `EXPOSE 8080` line is documentation that is intended to inform users which port(s) and protocols (TCP is the default protocol) the service listens on.

Let’s try to build this and see what happens. First, create a directory where you can work, and then, using your favorite text editor, paste the content from the preceding example into a file called _Dockerfile_:

```
$ mkdir /tmp/multi-build
$ cd /tmp/multi-build
$ vi Dockerfile
```

**TIP**

You can download a copy of this _Dockerfile_ from [GitHub](https://oreil.ly/C1TSz).[5](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#idm46803152409712)

We can now start the multistage build:

```
$ docker image build .
[+] Building 9.7s (7/7) FINISHED
 => [internal] load build definition from Dockerfile
 => => transferring dockerfile: 37B
 => [internal] load .dockerignore
 => => transferring context: 2B
 => [internal] load metadata for docker.io/library/golang:alpine
 => CACHED [builder 1/2] FROM docker.io/library/golang:alpine@sha256:7cc6257…
 => [builder 2/2] RUN apk update && apk add git && CGO_ENABLED=0 go install …
 => [stage-1 1/1] COPY --from=builder /go/bin/scratch-helloworld /helloworld
 => exporting to image
 => => exporting layers
 => => writing image sha256:bb853f23418161927498b9631f54692cf11d84d6bde3af2d…
```

You’ll notice that the output looks like most other builds and still ends by reporting the successful creation of our final, very minimal image.

**WARNING**

If you are compiling binaries on your local system that use shared libraries, you need to be careful to ensure that the correct versions of those shared libraries are also available to the process inside the container.

You are not limited to two stages, and in fact, none of the stages need to even be related to one another. They will be run in order. You could, for example, have a stage based on the public Go image that builds your underlying Go application to serve an API, and another stage based on the Angular container to build your frontend web UI. The final stage could then combine outputs from both.

**TIP**

As you start to build more complex images, you may find that being limited to a single build context is challenging. The `docker-buildx` plug-in, which we discuss near the end of this chapter, is capable of supporting [multiple build contexts](https://www.docker.com/blog/dockerfiles-now-support-multiple-build-contexts), which can be used to support some very advanced workflows.

### Layers Are Additive

Something that is not apparent until you dig much deeper into how images are built is that the filesystem layers that make up your images are strictly additive by design. Although you can shadow/mask files in previous layers, you cannot delete those files. In practice, this means that you cannot make your image smaller by simply deleting files that were generated in earlier steps.

**NOTE**

If you enable experimental features on your Docker server, it is possible to squash a bunch of layers into a single layer using `docker image build --squash`. This will cause all of the files that were deleted in the intermediate layers to actually disappear from the final image and can therefore recover a lot of wasted space, but it also means that the whole layer must be downloaded by every system that requires it, even when only a single line of source code was updated, so there are real trade-offs to using this approach.

The easiest way to explain the additive nature of image layers is by using some practical examples. In a new directory, [download](https://github.com/bluewhalebook/docker-up-and-running-3rd-edition/blob/main/chapter\_04/additive) or create the following file, which will generate an image that launches the Apache web server running on Fedora Linux:

```
FROM docker.io/fedora
RUN dnf install -y httpd
CMD ["/usr/sbin/httpd", "-DFOREGROUND"]
```

and then build it like this:

```
$ docker image build .
[+] Building 63.5s (6/6) FINISHED
 => [internal] load build definition from Dockerfile
 => => transferring dockerfile: 130B
 => [internal] load .dockerignore
 => => transferring context: 2B
 => [internal] load metadata for docker.io/library/fedora:latest
 => [1/2] FROM docker.io/library/fedora
 => [2/2] RUN dnf install -y httpd
 => exporting to image
 => => exporting layers
 => => writing image sha256:543d61c956778b8ea3b32f1e09a9354a864467772e6…
```

Let’s go ahead and tag the resulting image so that you can easily refer to it in subsequent commands:

```
$ docker image tag sha256:543d61c956778b8ea3b32f1e09a9354a864467772e6… size1
```

Now let’s take a look at our image with the `docker image history` command. This command will give us some insight into the filesystem layers and build steps that our image uses:

```
$ docker image history size1
IMAGE        CREATED            CREATED BY                            SIZE  …
543d61c95677 About a minute ago CMD ["/usr/sbin/httpd" "-DFOREGROU…"] 0B
<missing>    About a minute ago RUN /bin/sh -c dnf install -y httpd … 273MB
<missing>    6 weeks ago        /bin/sh -c #(nop)  CMD ["/bin/bash"]… 0B
<missing>    6 weeks ago        /bin/sh -c #(nop) ADD file:58865512c… 163MB
<missing>    3 months ago       /bin/sh -c #(nop)  ENV DISTTAG=f36co… 0B
<missing>    15 months ago      /bin/sh -c #(nop)  LABEL maintainer=… 0B
```

You’ll notice that three of the layers added no size to our final image, but two of them increase the size a great deal. The layer that is 163 MB makes sense, as this is the base Fedora image that includes a minimal Linux distribution; however, the 273 MB layer is surprising. The Apache web server shouldn’t be nearly that large, so what’s going on here, exactly?

If you have experience with package managers like `apk`, `apt`, `dnf`, or `yum`, then you may know that most of these tools rely heavily on a large cache that includes details about all the packages that are available for installation on the platform in question. This cache uses up a huge amount of space and is completely useless once you have installed the packages you need. The most obvious next step is to simply delete the cache. On Fedora systems, you could do this by editing your _Dockerfile_ so that it looks like this:

```
FROM docker.io/fedora
RUN dnf install -y httpd
RUN dnf clean all
CMD ["/usr/sbin/httpd", "-DFOREGROUND"]
```

and then building, tagging, and examining the resulting image:

```
$ docker image build .
[+] Building 0.5s (7/7) FINISHED
…
 => => writing image sha256:b6bf99c6e7a69a1229ef63fc086836ada20265a793cb8f2d…

$ docker image tag sha256:b6bf99c6e7a69a1229ef63fc086836ada20265a793cb8f2d17…
IMAGE        CREATED            CREATED BY                            SIZE  …
b6bf99c6e7a6 About a minute ago CMD ["/usr/sbin/httpd" "-DFOREGROU…"] 0B
<missing>    About a minute ago RUN /bin/sh -c dnf clean all # build… 71.8kB
<missing>    10 minutes ago     RUN /bin/sh -c dnf install -y httpd … 273MB
<missing>    6 weeks ago        /bin/sh -c #(nop)  CMD ["/bin/bash"]… 0B
<missing>    6 weeks ago        /bin/sh -c #(nop) ADD file:58865512c… 163MB
<missing>    3 months ago       /bin/sh -c #(nop)  ENV DISTTAG=f36co… 0B
<missing>    15 months ago      /bin/sh -c #(nop)  LABEL maintainer=… 0B
```

If you look carefully at the output from the `docker image history` command, you’ll notice that you have created a new layer that adds `71.8kB` to the image, but you have not decreased the size of the problematic layer at all. What is happening, exactly?

The important thing to understand is that image layers are strictly _additive_ in nature. Once a layer is created, nothing can be removed from it. This means that you cannot make earlier layers in an image smaller by deleting files in subsequent layers. When you delete or edit files in subsequent layers, you’re simply masking the older version with the modified or removed version in the new layer. This means that the only way you can make a layer smaller is by removing files before you save the layer.

The most common way to deal with this is by stringing commands together on a single _Dockerfile_ line. You can do this very easily by taking advantage of the `&&` operator. This operator acts as a Boolean `AND` statement and basically translates into English as “and if the previous command ran successfully, run this command.” In addition to this, you can take advantage of the `\` operator, which is used to indicate that a command continues after the newline. This can help improve the readability of long commands.

With this knowledge in hand, you can rewrite the _Dockerfile_ like this:

```
FROM docker.io/fedora
RUN dnf install -y httpd && \
    dnf clean all
CMD ["/usr/sbin/httpd", "-DFOREGROUND"]
```

Now you can rebuild the image and see how this change has impacted the size of the layer that includes the `http` daemon:

```
$ docker image build .
[+] Building 0.5s (7/7) FINISHED
…
 => => writing image sha256:14fe7924bb0b641ddf11e08d3dd56f40aff4271cad7a421fe…

$ docker image tag sha256:14fe7924bb0b641ddf11e08d3dd56f40aff4271cad7a421fe9b…
IMAGE        CREATED            CREATED BY                            SIZE   …
14fe7924bb0b About a minute ago CMD ["/usr/sbin/httpd" "-DFOREGROUN"]… 0B
<missing>    About a minute ago RUN /bin/sh -c dnf install -y httpd &… 44.8MB
<missing>    6 weeks ago        /bin/sh -c #(nop)  CMD ["/bin/bash"] … 0B
<missing>    6 weeks ago        /bin/sh -c #(nop) ADD file:58865512ca… 163MB
<missing>    3 months ago       /bin/sh -c #(nop)  ENV DISTTAG=f36con… 0B
<missing>    15 months ago      /bin/sh -c #(nop)  LABEL maintainer=C… 0B
```

In the first two examples, the layer in question was 273 MB in size, but now that you have removed many unnecessary files that were added to that layer, you can shrink the layer down to 44.8 MB. This is a very large saving of space, especially when you consider how many servers might be pulling the image down during any given deployment.

### Utilizing the Layer Cache

The final building technique that we will cover here is related to keeping build times as fast as possible. One of the important goals of the DevOps movement is to keep feedback loops as tight as possible. This means that it is important to try to ensure that problems are discovered and reported as quickly as possible so that they can be fixed when people are still completely focused on the code in question and haven’t moved on to other unrelated tasks.

During any standard build process, Docker uses a layer cache to try to avoid rebuilding any image layers that it has already built and that do not contain any noticeable changes. Because of this cache, the order in which you do things inside your _Dockerfile_ can have a dramatic impact on how long your builds take on average.

For starters, let’s take the _Dockerfile_ from the previous example and customize it just a bit so that it looks like this.

**TIP**

Along with the other examples, you can also find these files on [GitHub](https://github.com/bluewhalebook/docker-up-and-running-3rd-edition/blob/main/chapter\_04/cache).

```
FROM docker.io/fedora
RUN dnf install -y httpd && \
    dnf clean all
RUN mkdir -p /var/www && \
    mkdir -p /var/www/html
ADD index.html /var/www/html
CMD ["/usr/sbin/httpd", "-DFOREGROUND"]
```

Now, in the same directory, let’s also create a new file called _index.html_ that looks like this:

```
<html>
  <head>
    <title>My custom Web Site</title>
  </head>
  <body>
    <p>Welcome to my custom Web Site</p>
  </body>
</html>
```

For the first test, let’s time the build without using the Docker cache at all, by using the following command:

```
$ time docker image build --no-cache .
time docker image build --no-cache .
[+] Building 48.3s (9/9) FINISHED
 => [internal] load build definition from Dockerfile
 => => transferring dockerfile: 238B
 => [internal] load .dockerignore
 => => transferring context: 2B
 => [internal] load metadata for docker.io/library/fedora:latest
 => CACHED [1/4] FROM docker.io/library/fedora
 => [internal] load build context
 => => transferring context: 32B
 => [2/4] RUN dnf install -y httpd &&     dnf clean all
 => [3/4] RUN mkdir -p /var/www &&     mkdir -p /var/www/html
 => [4/4] ADD index.html /var/www/html
 => exporting to image
 => => exporting layers
 => => writing image sha256:7f94d0d6492f2d2c0b8576f0f492e03334e6a535cac85576c…

real  1m21.645s
user  0m0.428s
sys   0m0.323s
```

**TIP**

Windows users should be able to run this command in a WSL2 session or use the PowerShell [`Measure-Command`](https://oreil.ly/MQQY\_)[6](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#idm46803151763616) function to replace the Unix `time` command used in these examples.

The output from the `time` command tells us that the build without the cache took about a minute and 21 seconds and only pulled the base image from the layer cache. If you rebuild the image immediately afterward and allow Docker to use the cache, you will see that the build is very fast:

```
$ time docker image build .
[+] Building 0.1s (9/9) FINISHED
 => [internal] load build definition from Dockerfile
 => => transferring dockerfile: 37B
 => [internal] load .dockerignore
 => => transferring context: 2B
 => [internal] load metadata for docker.io/library/fedora:latest
 => [1/4] FROM docker.io/library/fedora
 => [internal] load build context
 => => transferring context: 32B
 => CACHED [2/4] RUN dnf install -y httpd &&     dnf clean all
 => CACHED [3/4] RUN mkdir -p /var/www &&     mkdir -p /var/www/html
 => CACHED [4/4] ADD index.html /var/www/html
 => exporting to image
 => => exporting layers
 => => writing image sha256:0d3aeeeeebd09606d99719e0c5197c1f3e59a843c4d7a21af…

real  0m0.416s
user  0m0.120s
sys   0m0.087s
```

Since none of the layers changed, and the cache could be fully leveraged for all four build steps, the build took only a fraction of a second to complete. Now, let’s make a small improvement to the _index.html_ file so that it looks like this:

```
<html>
  <head>
    <title>My custom Web Site</title>
  </head>
  <body>
    <div align="center">
      <p>Welcome to my custom Web Site!!!</p>
    </div>
  </body>
</html>
```

and then let’s time the rebuild again:

```
$ time docker image build .
[+] Building 0.1s (9/9) FINISHED
 => [internal] load build definition from Dockerfile
 => => transferring dockerfile: 37B
 => [internal] load .dockerignore
 => => transferring context: 2B
 => [internal] load metadata for docker.io/library/fedora:latest
 => [internal] load build context
 => => transferring context: 214B
 => [1/4] FROM docker.io/library/fedora
 => CACHED [2/4] RUN dnf install -y httpd &&     dnf clean all
 => CACHED [3/4] RUN mkdir -p /var/www &&     mkdir -p /var/www/html
 => [4/4] ADD index.html /var/www/html
 =>  ADD index.html /var/www/html
 => exporting to image
 => => exporting layers
 => => writing image sha256:daf792da1b6a0ae7cfb2673b29f98ef2123d666b8d14e0b74…

real  0m0.456s
user  0m0.120s
sys   0m0.068s
```

If you look at the output carefully, you will see that the cache was used for most of the build. It wasn’t until step `4/4`, when Docker needed to copy _index.html_, that the cache was invalidated and the layers had to be re-created. Because the cache could be used for most of the build, the build still did not exceed a second.

But what would happen if you changed the order of the commands in the _Dockerfile_ so that they looked like this:

```
FROM docker.io/fedora
RUN mkdir -p /var/www && \
    mkdir -p /var/www/html
ADD index.html /var/www/html
RUN dnf install -y httpd && \
    dnf clean all
CMD ["/usr/sbin/httpd", "-DFOREGROUND"]
```

Let’s quickly time another test build without the cache to get a baseline:

```
$ time docker image build --no-cache .
[+] Building 51.5s (9/9) FINISHED
…
 => => writing image sha256:1cc5f2c5e4a4d1cf384f6fb3a34fd4d00e7f5e7a7308d5f1f…

real  0m51.859s
user  0m0.237s
sys   0m0.159s
```

In this case, the build took 51 seconds to complete: since we used the `--no-cache` argument, we know that nothing was pulled from the layer cache, except for the base image. The difference in time from the very first test is entirely due to fluctuating network speeds and has nothing to do with the changes that you have made to the _Dockerfile_.

Now, let’s edit _index.html_ again, like so:

```
<html>
  <head>
    <title>My custom Web Site</title>
  </head>
  <body>
    <div align="center" style="font-size:180%">
      <p>Welcome to my custom Web Site</p>
    </div>
  </body>
</html>
```

And now, let’s time the image rebuild while using the cache:

```
$ time docker image build .
[+] Building 43.4s (9/9) FINISHED
 => [internal] load build definition from Dockerfile
 => => transferring dockerfile: 37B
 => [internal] load .dockerignore
 => => transferring context: 2B
 => [internal] load metadata for docker.io/library/fedora:latest
 => [1/4] FROM docker.io/library/fedora
 => [internal] load build context
 => => transferring context: 233B
 => CACHED [2/4] RUN mkdir -p /var/www &&     mkdir -p /var/www/html
 => [3/4] ADD index.html /var/www/html
 => [4/4] RUN dnf install -y httpd &&     dnf clean all
 => exporting to image
 => => exporting layers
 => => writing image sha256:9a05b2d01b5870649e0ad1d7ad68858e0667f402c8087f0b4…

real  0m43.695s
user  0m0.211s
sys   0m0.133s
```

The first time that you rebuilt the image, after editing the _index.html_ file, it took only .456 seconds, but this time it took 43.695 seconds, almost exactly as long as it took to build the whole image without using the cache at all.

This is because you have modified the _Dockerfile_ so that the _index.html_ file is copied into the image very early in the process. The problem with doing it this way is that the _index.html_ file changes frequently and will often invalidate the cache. The other issue is that it is unnecessarily placed before a very time-consuming step in our _Dockerfile_: installing the Apache web server.

The important lesson to take away from all of this is that order matters, and in general, you should always try to order your _Dockerfile_ so that the most stable and time-consuming portions of your build process happen first and your code is added as late in the process as possible.

For projects that require you to install dependencies based on your code using tools like `npm` and `bundle`, it is also a good idea to do some research about optimizing your Docker builds for those platforms. This often includes locking down your dependency versions and storing them along with your code so that they do not need to be downloaded for each and every build.

### Directory Caching

One of the many features that BuildKit adds to the image-building experience is directory caching. Directory caching is an incredibly useful tool for speeding up build times without saving a lot of files that are unnecessary for the runtime into your image. In essence, it allows you to save the contents of a directory inside your image in a special layer that can be bind-mounted at build time and then unmounted before the image snapshot is made. This is often used to handle directories where tools like Linux software installers (`apt`, `apk`, `dnf`, etc.), and language dependency managers (`npm`, `bundler`, `pip`, etc.), download their databases and archive files.

**TIP**

If you are unfamiliar with bind mounts and what they are, you can find a [bind mount overview](https://docs.docker.com/storage/bind-mounts) in the Docker documentation.

To make use of directory caching, you must have BuildKit enabled. In most circumstances, this should already be the case, but you can force it from the client side, by setting the environment variable `DOCKER_BUILDKIT=` to `1`:

```
$ export DOCKER_BUILDKIT=1
```

Let’s explore directory caching by checking out the following git repository and seeing how utilizing directory caching can significantly improve consecutive builds while still keeping the resulting image sizes smaller:

```
$ git clone https://github.com/spkane/open-mastermind.git \
  --config core.autocrlf=input

$ cd open-mastermind
$ cat Dockerfile
```

```
FROM python:3.9.15-slim-bullseye
RUN mkdir /app
WORKDIR /app
COPY . /app
RUN pip install -r requirements.txt
WORKDIR /app/mastermind
CMD ["python", "mastermind.py"]
```

This codebase has a very generic _Dockerfile_ checked into the repo. Let’s go ahead and see how long it takes to build this image, with and without the layer cache, and let’s also examine how large the resulting image is:

```
$ time docker build --no-cache -t docker.io/spkane/open-mastermind:latest .

[+] Building 67.5s (12/12) FINISHED
…
 => => naming to docker.io/spkane/open-mastermind:latest                  0.0s

real    0m28.934s
user    0m0.222s
sys     0m0.248s

$ docker image ls --format "{{ .Size }}" spkane/open-mastermind:latest
293MB

$ time docker build -t docker.io/spkane/open-mastermind:latest .

[+] Building 1.5s (12/12) FINISHED
…
 => => naming to docker.io/spkane/open-mastermind:latest                  0.0s

real    0m1.083s
user    0m0.098s
sys     0m0.095s
```

From this output, we can see that this image takes just under 29 seconds to build without the layer cache and just under 2 seconds to build when it can fully utilize the layer cache. The resulting image size is 293 MB in total.

**TIP**

BuildKit finally has support for [modifying or completely disabling the colors used for the output](https://github.com/moby/buildkit#color-output-controls). This is particularly nice for anyone who uses a dark background in their terminal. You can configure these colors by setting something like `export BUILDKIT_` `COLORS=run=green:warning=yellow:error=red:cancel=cyan` in your environment, or you can completely disable the colors by setting `export NO_COLOR=true`.

Note that the BuildKit version used in various `docker` components and third-party tools is still being updated, so it might not work yet in every situation.

If you want to test the build, go ahead and run it:

```
$ docker container run -ti --rm docker.io/spkane/open-mastermind:latest
```

This will launch a [terminal-based open source version of the Mastermind game](https://github.com/philshem/open-mastermind). There are on-screen directions for the game, and as a fallback, you can always exit by typing Ctrl-C.

Since this is a Python application, it uses the _requirements.txt_ file to list all of the libraries that the application requires, and then the `pip` application is used in the _Dockerfile_ to install these dependencies.

**NOTE**

We are installing some unnecessary dependencies simply to make the benefits of directory caching more obvious.

Go ahead and open up the _requirements.txt_ file and add a line that reads `log-symbols`, so that it looks like this:

```
colorama
# These are not required - but are used for demonstration purposes
pandas
flask
log-symbols
```

Let’s rerun the build now:

```
$ time docker build -t docker.io/spkane/open-mastermind:latest \
  --progress=plain .

#1 [internal] load build definition from Dockerfile
…
#9 [5/6] RUN pip install -r requirements.txt
#9 sha256:82dbc10f1bb9fa476d93cc0d8104b76f46af8ece7991eb55393d6d72a230919e
#9 1.954 Collecting colorama
#9 2.058   Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)
…
real    0m16.379s
user    0m0.112s
sys     0m0.082s
```

If you look at the full output for step `5/6`, you will notice that all the dependencies are downloaded again, even though `pip` would normally have most of those dependencies cached in _/root/.cache_. This inefficiency results from the builder seeing that we have made a change that impacts this layer and therefore completely re-creates the layer, so we lose that cache, even though we had it stored in the image layer.

Let’s go ahead and improve this situation. To do this, we need to leverage the [BuildKit directory cache](https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/reference.md#run---mounttypecache), and to do that we need to make a few changes to the _Dockerfile_ so that it looks like this:

```
# syntax=docker/dockerfile:1
FROM python:3.9.15-slim-bullseye
RUN mkdir /app
WORKDIR /app
COPY . /app
RUN --mount=type=cache,target=/root/.cache pip install -r requirements.txt
WORKDIR /app/mastermind
CMD ["python", "mastermind.py"]
```

There are two important changes in there. First, we added the following line:

```
# syntax=docker/dockerfile:1
```

This tells Docker that we are going to use a newer version of the [_Dockerfile_ frontend](https://hub.docker.com/r/docker/dockerfile), which provides us with access to BuildKit’s new features.

Then we edited the `RUN` line to look like this:

```
RUN --mount=type=cache,target=/root/.cache pip install -r requirements.txt
```

This line tells BuildKit to mount a caching layer into the container at _/root/.cache_ for the duration of this one build step. This will accomplish two goals for us. It will remove the contents of that directory from the resulting image, and it will also be remounted and available to `pip` in consecutive builds.

Let’s go ahead and do a full rebuild of the image with these changes, to generate the initial cache directory contents. If you follow the output, you will see that `pip` downloads all the dependencies, exactly as before:

```
$ time docker build --no-cache -t docker.io/spkane/open-mastermind:latest .

[+] Building 15.2s (15/15) FINISHED
…
 => => naming to docker.io/spkane/open-mastermind:latest                  0.0s
…
real    0m15.493s
user    0m0.137s
sys     0m0.096s
```

So, now let’s open up the _requirements.txt_ file and add a line that reads `py-events`:

```
colorama
# These are not required - but are used for demonstration purposes
pandas
flask
log-symbols
py-events
```

This is where the changes pay off. When we rebuild the image now, we will see that `py-events` and its dependencies are the only things that are downloaded; everything else uses the existing cache from our previous build, which has been mounted into the image for this build step:

```
$ time docker build -t docker.io/spkane/open-mastermind:latest \
  --progress=plain .

#1 [internal] load build definition from Dockerfile
…
#14 [stage-0 5/6] RUN --mount=type=cache,target=/root/.cache pip install …
#14 sha256:9bc72441fdf2ec5f5803d4d5df43dbe7bc6eeef88ebee98ed18d8dbb478270ba
#14 1.711 Collecting colorama
#14 1.714   Using cached colorama-0.4.5-py2.py3-none-any.whl (16 kB)
…
#14 2.236 Collecting py-events
#14 2.356   Downloading py_events-0.1.2-py3-none-any.whl (5.8 kB)
…
#16 DONE 1.4s

real    0m12.624s
user    0m0.180s
sys     0m0.112s

$ docker image ls --format "{{ .Size }}" spkane/open-mastermind:latest
261MB
```

The build time has shrunk since there is no longer a need to re-download everything each time, and the image size is also 32 MB smaller, even though we have added new dependencies to the image. This is simply because the cache directory is no longer stored directly in the image that contains the application.

BuildKit and the new _Dockerfile_ frontends bring a lot of very useful features to the image-building process that you will want to be aware of. We highly recommend that you take the time to read through [the reference guide](https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/reference.md) and become acquainted with all the available capabilities.

## Troubleshooting Broken Builds

We normally expect builds to just work, especially when we’ve scripted them, but in the real world things go wrong. Let’s spend a little bit of time discussing what you can do to troubleshoot a Docker build that is failing. In this section, we will explore two options: one that works with the pre-BuildKit approach to image building and one that works with BuildKit.

For this demonstration, we are going to reuse the `docker-hello-node` repo from earlier in the chapter. If required, you can clone it again, like this:

```
$ git clone https://github.com/spkane/docker-node-hello.git \
    --config core.autocrlf=input
Cloning into 'docker-node-hello'…
remote: Counting objects: 41, done.
remote: Total 41 (delta 0), reused 0 (delta 0), pack-reused 41
Unpacking objects: 100% (41/41), done.

$ cd docker-node-hello
```

### Debugging Pre-BuildKit Images

We need a patient for the next set of exercises, so let’s create a failing build. To do that, edit the _Dockerfile_ so that the line that reads:

```
RUN apt-get -y update
```

now reads:

```
RUN apt-get -y update-all
```

**WARNING**

If you are using PowerShell on Windows, you will likely need to set the environment variable that disables BuildKit before running the following `docker image build` command, and then reset it afterward:

```
PS C:\> $env:DOCKER_BUILDKIT = 0
PS C:\> docker image build `
         -t example/docker-node-hello:latest `
         --no-cache .
PS C:\> $env:DOCKER_BUILDKIT = 1
```

If you try to build the image now, you should get the following error:

```
$ DOCKER_BUILDKIT=0 docker image build -t example/docker-node-hello:latest \
  --no-cache .

Sending build context to Docker daemon  9.216kB
Step 1/14 : FROM docker.io/node:18.13.0
 ---> 9ff38e3a6d9d
…
Step 6/14 : ENV SCPATH /etc/supervisor/conf.d
 ---> Running in e903367eaeb8
Removing intermediate container e903367eaeb8
 ---> 2a236efc3f06
Step 7/14 : RUN apt-get -y update-all
 ---> Running in c7cd72f7d9bf
E: Invalid operation update-all
The command '/bin/sh -c apt-get -y update-all' returned a non-zero code: 100
```

So, how can we troubleshoot this, especially if we are not developing on a Linux system? The real trick here is to remember that almost all Docker images are layered on top of other Docker images and that you can start a container from any image. Although the meaning is not obvious on the surface, if you look at the output for `Step 6`, you will see this:

```
Step 6/14 : ENV SCPATH /etc/supervisor/conf.d
 ---> Running in e903367eaeb8
Removing intermediate container e903367eaeb8
 ---> 2a236efc3f06
```

The first line that reads `Running in e903367eaeb8` is telling you that the build process has started a new container, based on the image created in `Step 5`. The next line, which reads `Removing intermediate container e903367eaeb8`, is telling you that Docker is now removing the container after having altered it based on the instruction in `Step 6`. In this case, it was simply adding a default environment variable via `ENV SCPATH /etc/supervisor/conf.d`. The final line, which reads `--→ 2a236efc3f06`, is the one we really care about because this is giving us the image ID for the image that was generated by `Step 6`. You need this to troubleshoot the build because it is the image from the last successful step in the build.

With this information, it is possible to run an interactive container so that you can try to determine why your build is not working properly. Remember that every container image is based on the image layers below it. One of the great benefits of this is that we can just run the lower layer as a container itself, using a shell to look around!

```
$ docker container run --rm -ti 2a236efc3f06 /bin/bash
root@b83048106b0f:/#
```

From inside the container, you can now run any commands that you might need to determine what is causing your build to fail and what you need to do to fix your _Dockerfile_:

```
root@b83048106b0f:/# apt-get -y update-all
E: Invalid operation update-all

root@b83048106b0f:/# apt-get --help
apt 1.4.9 (amd64)
…

Most used commands:
  update - Retrieve new lists of packages
…

root@b83048106b0f:/# apt-get -y update
Get:1 http://security.debian.org/debian-security stretch/updates … [53.0 kB]
…
Reading package lists… Done

root@b83048106b0f:/# exit
exit
```

Once the root cause has been determined, the _Dockerfile_ can be fixed, so that `RUN apt-get -y update-all` now reads `RUN apt-get -y update`, and then rebuilding the image should result in success:

```
$ DOCKER_BUILDKIT=0 docker image build -t example/docker-node-hello:latest .
Sending build context to Docker daemon  15.87kB
…
Successfully built 69f5e83bb86e
Successfully tagged example/docker-node-hello:latest
```

### Debugging BuildKit Images

When using BuildKit, we have to take a slightly different approach to get access to the point where the build fails, because none of the intermediate build layers are exported from the build container to the Docker daemon.

The options for debugging BuildKit will almost certainly evolve as we move forward, but let’s take a look at one approach that works now.

Assuming that the _Dockerfile_ has been reverted to its original state, let’s change the line that reads:

```
RUN npm install
```

so that it now reads:

```
RUN npm installer
```

and then attempt to build the image.

**TIP**

Make sure that you have BuildKit enabled!

```
$ docker image build -t example/docker-node-hello:debug --no-cache .

[+] Building 51.7s (13/13) FINISHED
 => [internal] load build definition from Dockerfile                      0.0s
…
 => [7/8] WORKDIR /data/app                                               0.0s
 => ERROR [8/8] RUN npm installer                                         0.4s
______
 > [8/8] RUN npm installer:
#13 0.399
#13 0.399 Usage: npm <command>
…
#13 0.402 Did you mean one of these?
#13 0.402     install
#13 0.402     install-test
#13 0.402     uninstall
______
executor failed running [/bin/sh -c npm installer]: exit code: 1
```

We see an error as we expected, but how are we going to get access to that layer so that we can troubleshoot this?

One approach that works is to leverage multistage builds and the `--target` argument of `docker image build`.

Let’s start by modifying the _Dockerfile_ in two places. Change this line:

```
FROM docker.io/node:18.13.0
```

so that it now reads:

```
FROM docker.io/node:18.13.0 as deploy
```

and then immediately before the line that causes the error, we are going to add a new `FROM` line:

```
FROM deploy
RUN npm installer
```

By doing this, we are creating a multistage build, where the first stage contains all of the steps that we know are working and the second stage starts with our problematic step.

If we try to rebuild this using the same command as before, it will still fail:

```
$ docker image build -t example/docker-node-hello:debug .

[+] Building 51.7s (13/13) FINISHED
…
executor failed running [/bin/sh -c npm installer]: exit code: 1
```

So, instead of doing that, let’s tell Docker that we only want to build the first image in our multistage _Dockerfile_:

```
$ docker image build -t example/docker-node-hello:debug --target deploy .

[+] Building 0.8s (12/12) FINISHED
 => [internal] load build definition from Dockerfile                   0.0s
 => => transferring dockerfile: 37B                                    0.0s
…
 => exporting to image                                                 0.1s
 => => exporting layers                                                0.1s
 => => writing image sha256:a42dfbcfc7b18ee3d30ace944ad4134ea2239a2c0  0.0s
 => => naming to docker.io/example/docker-node-hello:debug             0.0s
```

Now, we can create a container from this image and do whatever testing we require:

```
$ docker container run --rm -ti docker.io/example/docker-node-hello:debug \
  /bin/bash

root@17807997176e:/data/app# ls
index.js  package.json

root@17807997176e:/data/app# npm install
…
added 18 packages from 16 contributors and audited 18 packages in 1.248s
…

root@17807997176e:/data/app# exit
exit
```

And then once we understand what is wrong with the _Dockerfile_, we can revert our debugging changes and fix the `npm` line so that the whole build works as expected.

## Multiarchitecture Builds

Since the launch of Docker, the _AMD64/X86\_64_ architecture has been the primary platform that most containers have targeted. However, this has started to change significantly. More and more developers are using systems based on ARM64/AArch64, and cloud companies are starting to make ARM-based VMs available through their platforms, due to the lower computing costs associated with the ARM platform.

This can cause some interesting challenges for anyone who needs to build and maintain images that will target multiple architectures. How can you maintain a single, streamlined codebase and pipeline while still supporting all of these different targets?

Luckily, Docker has released a plug-in for the `docker` CLI, called `buildx`, which can help make this process pretty straightforward. In many cases, `docker-buildx` will already be installed on your system, and you can verify this like so:

```
$ docker buildx version
github.com/docker/buildx v0.9.1 ed00243a0ce2a0aee75311b06e32d33b44729689
```

**TIP**

If you need to install the plug-in, you can follow [the directions from the GitHub repo](https://github.com/docker/buildx#installing).

By default, `docker-buildx` will leverage [QEMU-based virtualization](https://www.qemu.org/) and [`binfmt_misc`](https://docs.kernel.org/admin-guide/binfmt-misc.html) to support architectures that differ from the underlying system. This may already be set up on your Linux system, but just in case, it is a good idea to run the following command when you are first setting up a new Docker server, just to ensure that the QEMU files are properly registered and up to date:

```
$ docker container run --rm --privileged multiarch/qemu-user-static \
    --reset -p yes

Setting /usr/bin/qemu-alpha-static as binfmt interpreter for alpha
Setting /usr/bin/qemu-arm-static as binfmt interpreter for arm
Setting /usr/bin/qemu-armeb-static as binfmt interpreter for armeb
…
Setting /usr/bin/qemu-aarch64-static as binfmt interpreter for aarch64
Setting /usr/bin/qemu-aarch64_be-static as binfmt interpreter for aarch64_be
…
```

Unlike the original embedded Docker build functionality, which ran directly on the server, BuildKit can utilize a build container when it builds images, which means that there is a lot of functional flexibility that can be delivered with that build container. In the next step, we are going to create a default `buildx` container called `builder`.

**TIP**

If you have an existing `buildx` container by this name, you can either remove it by running `docker buildx rm builder` or you can change the name in the upcoming `docker buildx create` command.

With the next two commands, we are going to create the build container, set it as the default, and then start it up:

```
$ docker buildx create --name builder --driver docker-container --use
builder

$ docker buildx inspect --bootstrap
[+] Building 9.6s (1/1) FINISHED
 => [internal] booting buildkit                                           9.6s
 => => pulling image moby/buildkit:buildx-stable-1                        8.6s
 => => creating container buildx_buildkit_builder0                        0.9s
Name:   builder
Driver: docker-container

Nodes:
Name:      builder0
Endpoint:  unix:///var/run/docker.sock
Status:    running
Buildkit:  v0.10.5
Platforms: linux/amd64, linux/amd64/v2, linux/arm64, linux/riscv64,
           linux/ppc64le, linux/s390x, linux/386, linux/mips64le,
           linux/mips64, linux/arm/v7, linux/arm/v6
```

For this example, let’s go ahead and download the `wordchain` Git repository, which contains a useful tool that can generate random and deterministic word sequences to help with dynamic naming needs:

```
$ git clone https://github.com/spkane/wordchain.git \
  --config core.autocrlf=input
$ cd wordchain
```

Let’s go ahead and take a look at the included _Dockerfile_. You’ll notice that it is a pretty normal multistage _Dockerfile_ and does not have anything special in it related to the platform architecture:

```
FROM golang:1.18-alpine3.15 AS build

RUN apk --no-cache add \
    bash \
    gcc \
    musl-dev \
    openssl

ENV CGO_ENABLED=0

COPY . /build
WORKDIR /build

RUN go install github.com/markbates/pkger/cmd/pkger@latest && \
    pkger -include /data/words.json && \
    go build .

FROM alpine:3.15 AS deploy

WORKDIR /
COPY --from=build /build/wordchain /

USER 500
EXPOSE 8080

ENTRYPOINT ["/wordchain"]
CMD ["listen"]
```

In the first step, we are going to build our statically compiled Go binary, and then in the second step, we are going to package it up into a small deployment image.

**NOTE**

The `ENTRYPOINT` instruction in the _Dockerfile_ is an advanced instruction that allows you to separate the default process that is run by the container (`ENTRYPOINT`) from the command-line arguments that are passed to that process (`CMD`). When `ENTRYPOINT` is missing from the _Dockerfile_, the `CMD` instruction is expected to contain both the process and all the required command-line arguments.

We can go ahead and build this image and side-load it into our local Docker server by running the following command:

```
$ docker buildx build --tag wordchain:test --load .

[+] Building 2.4s (16/16) FINISHED
 => [internal] load .dockerignore                                         0.0s
 => => transferring context: 93B                                          0.0s
 => [internal] load build definition from Dockerfile                      0.0s
 => => transferring dockerfile: 461B                                      0.0s
…
 => exporting to oci image format                                         0.3s
 => => exporting layers                                                   0.0s
 => => exporting manifest sha256:4bd1971f2ed820b4f64ffda97707c27aac3e8eb7 0.0s
 => => exporting config sha256:ce8f8564bf53b283d486bddeb8cbb074ff9a9d4ce9 0.0s
 => => sending tarball                                                    0.2s
 => importing to docker                                                   0.0s
```

We can quickly test out the image by running the following commands:

```
$ docker container run wordchain:test random

witty-stack

$ docker container run wordchain:test random -l 3 -d .

odd.goo

$ docker container run wordchain:test --help

wordchain is an application that can generate a readable chain
  of customizable words for naming things like
  containers, clusters, and other objects.
…
```

As long as you got some random word pairs back with the first two commands, then everything is working as expected.

Now, to build this image for multiple architectures, we need to simply add the `--platform` argument to our build.

**NOTE**

Typically we would also replace `--load` with `--push`, which would push all the resulting images to the tagged repository, but in this case, we need to simply remove `--load`, because the Docker server cannot load images for multiple platforms at the moment, and we do not have a repository set up to push these images to. If we did have a repository and we tagged the images correctly, then we could very easily build and push all the resulting images in one step, with a command like this:

`docker buildx build --platform linux/amd64,linux/arm64 --tag docker.io/spkane/wordchain:latest --push .`

You can build this image for both the linux/amd64 and the linux/arm64 platforms like this:

```
$ docker buildx build --platform linux/amd64,linux/arm64 \
    --tag wordchain:test .

[+] Building 114.9s (23/23) FINISHED
…
 => [linux/arm64 internal] load metadata for docker.io/library/alpine:3.1 2.7s
 => [linux/amd64 internal] load metadata for docker.io/library/alpine:3.1 2.7s
 => [linux/arm64 internal] load metadata for docker.io/library/golang:1.1 3.0s
 => [linux/amd64 internal] load metadata for docker.io/library/golang:1.1 2.8s
…
 => CACHED [linux/amd64 build 5/5] RUN go install github.com/markbates/pk 0.0s
 => CACHED [linux/amd64 deploy 2/3] COPY --from=build /build/wordchain /  0.0s
 => [linux/arm64 build 5/5] RUN go install github.com/markbates/pkger/c 111.7s
 => [linux/arm64 deploy 2/3] COPY --from=build /build/wordchain /         0.0s
WARNING: No output specified with docker-container driver. Build result will
         only remain in the build cache. To push result image into registry
         use --push or to load image into docker use --load
```

**NOTE**

Due to the emulation that is required when building images for nonnative architectures, you may notice that some steps take much longer than normal. This is to be expected due to the additional computational overhead from the emulation.

It is possible to set up Docker so that it will build each image on a worker with a matching architecture, which should speed things up significantly in many cases. You can find some information about this in this [Docker blog article](https://www.docker.com/blog/speed-up-building-with-docker-buildx-and-graviton2-ec2).

In the output for the build, you will notice lines that start with something like `=> [linux/amd64 *]` or `=> [linux/arm64 *]`. Each of these lines represents the builder working on this build step for the stated platform. Many of these steps will run in parallel, and due to caching and other considerations, each build might progress at differing speeds.

Since we did not add `--push` to our build, you will also notice that we received a warning at the end of the build. This is because the _docker-container_ driver that the builder is using just left everything in the build cache, which means that we can’t run the resulting images; at this point, we can only feel confident that the build is working.

**TIP**

There are a few [`build` arguments](https://docs.docker.com/engine/reference/builder/#automatic-platform-args-in-the-global-scope) that are automatically set by Docker that can be especially helpful to leverage inside your _Dockerfile_ when you are doing multiarchitecture builds. As an example, `TARGETARCH` is frequently used to make sure that a given build step downloads the correct prebuilt binary for the current image’s platform.

So, when we upload this image to a repository, how does Docker know which image to use for the local platform? This information is provided to the Docker server through something called an _image manifest_. We can look at the manifest for _docker.io/spkane/workdchain_ by running the following:

```
$ docker manifest inspect docker.io/spkane/wordchain:latest
```

```
{
   "schemaVersion": 2,
   "mediaType": "application/vnd.docker.distribution.manifest.list.v2+json",
   "manifests": [
      {
         "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
         "size": 739,
         "digest": "sha256:4bd1…bfc0",
         "platform": {
            "architecture": "amd64",
            "os": "linux"
         }
      },
      {
…
         "platform": {
            "architecture": "arm64",
            "os": "linux"
         }
      },
…
   ]
}
```

If you look through the output, you will see that there are blocks that identify the image that is required for every platform the image supports. This is accomplished via the individual [`digest`](https://github.com/opencontainers/image-spec/blob/main/descriptor.md#digests) entries that are then paired with a _platform_ block. This manifest file is downloaded by the server when it requires an image, and then after referencing the manifest, the server will download the correct image for the local platform. This is why our _Dockerfile_ works at all. Each `FROM` line lists a base image that we want to use, but it is the Docker server that utilizes this manifest file to determine exactly which image to download for each platform that the build is targeting.

## Wrap-Up

At this point, you should feel pretty comfortable with image creation for Docker and should have a solid understanding of many of the core tools and functionality you can leverage to streamline your build pipeline. In the next chapter, we will start to dig into how you can use your images to create containerized processes for your projects.

[1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#idm46803155693712-marker) Full URL: [_https://github.com/torvalds/linux/commit/e9be9d5e76e34872f0c37d72e25bc27fe9e2c54c_](https://github.com/torvalds/linux/commit/e9be9d5e76e34872f0c37d72e25bc27fe9e2c54c)

[2](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#idm46803155211200-marker) This code was originally forked from [GitHub](https://github.com/enokd/docker-node-hello).

[3](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#idm46803154385056-marker) Full URL: [_https://docs.docker.com/registry/recipes/mirror/#configure-the-docker-daemon_](https://docs.docker.com/registry/recipes/mirror/#configure-the-docker-daemon)

[4](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#idm46803154382192-marker) Full URL: [_https://docs.docker.com/registry/recipes/mirror/#run-a-registry-as-a-pull-through-cache_](https://docs.docker.com/registry/recipes/mirror/#run-a-registry-as-a-pull-through-cache)

[5](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#idm46803152409712-marker) Full URL: [_https://github.com/bluewhalebook/docker-up-and-running-3rd-edition/blob/main/chapter\_04/multistage/Dockerfile_](https://github.com/bluewhalebook/docker-up-and-running-3rd-edition/blob/main/chapter\_04/multistage/Dockerfile)

[6](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#idm46803151763616-marker) Full URL: [_https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/measure-command?view=powershell-7.3_](https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/measure-command?view=powershell-7.3)
