# 6. Exploring Docker

## Chapter 6. Exploring Docker

Now that you have some experience working with containers and images, we can explore some of Docker’s other capabilities. In this chapter, we’ll continue to use the `docker` command-line tool to talk to the running `dockerd` server that you’ve configured while visiting some of the other fundamental commands.

Docker provides commands to do several additional things easily:

* Printing the Docker version
* Viewing the server information
* Downloading image updates
* Inspecting containers
* Entering a running container
* Returning a result
* Viewing logs
* Monitoring statistics
* And much more…

Let’s take a look at these as well as some of the additional community tooling that augments Docker’s native capabilities.

## Printing the Docker Version

If you completed the last chapter, you have a working Docker daemon on a Linux server or VM, and you’ve started a base container to make sure it’s all working. If you haven’t set that up already and you want to try out the steps in the rest of the book, you’ll want to follow the installation steps in [Chapter 3](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch03.html#installing\_docker) before you move on with this section.

The absolute simplest thing you can do with Docker is print the versions of the various components. It might not sound like much, but this is a useful tool to have because Docker is built from a multitude of components whose versions will directly dictate what functionality is available to you. Knowing how to show the version will also help you troubleshoot certain types of connection issues between the client and server. For example, the Docker client might give you a cryptic message about mismatched API versions, and it’s nice to be able to translate that into Docker versions so you know which component needs updating. This command talks to the remote Docker server, so if the client can’t connect to the server for any reason, the client will report an error and then only print out the client version information. If you find that you are having connectivity problems, you should probably revisit the steps in the last chapter.

**NOTE**

You can always directly log in to the Docker server and run `docker` commands from a shell on the server if you are troubleshooting issues or simply do not want to use the `docker` client to connect to a remote system. On most Docker servers, this will require either `root` privileges or membership in the `docker` group to connect to the Unix domain socket that Docker is listening on.

Since we just installed all of the Docker components at the same time, when we run `docker version`, we should see that all of our versions match:

```
$ docker version
Client:
 Cloud integration: v1.0.24
 Version:           20.10.17
 API version:       1.41
 Go version:        go1.17.11
 Git commit:        100c701
 Built:             Mon Jun  6 23:04:45 2022
 OS/Arch:           darwin/amd64
 Context:           default
 Experimental:      true

Server: Docker Desktop 4.10.1 (82475)
 Engine:
  Version:          20.10.17
  API version:      1.41 (minimum version 1.12)
  Go version:       go1.17.11
  Git commit:       a89b842
  Built:            Mon Jun  6 23:01:23 2022
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.6.6
  GitCommit:        10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1
 runc:
  Version:          1.1.2
  GitCommit:        v1.1.2-0-ga916309
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
```

Notice how we have different sections representing the client and server. In this case, we have a matching client and server since we just installed them together. But it’s important to note that this won’t always be the case. Hopefully, in your production systems, you can manage to keep the same version running on most systems. But it’s not uncommon for development environments and build systems to have slightly different versions.

API clients and libraries will usually work across a large number of Docker versions, depending on which API version they require. In the `Server` section, we can see that the current API version is 1.41 and the minimum API it will serve is 1.12. This is useful information when you’re working with third-party clients, and now you know how to verify this information.

## Server Information

We can also find out a lot about the Docker server via the Docker client. Later we’ll talk more about what all of this means, but you can find out which filesystem backend the Docker server is running, which kernel version it is on, which operating system it is running on, which plug-ins are installed, which runtime is being used, and how many containers and images are currently stored there. `docker system info` will present you with something similar to this, which has been shortened for brevity:

```
$ docker system info
Client:
…
 Plugins:
  buildx: Docker Buildx (Docker Inc., v0.8.2)
  compose: Docker Compose (Docker Inc., v2.6.1)
  extension: Manages Docker extensions (Docker Inc., v0.2.7)
  sbom: View the packaged-based Software Bill Of Materials (SBOM) …
  scan: Docker Scan (Docker Inc., v0.17.0)

Server:
 Containers: 11
…
 Images: 6
 Server Version: 20.10.17
 Storage Driver: overlay2
…
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local logentries …
…
 Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc
 Default Runtime: runc
…
 Kernel Version: 5.10.104-linuxkit
 Operating System: Docker Desktop
 OSType: linux
 Architecture: x86_64
…
```

Depending on how your Docker daemon is set up, this might look somewhat different. Don’t be concerned about that; this is just to give you an example. Here we can see that our server is a Docker Desktop release running the 5.10.104 Linux kernel and backed with the `overlay2` filesystem driver. We also have a few images and containers on the server. With a fresh install, this number should be zero.

The information about plug-ins is worth pointing out here. It’s telling us about all the things this installation of Docker supports. On a fresh install, things will look more or less like this, depending on which new plug-ins are distributed with Docker. Docker itself is made up of many different plug-ins all working together. This is powerful because it means it’s also possible to install several other plug-ins contributed by members of the community. It’s useful to be able to see which are installed even if you just want to make sure Docker has recognized one that you recently added.

In most installations, _/var/lib/docker_ will be the default root directory used to store images and containers. If you need to change this, you can edit your Docker startup scripts to launch the daemon, with the `--data-root` argument pointing to a new storage location. To test this by hand, you could run something like this:

```
$ sudo dockerd \
    -H unix:///var/run/docker.sock \
    --data-root="/data/docker"
```

**NOTE**

By default, [the configuration file for the Docker server](https://oreil.ly/jp7iK)[1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch06.html#idm46803145473376) can be found in _/etc/docker/daemon.json_. Most of the arguments that we discuss passing directly to `dockerd` can be permanently set in this file. If you are using Docker Desktop, you are advised to modify this file in the Docker Desktop UI.

We will talk more about runtimes later, but here you can see that we have three runtimes installed. The `runc` runtime is the default Docker runtime. If you think of Linux containers, you are usually thinking about the type of container that `runc` builds. On this server, we also have the `io.containerd.runc.v2` and `io.containerd.runtime.v1.linux` runtimes installed. We’ll talk more about some other runtimes in [Chapter 11](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch11.html#advanced\_topics).

## Downloading Image Updates

We’re going to use an Ubuntu base image for the following examples. Even if you already grabbed the `ubuntu:latest` base image once, you can `pull` it again and it will automatically pick up any updates that have been published since you last ran it.

This is because `latest` is a tag that, by convention, is supposed to represent the latest build of the container. However, the `latest` tag is controversial, since it is not permanently pinned to a specific image and can have different meanings across different projects. Some people use it to point to the most recent stable release, some use it to point to the last build produced by their CI/CD system, and others simply refuse to tag any of their images with `latest`. That being said, it is still in wide use and can be useful in preproduction environments where the convenience of using it outweighs the lack of assurances that a real version provides:

Invoking `docker image pull` will look like this:

```
$ docker image pull ubuntu:latest

latest: Pulling from library/ubuntu
405f018f9d1d: Pull complete
Digest: sha256:b6b83d3c331794420340093eb706a6f152d9c1fa51b262d9bf34594887c2c7ac
Status: Downloaded newer image for ubuntu:latest
docker.io/library/ubuntu:latest
```

That command pulled down only the layers that have changed since we last ran the command. You might see a longer or shorter list, or even an empty list, depending on when you last pulled the image, what changes have been pushed to the registry since then, and how many layers the target image contains.

**TIP**

It’s good to remember that even though you pulled `latest`, Docker won’t automatically keep the local image up to date for you. You’ll be responsible for doing that yourself. However, if you deploy an image based on a newer copy of `ubuntu:latest`, the Docker client will download the missing layers during the deployment just like you would expect. Keep in mind that this is the behavior of the Docker client, and other libraries or API tools may not behave this way. It’s highly recommended that you always deploy production code using a fixed version tag rather than the `latest` tag. This helps guarantee that you get the version you expect and there are no unexpected surprises.

In addition to referring to items in the registry by the `latest` tag or another version number tag, you can refer to them by their content-addressable tag, which looks like this:

```
sha256:b6b83d3c331794420340093eb706a6f152d9c1fa51b262d9bf34594887c2c7ac
```

These are generated as a hashed sum of the contents of the image and are a very precise identifier. This is by far the safest way to refer to Docker images when you need to make sure you are getting the exact version you expect because these can’t be moved like a version tag. The syntax for pulling them from the registry is very similar, but note the `@` in the tag:

```
$ docker image pull ubuntu@sha256:b6b83d3c331794420340093eb706a6f152d…
```

Unlike most Docker commands where you may shorten the hash, you cannot do that with SHA-256 hashes. You must use the full hash here.

## Inspecting a Container

Once you have a container created, running or not, you can now use `docker` to see how it was configured. This is often useful in debugging and also has some other information that can be useful for identifying a container.

For this example, go ahead and start up a container:

```
$ docker container run --rm -d -t ubuntu /bin/bash
3c4f916619a5dfc420396d823b42e8bd30a2f94ab5b0f42f052357a68a67309b
```

We can list all our running containers with `docker container ls` to ensure everything is running as expected, and to copy the container ID:

```
$ docker container ls
CONTAINER ID  IMAGE         COMMAND     … STATUS        …  NAMES
3c4f916619a5  ubuntu:latest "/bin/bash" … Up 31 seconds …  angry_mestorf
```

In this case, our ID is `3c4f916619a5`. We could also use `angry_mestorf`, which is the dynamic name assigned to our container. Many underlying tools need the unique container ID though, so it’s useful to get into the habit of looking at that first. As we mentioned earlier, the ID as shown is the truncated (or short) version, but Docker treats these interchangeably with the long versions. As is the case in many version control systems, this hash is just the prefix of a much longer hash. Internally, the kernel uses a 64-byte hash to identify the container. But that’s painful for humans to use, so Docker supports the shortened hash.

The output to `docker container inspect` is pretty verbose, so we’ll cut it down in the following code block to a few values worth pointing out. You should look at the full output to see what else you think is interesting:

```
$ docker container inspect 3c4f916619a5
```

```
[{
    "Id": "3c4f916619a5dfc420396d823b42e8bd30a2f94ab5b0f42f052357a68a67309b",
    "Created": "2022-07-17T17:26:53.611762541Z",
    …
    "Args": [],
    …
    "Image": "sha256:27941809078cc9b2802deb2b0bb6feed6c…7f200e24653533701ee",
    …
    "Config": {
        "Hostname": "3c4f916619a5",
        …
        "Env": [
          "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
        ],
        "Cmd": [
            "/bin/bash"
        ],
        …
        "Image": "ubuntu",
        …
    },
    …
}]
```

Note that long `"Id"` string. That’s the full unique identifier of this container. Luckily we can use the short version, even if that’s still not especially convenient. We can also see that the exact time when the container was created is much more precise than what `docker container ls` gives us.

Some other interesting things are shown here as well: the top-level command in the container, the environment that was passed to it at creation time, the image on which it’s based, and the hostname inside the container. All of these are configurable at container creation time if you need to do so. The usual method for passing configuration to containers, for example, is via environment variables, so being able to see how a container was configured via `docker container inspect` can reveal a lot when you’re debugging.

You can go ahead and stop the current container by running something like `docker container stop 3c4f916619a5`.

## Exploring the Shell

Let’s get a container running with just an interactive `bash` shell so we can take a look around. We’ll do that, as we did before, by running something like this:

```
$ docker container run --rm -it ubuntu:22.04 /bin/bash
```

That will run an Ubuntu 22.04 LTS container with the bash shell as the top-level process. By specifying the `22.04` tag, we can be sure to get a particular version of the image. So, when we start that container, what processes are running?

```
root@35fd1ad27228:/# ps -ef
UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 17:45 pts/0    00:00:00 /bin/bash
root         9     1  0 17:47 pts/0    00:00:00 ps -ef
```

Wow, that’s not much, is it? It turns out that when we told `docker` to start `bash`, we didn’t get anything but that. We’re inside a whole Linux distribution image, but no other processes started for us automatically. We only got what we asked for. It’s good to keep that in mind going forward.

**WARNING**

Linux containers don’t, by default, start anything in the background as a full VM would. They’re a lot lighter weight than that and therefore don’t start an `init` system. You can, of course, run a whole `init` system if you need to, or the [`tini init` system](https://github.com/krallin/tini) that is built into Docker, but you have to ask for it. We’ll talk about that more in [Chapter 7](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch07.html#debug\_docker).

That’s how we get a shell running in a container. Feel free to poke around and see what else looks interesting inside the container. You might have a pretty limited set of commands available. You’re in a base Ubuntu distribution, though, so you can fix that by using `apt-get update`, followed by `apt-get install…` to download more packages. However, these applications are only going to be around for the life of this container. You’re modifying the top layer of the container, not the base image! Containers are by nature ephemeral, so anything you do inside this container won’t outlast it.

When you are done in the container, make sure to `exit` the shell, which will then naturally stop the container:

```
root@35fd1ad27228:/# exit
```

## Returning a Result

How inefficient would it be to spin up a whole VM to run one command and get the results? You usually wouldn’t want to do this because it would be very time-consuming and would require booting a whole operating system to simply execute one command. But Docker and Linux containers do not work the same way as VMs do: containers are very lightweight and don’t have to boot up like an operating system does. Running something like a quick background job and waiting for the exit code is a normal use case for a Linux container. You can think of it as a way to get remote access to a containerized system and have access to any of the individual commands inside that container with the ability to pipe data to and from them and return exit codes.

This can be useful in lots of scenarios: you might, for instance, have system health checks run this way remotely or have a series of machines with processes that you spin up via Docker to process a workload and then return. The `docker` command-line tools proxy the results to the local machine. If you run the remote command in foreground mode and don’t specify doing otherwise, `docker` will redirect its `stdin` to the remote process, and the remote process’s `stdout` and `stderr` to your terminal. The only things we have to do to get this functionality are to run the command in the foreground and not allocate a TTY on the remote. This is also the default configuration! No command-line options are required.

When we run these commands, Docker creates a new container, executes the command that we requested inside the container’s namespaces and cgroups, removes the container, and then exits so that nothing is left running or taking up unnecessary disk space between invocations. The following code should give you an idea of the types of things that you can do:

```
$ docker container run --rm ubuntu:22.04 /bin/false
$ echo $?
1
```

```
$ docker container run --rm ubuntu:22.04 /bin/true
$ echo $?
0
```

```
$ docker container run --rm ubuntu:22.04 /bin/cat /etc/passwd

root:x:0:0:root:/root:/bin/bash
daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin
…
nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin
_apt:x:100:65534::/nonexistent:/usr/sbin/nologin

$ docker container run --rm ubuntu:22.04 /bin/cat /etc/passwd | wc -l

19
```

Here we executed `/bin/false` on the remote server, which will always exit with a status of `1`. Notice how `docker` proxied that result to us in the local terminal. Just to prove that it returns other results, we also run `/bin/true`, which will always return a `0`. And there it is.

Then we actually ask `docker` to run `cat /etc/passwd` on the remote container. What we get is a printout of the _/etc/passwd_ file contained inside that container’s filesystem. Because that’s just regular output on _stdout_, we can pipe it into local commands just like we would anything else.

**WARNING**

The previous code pipes the output into the local `wc` command, not a `wc` command in the container. The pipe itself is not passed to the container. If you want to pass the whole command, including the pipes, to the server, you need to invoke a complete shell on the remote side and pass a quoted command, like `bash -c "<your command> | <something else>"`. In the previous code, that would be `docker container run ubuntu:22.04 /bin/` `bash -c "` `/bin/cat /etc/passwd | wc -l"`.

## Getting Inside a Running Container

You can pretty easily get a shell running in a new container, based on almost any image, as we demonstrated earlier with `docker container run`. But it’s not the same as getting a new shell inside an existing container that is actively running your application. Every time you use `docker container run`, you get a new container. But if you have an existing container that is running an application and you need to debug it from inside the container, you need something else.

Using `docker container exec` is the Docker-native way to get a new interactive process in a container, but there is also a more Linux-native way to do it, called `nsenter`. We will take a look at `docker container exec` in this section and cover `nsenter` later in [“nsenter”](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch11.html#nsenter).

**NOTE**

You may be wondering why you would ever want to do this. In development, this can be very useful when you are actively building and testing your application. This is the mechanism that [development containers](https://containers.dev/) use in IDEs like [Visual Studio Code](https://code.visualstudio.com/docs/devcontainers/containers).

In production, it isn’t considered good practice to SSH into your production servers, and this is roughly the same thing; but there are times when it’s very important to see what’s going on inside the actual environment, and this can help you out in those situations.

### docker container exec

First, let’s take a look at the easiest and best way to get inside a running container. The `dockerd` server and `docker` command-line tool support remotely executing a new process in a running container via the `docker container exec` command. So let’s start up a container in background mode and then enter it using `docker container exec` and invoking a shell.

The command you invoke doesn’t have to be a shell: it’s possible to run individual commands inside the container and see their results outside it using `docker container exec`. But if you want to get inside the container to look around, a shell is the easiest way to do that.

To run `docker container exec`, we’ll need our container’s ID. For this demo, let’s create a container that will just run the `sleep` command for 600 seconds:

```
$ docker container run -d --rm  ubuntu:22.04 sleep 600
9f09ac4bcaa0f201e31895b15b479d2c82c30387cf2c8a46e487908d9c285eff
```

The short ID for this container is `9f09ac4bcaa0`. We can now use that to get inside the container with `docker container exec`. The command line for that, unsurprisingly, looks a lot like the command line for `docker container run`. We request an interactive session and a pseudo-TTY with the `-i` and `-t` flags:

```
$ docker container exec -it 9f09ac4bcaa0 /bin/bash
root@9f09ac4bcaa0:/#
```

Note that we got a command line back that tells us the ID of the container we’re running inside. That’s pretty useful for keeping track of where we are. We can now run a normal Linux `ps` to see what else is running inside our container. We should see the `sleep` process that was created when the container was originally started:

```
root@9f09ac4bcaa0:/# ps -ef
UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 20:22 ?        00:00:00 sleep 600
root         7     0  0 20:23 pts/0    00:00:00 /bin/bash
root        15     7  0 20:23 pts/0    00:00:00 ps -ef
```

Type `exit` to get out of the container when you are done.

**WARNING**

You can also run additional processes in the background via `docker container exec`. You use the `-d` option just like with `docker container run`. But you should think hard about doing that for anything but debugging because you lose the repeatability of the image deployment if you depend on this mechanism. Other people would then have to know what to pass to `docker container exec` to get the desired functionality. If you’re tempted to do this, you would probably reap bigger gains from rebuilding your container image to launch both processes in a repeatable way. If you need to signal to the software inside the container to take some action like rotating logs or reloading a configuration, it is cleaner to leverage `docker container kill -s <SIGNAL>` with the standard Unix signal name to pass information to the process inside the container.

### docker volume

Docker supports a `volume` subcommand that makes it possible to list all of the volumes stored in your root directory and then discover additional information about them, including where they are physically stored on the server.

These volumes are not bind-mounted; instead, they are special data containers that provide a useful method for persisting data.

If we run a normal `docker` command that bind-mounts a directory, we’ll notice that it does not create any Docker volumes:

```
$ docker volume ls
DRIVER              VOLUME NAME

$ docker container run --rm -d -v /tmp:/tmp ubuntu:latest sleep 120
6fc97c50fb888054e2d01f0a93ab3b3db172b2cd402fc1cd616858b2b5138857

$ docker volume ls
DRIVER              VOLUME NAME
```

However, you can easily create a new volume with a command like this:

```
$ docker volume create my-data
```

If you then list all your volumes, you should see something like this:

```
$ docker volume ls

DRIVER              VOLUME NAME
local               my-data

$ docker volume inspect my-data
```

```
[
    {
        "CreatedAt": "2022-07-31T16:19:42Z",
        "Driver": "local",
        "Labels": {},
        "Mountpoint": "/var/lib/docker/volumes/my-data/_data",
        "Name": "my-data",
        "Options": {},
        "Scope": "local"
    }
]
```

Now you can start a container with this data volume attached to it by running the following:

```
 $ docker container run --rm \
     --mount source=my-data,target=/app \
     ubuntu:latest touch /app/my-persistent-data
```

That container created a file in the data volume and then immediately exited.

If we now mount that data volume to a different container, we will see that our data is still there:

```
$ docker container run --rm \
    --mount source=my-data,target=/app \
    fedora:latest ls -lFa /app/my-persistent-data

-rw-r--r-- 1 root root 0 Jul 31 16:24 /app/my-persistent-data
```

And finally, you can delete the data volume when you are done with it by running the following:

```
$ docker volume rm my-data

my-data
```

**NOTE**

If you try to delete a volume that is in use by a container (whether it is running or not), you’ll get an error like this:

```
Error response from daemon: unable to remove volume:
    remove my-data: volume is in use - [
    d0763e6e8d79e55850a1d3ab21e9d…,
    4b40d52978ea5e784e66ddca8bc22…]
```

These commands should help you explore your containers in great detail. Once we’ve explained namespaces more in [Chapter 11](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch11.html#advanced\_topics), you’ll get a better understanding of exactly how all these pieces interact and combine to create a container.

## Logging

Logging is a critical part of any production application. When things go wrong, logs can be a critical tool in restoring service, so they need to be done well. There are some common ways in which we expect to interact with application logs on Linux systems, some better than others. If you’re running an application process on a box, you might expect the output to go to a local logfile that you could read through. Or perhaps you might expect the output to simply be logged to the kernel buffer where it can be read from `dmesg`. Or, as on many modern Linux distributions with `systemd`, you might expect logs to be available from `journalctl`. Because of the container’s restrictions and how Docker is constructed, none of these will work without at least some configuration on your part. But that’s OK because logging has first-class support in Docker.

Docker makes logging easier in a few critical ways. First, it captures all of the normal text output from applications in the containers it manages. Anything sent to `stdout` or `stderr` in the container is captured by the Docker daemon and streamed into a configurable logging backend. Secondly, like many other parts of Docker, this system is pluggable, and there are lots of powerful options available to you as plug-ins. But let’s not dive into the deep end just yet.

### docker container logs

We’ll start with the simplest Docker use case: the default logging mechanism. There are limitations to this mechanism, which we’ll explain in a minute, but for the most common use cases, it works well, and it’s very convenient. If you are running Docker in development, this is probably the only logging strategy you’ll use there. This logging method has been there from the very beginning and is well understood and supported. The mechanism is the `json-file` method. The `docker container logs` command exposes most users to this.

As implied by the name, when you run the default `json-file` logging plug-in, your application’s logs are streamed by the Docker daemon into a JSON file for each container. This lets us retrieve logs for any container at any time.

We can display some logs by starting an `nginx` container:

```
$ docker container run --rm -d --name nginx-test --rm nginx:latest
```

and then:

```
$ docker container logs nginx-test
…
2022/07/31 16:36:05 [notice] 1#1: using the "epoll" event method
2022/07/31 16:36:05 [notice] 1#1: nginx/1.23.1
2022/07/31 16:36:05 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6)
2022/07/31 16:36:05 [notice] 1#1: OS: Linux 5.10.104-linuxkit
…
```

This is nice because Docker allows you to get the logs remotely, right from the command line, on demand. That’s very useful for low-volume logging.

**NOTE**

To limit the log output to more recent logs, you can use the `--since` option to display logs only after a specified RFC 3339 date (e.g., 2002-10-02T10:00:00-05:00), Unix timestamp (e.g., 1450071961), standard timestamp (e.g., 20220731), or Go duration string (e.g., 5m45s). You can also use `--tail` followed by the number of lines you would like to tail.

The actual files backing this logging are on the Docker server itself, by default in _/var/lib/docker/containers/`<container_id>`/_ where the _`<container_id>`_ is replaced by the actual container ID. If you take a look at the file named _`<container_id>`_`-json.log`, you’ll see that it’s a file with each line representing a JSON object. It will look something like this:

```
{"log":"2022/07/31 16:36:05 [notice] 1#1: using the \"epoll\" event method\n",
  "stream":"stderr","time":"2022-07-31T16:36:05.189234362Z"}
```

That `log` field is exactly what was sent to `stdout` on the process in question; the `stream` field tells us that this was `stdout` and not `stderr`, and the precise time that the Docker daemon received it is provided in the `time` field. It’s an uncommon format for logging, but it’s structured rather than just a raw stream, which is beneficial if you want to do anything with the logs later.

Like a logfile, you can also tail the Docker logs live with `docker container logs -f`:

```
$ docker container logs -f nginx-test
…
2022/07/31 16:36:05 [notice] 1#1: start worker process 35
2022/07/31 16:36:05 [notice] 1#1: start worker process 36
2022/07/31 16:36:05 [notice] 1#1: start worker process 37
2022/07/31 16:36:05 [notice] 1#1: start worker process 38
```

This looks identical to the usual `docker container logs`, but the client will continue to wait for, and then display, new messages as they are received from the server, much like the Linux command line `tail -f`. You can type Ctrl-C to exit the logs stream at any time:

```
---
$ docker container stop nginx-test
---
```

**TIP**

By configuring the tag log option similar to `--log-opt tag="{{.ImageName}}/{{.ID}}"`, it is possible to change the default log tag (which every log line will start with) to something more useful. By default, Docker logs will be tagged with the first 12 characters of the container ID.

For single-host logging, this mechanism is pretty good. Its shortcomings are around log rotation, remote access to the logs once they’ve been rotated, and disk space usage for high-volume logging. Despite being backed by a JSON file, this mechanism performs well enough that most production applications can log this way if that’s the solution that works for you. But if you have a more complex environment, you’re going to want something more robust and with centralized logging capabilities.

**WARNING**

The default settings for `dockerd` do not currently enable log rotation. You’ll want to make sure you specify the `--log-opt` `max-size` and `--log-opt` `max-file` settings via the command line or the _daemon.json_ configuration file if you are running in production. Those settings limit the largest file size before rotation and the maximum number of logfiles to keep, respectively. `max-file` does not do anything unless you’ve also set `max-size` to tell Docker when to rotate the logs. When this is enabled, the `docker container logs` mechanism will return data only from the current logfile.

### More Advanced Logging

For those times when the default mechanism isn’t enough—and at scale, it’s probably not—Docker also supports configurable logging backends. This list of plug-ins is constantly growing. Currently supported are the `json-file` we described earlier, as well as `syslog`, `fluentd`, `journald`, `gelf`, `awslogs`, `splunk`, `gcplogs`, `local`, and `logentries`, which are used for sending logs to various popular logging frameworks and services.

That’s a big list of plug-ins we just threw out there. The supported option that currently is the simplest for running Docker at scale is sending your container logs to `syslog` directly from Docker. You can specify this on the Docker command line with the `--log-driver=syslog` option or set it as the default in the _daemon.json_ file for all containers.

**TIP**

The _daemon.json_ file is the configuration for the `dockerd` server. It can usually be found in the _/etc/docker/_ directory on the server. For Docker Desktop, this file can be edited in Preferences → Docker Engine from the UI. If you change this file, you will need to restart Docker Desktop or the `dockerd` daemon.

There are also several third-party plug-ins available. We’ve seen mixed results from third-party plug-ins, primarily because they complicate installing and maintaining Docker. However, you may find that there is a third-party implementation that’s perfect for your system, and it might be worth the installation and maintenance hassle.

**WARNING**

Some caveats apply to all of the logging drivers. For example, Docker supports only one at a time. This means that you can use the `syslog` or `gelf` logging driver, but not along with the `json-file` driver. Unless you run `json-file` or `journald`, you will lose the ability to use the `docker container logs` command! This may not be expected and is a big consideration when you are changing the driver.

Some plug-ins are designed to send the logs to a remote endpoint and keep a local JSON copy for the `docker container logs` command, but you will need to determine if the plug-in that you want to use supports this. There are too many gotchas to go through for each driver, but you should keep in mind the trade-off between guaranteed delivery of logs and the potential for breaking your Docker deployment. UDP-based solutions or other nonblocking options are recommended.

Traditionally, most Linux systems have some kind of syslog receiver, whether it be `syslog`, `rsyslog`, or any of the many other options. This protocol in its various forms has been around for a long time and is fairly well supported by most deployments. When migrating to Docker from a traditional Linux or Unix environment, many companies already have syslog infrastructure in place, which means this is often the easiest migration path as well.

**NOTE**

Many newer Linux distributions are based on the `systemd` init system and therefore use `journald` for logging by default, which is different from `syslog`.

While syslog is a traditional solution, it has its problems. The Docker syslog driver supports TLS, TCP, and UDP connection options, which sounds great, but you should be cautious about streaming logs from Docker to a remote log server over TCP or TLS. The problem with this is that they are both run on top of connection-oriented TCP sessions, and Docker tries to connect to the remote logging server at the time of container startup. If it fails to make the connection, it will block trying to start the container. If you are running this as your default logging mechanism, this can strike at any time on any deployment.

This is not a particularly usable state for production systems, and thus it is recommended that you use the UDP option for syslog logging if you intend to use the `syslog` driver. This does mean your logs are not encrypted and do not have guaranteed delivery. There are various philosophies around logging, and you’ll need to balance your need for logs against the reliability of your system. We tend to recommend erring on the side of reliability, but if you run in a secure audit environment, you may have different priorities.

**TIP**

You can log directly to a remote syslog-compatible server from a single container by setting the log option `syslog-address` similar to this: `--log-opt syslog-address=udp://192.168.42.42:123`.

One final caveat to be aware of regarding most of the logging plug-ins: they are blocking by default, which means that logging back-pressure can cause issues with your application. You can change this behavior by setting `--log-opt mode=non-blocking` and then setting the maximum buffer size for logs to something like `--log-opt max-buffer-size=4m`. Once these are set, the application will no longer block when that buffer fills up. Instead, the oldest loglines in memory will be dropped. Again, reliability needs to be weighed here against your business’s need to receive all the logs.

**WARNING**

Some third-party libraries and programs write to the filesystem for various (and sometimes unexpected) reasons. If you are trying to design clean containers that do not write directly into the container filesystem, you should consider utilizing the `--read-only` and `--mount type=tmpfs` options to `docker container run` that we discussed in [Chapter 4](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#docker\_images). Writing logs _inside_ the container is not recommended. It makes them hard to get to, prevents them from being preserved beyond the container life span, and can wreak havoc with the Docker filesystem backend.

## Monitoring Docker

Among the most important requirements for production systems is that they are observable and measurable. A production system where you are blind to how it’s behaving won’t serve you well. In modern operations environments, we want to monitor everything meaningful and report as many useful statistics as we can. Docker supports container health checks and some basic reporting capabilities via `docker container stats` and `docker system events`. We’ll show you those and then look at a community offering from Google that does some nice graphing output, and then we’ll take a look at a—currently experimental—feature of Docker that exports container metrics to the Prometheus monitoring system.

### Container Statistics

Let’s start with the CLI tools that ship with Docker itself. The `docker` CLI has an endpoint for viewing important statistics of running containers. The command-line tool can stream from this endpoint and every few seconds report back on one or more listed containers, giving basic statistics information about what’s happening. `docker container stats`, like the Linux `top` command, takes over the current terminal and updates the same lines on the screen with the current information. It’s hard to show that in print so we’ll just give an example, but this updates every few seconds by default.

#### Command-line statistics

Start an active container:

```
$ docker container run --rm -d --name stress \
    docker.io/spkane/train-os:latest \
    stress -v --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout 60s
```

Then run the `stats` command to look at the new container:

```
$ docker container stats stress
CONTAINER ID NAME   CPU %   MEM USAGE/LIMIT   MEM % NET I/O   BLOCK I/O PIDS
1a9f52f0855f stress 476.50% 36.09MiB/7.773GiB 0.45% 1.05kB/0B 0B/0B     6
```

You can type Ctrl-C to exit the `stats` stream at any time.

**TIP**

You can use the `--no-stream` option to get a single-point-in-time set of statistics that will not update and will return you back to the command line after the command completes.

Let’s break that rather dense output down into some manageable chunks. We have the following:

* The container ID (but not the name).
* The amount of CPU it’s currently consuming. One hundred percent is equivalent to one whole CPU core.
* The amount of memory it has in use, followed by the maximum amount it’s allowed to use.
* Network and block I/O statistics.
* The number of active processes inside the container.

Some of these will be more useful than others for debugging, so let’s take a look at what you can do with them.

One of the more helpful pieces of output here is the percentage of memory used versus the limit that was set for the container. One common problem with running production containers is that overly aggressive memory limits can cause the Linux kernel’s OOM killer to stop the container over and over again. The `stats` command can help you identify and troubleshoot these types of issues.

Concerning I/O statistics, if you run all of your applications in containers, then this summary can make it very clear where your I/O is going from the system. Before containers, this was much harder to figure out!

The number of active processes inside the container helps debug as well. If you have an application that is spawning children without reaping them, this can expose it pretty quickly.

One great feature of `docker container stats` is that it can show not just one container but all of them in a single summary. That can be pretty revealing, even on boxes where you think you know what they are doing.

That is all useful and easy to digest because it’s human formatted and available on the command line. But there is an additional endpoint on the Docker API that provides a _lot_ more information than is shown in the client. We’ve steered away from directly utilizing the API in this book so far, but in this case, the data provided by the API is so much richer than the client that we’ll go ahead and use `curl` to make an API request and see what our container is doing. It’s nowhere near as nice to read, but there is a lot more detail.

**NOTE**

Remember that basically everything that the `docker` client can do can be done directly through the Docker APIs. This means that you can programmatically do very similar things in your applications if there is a need.

The example in [“stats API endpoint”](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch06.html#stats\_api\_endpoint) is a good intro to calling the API directly.

#### stats API endpoint

The `/stats/` endpoint that we’ll hit on the API will continue to stream statistics to us as long as we keep the connection open. Since as humans we can’t easily parse the JSON, we’ll just ask for one line and then use the tool `jq` to “pretty-print” it. For this command to work, you’ll need to have `jq` installed (version 2.6 or later). If you don’t and you still want to see the JSON output, you can skip the pipe to `jq`, but you’ll get plain, ugly JSON back. If you already have a favorite JSON pretty printer, feel free to use that instead.

Most Docker daemons will be installed with the API available only on the Unix domain socket and not published on TCP. So we’ll use `curl` from the Docker server host itself to call the API. If you plan to monitor this endpoint in production, you would need to expose the Docker API on a TCP port. This is not something that we recommend, but the [Docker documentation](https://dockr.ly/2Lzuox2) will walk you through this.

**NOTE**

If you are not on the Docker server or using Docker Desktop locally, you may need to inspect the contents of the `DOCKER_HOST` environment variable, using something like `echo $DOCKER_HOST`, to discover the hostname or IP address of the Docker server that you are using.

First, start up a container that you can read statistics from:

```
$ docker container run --rm -d --name stress \
    docker.io/spkane/train-os:latest \
    stress -v --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout 60s
```

Now that the container is running, you can get an ongoing stream of statistics about the container in JSON format by running something like `curl` with your container’s name or hash.

**NOTE**

In the following examples, we are running `curl` against the Docker socket, but you could just as easily run it against the Docker port if it is available.

```
$ curl --no-buffer -XGET --unix-socket /var/run/docker.sock \
    http://docker/containers/stress/stats
```

**NOTE**

This JSON stream of statistics will not stop on its own. So for now, we can use the Ctrl-C key combination to stop it.

To get a single group of statistics, we can run something similar to this:

```
$ curl -s -XGET --unix-socket /var/run/docker.sock \
    http://docker/containers/stress/stats | head -n 1 | jq
```

And finally, if we have [jq](https://stedolan.github.io/jq) or another tool capable of pretty-printing JSON, we can make this output human readable, as shown here:

```
$ curl -s -XGET --unix-socket /var/run/docker.sock \
    http://docker/containers/stress/stats | head -n 1 | jq
```

```
{
  "read": "2022-07-31T17:41:59.10594836Z",
  "preread": "0001-01-01T00:00:00Z",
  "pids_stats": {
    "current": 6,
    "limit": 18446744073709552000
  },
  "blkio_stats": {
    "io_service_bytes_recursive": [
      {
        "major": 254,
        "minor": 0,
        "op": "read",
        "value": 0
      },
…
    ]
  },
  "num_procs": 0,
  "storage_stats": {},
  "cpu_stats": {
    "cpu_usage": {
      "total_usage": 101883204000,
      "usage_in_kernelmode": 43818021000,
      "usage_in_usermode": 58065183000
…
    },
  },
  "memory_stats": {
    "usage": 183717888,
    "stats": {
      "active_anon": 0,
      "active_file": 0,
…
    },
    "limit": 8346021888
  },
  "name": "/stress",
  "id": "9be7c9de26864ac97e07fc3d8e3ffb5bb52cc2ba49f569d4ba8d407f8747851f",
  "networks": {
    "eth0": {
      "rx_bytes": 1046,
      "rx_packets": 9,
…
    }
  }
}
```

There is _a lot_ of information in there. We’ve cut it down to prevent wasting any more trees or electrons than necessary, but even so, there is a lot to digest. The main idea is to let you see how much data is available from the API about each container. We won’t spend much time going into the details, but you can get quite detailed memory usage information, as well as block I/O and CPU usage information.

If you are doing your own monitoring, this is a great endpoint to hit as well. A drawback, however, is that it’s one endpoint per container, so you can’t get the statistics about all containers from a single call.

### Container Health Checks

As with any other application, when you launch a container it is possible that it will start and run but never actually enter a healthy state where it could receive traffic. Production systems also fail, and your application may become unhealthy at some point during its life, so you need to be able to deal with that.

Many production environments have standardized ways to health-check applications. Unfortunately, there’s no clear standard for how to do that across organizations, and it’s unlikely that many companies do it in the same way. For this reason, monitoring systems have been built to handle that complexity so that they can work in a lot of different production systems. It’s a clear place where a standard would be a big win.

To help remove this complexity and standardize on a universal interface, Docker has added a health-check mechanism. Following the shipping container metaphor, Linux containers should really look the same to the outside world no matter what is inside the container, so Docker’s health-check mechanism not only standardizes health checking for containers but also maintains the isolation between what is inside the container and what it looks like on the outside. This means that containers from Docker Hub or other shared repositories can implement a standardized health-checking mechanism, and it will work in any other Docker environment designed to run production containers.

Health checks are a build-time configuration item and are created with a `HEALTHCHECK` definition in the _Dockerfile_. This directive tells the Docker daemon what command it can run inside the container to ensure the container is in a healthy state. As long as the command exits with a code of zero (0), Docker will consider the container to be healthy. Any other exit code will indicate to Docker that the container is not in a healthy state, at which point appropriate action can be taken by a scheduler or monitoring system.

We will be using the following project to explore Docker Compose in a few chapters. But, for the moment, it includes a useful example of Docker health checks. Go ahead and pull down a copy of the code, and then navigate into the _rocketchat-hubot-demo/mongodb/docker/_ directory:

```
$ git clone https://github.com/spkane/rocketchat-hubot-demo.git \
    --config core.autocrlf=input
$ cd rocketchat-hubot-demo/mongodb/docker
```

In this directory, you will see a _Dockerfile_ and a script called `docker-healthcheck`. If you view the _Dockerfile_, this is all that you will see:

```
FROM docker.io/bitnami/mongodb:4.4
# Newer Upstream Dockerfile:
# https://github.com/bitnami/containers/blob/
# f9fb3f8a6323fb768fd488c77d4f111b1330bd0e/bitnami/mongodb
# /5.0/debian-11/Dockerfile

COPY docker-healthcheck /usr/local/bin/

# Useful Information:
# https://docs.docker.com/engine/reference/builder/#healthcheck
# https://docs.docker.com/compose/compose-file/#healthcheck
HEALTHCHECK CMD ["docker-healthcheck"]
```

It is very short because we are basing this on the [upstream Mongo image](https://oreil.ly/Is1yt),[2](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch06.html#idm46803143337184) and our image inherits a lot of things from that, including the entry point, default command, and port to expose.

**NOTE**

Bitnami significantly refactored their container repositories in early 2023, so this link points to a slightly newer version of the _Dockerfile_ that targets MongoDB 5.0. We are using MongoDB 4.4 in this example, but the link should still get the point across.

```
EXPOSE 27017
ENTRYPOINT [ "/opt/bitnami/scripts/mongodb/entrypoint.sh" ]
CMD [ "/opt/bitnami/scripts/mongodb/run.sh" ]
```

**TIP**

Be aware that Docker will forward traffic to a container’s ports even when the container and underlying processes are still spinning up.

So, in our _Dockerfile_ we are only adding a single script that can health-check our container, and defining a health-check command that runs that script.

You can build the container like this:

```
$ docker image build -t mongo-with-check:4.4 .
 => [internal] load build definition from Dockerfile                      0.0s
 => => transferring dockerfile: 37B                                       0.0s
 => [internal] load .dockerignore                                         0.0s
 => => transferring context: 2B                                           0.0s
 => [internal] load metadata for docker.io/bitnami/mongodb:4.4            0.5s
 => [internal] load build context                                         0.0s
 => => transferring context: 40B                                          0.0s
 => CACHED [1/2] FROM docker.io/bitnami/mongodb:4.4@sha256:9162…ae209     0.0s
 => [2/2] COPY docker-healthcheck /usr/local/bin/                         0.0s
 => exporting to image                                                    0.0s
 => => exporting layers                                                   0.0s
 => => writing image sha256:a6ef…da808                                    0.0s
 => => naming to docker.io/library/mongo-with-check:4.4                   0.0s
```

And then run the container and look at the `docker container ls` output:

```
$ docker container run -d --rm --name mongo-hc mongo-with-check:4.4
5a807c892428ab0641232c82bd477fc8d1142c9e15c27d5946b8bfe7056e2695

$ docker container ls
… IMAGE                   … STATUS                      PORTS     …
… mongo-with-check:4.4 … Up 1 second (health: starting) 27017/tcp …
```

You should notice that the `STATUS` column now has a `health` section in parentheses. Initially, this will display `health: starting` as the container is starting up. You can change the amount of time that Docker waits for the container to initialize using the `--health-start-period` argument to `docker container run`. The status will change to `healthy` once the container is up and the health check is successful. It might take this container 40+ seconds to transition into a healthy state:

```
$ docker container ls
… IMAGE                   … STATUS               PORTS     …
… mongo-with-check:4.4 … Up 32 seconds (healthy) 27017/tcp …
```

You can query this status directly, using the `docker container inspect` command:

```
$ docker container inspect --format='{{.State.Health.Status}}' mongo-hc
healthy

$ docker container inspect --format='{{json .State.Health}}' mongo-hc | jq
```

```
{
  "Status": "healthy",
  "FailingStreak": 0,
  "Log": [
    …
  ]
}
```

If your container begins failing its health check, the status will change to `unhealthy`, and you can then determine how to handle the situation:

```
$ docker container ls
… IMAGE                   … STATUS                PORTS     …
… mongo-with-check:4.4 … Up 9 minutes (unhealthy) 27017/tcp …
```

At this point, you can stop the container by simply running `docker container stop mongo-hc`.

**TIP**

As with most systems, you can configure a lot of details about your health checks, including how often Docker checks the health (`--health-interval`), how many failures are required to cause the container to be marked unhealthy (`--health-retries`), and more. You can even disable the health check completely (`--no-healthcheck`) if needed.

This feature is very useful, and you should strongly consider using it in all of your containers. This will help you improve both the reliability of your environment and the visibility you have into how things are running in it. It is also supported by many production schedulers and monitoring systems, so it should be easy to implement.

**WARNING**

As always, the usefulness of a health check is largely determined by how well written it is and how accurately it determines the state of the service.

### docker system events

The `dockerd` daemon internally generates an events stream around the container lifecycle. This is how various parts of the system find out what is going on in other parts. You can also tap into this stream to see what lifecycle events are happening for containers on your Docker server. This, as you probably expect by now, is implemented in the `docker` CLI tool as another command-line argument. When you run this command, it will block and continually stream messages to you. Behind the scenes, this is a long-lived HTTP request to the Docker API that returns messages in JSON blobs as they occur. The `docker` CLI tool decodes them and prints some data to the terminal.

This events stream is useful in monitoring scenarios or triggering additional actions, like wanting to be alerted when a job completes. For debugging purposes, it allows you to see when a container died even if Docker restarts it later. Down the road, this is a place where you might also find yourself directly implementing some tooling against the API.

In one terminal, go ahead and run the `events` command:

```
$ docker system events
```

You will notice that nothing happens.

In another terminal, go ahead and launch the following short-lived container:

```
$ docker container run --rm --name sleeper debian:latest sleep 5
```

In the original terminal that is running the `events` command, you should now see something like this:

```
…09:59.606… container create d6… (image=debian:latest, name=sleeper)
…09:59.610… container attach d6… (image=debian:latest, name=sleeper)
…09:59.631… network connect ea… (container=d60b…, name=bridge, type=bridge)
…09:59.827… container start d6… (image=debian:latest, name=sleeper)
…10:04.854… container die d6… (exitCode=0, image=debian:latest, name=sleeper)
…10:04.907… network disconnect ea… (container=d60b…, name=bridge, type=bridge)
…10:04.922… container destroy d6… (image=debian:latest, name=sleeper)
```

You can type Ctrl-C to exit the events stream at any time.

**TIP**

As with the Docker statistics, you can access the Docker system events via `curl` using a command like `curl --no-buffer -XGET --unix-socket /var/run/docker.sock` [_`http://docker/events`_](http://docker/events).

In this example, we ran a short-lived container that simply counted 5 seconds and then exited.

The `container create`, `container attach`, `network connect`, and `container start` events are all the steps required to get the container into a running state. When the container exits, the events stream logs a `container die`, `network disconnect`, and `container destroy` message. Each one of these marks a step in completely tearing down the container. Docker also helpfully tells us the ID of the image that the container is running on. This can be useful for tying deployments to events, for example, because a deployment usually involves a new image.

If you have a server where containers are not staying up, the `docker system events` stream is pretty helpful in seeing what’s going on and when. But if you’re not watching it at the time, Docker very helpfully caches some of the events, and you can still get at them for some time afterward. You can ask it to display events after a time with the `--since` option, or before with the `--until` option. You can also use both to limit the window to a narrow scope of time when an issue you are investigating may have occurred. Both options take ISO time formats like those in the previous example (e.g., 2018-02-18T14:03:31-08:00).

**TIP**

There are a few specific event types that you should go out of your way to monitor:

`container oom`

Appears when a container runs out of memory

`container exec_createcontainer exec_startcontainer exec_die`

Appear when someone has used `docker container exec` to enter a container, which could signal a security incident

### cAdvisor

`docker container stats` and `docker system events` are useful but don’t get us graphs to look at yet. And graphs are pretty helpful when we’re trying to see trends. Of course, other people have filled some of this gap. When you begin to explore the options for monitoring Docker, you will find that many of the major monitoring tools now provide some functionality to help you improve the visibility into your containers’ performance and ongoing state.

In addition to the commercial tooling provided by companies like Datadog, GroundWork, and New Relic, there are plenty of options for free, open source tools like Prometheus or even Nagios. We’ll talk about Prometheus in [“Prometheus Monitoring”](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch06.html#prom\_monitoring). Soon after Docker was introduced, Google released its internal container monitoring tool as a well-maintained open source project on GitHub, called [cAdvisor](https://github.com/google/cadvisor). Although cAdvisor can be run outside of Docker, by now you’re probably not surprised to hear that the easiest implementation of cAdvisor is to simply run it as a Linux container.

To install cAdvisor on most Linux systems, all you need to do is run this code.

**WARNING**

This command is intended to be run directly on a Linux Docker server. It will not work properly when run from a Windows or macOS system.

```
$ docker container run \
  --volume=/:/rootfs:ro \
  --volume=/var/run:/var/run:ro \
  --volume=/sys:/sys:ro \
  --volume=/var/lib/docker/:/var/lib/docker:ro \
  --volume=/dev/disk/:/dev/disk:ro \
  --publish=8080:8080 \
  --detach=true \
  --name=cadvisor \
  --privileged \
  --rm \
  --device=/dev/kmsg \
  gcr.io/cadvisor/cadvisor:latest

Unable to find image 'cadvisor/cadvisor:latest' locally
Pulling repository cadvisor/cadvisor
f0643dafd7f5: Download complete
…
ba9b663a8908: Download complete
Status: Downloaded newer image for cadvisor/cadvisor:latest
f54e6bc0469f60fd74ddf30770039f1a7aa36a5eda6ef5100cddd9ad5fda350b
```

**NOTE**

On Red Hat Enterprise Linux (RHEL)-based systems, you may need to add the following line to the `docker container run` command shown here: `--volume=/cgroup:/cgroup \`.

Once you have done this, you will be able to navigate to your Docker host on port 8080 to see the cAdvisor web interface (e.g., _http://172.17.42.10:8080/_) and the various detailed charts it has for the host and individual containers (see [Figure 6-1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch06.html#figure6-1)).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098131814/files/assets/dur3_0601.png" alt="cAdvisor CPU Graphs" height="656" width="600"><figcaption></figcaption></figure>

**Figure 6-1. cAdvisor CPU graphs (example)**

cAdvisor provides a REST API endpoint, which can easily be queried for detailed information by your monitoring systems:

```
$ curl http://172.17.42.10:8080/api/v2.1/machine/
```

You can find details about the cAdvisor API in [the official documentation](https://github.com/google/cadvisor/blob/master/docs/api\_v2.md).

The amount of detail provided by cAdvisor should be sufficient for many of your graphing and monitoring needs.

## Prometheus Monitoring

The [Prometheus](https://prometheus.io/) monitoring system has become a popular solution for monitoring distributed systems. It works largely on a pull model, where it reaches out and gathers statistics from endpoints on a timed basis. Docker has an endpoint that was built for Prometheus and makes it easy to integrate your container stats into a Prometheus monitoring system. At the time of this writing, the endpoint is currently experimental and not enabled in the `dockerd` server by default. Our brief experience with it shows that it seems to work well, and it’s a pretty slick solution, as we’ll show you. We should point out that this solution is for monitoring the `dockerd` server, in contrast to the other solutions, which exposed information about the containers.

To export metrics to Prometheus, we need to reconfigure the `dockerd` server to enable the experimental features and to expose the metrics listener on a port of our choice. This is nice because we don’t have to expose the whole Docker API on a TCP listener to get metrics out of the system—a security win at the expense of a little more configuration. To do that, we can either provide the `--experimental` and `--metrics-addr=` options on the command line, or we can put them into the _daemon.json_ file that the daemon uses to configure itself. Because many current distributions run `systemd`, and changing configurations there is highly dependent on your installation, we’ll use the _daemon.json_ option since it’s more portable. We’ll demonstrate this on Ubuntu Linux 22.04 LTS. On this distribution, the file is usually not present to begin with. So let’s put one there using your favorite editor.

**TIP**

As previously mentioned, the _daemon.json_ file for Docker Desktop can be edited in Preferences → Docker Engine from the UI. If you change this file, you will need to restart Docker Desktop or the `dockerd` daemon.

Adjust or add the following lines to the _daemon.json_ file:

```
{
  "experimental": true,
  "metrics-addr": "0.0.0.0:9323"
}
```

You should now have a file that contains only what you just pasted and nothing else.

**WARNING**

Any time you make a service available on the network, you need to consider what security risks you might introduce. We believe the benefit of making metrics available is worth the trade-off, but you should think through the repercussions in your scenario. For example, making metrics available on the public internet is probably not a good idea in almost all cases.

When we restart Docker, we’ll now have a listener on all addresses on port 9323. That’s where Prometheus will connect to get the metrics. But first, we need to restart the `dockerd` server. Docker Desktop automatically takes care of the restart for you, but if you are on the Linux Docker server, then you can run something like `sudo systemctl restart docker` to restart the daemon. You should not get any errors returned from the restart. If you do, you likely have something set incorrectly in the _daemon.json_ file.

Now you can test the metrics endpoint with `curl`:

```
$ curl -s http://localhost:9323/metrics | head -15

# HELP builder_builds_failed_total Number of failed image builds
# TYPE builder_builds_failed_total counter
builder_builds_failed_total{reason="build_canceled"} 0
builder_builds_failed_total{reason="build_target_not_reachable_error"} 0
builder_builds_failed_total{reason="command_not_supported_error"} 0
builder_builds_failed_total{reason="dockerfile_empty_error"} 0
builder_builds_failed_total{reason="dockerfile_syntax_error"} 0
builder_builds_failed_total{reason="error_processing_commands_error"} 0
builder_builds_failed_total{reason="missing_onbuild_arguments_error"} 0
builder_builds_failed_total{reason="unknown_instruction_error"} 0
# HELP builder_builds_triggered_total Number of triggered image builds
# TYPE builder_builds_triggered_total counter
builder_builds_triggered_total 0
# HELP engine_daemon_container_actions_seconds The number of seconds it
# takes to process each container action
# TYPE engine_daemon_container_actions_seconds histogram
```

If you run this locally, you should get very similar output. It might not be identical, and that’s OK as long as you get something that is not an error message.

So now we have a place where Prometheus can get to our statistics. But we need to have Prometheus running somewhere, right? We can easily do that by spinning up a container. But first, we need to write a simple config. We’ll put it in _/tmp/prometheus/prometheus.yaml_. You can use your favorite editor to put the following into the file:

```
# Scrape metrics every 5 seconds and name the monitor 'stats-monitor'
global:
  scrape_interval: 5s
  external_labels:
    monitor: 'stats-monitor'

# We're going to name our job 'DockerStats' and we'll connect to the docker0
# bridge address to get the stats. If your docker0 has a different IP address
# then use that instead. 127.0.0.1 and localhost will not work.
scrape_configs:
  - job_name: 'DockerStats'
    static_configs:
    - targets: ['172.17.0.1:9323']
```

**NOTE**

For Docker Desktop, you can also use `host.docker.internal:9323` or `gateway.docker.internal:9323` in place of the `172.17.0.1:9323` shown here. Both of these hostnames will point to the container’s IP address.

As noted in the file, you should use the IP address of your `docker0` bridge here, or the IP address of your `ens3` or `eth0` interface since `localhost` and `127.0.0.1` are not routable from the container. The address we used here is the usual default for `docker0`, so it’s probably the right one for you.

Now that we’ve written that out, we need to start up the container using this config:

```
$ docker container run --rm -d -p 9090:9090 \
    -v /tmp/prometheus/prometheus.yaml:/etc/prometheus.yaml \
    prom/prometheus --config.file=/etc/prometheus.yaml
```

That will run the container and volume-mount the config file we made into the container so that it will find the settings it needs to monitor our Docker endpoint. If it starts up cleanly, you should now be able to open your browser and navigate to port 9090 on your host. There you will get a Prometheus window, something like [Figure 6-2](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch06.html#figure6-2).

In the following figure, you’ll see that we’ve selected one of the metrics, the `engine_daemon_events_total`, and graphed it over a short period. You can easily query any of the other metrics in the drop-down. Further work and exploration with Prometheus would allow you to define alerts and alerting policies based on these metrics as well. And it is easy to monitor so much more than just the `dockerd` server. You can also expose metrics for Prometheus from your applications. If you’re intrigued and want to look at something more advanced, you might take a look at [dockprom](https://github.com/stefanprodan/dockprom), which leverages Grafana to make nice dashboards and also queries your container metrics like those in the Docker API `/stats` endpoint.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098131814/files/assets/dur3_0602.png" alt="Prometheus web UI" height="420" width="600"><figcaption></figcaption></figure>

**Figure 6-2. Prometheus event graph (example)**

## Exploration

This should give you all the basics you need to start running containers. It’s probably worth downloading a container or two from the Docker Hub registry and exploring a bit on your own to get used to the commands we just learned. There are many other things you can do with Docker, including but not limited to the following:

* Copying files in and out of the container with `docker container cp`
* Saving an image to a tarball with `docker image save`
* Loading an image from a tarball with `docker image import`

Docker has a huge feature set that you will likely grow into over time. Each new release adds more functionality as well. We’ll get into a lot more detail later on about many of the other commands and features, but keep in mind that Docker’s whole feature set is very large.

## Wrap-Up

In the next chapter, we’ll dive into more technical details about how Docker works and how you can use this knowledge to debug your containerized applications.

[1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch06.html#idm46803145473376-marker) Full URL: [_https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file_](https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file)

[2](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch06.html#idm46803143337184-marker) Full URL: [_https://github.com/bitnami/containers/blob/f9fb3f8a6323fb768fd488c77d4f111b1330bd0e/bitnami/mongodb/5.0/debian-11/Dockerfile_](https://github.com/bitnami/containers/blob/f9fb3f8a6323fb768fd488c77d4f111b1330bd0e/bitnami/mongodb/5.0/debian-11/Dockerfile)
