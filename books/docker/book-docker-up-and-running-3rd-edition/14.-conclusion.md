# 14. Conclusion

## Chapter 14. Conclusion

At this point, you have had a solid tour through the Docker ecosystem and have seen many examples of how Docker and Linux containers can benefit you and your organization. We have tried to map out some of the common pitfalls and impart some of the wisdom that we have picked up over the many years that we’ve run Linux containers in production. Our experience has shown that the promise of Docker is quite achievable, and we’ve seen significant benefits in our organizations as a result. Like other powerful technologies, Docker is not without its compromises, but the net result has been a big positive for us, our teams, and our organizations. If you implement the Docker workflow and integrate it into the processes you already have in your organization, there is every reason to believe that you can significantly benefit from it as well.

In this chapter, we will take a moment to consider Docker’s evolving place in the technology landscape, and then quickly review the problems that Docker is designed to help you solve and some of the power it brings to the table.

## The Road Ahead

There is no doubt that containers are here to stay for a very long time, but some people have predicted the ultimate demise of Docker on and off for a long time. Much of this is simply because the word _Docker_ represents [so many things in so many people’s minds](https://oreil.ly/pvSEl).[1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch14.html#idm46803122954992) Are you talking about the company, which was sold to Mirantis in 2019 and reported $50 million USD in annual recurring revenue (ARR) two years after the restructuring? Or maybe the `docker` client tool, whose source code can be [downloaded](https://github.com/docker/cli), modified, and built by anyone who might need it? It is hard to know. People often like to try and predict the future, but reality often lies somewhere in the middle, hidden in the often-overlooked details.

In 2020, Kubernetes announced the [deprecation of dockershim](https://kubernetes.io/blog/2022/02/17/dockershim-faq), which went fully into effect with the release of Kubernetes v1.24. At the time, lots of people took this to mean that Docker was dead, but the point many people were missing is that Docker has always primarily been a developer tool, not a production component. Sure it can be used on a production system for various reasons, but its true power lies in its ability to streamline much of the software packaging and testing workflow into a consolidated toolset. Kubernetes uses the [Container Runtime Interface (CRI)](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes), which is not implemented by Docker and therefore required them to maintain another piece of wrapper software called `dockershim` to support using Docker Engine via the CRI. This announcement was not given to make some statement about Docker’s place in the ecosystem; it was simply given to make maintaining a large volunteer-driven open source project easier. Docker may not run on your Kubernetes servers, but in most cases, this will have no impact at all on the development and release cycle for your software. Unless you are a Kubernetes operator who used the `docker` CLI to directly query the containers running on a Kubernetes node, you are unlikely to notice any change as this transition occurs.

And as it turns out, Docker’s parent company has developed and continues to support a new shim, called [`cri-dockerd`](https://github.com/Mirantis/cri-dockerd), that allows Kubernetes to continue to interface with Docker for those who need that workflow to be supported.

Interestingly enough, Docker is also diversifying into noncontainer technologies, like [WebAssembly](https://docs.docker.com/desktop/wasm) (Wasm), that can complement containers while improving the developer experience.

So, Docker as a developer-friendly toolset is likely here to stay for a long while, but that doesn’t mean that there are not any other tools in the ecosystem that can complement or even replace it if that is something that you want or need. The beauty of the various standards that exist, like the OCI, and their broad adoption, is that many of these tools can interoperate with the same images and containers that other tools generate and manage.

## The Challenges Docker Addresses

In traditional deployment workflows, there is often a multitude of required steps that significantly contribute to the overall pain felt by teams. Every step you add to the deployment process for an application increases the risk inherent in shipping it to production. Docker combines a workflow with a simple toolset that is directly targeted at addressing these concerns. Along the way, it squarely aims your development processes toward some of the industry’s best practices, and its opinionated approach often leads to better communication and more robustly crafted applications.

Some of the specific problems that Docker and Linux containers can help mitigate include the following:

* Avoiding significant divergence between deployment environments.
* Requiring application developers to re-create configuration and logging logic in applications.
* Using outdated build and release processes that require multiple levels of handoff between development and operations teams.
* Requiring complex and fragile build and deploy processes.
* Managing divergent dependency versions that are required by applications that need to share the same hardware.
* Managing multiple Linux distributions in the same organization.
* Building one-off deployment processes for each application you put into production.
* Needing to treat each application as a unique codebase when it comes to patching and auditing security vulnerabilities.
* And much more.

By using the registry as a handoff point, Docker eases and simplifies communication between operations and development teams, or between multiple development teams on the same project. By bundling all of the dependencies for an application into one shipping artifact, Docker eliminates concerns about which Linux distribution developers want to work on, which versions of libraries they need to use, and how they compile their assets or bundle their software. It isolates operations teams from the build process and puts developers in charge of their dependencies.

## The Docker Workflow

Docker’s workflow helps organizations tackle really hard problems—some of the same problems that DevOps processes are aimed at solving. A major problem in incorporating DevOps successfully into a company’s processes is that many people have no idea where to start. Tools are often incorrectly presented as the solution to what are fundamentally process problems. Adding virtualization, automated testing, deployment tools, or configuration management suites to the environment often just changes the nature of the problem without delivering a resolution.

It would be easy to dismiss Docker as just another tool making unfulfillable promises about fixing your business processes, but that would be selling it short. Docker’s power is in the way that its natural workflow allows applications to travel through their whole lifecycle, from conception to retirement, within one ecosystem. Unlike other tools that often target only a single aspect of the DevOps pipeline, Docker significantly improves almost every step of the process. That workflow is often opinionated, but it simplifies the adoption of some of the core principles of DevOps. It encourages development teams to understand the whole lifecycle of their application and allows operations teams to support a much wider variety of applications on the same runtime environment. And that delivers value across the board.

## Minimizing Deployment Artifacts

Docker alleviates the pain that is often induced by sprawling deployment artifacts. It does this by defining the result of a build as a single artifact, the Docker image, which contains everything your Linux application requires to run, and it executes this within a protected runtime environment. Containers can then be easily deployed on modern Linux distributions. But because of the clean split between the Docker client and server, developers can build their applications on non-Linux systems and still participate in the Linux container environment remotely.

Leveraging Docker allows software developers to create Docker images that, starting with the very first proof of concept, can be run locally, tested with automated tools, and deployed into integration or production environments without ever having to be rebuilt. This ensures that the application that is launched in production is the same as what was tested. Nothing needs to be recompiled or repackaged during the deployment workflow, which significantly lowers the risks normally inherent in most deployment processes. It also means that a single build step replaces a typically error-prone process that involves compiling and packaging multiple complex components for distribution.

Docker images also simplify the installation and configuration of an application. Every single piece of software that an application requires to run on a modern Linux kernel is contained in the image, and the dependency conflicts you might find in a traditional environment are eliminated. This makes it trivial to run multiple applications that rely on different versions of core system software on the same server.

## Optimizing Storage and Retrieval

Docker leverages filesystem layers to allow containers to be built from a composite of multiple images. This shaves a vast amount of time and effort off of many deployment processes by shipping only significant changes across the wire. It also saves considerable disk space by allowing multiple containers to be based on the same lower-level base image and then utilizing a copy-on-write process to write new or modified files into a top layer. This also helps in scaling an application by allowing more copies of an application to be started on the same servers without the need to push the binaries across the wire for each new instance.

To support image retrieval, Docker leverages the image registry for hosting images. While not revolutionary on the face of it, the registry helps split team responsibilities clearly along the lines embraced by DevOps principles. Developers can build their application, test it, ship the final image to the registry, and deploy the image to the production environment, while the operations team can focus on building excellent deployment and cluster management tooling that pulls from the registry, runs reliably, and ensures environmental health. Operations teams can provide feedback to developers and see the results of all the test runs at build time rather than waiting to find problems when the application is shipped to production. This enables both teams to focus on what they do best without a multiphase handoff process.

## The Payoff

As teams become more confident with Docker and its workflow, the realization often dawns that containers create a powerful abstraction layer between all of their software components and the underlying operating system. Organizations can begin to move away from having to create custom physical servers or VMs for most applications and instead deploy fleets of identical Docker hosts that can be used as a large pool of resources to dynamically deploy their applications to, with an ease that was previously unheard of.

When these process changes are successful, the cultural impact within a software engineering organization can be dramatic. Developers gain more ownership of their complete application stack, including many of the smallest details, which would typically be handled by a completely different group. Operations teams are simultaneously freed from trying to package and deploy complicated dependency trees with little or no detailed knowledge of the application.

In a well-designed Docker workflow, developers compile and package the application, which makes it much easier for them to focus on ensuring that the application is running properly in all environments, without worrying about significant changes introduced to the application environment by the operations teams. At the same time, operations teams are freed from spending most of their time supporting the application and can focus on creating a robust and stable platform for the application to run on. This dynamic creates a very healthy environment in which teams have clearer ownership and responsibilities in the application delivery process, and friction between them is significantly decreased.

Getting the process right has a huge benefit to both the company and the customers. With organizational friction removed, software quality is improved, processes are streamlined, and code ships to production faster. This all helps free the organization to spend more time providing a satisfying customer experience and delivering directly to the broader business objectives. A well-implemented Docker-based workflow can greatly help organizations achieve those goals.

## The Final Word

You should now be equipped with the knowledge that can help you make the transition to a modern, container-based build and deployment process. We encourage you to experiment with Docker on a small scale on your laptop or in a VM to further your understanding of how all of the pieces fit together, and then consider how you might begin to implement it for your organization. Every company or individual developer will follow a different path determined by their own needs and competencies. If you’re looking for guidance on how to start, we’ve found success in tackling the deployment problem first with simpler tools and then moving on to tasks like service discovery and distributed scheduling. Docker can be made as complicated as you like, but as with anything, starting simple usually pays off.

We hope you can now take all of this newfound knowledge and make good on some of Docker and Linux containers’ promises for yourself.

[1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch14.html#idm46803122954992-marker) Full URL: [_https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci_](https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci)
