# 9. The Path To Production Containers

## Chapter 9. The Path to Production Containers

Now that we’ve explored tooling for bringing up a stack of containers on a single host, we need to look at how we’d do this in a large-scale production environment. In this chapter, our goal is to show you how you might take containers to production based on our own experiences. There are myriad ways in which you will probably need to tailor this to your applications and environments, but this should provide you with a solid starting point to help you understand the Docker philosophy in practical terms.

## Getting to Production

Getting an application from the point where it is built and configurable to the point where it is running on production systems is one of the most mine-ridden steps in going from zero to production. This has traditionally been complicated but is vastly simplified by the shipping container model. If you can imagine what it was like to load goods into a ship to take across the ocean before shipping containers existed, you have a sense of what most traditional deployment systems look like. In that old shipping model, randomly sized boxes, crates, barrels, and all manner of other packages were loaded by hand onto ships. They then had to be manually unloaded by someone who could tell which pieces needed to be unloaded first so that the whole pile wouldn’t collapse like a [Jenga](https://en.wikipedia.org/wiki/Jenga) puzzle.

Shipping containers changed all that: we now have a standardized box with well-known dimensions. These containers can be packed and unloaded in a logical order, and whole groups of items arrive together when expected. The shipping industry built machinery to manage them very efficiently. The Docker deployment model is very similar. All Linux containers support the same external interface, and the tooling just drops them on the servers they are supposed to be on without any concern for what’s inside.

In the new model, when we have a running build of our application, we don’t have to write much custom tooling to kick off deployment. If we only want to ship it to one server, the `docker` command-line tooling will handle most of that for us. If we want to send it to more servers, then we will have to look at some of the more advanced tooling from the broader container ecosystem. In either case, there are things your application will need to be aware of and concerns you will need to consider before taking your containerized application to production.

There is a progression you will follow while getting your applications to production with Docker:

1. Locally build and test a Docker image on your development box.
2. Build your official image for testing and deployment, usually from a continuous integration (CI) or build system.
3. Push the image to a registry.
4. Deploy your Docker image to your server, then configure and start the container.

As your workflow evolves, you will eventually collapse all of those steps into a single fluid workflow:

1. Orchestrate the building, testing, and storage of images and the deployment of containers to production servers.

But there is a lot more to the story than that. At the most basic level, a production story must encompass three things:

* It must be a repeatable process. Each time you invoke it, it needs to do the same thing. Ideally, it will do the same thing for all your applications.
* It needs to handle configuration for you. You must be able to define your application’s configuration in a particular environment and then guarantee that it will ship that configuration on each deployment.
* It must deliver an executable artifact that can be started.

To accomplish that, there are several things you need to think about. We’ll try to help with that by presenting a framework you can use to think about your application in its environment.

## Docker’s Role in Production Environments

We’ve covered a lot of capabilities that Docker brings to the table, and we’ve talked about some general production strategies. Before we dive deeper into production containers, let’s look at how Docker fits into both a traditional and more modern production environment. If you are moving to Docker from a more traditional system, you can pick and choose which pieces you will delegate to Docker, to a deployment tool, or to a larger platform like Kubernetes or a cloud-based container system, or perhaps you’ll even decide to leave it on your more traditional infrastructure. We have successfully transitioned multiple systems from traditional deployments to containerized systems, and there is a wide spectrum of good solutions. But understanding the required components and what makes up the modern and more traditional variants will put you on the right path to making good choices.

In [Figure 9-1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch09.html#figure07-1) we describe several concerns that need to be considered in a production system, the modern components that address them, and the systems they might replace in a more traditional environment. We divide these up into concerns that are addressed by Docker itself and those we ascribe to what we call the _platform_. The platform is a system that usually wraps around a cluster of servers and presents a common interface for Linux container management. This might be a unified system like Kubernetes or Docker Swarm, or it might consist of separate components that combine to form a platform. During the transition to a fully containerized system with a scheduler, the platform might be more than one thing at a time. So let’s take a look at each of these concerns and see how they fit together.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098131814/files/assets/dur3_0901.png" alt="Docker&#x27;s Role in Production" height="359" width="600"><figcaption></figcaption></figure>

**Figure 9-1. Docker’s role in a production system**

In [Figure 9-1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch09.html#figure07-1), you can see that the application is sitting on the top of the stack. It relies on all of the concerns below it in a production system. In some cases, your environment may call these concerns out specifically, and in others, they may be addressed by something you don’t necessarily think of as filling that concern. But your production applications will rely on most of these in one way or another, and they will need to be addressed in your production environment. If you want to transition from an existing environment to a Linux Container-based environment, you’ll want to think about how you are providing these today and how they might be addressed in the new system.

We’ll start with familiar territory and then go from the bottom to the top. That familiar territory is your application. Your application is on the top! Everything else is there to deliver functionality to your application. After all, it’s the application that delivers business value, and everything else is there to make that possible, to facilitate doing it at scale and reliably, and to standardize how it works across applications. While the order of the items underneath your application is intentional, it’s not the case that each layer provides functionality to the one above. They are all providing that functionality to the application itself.

Because Linux containers and Docker can facilitate a lot of this functionality, containerizing your system will make many of these choices easier. As we get closer to the platform part of the stack, we’ll have more to think about, but understanding everything that lies below it will make that much more manageable.

Let’s start with application job control.

### Job Control

Job control is a fundamental requirement for a modern deployment. This is part of the blue block in the drawing of concerns. You basically can’t have a system of any kind without job control. It’s something we have more traditionally left to the operating system, or one of the Linux init systems (`systemd`, System V `init`, `runit`, BSD `rc` scripts, etc.) more specifically. We tell the operating system about a process we want to have running, and then we configure what the behavior should be when restarting it, reloading its configuration, and managing the lifecycle of the application.

When we want to start or stop the application, we rely on these systems to handle that. We also rely on them in some cases to keep the application running more robustly by, for example, restarting it when it fails. Different applications require different job control. In a traditional Linux system, you might use `cron` to start and stop jobs on a timed basis. `systemd` might be responsible for restarting your application if it crashes. But, how the system does so is up to the specifics of that system, and there are many different implementations to deal with, which is not great.

If we’re moving to the shipping container model, we want to be able to treat all jobs more or less the same way from the outside. We might need a little more metadata about them to get them to do the right thing, but we don’t want to look inside the container. The Docker engine provides a strong set of primitives around job control—for example, `docker container start`, `docker container stop`, `docker container run`, and `docker container kill`—which map to most of the critical steps in the lifecycle of an application. All of the platforms that are built around Docker containers, including Kubernetes, follow these lifecycle behaviors as well. We’ve placed this at the bottom of the stack of concerns because it’s fundamentally the lowest abstraction that Docker provides for your application. Even if we didn’t use any other part of Docker, this would be a big win because it’s the same for all applications and for all the platforms that run Docker containers.

### Resource Limits

Sitting above job control are resource limits. In Linux systems, it is possible to use [Linux control groups (cgroups)](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html) directly to manage resource limits if we want to, and some production environments have done exactly that. But more traditionally we have relied on things like [`ulimit`](https://linuxconfig.org/limit-user-environment-with-ulimit-linux-command) and the different settings of application runtime environments like the Java, Ruby, or Python VMs. In cloud systems, one of the early wins was that we could spin up individual virtual servers to limit the resources around a single business application. This was a nice innovation: no more noisy neighbor applications. Compared to containers, however, that is a pretty coarse-grained control.

With Linux containers, you can easily apply a wide set of resource controls to your containers via cgroups. It’s up to you to decide whether or not you’ll restrict your application’s access to things like memory, disk space, or I/O when running in production. However, we highly recommend that you take the time to do this once you’re comfortable with the needs of your application. If you don’t, you won’t be able to take advantage of one of the core features of containerized applications: running multiple applications on the same machine, largely without interference. As we’ve discussed, Docker gives this to you for free, and it’s a core part of what makes a container valuable. You can review the specific arguments that Docker uses to manage these resources in [Chapter 5](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch05.html#docker\_containers).

### Networking

There is a lot of detail about Docker networking in [Chapter 11](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch11.html#advanced\_topics), so we won’t touch on it too heavily here, but your containerized system will need to manage connecting your applications on the network. Docker provides a rich set of configuration options for networking. You should decide on one mechanism to use in your production environment and standardize that across containers. Trying to mix them is not an easy path to success. If you are running a platform like Kubernetes, then some of these decisions will be made for you. But the good part is that generally, the complexity of how the network is constructed is outside the concern of the application in the container. Consider that Docker or your bigger platform will provide this to you, and your application can work the same way inside the container on a local machine as it would in production as long as you follow a few rules:

1. Rely on Docker or your platform to map your ports dynamically and tell your application what they are mapped to. This is often provided to the application in the form of an environment variable.
2. Avoid protocols like FTP or RTSP that map random ports for return traffic. This is very difficult to support in a containerized platform.
3. Rely on the DNS provided to your container by Docker or your production runtime.

If you follow these rules, then generally your application can be quite agnostic about where it is deployed. Most production environments will provide you the ability to define the actual configuration and apply them at runtime. Docker Compose, Docker Swarm mode, Kubernetes, and cloud provider runtimes, like ECS, all do this for you.

### Configuration

All applications need to somehow have access to their configuration. There are two levels of configuration for an application. The lowest level is how it expects the Linux environment around it to be configured. Containers handle this by providing a _Dockerfile_ that we can use to build the same environment repeatably. In a more traditional system, we might have used a configuration management system like Chef, Puppet, or Ansible to do this. You may still use those systems in a containerized world, but you are usually not using them to provide dependencies to applications. That job belongs to Docker and the _Dockerfile_. Even if the contents of the _Dockerfile_ are different for different applications, the mechanism and tooling are all the same—and that’s a huge win.

The next level of configuration is the configuration directly applied to the application. We talked earlier about this in detail. Docker’s native mechanism is to use environment variables, and this works across all modern platforms. Some systems, notably, make it easier to rely on more traditional configuration files. Kubernetes, in particular, makes it relatively easy to rely on files, but we recommend against it if you truly want a portable, container-native application. We find that this can significantly impact the observability of the application and discourage you from relying on that crutch. There is more about the reasoning behind environment variables in [Chapter 13](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch13.html#container\_thinking).

### Packaging and Delivery

We’ll lump packaging and delivery together in our discussion here. This is an area where a containerized system has major advantages over a traditional one. Here we don’t have to stretch our imaginations to see the parallels to the shipping container model: we have a consistent package, the container image, and a standardized way to get them places—Docker’s registry and the `image pull` and `image push` facilities. In more traditional systems, we would have built handcrafted deployment tooling, some of which we hopefully standardized across our applications. But if we needed to have a multilanguage environment, this would have been trouble. In your containerized environment, you’ll need to consider how you handle packaging your applications into images and how you store those images.

The easiest path for the latter is a paid subscription to a hosted, commercial image registry. If that’s acceptable to your company, then you should consider it. Several cloud providers, including Amazon, have image-hosting services that you can deploy inside your environment, which is another good option. You can, of course, also build and maintain an internal private registry, as we talked about in [“Running a Private Registry”](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#private\_registry). There is a broad ecosystem of providers available to you, and you should survey your options.

### Logging

Logging sits on the boundary of concerns that you can rely on Docker to provide in your containerized environment and concerns that the platform needs to manage. That’s because, as we detailed in [Chapter 6](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch06.html#exploring\_docker), Docker can collect all the logs from your containers and ship them somewhere. But by default, that somewhere is not even off of the local system. That might be great for a limited-size environment, and you could stop considering it there if local host storage is good enough for you. But your platform will be responsible for handling logs from lots of applications on lots of systems, so you’ll probably want to centralize these logs into a system that significantly improves visibility and simplifies troubleshooting. When designing this, refer back to [Chapter 6](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch06.html#exploring\_docker) for more details on logging. Some systems, like Kubernetes, are opinionated about the collecting of logs. But from the application’s standpoint, you only need to make sure it sends them to `stdout` or `stderr` and let Docker or the platform handle the rest.

### Monitoring

The first part of the system not neatly tied up in a bow by Docker or Linux containers in general is still improved by the standardization that Docker brings to the table. The ability to health-check applications in a standardized way, as discussed in [Chapter 6](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch06.html#exploring\_docker), means that the process for monitoring application health is simplified. In many systems, the platform itself handles monitoring, and the scheduler will dynamically shut down unhealthy containers and potentially move the workload to a different server or restart the workload on the same system. In older systems, containers are often monitored by existing systems like Nagios, Zabbix, or other traditional monitoring systems. As we showed in [Chapter 6](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch06.html#exploring\_docker), there are also newer options, including systems like Prometheus. The Application Performance Monitoring (APM) vendors, like New Relic, Datadog, or Honeycomb, all have first-class support for Linux containers and the applications that they contain as well. So if your application is already monitored by one of them, chances are that you don’t need to change much.

In older systems, it is generally engineers who are paged and respond to issues and make decisions about how to handle failed applications. In dynamic systems, this work generally moves into more automated processes that belong inside the platform. In a transitional period, your system may have both while moving to an automated system where engineers are paged only when the platform really can’t intervene. In any case, a human will still need to be the final line of defense. But the containerized system is much easier to handle when things do go wrong because the mechanisms are standardized across applications.

### Scheduling

How do you decide which services run on which servers? Containers are easy to move around because Docker provides such good mechanisms for doing so. And that opens up lots of possibilities for better resource usage, better reliability, self-healing services, and dynamic scaling. But something has to make those decisions.

In older systems, this was often handled with dedicated servers per service. You often configured a list of servers into the deployment scripts, and the same set of servers would receive the new application on each deployment. One-service-per-server models drove early virtualization in private data centers. Cloud systems encouraged the one-service-per-server model by making it easy to slice and dice servers into commodity virtual servers. Autoscaling in systems like AWS handled part of this dynamic behavior. But if you move to containers, where many services may be running on the same virtual server, scaling and dynamic behaviors at the server level don’t help you.

#### Distributed schedulers

Distributed schedulers leverage Docker to let you reason about your entire network of servers almost as if it were a single computer. The idea here is that you define some policies about how you want your application to run, and you let the system figure out where to run it and how many instances of it to run. If something goes wrong on a server or with the application, you let the scheduler start it up again on any available healthy resource that meets the application’s requirements. This fits more into Docker, Inc., founder [Solomon Hykes’s](https://www.linkedin.com/in/solomonhykes) original vision for Docker: a way to run your application anywhere without worrying about how it gets there. Generally, zero downtime deployment in this model is done in the [blue-green style](https://martinfowler.com/bliki/BlueGreenDeployment.html), where you launch the new generation of an application alongside the old generation and then slowly migrate work from the old stack to the new one.

Using the metaphor now [made famous by Kelsey Hightower](https://youtu.be/HlAXp0-M6SY?t=10m23s), the scheduler is the system that plays Tetris for you, placing services on servers for the best fit, on the fly.

While it was not the first—that honor goes to platforms like Mesos and Cloud Foundry—today [Kubernetes](https://kubernetes.io/), which came out of Google in 2014, is the undoubted leader when it comes to container-based schedulers. The early releases of Kubernetes took the lessons that Google learned from its own internal [Borg](https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes) system and brought those to the open source community. It was built on Docker and Linux containers from the beginning and supports not only Docker’s `containerd` but also a few of the other container runtimes—all of which use Docker containers. Kubernetes is a big system with a lot of moving pieces. There are many different commercial and cloud-based distributions of Kubernetes. The [Cloud Native Computing Foundation](https://landscape.cncf.io/members?category=certified-kubernetes-distribution,certified-kubernetes-hosted,certified-kubernetes-installer\&grouping=category) provides certifications to ensure that each distribution meets certain standards within the broader Kubernetes community. This space continues to change rapidly, and while Kubernetes is really powerful, it’s an actively evolving target that can be hard to stay on top of. If you are building a brand-new system from scratch, you will probably want to strongly consider Kubernetes. In the absence of other experience, if you are running on a cloud, your provider’s implementation will likely be the easiest path to follow. While we encourage you to consider it for any complex system, Kubernetes is not the only option.

Docker Swarm mode came out of Docker, Inc., in 2015 and is built as a Docker-native system from the ground up. It might be an attractive option if you are looking for a very simple orchestration tool that stays completely within the Docker platform and is supported by a single vendor. Docker Swarm mode has not seen much adoption in the market, and since Docker is integrating Kubernetes so heavily into its tooling, this is probably not as clear a path as it once was.

#### Orchestration

When we talk about schedulers, we often talk about not just their ability to match jobs to resources but their orchestration capabilities as well. By that, we mean the ability to command and organize applications and deployments across a whole system. Your scheduler might move jobs for you on the fly or allow you to run tasks on each server specifically. This was more commonly handled in older systems by specific orchestration tools.

In most modern container systems, all the orchestration tasks, including scheduling, are handled by the core cluster software, whether it be Kubernetes, Swarm, a cloud provider’s bespoke container-management system, or something else.

Of all the features delivered by the platform, scheduling is undoubtedly the most powerful. It also has the most impact on applications when moving them into containers. Many traditional applications are not designed to have service discovery and resource allocation change underneath them and require a significant number of changes to work well in a truly dynamic environment. For this reason, your move to a containerized system may not necessarily encompass moving to a scheduled platform initially. Often the best path to production containers lies in containerizing your applications while running inside the traditional system and then moving on to a more dynamic, scheduled system. This might mean initially running your applications as containers on the same servers they are currently deployed to, and then once that is working well, you can introduce a scheduler to the mix.

### Service Discovery

You can think of service discovery as the mechanism by which the application finds all the other services and resources it needs on the network. Rare is the application that has no dependency on anything else. Stateless, static websites are perhaps one of the only systems that may not need any service discovery. Nearly everything else needs to know something about the surrounding system and requires a way to discover that information. Most of the time this involves more than one system, but they are usually tightly coupled.

You might not think of them this way, but in traditional systems, load balancers are one of the primary means for service discovery. Load balancers are used for reliability and scaling, but they also keep track of all of the endpoints associated with a particular service. This is sometimes manually configured and sometimes more dynamic, but the way other systems find endpoints for a service is by using a known address or name for the load balancer. That’s a form of service discovery, and load balancers are a common way to do this in older systems. They often are used for this in modern environments, too, even if they don’t look much like traditional load balancers. Other means for service discovery in older systems are static database configurations or application configuration files.

As you saw back in [Figure 9-1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch09.html#figure07-1), Docker does not address service discovery in your environment, except when using Docker Swarm mode. For the vast majority of systems, service discovery is left to the platform. This means it’s one of the first things you’ll need to resolve in a more dynamic system. Containers are by nature easily moved, and that can break traditional systems if they were built around more statically deployed applications. Each platform handles this differently, and you’ll want to understand what works best with your system.

**NOTE**

[Docker Swarm (classic Swarm)](https://github.com/docker-archive/classicswarm) and [Docker Swarm mode](https://docs.docker.com/engine/swarm) are not the same things. We will discuss Docker Swarm mode in more detail in [Chapter 10](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch10.html#containers\_scale).

Some examples of service discovery mechanisms you might be familiar with include the following:

* Load balancers with well-known addresses
* Round-robin DNS
* DNS SRV records
* Dynamic DNS systems
* Multicast DNS
* Overlay networks with well-known addresses
* Gossip protocols
* Apple’s [Bonjour protocol](https://en.wikipedia.org/wiki/Bonjour\_\(software\))
* [Apache ZooKeeper](https://zookeeper.apache.org/)
* [HashiCorp’s Consul](https://www.consul.io/)
* [etcd](https://etcd.io/)

That’s a big list, and there are a lot more options than that. Some of these systems also do a lot more than just service discovery, which can confuse the issue. An example of service discovery that may be closer to hand while you’re trying to understand this concept is the linking mechanism used by Docker Compose in [Chapter 8](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch08.html#docker\_compose). This mechanism relies on a DNS system that the `dockerd` server supplies, which allows one service in Docker Compose to reference another peer service’s name and return the correct container IP address. Kubernetes, at its simplest, also has a system that works like this, with injected environment variables. But these are the simplest forms of discovery on modern systems.

Often you find that the interface to these systems relies on having well-known names and/or ports for a service. You might call out to _http://service-a.example.com_ to reach service A on a well-known name. Or you might call out to _http://services.example.com:service-a-port_ to reach the same service on a well-known name and port. Modern environments often handle this differently. Usually, within a new system, this process will be managed and fairly seamless. And it’s frequently easy for new applications to call out of the platform to more traditional systems, but sometimes it’s not as easy going the other way. Often, the best initial system (though not necessarily longer term) is one in which you present dynamically configured load balancers that are easily reachable by systems in your older environment. Kubernetes provides for this in the form of `Ingress` routes and might be one path to consider if you are using that platform.

Examples of this include the following:

* Kubernetes’s [`Ingress` controllers](https://oreil.ly/7ucPN),[1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch09.html#idm46803137249216) including [Traefik](https://oreil.ly/RbuvY)[2](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch09.html#idm46803137246480) or [Contour](https://projectcontour.io/), among others
* [Linkerd](https://linkerd.io/) service mesh
* Standalone [Sidecar service discovery](https://github.com/NinesStack/sidecar) with Lyft’s [Envoy](https://github.com/envoyproxy/envoy) proxy
* [Istio](https://istio.io/) service mesh and Lyft’s Envoy

If you are running a blended modern and traditional system, getting traffic into the newer containerized system is generally the harder problem to solve and the one you should think through first.

### Production Wrap-Up

Many people will start by using simple Docker orchestration tools. However, as the number of containers and frequency with which you deploy containers grows, the appeal of distributed schedulers will quickly become apparent. Tools like Kubernetes allow you to abstract individual servers and whole data centers into large pools of resources in which to run container-based tasks.

There are undoubtedly many other worthy projects out there in the deployment space. But these are the most commonly cited and have the most publicly available information at the time of this writing. It’s a fast-evolving space, so it’s worth taking a look around to see what new tools are being shipped.

In any case, you should start by getting a Linux container infrastructure up and running and then look at outside tooling. Docker’s built-in tooling might be good enough for you. We suggest using the lightest-weight tool for the job, but having flexibility is a great place to be, and Linux containers are increasingly supported by more and more powerful tooling.

## Docker and the DevOps Pipeline

So once we have considered and implemented all of that functionality, we should have our production environment in robust shape. But how do we know it works? One of Docker’s key promises is the ability to test your application and all of its dependencies in exactly the operating environment it would have in production. It can’t guarantee that you have properly tested external dependencies like databases, nor does it provide any magical test framework, but it can make sure that your libraries and other code dependencies are all tested together. Changing underlying dependencies is a critical place where things go wrong, even for organizations with strong testing discipline. With Docker, you can build your image, run it on your development box, and then test the same image in your continuous-integration pipeline before shipping it to production servers.

Testing your containerized application is not much more complicated than testing your application itself, as long as your test environment is designed to manage Linux container workloads. Next, let’s cover one example of how you might do this.

### Quick Overview

Let’s draw up an example production environment for a fictional company. We’ll try to describe something similar to the environment at a lot of companies, with Docker thrown into the mix for illustration purposes.

Our fictional company’s environment has a pool of production servers that run Docker daemons and an assortment of applications deployed there. There are multiple build and test workers that are tied to the pipeline coordination server. We’ll ignore deployment for now and talk about it once our fictional application has been tested and is ready to ship.

[Figure 9-2](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch09.html#docker\_testing\_workflow\_chart) shows what a common workflow looks like for testing containerized applications, including the following steps:

1. A build is triggered by some outside means—for example, from a webhook call from a source code repository or a manual trigger by a developer.
2. The build server kicks off a container image build.
3. The image is created on the local server.
4. The image is tagged with a build or version number or a commit hash.
5. A new container, based on the newly built image, is configured to run the test suite.
6. The test suite is run against the container, and the result is captured by the build server.
7. The build is marked as passing or failing.
8. Passed builds are shipped to an image registry or other storage mechanism.

You’ll notice that this isn’t too different from common patterns for testing applications. At a minimum, you need to have a job that can kick off a test suite. The steps we’re adding here are just to create a container image first and invoke the test suite inside of the container.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098131814/files/assets/dur3_0902.png" alt="A typical workflow when testing with Docker" height="338" width="600"><figcaption></figcaption></figure>

**Figure 9-2. Docker testing workflow chart**

Let’s look at how this works for the application we’re deploying at our fictional company. We just updated our application and pushed the latest code to our Git repository. We have a post-commit hook that triggers a build on each commit, so that job is kicked off on the build server, which is also running the `dockerd` daemon. The job on the build server assigns the task to a test worker. The worker doesn’t have `dockerd` running, but it has the `docker` command-line tool installed. So we run our `docker image build` against the remote `dockerd` daemon, generating a new image on the remote Docker server.

**NOTE**

You should build your container image exactly as you’ll ship it to production. If you need to make concessions for testing, they should be externally provided switches, either via environment variables or through command-line arguments. The whole idea is to test the exact build that you’ll ship, so this is a critical point.

Once the image has been built, our test job will create and run a new container based on our new production image. Our image is configured to run the application in production, but we need to run a different command for testing. That’s OK! Docker lets us do that simply by providing the command at the end of the `docker container run` command. In production, our imaginary container would start `supervisor`, which in turn would start up an `nginx` instance and some Ruby unicorn web server instances behind that. But for testing, we don’t need that `nginx`, and we don’t need to run our web application. Instead, our build job invokes the container like this:

```
$ docker container run -e ENVIRONMENT=testing -e API_KEY=12345 \
    -it awesome_app:version1 /opt/awesome_app/test.sh
```

We called `docker container run`, but we did a couple of extra things here, too. We passed a couple of environment variables into the container: `ENVIRONMENT` and `API_KEY`. These can either be new or overrides for the ones Docker already exports for us. We also asked for a particular tag—in this case, `version1`. That will make sure we build on top of the correct image even if another build is running simultaneously. Then we override the command that our container was configured to start in the _Dockerfile_’s `CMD` line. Instead, we call our test script, _/opt/awesome\_app/test.sh_. Although it is not necessary in this example, you should note that in some cases you will need to override the _Dockerfile_’s `ENTRYPOINT` (`--entrypoint`) to run something other than the default command for that container.

**TIP**

Always pass the precise Docker tag (usually a version or commit hash) for your image into the test job. If you always use `latest`, then you won’t be able to guarantee that another job has not moved that tag just after your build was kicked off. If you use the most precise tag possible, then you can be sure you’re testing the right build of the application.

A critical point to make here is that `docker container run` will exit with the exit status of the command that was invoked in the container. That means we can just look at the exit status to see if our tests were successful. If your test suite is properly designed, this is probably all you need. If you need to run multiple steps, or the exit code can’t be relied on, one way to handle this is to capture all of the output of the test run into a file and then sift through the output to look for status messages. Our fictional build system does just that. We write out the output from the test suite, and our _test.sh_ echoes either `Result: SUCCESS!` or `Result: FAILURE!` on the last line to signify if our tests passed. If you need to rely on this mechanism, be sure to look for some output string that won’t appear by happenstance in your normal test suite output. If we need to look for “success,” for example, we should limit it to looking at the last line of the file, and maybe also ensure that the whole line matched the exact output we would normally expect. In this case, we look at just the last line of the file and find our success string, so we mark the build as passed.

There is one more container-specific step. We want to take our passed build and push that image to our registry. The registry is the interchange point between builds and deployments. It also allows us to share the image with our peers and other builds that might be built on top of it. But for now, let’s just think of it as the place where we put and tag successful builds. Our build script will now do a `docker image tag` to give the image the right build tag(s), potentially including `latest`, and then perform a `docker image push` to push the build to the registry.

That’s it! As you can see, there is not much to this compared with testing a normal application. We took advantage of Docker’s client/server model to invoke the test on a different server from our primary test server, and we wrapped up our tests into a consolidated shell script to generate our output status. Overall it is very similar to most other modern build system approaches.

The most critical takeaway is that our fictional company’s system makes sure that they only ship applications whose test suites have passed on the same Linux distribution, with the same libraries and the same build settings. That container might then also be tested against any outside dependencies like databases or caches without having to mock them. None of this guarantees success, but it gets us a lot closer to that than the dependency roulette often experienced by production deployment systems that are not built on container technology.

**NOTE**

If you use Jenkins for continuous integration or are looking for a good way to test scaling Docker, there are many [plug-ins](https://plugins.jenkins.io/) for Docker, Mesos, and Kubernetes that are worth investigating. Many hosted, commercial platforms now provide containerized CI environments as well, including [CircleCI](https://circleci.com/) and [GitHub Actions](https://github.blog/2022-02-02-build-ci-cd-pipeline-github-actions-four-steps).

### Outside Dependencies

But what about those external dependencies that we glossed over? Things like the database, or Memcached or Redis instances that we need to run our tests against our container? If our fictional company’s application needs a database to run, or a Memcached or Redis instance, we need to solve that external dependency to have a clean test environment. It would be nice to use the container model to support that dependency. With some work, you can do this with tools like [Docker Compose](https://github.com/docker/compose), which we described in detail in [Chapter 8](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch08.html#docker\_compose). In Docker Compose, our build job could express some dependencies between containers, and then Compose will connect them seamlessly.

Being able to test your application in an environment that looks like where it will live is a huge win. Compose makes this pretty easy to set up. You’ll still need to rely on your own language’s testing framework for the tests, but the environment is really easy to orchestrate.

## Wrap-Up

Now that we’ve surveyed how a containerized application interacts with the outside environment, and where the boundaries lie in each of those areas, we’re ready to explore how Docker clusters can be built to support the global, always-on, on-demand nature of many modern technology operations.

[1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch09.html#idm46803137249216-marker) Full URL: [_https://kubernetes.io/docs/concepts/services-networking/ingress_](https://kubernetes.io/docs/concepts/services-networking/ingress)

[2](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch09.html#idm46803137246480-marker) Full URL: [_https://doc.traefik.io/traefik/providers/kubernetes-ingress_](https://doc.traefik.io/traefik/providers/kubernetes-ingress)
