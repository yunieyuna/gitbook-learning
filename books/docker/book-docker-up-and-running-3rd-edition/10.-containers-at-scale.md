# 10. Containers At Scale

## Chapter 10. Containers at Scale

A major strength of containers is their ability to abstract away the underlying hardware and operating system so that your application is not constrained to any particular host or environment. It facilitates scaling a stateless application not just horizontally within your data center but also across cloud providers without many of the traditional barriers you would encounter. True to the shipping container metaphor, a container on one cloud looks like a container on another.

Many organizations find turnkey cloud deployments of Linux containers appealing because they can gain many of the immediate benefits of a scalable container-based platform without needing to completely build something in-house. Even though this is true, the barrier is actually pretty low for building your own platform in the cloud or in your own data center, and we’ll cover some options for doing that shortly.

The major public cloud providers have all worked to support Linux containers natively in their offerings. Some of the largest efforts to support Linux containers in the public cloud include the following:

* [Amazon Elastic Container Service](https://aws.amazon.com/ecs)
* [Google Cloud Run](https://cloud.google.com/run)
* [Azure Container Apps](https://azure.microsoft.com/en-us/services/container-apps)

Many of the same companies also have robust hosted Kubernetes offerings like these:

* [Amazon Elastic Kubernetes Service](https://aws.amazon.com/eks)
* [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine)
* [Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service)

It’s trivial to install Docker on a Linux instance in one of the public clouds. But getting Docker onto the server is usually just one step in creating a full production environment. You could do this completely on your own, or you could use the many tools available from the major cloud providers, Docker, Inc., and the broader container community. Much of the tooling will work equally well in either a public cloud or your own data center.

In the realm of schedulers and more complex tooling systems, we have plenty of choices for systems that replicate much of the functionality you would get from a public cloud provider. Even if you run in a public cloud, there are some compelling reasons why you might choose to run your own Linux container environment rather than use one of the off-the-shelf offerings.

In this chapter, we’ll cover some options for running Linux containers at scale, first going through the much simpler Docker Swarm mode and then diving into some more advanced tools like Kubernetes and some of the larger cloud offerings. All of these examples should give you a view of how you can leverage Docker to provide an incredibly flexible platform for your application workloads.

## Docker Swarm Mode

After building the container runtime in the form of the Docker engine, the engineers at Docker turned to the problems of orchestrating a fleet of individual Docker hosts and effectively packing those hosts full of containers. The first tool that evolved from this work was called Docker Swarm. As we explained early on, and rather confusingly, there are now two things called “Swarm,” both of which come from Docker, Inc.

The original standalone Docker Swarm is now commonly referred to as [Docker Swarm (classic)](https://github.com/docker-archive/classicswarm), but there is a second “Swarm” implementation that is more specifically called [Swarm mode](https://docs.docker.com/engine/swarm). Instead of being a separate product, this is built into the Docker client. The built-in Swarm mode is a lot more capable than the original Docker Swarm and is intended to replace it entirely. Swarm mode has the major advantage of not requiring you to install anything separately. You already have this clustering capability on any of your systems that are running Docker! This is the Docker Swarm implementation that we’ll focus on here. Hopefully, now that you know that there have been two different Docker Swarm implementations, you won’t get confused by contradictory information on the internet.

The idea behind Docker Swarm mode is to present a single interface to the `docker` client tool but have that interface be backed by a whole cluster rather than a single Docker daemon. Swarm is primarily aimed at managing clustered computing resources via the Docker tools. It has grown a lot since its first release and now contains several scheduler plug-ins with different strategies for assigning containers to hosts, and it comes with some basic service discovery built in. But it remains only one building block of a more complex solution.

Swarm clusters can contain one or more managers that act as the central management hub for your Docker cluster. It is best to set up an odd number of managers. Only one manager will act as the cluster leader at a time. As you add more nodes to Swarm, you are merging them into a single, cohesive cluster that can be easily controlled with the Docker tooling.

Let’s get a Swarm cluster up and running. To start, you will need three or more Linux servers that can talk to each other over the network. Each of these servers should be running recent releases of Docker Community Edition from the official Docker software repositories.

**TIP**

Refer to [Chapter 3](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch03.html#installing\_docker) for details on installing the `docker-ce` packages on Linux.

For this example, we will use three Ubuntu servers running `docker-ce`. The very first thing you’ll need to do is `ssh` to the server that you want to use as the Swarm manager, and then run the `swarm init` command using the IP address for your Swarm manager:

```
$ ssh 172.17.4.1
…

ubuntu@172.17.4.1:$ sudo docker swarm init --advertise-addr 172.17.4.1

Swarm initialized: current node (hypysglii5syybd2zew6ovuwq) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-14……a4o55z01zq 172.17.4.1:2377

To add a manager to this swarm, run 'docker swarm join-token manager'
and follow the instructions.
```

**WARNING**

There are steps that you must take to secure a Docker Swarm mode cluster, which we are not covering here. Before you run Docker Swarm mode on any long-lived systems, make sure that you understand the options and have taken proper steps to secure the environment.

**TIP**

In many of this chapter’s examples, you must use the correct IP addresses for your manager and worker nodes.

This step will initialize the Swarm manager and give you the token that is required for nodes that want to join the cluster. Make note of this token somewhere safe, like a password manager. Don’t worry too much if you lose this token; you can always get it again by running the following command on the manager:

```
sudo docker swarm join-token --quiet worker
```

You can inspect your progress so far by running your local `docker` client pointed at the new manager node’s IP address:

```
$ docker -H 172.17.4.1 system info
```

```
…
Swarm: active
  NodeID: l9gfcj7xwii5deveu3raf4782
  Is Manager: true
  ClusterID: mvdaf2xsqwjwrb94kgtn2mzsm
  Managers: 1
  Nodes: 1
  Default Address Pool: 10.0.0.0/8
  SubnetSize: 24
  Data Path Port: 4789
  Orchestration:
   Task History Retention Limit: 5
  Raft:
   Snapshot Interval: 10000
   Number of Old Snapshots to Retain: 0
   Heartbeat Tick: 1
   Election Tick: 10
  Dispatcher:
   Heartbeat Period: 5 seconds
  CA Configuration:
   Expiry Duration: 3 months
   Force Rotate: 0
  Autolock Managers: false
  Root Rotation In Progress: false
  Node Address: 172.17.4.1
  Manager Addresses:
   172.17.4.1:2377
…
```

You can also list all of the nodes that are currently in the cluster with the following command:

```
$ docker -H 172.17.4.1 node ls

ID      HOSTNAME      STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION
l9…82 * ip-172-17-4-1 Ready  Active       Leader         20.10.7
```

At this point, you can add the two additional servers as workers to the Swarm cluster. This is what you’d do in production if you were going to scale up, and Swarm makes this pretty easy:

```
$ ssh 172.17.4.2 \
    "sudo docker swarm join --token SWMTKN-1-14……a4o55z01zq 172.17.4.1:2377"

This node joined a swarm as a worker.

$ ssh 172.17.4.3 \
    "sudo docker swarm join --token SWMTKN-1-14……a4o55z01zq 172.17.4.1:2377"

This node joined a swarm as a worker.
```

**TIP**

Adding additional managers is important and can be done as easily as adding the workers. You just need to pass in the manager join token instead of the worker join token. You can get this token by running `docker swarm join-token manager` on any of the active nodes.

If you rerun `docker node ls`, you should now see that you have a total of three nodes in your cluster, and only one of them is marked as the `Leader`:

```
$ docker -H 172.17.4.1 node ls

ID      HOSTNAME      STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION
l9…82 * ip-172-17-4-1 Ready  Active       Leader         20.10.7
3d…7b   ip-172-17-4-2 Ready  Active                      20.10.7
ip…qe   ip-172-17-4-3 Ready  Active                      20.10.7
```

This is all that’s required to get a Swarm cluster up and running in Swarm mode ([Figure 10-1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch10.html#figure10-1))!

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098131814/files/assets/dur3_1001.png" alt="Simple Docker Swarm Cluster" height="344" width="600"><figcaption></figcaption></figure>

**Figure 10-1. Simple Docker Swarm mode cluster**

The next thing you should do is create a network for your services to use. There is a default network called `ingress` in Swarm, but it is very easy to create additional ones for better isolation:

```
$ docker -H 172.17.4.1 network create --driver=overlay default-net

ckwh5ph4ksthvx6843ytrl5ik

$ docker -H 172.17.4.1 network ls

NETWORK ID     NAME              DRIVER    SCOPE
494e1a1bf8f3   bridge            bridge    local
xqgshg0nurzu   default-net       overlay   swarm
2e7d2d7aaf0f   docker_gwbridge   bridge    local
df0376841891   host              host      local
n8kjd6oa44fr   ingress           overlay   swarm
b4720ea133d6   none              null      local
```

Up to this point, we’ve just been getting the underlying pieces running, and so far we haven’t deployed any real business logic. So let’s launch your first service into the cluster. You can do that with a command like this:

```
$ docker -H 172.17.4.1 service create --detach=true --name quantum \
    --replicas 2 --publish published=80,target=8080 --network default-net \
    spkane/quantum-game:latest

tiwtsbf270mh83032kuhwv07c
```

The service we’re launching with starts containers that host the [_Quantum game_](https://github.com/stared/quantum-game). This is a browser-based puzzle game that uses real quantum mechanics. We hope that this is a more interesting example than another Hello World!

**WARNING**

Although we’re using the `latest` tag in many of these examples, you shouldn’t ever use this tag in production. It is convenient for this book since we can easily push out updates to the code, but this tag floats and cannot be pinned to a specific release over a long period. That means if you use `latest`, then your deployments are not repeatable! It can also easily lead to a situation where you don’t have the same version of an application running on all the servers.

Let’s see where those containers ended up by running `docker service ps` against the service name you created:

```
$ docker -H 172.17.4.1 service ps quantum

ID    NAME      IMAGE       NODE          DESIRED… CURRENT… ERROR PORTS
rk…13 quantum.1 spkane/qua… ip-172-17-4-1 Running  Running…
lz…t3 quantum.2 spkane/qua… ip-172-17-4-2 Running  Running…
```

Swarm mode uses a routing mesh between the nodes to automatically route traffic to a container that can serve the request. When you specify a published port in the `docker service create` command, the mesh makes it possible to hit this port on any of your three nodes and will route you to the web application. Notice that we said any of the _three_ nodes even though you only have two instances running. Traditionally, you would have also had to set up a separate reverse proxy layer to accomplish this, but its batteries are included with Swarm mode.

To prove it, you can test the service now by pointing a web browser to the IP address of any of your nodes:

```
http://172.17.4.1/
```

If everything is working as expected, you should see the first puzzle board for [the _Quantum Game_](https://quantumgame.io/):

```
To get a list of all the services, we can use +service ls+:
```

```
$ docker -H 172.17.4.1 service ls

ID    NAME    MODE       REPLICAS IMAGE                      PORTS
iu…9f quantum replicated 2/2      spkane/quantum-game:latest *:80->8080/tcp
```

This gives us a summary view of the most commonly needed information, but sometimes that’s not enough. Docker maintains a lot of other metadata about services, just like it does for containers. We can get detailed information about a service with `service inspect`:

```
$ docker -H 172.17.4.1 service inspect --pretty quantum
```

```
ID:    iuoh6oxrec9fk67ybwuikutqa
Name:    quantum
Service Mode:  Replicated
 Replicas:  2
Placement:
UpdateConfig:
 Parallelism:  1
 On failure:  pause
 Monitoring Period: 5s
 Max failure ratio: 0
 Update order:      stop-first
RollbackConfig:
 Parallelism:  1
 On failure:  pause
 Monitoring Period: 5s
 Max failure ratio: 0
 Rollback order:    stop-first
ContainerSpec:
 Image:    spkane/quantum-game:latest@sha256:1f57…4a8c
 Init:    false
Resources:
Networks: default-net
Endpoint Mode:  vip
Ports:
  PublishedPort = 80
  Protocol = tcp
  TargetPort = 8080
  PublishMode = ingress
```

There is a lot of info here, so let’s point out some of the more important things. First, we can see that this is a replicated service with two replicas, just like we saw in the `service ls` command. We can also see that Docker is health-checking the service at 5-second intervals. Running an update to the service will use the `stop-first` method, which means it will take our service first to _N_−1 and then spin up a new instance to take us back to _N_. You might want to always run in _N_+1 mode so that you are never down a node during updates in production. You can change that with the `--update-order=start-first` option to the `service update` command. It will exhibit the same behavior in a rollback scenario, and we can likewise change that with `--rollback-order=start-first`.

In a real-world scenario, we not only need to be able to launch our service, but we also need to be able to scale it up and down. It would be a shame if we had to redeploy it to do that, not to mention it could introduce any number of additional issues. Luckily, Swarm mode makes it easy to scale our services with a single command. To double the number of instances you have running from two to four, you can simply run this:

```
$ docker -H 172.17.4.1 service scale --detach=false quantum=4

quantum scaled to 4
overall progress: 4 out of 4 tasks
1/4: running   [==================================================>]
2/4: running   [==================================================>]
3/4: running   [==================================================>]
4/4: running   [==================================================>]
verify: Service converged
```

**NOTE**

We used `--detach=false` in the previous command so that it was easier to see what was happening.

We can now use `service ps` to show us that Swarm did what we asked. This is the same command we ran earlier, but now we should have more copies running! But wait, didn’t we ask for more copies than we have nodes?

```
$ docker -H 172.17.4.1 service ps quantum

ID    NAME      IMAGE        NODE          DESIRED… CURRENT… ERROR PORTS
rk…13 quantum.1 spkane/quan… ip-172-17-4-1 Running  Running…
lz…t3 quantum.2 spkane/quan… ip-172-17-4-2 Running  Running…
mh…g8 quantum.3 spkane/quan… ip-172-17-4-3 Running  Running…
cn…xb quantum.4 spkane/quan… ip-172-17-4-1 Running  Running…
```

You’ll notice that you have two services running on the same host. Did you expect that? This may not be ideal for host resiliency, but by default Swarm will prioritize ensuring that you have the number of instances that you requested over spreading individual containers across hosts when possible. If you don’t have enough nodes, you will get multiple copies on each node. In a real-world scenario, you need to think carefully about placement and scaling. You might not be able to get away with running multiple copies on the same host when you lose a whole node. Would your application still serve users at that reduced scale?

When you need to deploy a new release of your software, you will want to use the `docker service update` command. There are a lot of options for this command, but here’s one example:

```
$ docker -H 172.17.4.1 service update --update-delay 10s \
    --update-failure-action rollback --update-monitor 5s \
    --update-order start-first --update-parallelism 1 \
    --detach=false \
    --image spkane/quantum-game:latest-plus quantum

quantum
overall progress: 4 out of 4 tasks
1/4: running   [==================================================>]
2/4: running   [==================================================>]
3/4: running   [==================================================>]
4/4: running   [==================================================>]
verify: Service converged
```

Running this command will cause Swarm to update your service one container at a time, pausing in between each update. Once this is done, you should be able to open up the service’s URL in a new private or incognito browsing session (to sidestep the browser’s local cache) and see that the game background is now green instead of blue.

Great, you have now successfully applied an update, but what if something were to go wrong? We might need to deploy a previous release to get back to working order. You could now roll back to the previous version, with the correct blue background, by using the `service rollback` command, which we discussed in passing a little bit earlier:

```
$ docker -H 172.17.4.1 service rollback quantum

quantum
rollback: manually requested rollback
overall progress: rolling back update: 4 out of 4 tasks
1/4: running   [>                                                  ]
2/4: running   [>                                                  ]
3/4: running   [>                                                  ]
4/4: running   [>                                                  ]
verify: Service converged
```

That’s about as nice a rollback mechanism as you could ask for a stateless service. You don’t have to keep track of the previous version; Docker does that for you. All you need to do is tell it to roll back and it pulls the previous metadata out of its internal storage and performs the rollback. Just like during deployment, Docker can health-check your containers to make sure the rollback is working correctly.

**NOTE**

This rollback mechanism will always go back to the last deployed version, so if you run it multiple times in a row, it will just flip between two versions.

Building on `docker service` is a command called `docker stack`, which enables you to deploy a specially designed _docker-compose.yaml_ file to a Docker Swarm mode or Kubernetes cluster. If you go back and check out the Git repo that we used in [Chapter 8](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch08.html#docker\_compose), we can deploy a modified version of that container stack into our current Swarm mode cluster:

```
$ git clone https://github.com/spkane/rocketchat-hubot-demo.git \
    --config core.autocrlf=input
```

Inside that repository is a directory called _stack_ that contains a modified version of the _docker-compose.yaml_ file that we used earlier:

```
$ cd rocketchat-hubot-demo/stack
```

If you wanted to spin up this setup in the Swarm mode cluster, you could run the following command:

```
$ docker -H 172.17.4.1 stack deploy --compose-file docker-compose-stack.yaml \
    rocketchat

Creating network rocketchat_default
Creating service rocketchat_hubot
Creating service rocketchat_mongo
Creating service rocketchat_rocketchat
Creating service rocketchat_zmachine
```

Now you can list what stacks are in the cluster and see what services were added by the stack:

```
$ docker -H 172.17.4.1 stack ls

NAME         SERVICES   ORCHESTRATOR
rocketchat   4          Swarm

$ docker -H 172.17.4.1 service ls

ID    NAME         …  …  IMAGE                              PORTS
iu…9f quantum      … 2/2 spkane/quantum-game:latest         *:80->8080/tcp
nh…jd …_hubot      … 1/1 rocketchat/hubot-rocketchat:latest *:3001->8080/tcp
gw…qv …_mongo      … 1/1 spkane/mongo:4.4
m3…vd …_rocketchat … 1/1 rocketchat/rocket.chat:5.0.4       *:3000->3000/tcp
lb…91 …_zmachine   … 1/1 spkane/zmachine-api:latest
```

**NOTE**

This stack is for basic demonstration purposes and has not been well tested for this use case; however, it should give you an idea of how you could assemble something similar.

You may notice that it takes a while for all the containers to come up and that Hubot will continue to restart. This is expected since Rocket.Chat has not been configured yet. The Rocket.Chat setup is covered in [Chapter 8](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch08.html#docker\_compose).

At this point, you could point your web browser at port 3000 on one of the Swarm nodes (e.g., _http://172.17.4.1:3000/_ in these examples), and you should see the initial setup page for Rocket.Chat.

You can see all the containers that are managed by the stack, with `docker stack ps`:

```
$ docker -H 172.17.4.1 stack ps -f "desired-state=running" rocketchat

ID    NAME           IMAGE                    NODE … CURRENT STATE           …
b5…1h …_hubot.1      rocketchat/hubot-rocket… …-1  … Running 14 seconds ago
eq…88 …_mongo.1      spkane/mongo:4.4         …-2  … Running 11 minutes ago
5x…8u …_rocketchat.1 rocketchat/rocket.chat:… …-3  … Running 11 minutes ago
r5…x4 …_zmachine.1   spkane/zmachine-api:lat… …-4  … Running 12 minutes ago
```

When you are done, you can go ahead and tear down the stack like this:

```
$ docker -H 172.17.4.1 stack rm rocketchat

Removing service rocketchat_hubot
Removing service rocketchat_mongo
Removing service rocketchat_rocketchat
Removing service rocketchat_zmachine
Removing network rocketchat_default
```

**NOTE**

If you try to immediately spin everything back up, you might get some unexpected errors. Just waiting a few moments should fix things while the cluster finishes tearing down the old network for the stack, etc.

So, what happens if one of your servers is experiencing an issue and you need to take it offline? In this case, you can easily drain all the services off of a single node by using the `--availability` option to the `docker node update` command.

Let’s take a look at the nodes that you have in the cluster again:

```
 docker -H 172.17.4.1 node ls

ID      HOSTNAME      STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION
l9…82 * ip-172-17-4-1 Ready  Active       Leader         20.10.7
3d…7b   ip-172-17-4-2 Ready  Active                      20.10.7
ip…qe   ip-172-17-4-3 Ready  Active                      20.10.7
```

Let’s also check where our containers are currently running:

```
$ docker -H 172.17.4.1 service ps -f "desired-state=running" quantum

ID    NAME        IMAGE       NODE          DESIRED… CURRENT… ERROR   PORTS
sc…1h quantum.1   spkane/qua… ip-172-17-4-1 Running  Running…
ax…om quantum.2   spkane/qua… ip-172-17-4-2 Running  Running…
p4…8h quantum.3   spkane/qua… ip-172-17-4-3 Running  Running…
g8…tw quantum.4   spkane/qua… ip-172-17-4-1 Running  Running…
```

**TIP**

In the previous command, we used a filter so that the output showed only the currently running processes. By default, Docker will also show you the previous containers that were running in a tree format so that you can see things like updates and rollbacks in the output.

If you have determined that the server at 172.17.4.3 needs downtime, you could drain the tasks of that node and move them to another host by modifying the `availability` state to `drain` in Swarm:

```
$ docker -H 172.17.4.1 node update --availability drain ip-172-17-4-3

ip-172-17-4-3
```

If we inspect the node, we can see that the availability is now set to `drain`:

```
$ docker -H 172.17.4.1 node inspect --pretty ip-172-17-4-3
```

```
ID:      ipohyw73hvf70td9licnls9qe
Hostname:                ip-172-17-4-3
Joined at:               2022-09-04 16:59:52.922451346 +0000 utc
Status:
 State:      Ready
 Availability:           Drain
 Address:    172.17.4.3
Platform:
 Operating System:  linux
 Architecture:    x86_64
Resources:
 CPUs:      2
 Memory:    7.795GiB
Plugins:
 Log:    awslogs, fluentd, gcplogs, gelf, journald, json-file, local,
         logentries, splunk, syslog
 Network:    bridge, host, ipvlan, macvlan, null, overlay
 Volume:    local
Engine Version:    20.10.7
TLS Info:
 TrustRoot:
…

 Issuer Subject:  …
 Issuer Public Key:  …
```

You might be wondering what effect that has on the service. We told one of the nodes to stop running copies of the service, and they either have to go away or migrate somewhere else. What did it do? We can look at the details of our service again and see that all the running containers on that host have been moved to a different node:

```
$ docker -H 172.17.4.1 service ps -f "desired-state=running" quantum

ID    NAME        IMAGE       NODE          DESIRED… CURRENT… ERROR   PORTS
sc…1h quantum.1   spkane/qua… ip-172-17-4-1 Running  Running…
ax…om quantum.2   spkane/qua… ip-172-17-4-2 Running  Running…
p4…8h quantum.3   spkane/qua… ip-172-17-4-2 Running  Running…
g8…tw quantum.4   spkane/qua… ip-172-17-4-1 Running  Running…
```

At this point, it is safe to bring down the node and do whatever work is required to make it healthy again. When you are ready to add the node back into the Swarm cluster, you can do so by running the following:

```
$ docker -H 172.17.4.1 node update --availability active ip-172-17-4-3

ip-172-17-4-3
```

We’ll spare you from reinspecting the node at the moment, but you can always rerun the `node inspect` command if you want to see what this looks like.

**WARNING**

When you add a node back to the cluster, containers will not automatically balance! However, a new deployment or update should result in the containers being evenly spread across the nodes.

Once you are done, you can remove your service and network with the following commands:

```
$ docker -H 172.17.4.1 service rm quantum

quantum

$ docker -H 172.17.4.1 network rm default-net

default-net
```

And then verify that they are both indeed completely gone:

```
$ docker -H 172.17.4.1 service ps quantum

no such service: quantum

$ docker -H 172.17.4.1 network ls

NETWORK ID     NAME              DRIVER    SCOPE
494e1a1bf8f3   bridge            bridge    local
2e7d2d7aaf0f   docker_gwbridge   bridge    local
df0376841891   host              host      local
n8kjd6oa44fr   ingress           overlay   swarm
b4720ea133d6   none              null      local
```

That’s all for now! At this point, you can safely tear down all of the servers that were a part of your Swarm cluster if you no longer need them.

That was kind of a whirlwind tour, but it covers the basics of using Swarm mode in Docker Engine and should help get you started building your own Docker clusters wherever you might decide to use them.

## Kubernetes

Now let’s take some time to look at Kubernetes. Since its release to the public during [DockerCon](https://events.docker.com/events/dockercon) 2014, Kubernetes has grown rapidly and is now probably the most widely adopted of the container platforms. It is not the oldest or most mature product today—that distinction goes to Mesos, which first launched in 2009 before containers were in widespread use—but Kubernetes was purpose-built for containerized workloads, has a great mix of functionality that is ever evolving, and also enjoys a very strong community that includes many early Docker and Linux container adopters. This mix has helped significantly increase its popularity over the years. At DockerCon EU 2017, Docker, Inc., announced that Kubernetes support will be coming to the Docker Engine tooling itself. Docker Desktop is capable of spinning up a single-node Kubernetes cluster, and the client can deploy container stacks for development purposes. This provides a nice bridge for developers who use Docker locally but deploy to Kubernetes.

Like Linux itself, Kubernetes is available in several distributions, both free and commercial. There is a wide variety of distributions that are available and supported to varying degrees. Kubernetes widespread adoption means that it now has some pretty nice tooling for local development installations.

**TIP**

The Kubernetes coverage in this book is intended to provide some basic guidance on how you can integrate your Linux container workflow with Kubernetes, but we do not go into a lot of detail about the Kubernetes ecosystem here. We highly recommend reading [_Kubernetes: Up & Running_, by Brendan Burns et al. (O’Reilly)](https://www.oreilly.com/library/view/kubernetes-up-and/9781098110192), or any of the other great materials out there to familiarize yourself with all the relevant concepts and terminology.

### Minikube

Minikube was one of the original tools for managing a local Kubernetes installation and is the first one that we will be focusing on here. Most of the concepts that you’ll learn while working with Minikube can be applied to any Kubernetes implementation, including the options that we’ll discuss after Minikube, so it’s a great place to start.

**TIP**

There are many other options for running a local Kubernetes cluster. We are starting with `minikube` because the container or VM that it spins up is a pretty standard single-node Kubernetes install. In addition to the tools that we will be discussing in this section, we highly recommend exploring [k3s](https://k3s.io/), [k3d](https://k3d.io/), [k0s](https://k0sproject.io/), and [microk8s](https://microk8s.io/) as well.

#### What Is Minikube?

Minikube is a whole distribution of Kubernetes for a single instance. It manages a container or VM on your computer that presents a working Kubernetes installation and allows you to use all the same tooling that you would use in a production system. In scope, it’s a little bit like Docker Compose: it will let you stand up a whole stack locally. It goes one step further than Compose, though, in that it has all the production APIs. As a result, if you run Kubernetes in production, you can have an environment on your desktop that is reasonably close in function, if not in scale, to what you are running in production.

Minikube is fairly unique in that all of the distribution is controlled from a single binary you download and run locally. It will automatically detect which containerization or VM manager you have locally and will set up and run a container or VM with all of the necessary Kubernetes services in it. That means getting started with it is pretty simple.

So let’s install it!

#### Installing Minikube

Most of the installation is the same across all platforms because once you have the tools installed, they will be your gateway to the VM running your Kubernetes installation. To get started, just skip to the section that applies to your operating system. Once you have the tool up and running, you can follow the shared documentation.

We need two tools to use Minikube effectively: `minikube` and `kubectl`. For our simple installation, we’re going to leverage the fact that both of these commands are static binaries with no outside dependencies, which makes them easy to install.

**NOTE**

There are several other ways to install Minikube. We’re going to show you what we think is the simplest path on each platform. If you have strong preferences about how to install these applications, feel free to use your preferred approach. On Windows, for example, you might prefer to use the [Chocolatey package manager](https://chocolatey.org/), or the [Snap package system](https://snapcraft.io/) on Linux.

**macOS**

Just as in [Chapter 3](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch03.html#installing\_docker), you will need to have Homebrew installed on your system. If you don’t, go back to [Chapter 3](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch03.html#installing\_docker) and make sure you have it set up. Once you do, it’s trivial to install the `minikube` client:

```
$ brew install minikube
```

This will cause Homebrew to download and install Minikube. It will look something like this depending, on your configuration:

```
==> Downloading https://ghcr.io/v2/homebrew/core/kubernetes-cli/…/1.25.0
Already downloaded: …/Homebrew/downloads/…kubernetes-cli…manifest.json
==> Downloading https://ghcr.io/v2/homebrew/core/kubernetes-cli/blobs/sha256…
Already downloaded: …/Homebrew/downloads/…kubernetes-cli--1.25…bottle.tar.gz
==> Downloading https://ghcr.io/v2/homebrew/core/minikube/manifests/1.26.1
Already downloaded: …/Homebrew/downloads/…minikube-1.26.1.…_manifest.json
==> Downloading https://ghcr.io/v2/homebrew/core/minikube/blobs/sha256:…
Already downloaded: …/Homebrew/downloads/…minikube--1.26.1…bottle.tar.gz
==> Installing dependencies for minikube: kubernetes-cli
==> Installing minikube dependency: kubernetes-cli
==> Pouring kubernetes-cli--1.25.0.arm64_monterey.bottle.tar.gz
  /opt/homebrew/Cellar/kubernetes-cli/1.25.0: 228 files, 52.8MB
==> Installing minikube
==> Pouring minikube--1.26.1.arm64_monterey.bottle.tar.gz
==> Caveats
Bash completion has been installed to:
  /opt/homebrew/etc/bash_completion.d
==> Summary
  /opt/homebrew/Cellar/minikube/1.26.1: 9 files, 70.6MB
==> Running `brew cleanup minikube`…
Disable this behavior by setting HOMEBREW_NO_INSTALL_CLEANUP.
Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).
==> Caveats
==> minikube
Bash completion has been installed to:
  /opt/homebrew/etc/bash_completion.d
```

That’s it! Let’s test to make sure it’s in your path:

```
$ which minikube
/opt/homebrew/bin/minikube
```

**NOTE**

Homebrew on _arm64_ systems install into _/opt/homebrew/bin_ instead of _/usr/local/bin_.

If you don’t get a response, you will need to make sure you have _/usr/local/bin_ and _/opt/homebrew/bin_ in your `PATH` environment variable. Assuming that passes, you now have the `minikube` tool installed.

`kubectl` should have been automatically installed since it is a dependency of `minikube`, but you can also do it explicitly with `brew` as well. Generally, the version of `kubectl` in Homebrew will match the current release of `minikube`, so using `brew install` should help prevent mismatches:

```
$ brew install kubernetes-cli
```

We’ll test that the same way we tested `minikube`:

```
$ which kubectl
/opt/homebrew/bin/kubectl
```

We’re good to go!

**Windows**

As with installing Docker Desktop on Windows, you may want to install Hyper-V or another supported virtualization platform to run a Kubernetes VM. To install `minikube`, you simply download the binary and put it in a place you have in your `PATH` so that you can execute it on the command line. You can download the [most recent release of `minikube` from GitHub](https://github.com/kubernetes/minikube/releases/latest). You’ll want to rename the Windows executable that you download to _minikube.exe_; otherwise, you’ll be doing a lot more typing than you probably want!

**TIP**

You can find more details about the Windows install process and that binary executable from the [Minikube install documentaton](https://minikube.sigs.k8s.io/docs/start).

You then need to get the latest Kubernetes CLI tool, `kubectl`, to interact with your distribution. Unfortunately, there is not a _/latest_ path for downloading that. So, to make sure you have the latest version, you need to [get the latest version](https://storage.googleapis.com/kubernetes-release/release/stable.txt) from the website and then plug it into a URL, like this:

_https://storage.googleapis.com/kubernetes-release/release/\<VERSION>/bin/windows/amd64/kubectl.exe_.

Once you’ve downloaded that, you again need to make sure it’s accessible from your `PATH` to make the rest of our exploration easier.

**Linux**

On Linux, you will want to have Docker installed and should consider installing either KVM (Linux’s Kernel-based Virtual Machine) or VirtualBox so that `minikube` can create and manage a Kubernetes VM for you. Because `minikube` is just a single binary, once you have it installed, there is no need to install any additional packages. And, because `minikube` is a statically linked binary, it should pretty much work on any distribution you want to run it on. Although we could do all the installation in a one-liner, we are going to break it up into a few steps to make it easier to understand and troubleshoot. Note that at the time of this writing, the binary is hosted on _googleapis_, which usually maintains very stable URLs. So, here we go:

```
# Download the file, save as 'minikube'
$ curl -Lo minikube \
  https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64

# Make it executable
$ chmod +x minikube

# Move it to /usr/local/bin
$ sudo mv minikube /usr/local/bin/
```

You’ll need to make sure that _/usr/local/bin_ is in your path. Now that we have `minikube`, we also need to fetch `kubectl`, which we can do like this:

```
# Get the latest version number
$ KUBE_VERSION=$(curl -s \
    https://storage.googleapis.com/kubernetes-release/release/stable.txt)

# Fetch the executable
$ curl -LO \
    https://storage.googleapis.com/kubernetes-release/\
release/$(KUBE_VERSION)/bin/linux/amd64/kubectl

# Make it executable
$ chmod +x kubectl

# Move it to /usr/local/bin
$ sudo mv kubectl /usr/local/bin/
```

**NOTE**

One of the URLs in the example has been continued on the following line so that it fits in the margins. You may find that you need to reassemble the URL and remove the backslashes for the command to work properly in your environment.

That’s it for installation—we’re ready to go.

#### Running Kubernetes

Now that we have the `minikube` tool, we can use it to bootstrap our Kubernetes cluster. This is normally pretty straightforward. You usually don’t need to do any configuration beforehand. In this example, you will see that `minikube` decided to use the _docker driver_, although there are others that could be selected.

To start `minikube`, go ahead and run the following:

```
$ minikube start

  minikube v1.26.1 on Darwin 12.5.1 (arm64)
  Automatically selected the docker driver. Other choices: parallels, ssh, …
  Using Docker Desktop driver with root privileges
  Starting control plane node minikube in cluster minikube
  Pulling base image …
  Downloading Kubernetes v1.24.3 preload …
    > preloaded-images-k8s-v18-v1…: 342.82 MiB / 342.82 MiB  100.00% 28.22 M
    > gcr.io/k8s-minikube/kicbase: 348.00 MiB / 348.00 MiB  100.00% 18.13 MiB
    > gcr.io/k8s-minikube/kicbase: 0 B [________________________] ?% ? p/s 16s
  Creating docker container (CPUs=2, Memory=4000MB) …
  Preparing Kubernetes v1.24.3 on Docker 20.10.17 …
    ▪ Generating certificates and keys …
    ▪ Booting up control plane …
    ▪ Configuring RBAC rules …
  Verifying Kubernetes components…
    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
  Enabled addons: storage-provisioner, default-storageclass
  Done! kubectl is now configured to use "minikube" cluster and
     "default" namespace by default
```

So what did we just do? Minikube packs a lot into that one command. In this case, we launched a single Linux container that is providing us a functioning Kubernetes installation on our local system. If we had used one of the virtualization drivers with `minikube`, then we would have created a complete VM running Kubernetes instead on a single container.

It then runs all of the necessary components of Kubernetes inside Linux containers on the host. You can easily explore the `minikube` container or VM to see what you got:

```
$ minikube ssh

docker@minikube:~$
```

On your Kubernetes cluster, you probably won’t be using SSH to get into the command line that often. But we want to see what’s installed and get a handle on the fact that when we run `minikube`, we’re controlling an environment that is running many processes. Let’s take a look at what is running on the Docker instance on our Kubernetes cluster:

```
docker@minikube:~$ docker container ls

…ID   IMAGE       COMMAND               …  NAMES
48…cf ba…57      "/storage-provisioner" … k8s_storage-provisioner_storage-…
4e…8d ed…e8      "/coredns -conf /etc…" … k8s_coredns_coredns-6d4b75cb6d-…
1d…3d …pause:3.6 "/pause"               … k8s_POD_coredns-6d4b75cb6d-…
82…d3 7a…dc      "/usr/local/bin/kube…" … k8s_kube-proxy_kube-proxy-…
27…10 …pause:3.6 "/pause"               … k8s_POD_kube-proxy-zb6w2_kube-…
15…ce …pause:3.6 "/pause"               … k8s_POD_storage-provisioner_kube-…
ff…3d f9…55      "kube-controller-man…" … k8s_kube-controller-manager_kube-…
33…c5 …pause:3.6 "/pause"               … k8s_POD_kube-controller-manager-…
30…97 a9…df      "etcd --advertise-cl…" … k8s_etcd_etcd-minikube_kube-…
f5…41 53…a6      "kube-apiserver --ad…" … k8s_kube-apiserver_kube-apiserver-…
5b…08 8f…73      "kube-scheduler --au…" … k8s_kube-scheduler_kube-scheduler-…
87…cc …pause:3.6 "/pause"               … k8s_POD_kube-apiserver-…
5a…14 …pause:3.6 "/pause"               … k8s_POD_etcd-minikube_kube-…
6f…0c …pause:3.6 "/pause"               … k8s_POD_kube-scheduler-…
```

We won’t dive too much into what each component is, but by now you should hopefully see how the mechanism works. Also, it’s pretty easy to upgrade the components since they are just containers, are versioned, and can be pulled from an upstream container repository.

Go ahead and exit the shell that you have on the Minikube system:

```
docker@minikube:~$ exit
```

**minikube commands**

In the interest of space and time, we won’t go through all of the commands for `minikube`. We encourage you to run it without any options, take a look at the output, and play around with what’s available. That being said, let’s take a quick look at some of the most interesting commands. We’ll cover a few more later in the course of installing an application stack, but here’s a quick survey.

To see what was going on inside the system, earlier we used `minikube ssh`, which is great for debugging or inspecting the system directly. Without directly accessing the Minikube system, we can always check on the cluster status using another `minikube` command:

```
$ minikube status

minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured
```

This shows us that everything is looking good. Two other useful commands include:

| Command                 | Action                                                          |
| ----------------------- | --------------------------------------------------------------- |
| `minikube ip`           | Retrieve the IP address of the Minikube VM.                     |
| `minikube update-check` | Check your version of Minikube against the most recent release. |

To apply an upgrade, you can simply use the same mechanism you used to install it originally.

Critically, the `minikube status` command also shows us that the `kubeconfig` is properly configured. We will need this so that `kubectl` knows how to connect to our cluster.

We started the Kubernetes cluster with `minikube start`. As you might expect, following the style of Docker CLI arguments, `minikube stop` will stop all the Kubernetes components and the Linux container or VM. To completely clean up your environment, you can also delete the cluster by running `minikube delete`.

#### Kubernetes Dashboard

Now that we have Minikube up and running, we don’t just have the command-line tools to interact with; we have a whole Kubernetes Dashboard installed that we can explore. We can reach it via the `minikube dashboard` command. Go ahead and run that—it should launch your web browser pointed to the correct IP address and port of the Kubernetes Dashboard! There is a lot of stuff on the dashboard, and we’re not able to cover it all, but feel free to click around and explore. Depending on your previous exposure to Kubernetes, some of the terms in the dashboard’s sidebar will be familiar to you, but many of them may be completely foreign. If you don’t have a computer in front of you, [Figure 10-2](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch10.html#figure10-2) shows a screenshot of what an empty Minikube installation looks like from the Service link in the dashboard sideboard.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098131814/files/assets/dur3_1002.png" alt="Kubernetes Dashboard" height="250" width="600"><figcaption></figcaption></figure>

**Figure 10-2. Kubernetes Dashboard (example)**

If you explore the Nodes link under Cluster in the left sidebar, you should see a single node in the cluster, named `minikube`. This is the container or VM that we started, and the dashboard, like the other components, is hosted in one of the containers we saw when we connected to the Minikube system earlier. We’ll take another look at the dashboard when we’ve deployed something into our cluster.

**NOTE**

Kubernetes exposes almost everything that you see on the dashboard with the `kubectl` command as well, which makes it very scriptable with shell scripts.

For example, running `kubectl get services` or `kubectl get nodes` should show you the same information that you can see on the dashboard.

While clicking around, you may notice that Kubernetes itself shows up as a component inside the system, just like your applications will.

**NOTE**

You will need to type Ctrl-C to exit the `minikube dashboard` process and return to your terminal prompt.

#### Kubernetes containers and pods

Now that we have a Kubernetes cluster up and running, and you’ve seen how easy that is to do locally, we need to pause to talk about a concept that Kubernetes adds on top of the container abstraction. Kubernetes came out of the experiences that Google had running its massive platform. Google encountered most of the situations you might see in a production platform and had to work out concepts to make it easier to understand and solve the kinds of problems you run into when managing a large installation. In doing so, Google created a complex set of new abstractions. Kubernetes embraces many of these and thus has a whole vocabulary unto itself. We won’t try to get into all of these, but it’s important to understand the most central of these new abstractions—​a concept that sits a layer above the container and is known as a _pod_.

**NOTE**

The term _pod_ came about because the Docker mascot is Moby, the whale, and a group of whales is called a _pod_.

In Kubernetes parlance, a pod is one or more containers sharing the same cgroups and namespaces. You can also isolate the containers themselves from one another inside the same pod using cgroups and namespaces. A pod is intended to encapsulate all of the processes or applications that need to be deployed together to create a functioning unit, which the scheduler can then manage. All of the containers in the pod can talk to one another on `localhost`, which eliminates any need to discover one another. So why not just deploy a big container with all the applications inside it? The advantage of a pod over a massive container is that you can still resource-limit the individual application separately and leverage the large library of public Linux containers to construct your application.

Additionally, Kubernetes administrators often leverage the pod abstraction to have a container run on pod startup to make sure things are configured properly for the others, to maintain a shared resource, or to announce the application to others, for example. This allows you to make finer-grained containers than you might if you have to group things into the same container. Another nice part of the pod abstraction is the ability to share mounted volumes.

Pods have a life span much like a Linux container. They are essentially ephemeral and can be redeployed to new hosts according to the lifecycle of the application or the host it runs on. Containers in a pod even share the same IP address when facing the outside world, which means they look like a single entity from the network level. Just as you would run only one instance of an application per container, you generally run one instance of a given container inside a pod. The easiest way to think about pods is that they are a group of Linux containers that work together as if they were one container, for most purposes. If you need only one container, then you still get a pod deployed by Kubernetes, but that pod contains only one container. The nice thing about this is that there is only one abstraction as far as the Kubernetes scheduler is concerned: the pod. Containers are managed by some of the runtime pieces that construct the pod and also by the configuration that you use to define them.

One critical difference between a pod and a container is that you don’t construct pods in a build step. They are a runtime abstraction that is defined in a JSON or YAML manifest and lives only inside Kubernetes. So you build your Linux containers and send them to a registry, then define and deploy your pods using Kubernetes. In reality, you don’t usually directly describe a pod either; the tools generate it for you through the concept of a deployment. But the pod is the unit of execution and scheduling in a Kubernetes cluster. There is a lot more to it, but that’s the basic concept, and it’s probably easiest to understand with a simple example. The pod abstraction is more complicated than thinking of your system in terms of individual containers, but it can be pretty powerful.

#### Let’s deploy something

When working with pods in Kubernetes, we usually manage them through the abstraction of a _deployment_. A deployment is just a pod definition with some additional information, including health monitoring and replication configuration. It contains the definition of the pod and a little metadata about it. So let’s look at a basic deployment and get it running.

The simplest thing we can deploy on Kubernetes is a pod that contains just one container. We are going to use the [`httpbin`](https://httpbin.org/) application to explore the basics of deployment on Kubernetes, and we’ll call our deployment `hello-minikube`.

We’ve used the `minikube` command, but to get things done on Kubernetes itself, we now need to leverage the `kubectl` command we installed earlier:

```
$ kubectl create deployment hello-minikube \
    --image=kennethreitz/httpbin:latest --port=80

deployment.apps/hello-minikube created
```

To see what that did for us, we can use the `kubectl get all` command to list the most important objects that are now in our cluster:

```
$ kubectl get all

NAME                                 READY   STATUS    RESTARTS   AGE
pod/hello-minikube-ff49df9b8-svl68   1/1     Running   0          2m39s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   98m

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/hello-minikube   1/1     1            1           2m39s

NAME                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/hello-minikube-ff49df9b8   1         1         1       2m39s
```

With that one command, Kubernetes created a deployment, a ReplicaSet to manage scaling, and a pod. We want to ensure that our pod shows a `STATUS` of `Running`. If yours isn’t, just wait and run the command a couple more times until you see the status change. The `service/kubernetes` entry is a running service that represents Kubernetes itself. But where is our service? We can’t get to it yet. It’s essentially in the same state a Linux container would be if you didn’t tell it to expose any ports. So we need to tell Kubernetes to do that for us:

```
$ kubectl expose deployment hello-minikube --type=NodePort
service/hello-minikube exposed
```

This has now created a service we can reach and interact with. A _service_ is a wrapper for one or more deployments of an application and can tell us how to contact the application. In this case, we get a `NodePort`, which exposes a port on every node in the cluster that will be routed to the underlying pods. Let’s get Kubernetes to tell us how to get to it:

```
$ kubectl get services

NAME           TYPE      CLUSTER-IP     EXTERNAL-IP PORT(S)        AGE
hello-minikube NodePort  10.105.184.177 <none>      80:32557/TCP   8s
kubernetes     ClusterIP 10.96.0.1      <none>      443/TCP        107m
```

You might think you could now connect to _http://10.105.184.177:8080_ to get to our service. But those addresses are not reachable from your host system because of the container or VM in which Minikube is running. So we need to get `minikube` to tell us where to find the service:

```
$ minikube service hello-minikube --url
http://192.168.99.100:30616
```

**TIP**

In some configurations, you may see a message like this:

```
 Because you are using a Docker driver on darwin,
   the terminal needs to be open to run it.
```

This indicates that transparently wiring the networking from your host to the Kubernetes services is not possible at the moment, and you will need to leave the command running while you explore your application. You can use a local web browser or open up another terminal to run commands like `curl`.

When you are done, you can type Ctrl-C in the original terminal session to kill the `minikube service` command.

The nice thing about this command, like many of the other Kubernetes commands, is that it is scriptable and command-line friendly under normal circumstances. If we want to open it with `curl` on the command line, we can often just include the `minikube` command call in our request:

```
$ curl -H foo:bar $(minikube service hello-minikube --url)/get
```

```
{
  "args": {},
  "headers": {
    "Accept": "*/*",
    "Foo": "bar",
    "Host": "127.0.0.1:56695",
    "User-Agent": "curl/7.85.0"
  },
  "origin": "172.17.0.1",
  "url": "http://127.0.0.1:56695/get"
}
```

`httpbin` is a simple HTTP request and response API that can be used to test and confirm HTTP services. Not the world’s most exciting application, but you can see that we are able to contact our service and get a response back from it via `curl`.

This is the simplest use case. We didn’t configure much and relied on Kubernetes to do the right thing using its defaults. In the next step, we’ll take a look at something more complicated. But first, let’s shut down our new service and deployment. It takes two commands to do that: one to remove the service and the other to delete it:

```
$ kubectl delete service hello-minikube
service "hello-minikube" deleted

$ kubectl delete deployment hello-minikube
deployment.apps "hello-minikube" deleted

$ kubectl get all

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   138m
```

#### Deploying a realistic stack

Let’s now deploy something that looks more like a production stack. We’ll deploy an application that can fetch PDF documents from an S3 bucket, cache them on disk locally, and rasterize individual pages to PNG images on request, using the cached document. To run this application, we’ll want to write our cache files somewhere other than inside the container. We want to have them go somewhere a little more permanent and stable. And this time we want to make things repeatable so that we’re not deploying our application through a series of CLI commands that we need to remember and hopefully get right each time. Kubernetes, much like Docker Compose, lets us define our stack in one or more YAML files that contain all of the definitions we care about in one place. This is what you want in a production environment and is similar to what you’ve seen for the other production tools.

The service we’ll now create will be called `lazyraster` (as in “rasterize on demand”), and each time you see that in the YAML definition, you’ll know we’re referring to our application. Our persistent volume will be called `cache-data`. Again, Kubernetes has a huge vocabulary that we can’t entirely address here, but to make it clear what we’re looking at, we need to introduce two more concepts: `PersistentVolume` and `PersistentVolumeClaim`. A `PersistentVolume` is a physical resource that we provision inside the cluster. Kubernetes has support for many kinds of volumes, from local storage on a node to [Amazon Elastic Block Store (Amazon EBS) volumes](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html) on AWS and similar on other cloud providers. It also supports [Network File System (NFS)](https://en.wikipedia.org/wiki/Network\_File\_System) and other more modern network filesystems. A `PersistentVolume` stores data with a lifecycle that is independent of our application or deployments. This lets us store data that persists between application deployments. For our cache, that’s what we’ll use. A `PersistentVolumeClaim` is a link between the physical resource of the `PersistentVolume` and the application that needs to consume it. We can set a policy on the claim that allows either a single read/write claim or many read claims. For our application we just want a single read/write claim to our `cache-data` `PersistentVolume`.

**TIP**

If you want more detail about some of the concepts we’ve talked about here, the Kubernetes project maintains a [glossary](https://kubernetes.io/docs/reference/glossary/?fundamental=true) of all the terms involved in operating Kubernetes. This can be very helpful. Each entry in the glossary is also linked to much more in-depth detail on other pages.

You can check out the file we will be using in this section by running the following:

```
$ git clone \
    https://github.com/bluewhalebook/\
docker-up-and-running-3rd-edition.git --config core.autocrlf=input

Cloning into 'docker-up-and-running-3rd-edition'…
…

$ cd docker-up-and-running-3rd-edition/chapter_10/kubernetes
```

**NOTE**

The URL in the example has been continued on the following line so that it fits in the margins. You may find that you need to reassemble the URL and remove the backslashes for the command to work properly.

We will start by looking at the manifest YAML file, called _lazyraster-service.yaml_. The full manifest contains multiple YAML documents separated by `---`. We will discuss each section individually here.

#### Service definition

```
apiVersion: v1
kind: Service
metadata:
  name: lazyraster
  labels:
    app: lazyraster
spec:
  type: NodePort
  ports:
    - port: 8000
      targetPort: 8000
      protocol: TCP
  selector:
    app: lazyraster
```

The first section defines our `Service`. The second and third sections, which we’ll see in a moment, respectively define our `PersistentVolumeClaim` and then our actual `Deployment`. We’ve told Kubernetes that our service will be called `lazyraster` and that it will be exposed on port 8000, which maps to the actual 8000 in our container. We’ve exposed that with the `NodePort` mechanism, which simply makes sure that our application is exposed on the same port on each host, much like the `--publish` flag to `docker container run`. This is helpful with `minikube` since we’ll run only one instance, and the `NodePort` type makes it easy for us to access it from our computer just like we did earlier. As with many parts of Kubernetes, there are several options other than `NodePort`, and you can probably find a mechanism that’s ideal for your production environment. `NodePort` is good for `minikube`, but it might work well for more statically configured load balancers as well.

So, back to our `Service` definition. The `Service` is going to be connected to the `Deployment` via the `selector`, which we apply in the `spec` section. Kubernetes widely uses labels as a way to reason about similar components and to help tie them all together. Labels are key/value pairs that are arbitrarily defined and that can then be queried to identify pieces of your system. Here the `selector` tells Kubernetes to look for `Deployments` with the label `app: lazyraster`. Notice that we also apply the same label to the `Service` itself. That’s helpful if we want to identify all the components together later, but it’s the `selector` section that ties the `Deployment` to our `Service`. So we now have a `Service`, but it doesn’t do anything yet. We need more definitions to make Kubernetes do what we want.

#### PersistentVolumeClaim definition

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cache-data-claim
  labels:
    app: lazyraster
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
```

The next section defines our `PersistentVolumeClaim` and likewise the `PersistentVolume` that backs it. A `PersistentVolumeClaim` is a way to name a volume and claim that you have a token to access that particular volume in a particular way. Notice, though, that we didn’t define the `PersistentVolume` here. That’s because Kubernetes is doing that work for us using what it calls _Dynamic Volume Provisioning_. In our case, the use is pretty simple: we want a read/write claim to a volume, and we’ll let Kubernetes put that in a volume container for us. But you can imagine a scenario where an application is going to be deployed into a cloud provider and where dynamic provisioning would truly come into its own. In that scenario, we don’t want to have to make separate calls to have our volume created in the cloud for us. We want Kubernetes to handle that. That’s what Dynamic Volume Provisioning is all about. Here, it will just create a container for us to hold our persistent data, and mount it into our pod when we stake our claim. We don’t do a lot in this section except name it, ask for 100 MB of data, and tell Kubernetes it’s a read/write mount-once-only volume.

**NOTE**

There’s a large number of possible volume providers in Kubernetes. Which ones are available to you is in part determined by which provider or cloud service you are running on. You should take a look and see which ones make the most sense for you when you are preparing to head into production.

#### Deployment definition

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lazyraster
  labels:
    app: lazyraster
spec:
  selector:
    matchLabels:
      app: lazyraster
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: lazyraster
    spec:
      containers:
      - image: relistan/lazyraster:demo
        name: lazyraster
        env:
        - name: RASTER_RING_TYPE
          value: memberlist
        - name: RASTER_BASE_DIR
          value: /data
        ports:
        - containerPort: 8000
          name: lazyraster
        volumeMounts:
        - name: cache-data
          mountPath: /data
      volumes:
      - name: cache-data
        persistentVolumeClaim:
          claimName: cache-data-claim
```

The `Deployment` creates the pods for us and uses the Linux container for our application. We define some metadata about the application, including its name and one label, just like we did for the other definitions. We also apply another `selector` here to find the other resources we’re tied to. In the `strategy` section, we say we want to have a `RollingUpdate`, which is a strategy that causes our pods to be cycled through one by one during deployment. We could also pick `Recreate`, which would simply destroy all existing pods and then create new ones afterward.

In the `template` section, we define how to stamp out copies of this deployment. The container definition includes the Docker image name, the ports to map, volumes to mount, and some environment variables that the `lazyraster` application needs. The very last part of the `spec` asks to have our `PersistentVolumeClaim` named `cache-data-claim`.

And that’s it for the application definition. Now let’s stand it up!

**NOTE**

There are many more options and a rich set of directives you can specify here to tell Kubernetes how to handle your application. We’ve walked through a couple of simple options, but we encourage you to explore the Kubernetes documentation to learn more.

#### Deploying the application

Before we continue, let’s see what’s in our Kubernetes cluster by using the `kubectl` command:

```
$ kubectl get all

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   160m
```

We have only one thing defined at the moment, a service called `service/kubernetes`. A naming convention used widely in Kubernetes is to preface the type of object with the object `Kind`, which is sometimes shortened to a two- or three-letter abbreviation. Sometimes you will see `service` represented as `svc`. If you are curious, you can see all of the resources and their short names by running the command `kubectl api-resources`. So let’s go ahead and get our service, deployment, and volume into the cluster!

```
$ kubectl apply -f ./lazyraster-service.yaml

service/lazyraster created
persistentvolumeclaim/cache-data-claim created
deployment.apps/lazyraster created
```

That output looks like what we expected: we have a service, a persistent volume claim, and a deployment. So let’s see what’s in the cluster now:

```
$ kubectl get all

NAME                              READY   STATUS    RESTARTS   AGE
pod/lazyraster-644cb5c66c-zsjxd   1/1     Running   0          17s

NAME               TYPE      CLUSTER-IP     EXTERNAL-IP PORT(S)        AGE
service/kubernetes ClusterIP 10.96.0.1      <none>      443/TCP        161m
service/lazyraster NodePort  10.109.116.225 <none>      8000:32544/TCP 17s

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/lazyraster   1/1     1            1           17s

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/lazyraster-644cb5c66c   1         1         1       17s
```

You can see that a bunch more happened behind the scenes. And also, where is our volume or persistent volume claim? We have to ask for that separately:

```
$ kubectl get pvc

NAME             STATUS VOLUME    CAPACITY ACCESS MODES STORAGECLASS AGE
cache-data-claim Bound  pvc-1a…41 100Mi    RWO          standard     65s
```

**TIP**

`kubectl get all` does nothing of the sort. It would be more aptly named `get all-of-the-most-common-resources`, but there are several other resources you can fetch. The Kubernetes project hosts a handy [cheat sheet](https://kubernetes.io/docs/reference/kubectl/cheatsheet) to make this more discoverable.

So what about that `replicaset.apps` that appeared in the `get all` output? That is a ReplicaSet. A ReplicaSet is a piece of Kubernetes that is responsible for making sure that our application is running the right number of instances all the time and that they are healthy. We don’t normally have to worry about what happens inside the ReplicaSet because the deployment we created manages it for us. You can manage the ReplicaSet yourself if need be, but most of the time you won’t need to or want to.

We didn’t tell `kubectl` any specific number of instances, so we got one. And we can see that both the desired and current states match. We’ll take a look at that in a moment. But first, let’s connect to our application and see what we’ve got:

```
$ minikube service --url lazyraster
http://192.168.99.100:32185
```

You will probably get a different IP address and port back. That’s fine! This is very dynamic stuff. And that’s why we use the `minikube` command to manage it for us.

Also, remember that `minikube` will warn you if you need to keep the `service` command running while you explore the `lazyraster` service. So grab the address that came back, open your web browser, and paste it into the URL bar like this: _http://<192.168.99.100:32185>/documents/docker-up-and-running-public/sample.pdf?page=1_. You’ll need to substitute the IP and port into the URL to make it work for you.

You’ll need to be connected to the internet because the `lazyraster` application is going to go out to the internet, fetch a PDF from a public S3 bucket, and then render the first page from the document as a PNG in a process called _rasterization_. If everything worked, you should see a copy of the front cover of an earlier edition of this book! This particular PDF has two pages, so feel free to try changing the argument to `?page=2`. If you do that, you may notice it renders _much_ faster than the first page. That’s because the application is using our persistent volume to cache the data. You can also specify `width=2048` or ask for a JPEG instead of a PNG with `imageType=image/jpeg`. You could rasterize the front page as a very large JPEG, like this:

_http://<192.168.99.100:32185>/documents/docker-up-and-running-public/sample.pdf?page=1\&imageType=image/jpeg\&width=2048_

If you have a public S3 bucket with other PDFs in it, you can simply substitute the bucket name for `docker-up-and-running-public` in the URL to hit your bucket instead. If you want to play with the application some more, check out [the _Nitro_/_lazyraster_ repo on GitHub](https://github.com/Nitro/lazyraster).

#### Scaling up

In real life you don’t just deploy applications; you operate them as well. One of the huge advantages of scheduled workloads is the ability to scale them up and down at will, within the resource constraints available to the system. In our case, we only have one Minikube node, but we can still scale up our service to better handle load and provide more reliability during deployments. Kubernetes, as you might imagine, allows scaling up and down quite easily. For our service, we will need only one command to do it. Then we’ll take another look at the `kubectl` output and also at the Kubernetes Dashboard we introduced earlier so we can prove that the service scaled.

In Kubernetes, the thing we will scale is not the service; it’s the deployment. Here’s what that looks like:

```
$ kubectl scale --replicas=2 deploy/lazyraster
deployment.apps/lazyraster scaled
```

Great, that did something! But what did we get?

```
$ kubectl get deployment/lazyraster

NAME         READY   UP-TO-DATE   AVAILABLE   AGE
lazyraster   2/2     2            2           16m
```

We now have two instances of our application running. Let’s see what we got in the logs:

```
$ kubectl logs deployment/lazyraster

Found 2 pods, using pod/lazyraster-644cb5c66c-zsjxd
Trying to clear existing Lazyraster cached files (if any) in the background…
Launching Lazyraster service…
time="2022-09-10T21:14:16Z" level=info msg="Settings -----------------…
time="2022-09-10T21:14:16Z" level=info msg="  * BaseDir: /data"
time="2022-09-10T21:14:16Z" level=info msg="  * HttpPort: 8000"
…
time="2022-09-10T21:14:16Z" level=info msg="  * LoggingLevel: info"
time="2022-09-10T21:14:16Z" level=info msg="--------------------------…
…
time="2022-09-10T21:14:16Z" level=info msg="Listening on tcp://:6379"
…
```

We asked for logs for the deployment, but Kubernetes tells us two pods are running, so it simply picked one of them to show us the logs from. We can see the replica starting up. If we want to specify a particular instance to look at, we can ask for the logs for that pod with something like `kubectl logs pod/lazyraster-644cb5c66c-zsjxd`, using the output from `kubectl get pods` to find the pod in question.

We now have a couple of copies of our application running. What does that look like on the Kubernetes Dashboard? Let’s navigate there with `minikube dashboard`. Once we’re there, we’ll select “Workloads - Deployments” from the left sidebar and then click on the `lazyraster` deployment, which should display a screen that looks like [Figure 10-3](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch10.html#figure10-3).

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098131814/files/assets/dur3_1003.png" alt="Lazyraster Service Dashboard" height="298" width="600"><figcaption></figcaption></figure>

**Figure 10-3. `lazyraster` service dashboard (example)**

We encourage you to click around some more in the Kubernetes Dashboard to see what else is presented. With the concepts you’ve picked up here, a lot should be clearer now, and you can probably figure out some more on your own. Likewise, `kubectl` has a lot of other options available as well, many of which you’ll need in a real production system. The [cheat sheet we discussed earlier](https://kubernetes.io/docs/reference/kubectl/cheatsheet) is a real lifesaver here!

As always, you can type Ctrl-C at any time to exit the running `minikube dashboard` command.

#### kubectl API

We haven’t shown you an API yet, and, as we’ve discussed with Docker, it can be really useful to have a simple API to interact with for scripting, programming, and other general operational needs. You can write programs to talk directly to the Kubernetes API, but for local development and other simple use cases, you can use `kubectl` as a nice proxy to Kubernetes, and it presents a clean API that is accessible with `curl` and JSON command-line tools. Here’s an example of what you can do:

```
$ kubectl proxy
Starting to serve on 127.0.0.1:8001
```

We’ve now got `kubectl` itself serving up a web API on the local system! You’ll need to read more about what’s possible, but let’s get it to show us the individual instances of the `lazyraster` application. We can do that by opening the following URL in a browser or by using `curl` in another terminal window: _http://localhost:8001/api/v1/namespaces/default/endpoints/lazyraster_.

There is a lot of output here, but the part we care about is the `subsets` section:

```
{
…
  "subsets": [
    {
      "addresses": [
        {
          "ip": "172.17.0.5",
          "nodeName": "minikube",
          "targetRef": {
            "kind": "Pod",
            "namespace": "default",
            "name": "lazyraster-644cb5c66c-zsjxd",
            "uid": "9631395d-7e68-47fa-bb9f-9641d724d8f7"
          }
        },
        {
          "ip": "172.17.0.6",
          "nodeName": "minikube",
          "targetRef": {
            "kind": "Pod",
            "namespace": "default",
            "name": "lazyraster-644cb5c66c-pvcmj",
            "uid": "e909d424-7a91-4a74-aed3-69562b74b422"
          }
        }
      ],
      "ports": [
        {
          "port": 8000,
          "protocol": "TCP"
        }
      ]
    }
  ]
}
```

What’s interesting here is that we can see that both instances are running on the Minikube host and that they have different IP addresses. If we were building a cloud-native application that needed to know where the other instances of the application were running, this would be a good way to do that.

You can type Ctrl-C at any time to exit the running `kubectl proxy` processes, and then you can remove the deployment and all of its components by running the following command. It may take Kubernetes a minute or so to delete everything and return you to the terminal prompt:

```
$ kubectl delete -f ./lazyraster-service.yaml

service "lazyraster" deleted
persistentvolumeclaim "cache-data-claim" deleted
deployment.apps "lazyraster" deleted
```

And then finally, you can go ahead and remove your Minikube cluster if you are done with everything in it for now:

```
$ minikube delete

  Deleting "minikube" in docker …
  Deleting container "minikube" …
  Removing /Users/spkane/.minikube/machines/minikube …
  Removed all traces of the "minikube" cluster.
```

**TIP**

Kubernetes is a really big system, with great community involvement. We’ve just shown you the tip of the iceberg with Minikube, but if you are interested, there are many other Kubernetes distributions and tools to explore.

### Docker Desktop-Integrated Kubernetes

Docker Desktop comes with support for an integrated single-node Kubernetes cluster that can be run by simply enabling an option in the application preferences.

The integrated Kubernetes cluster is not easily configurable, but it does provide a very accessible option for those who simply need to verify some basic functionality against a current Kubernetes installation.

To enable Docker Desktop’s built-in Kubernetes functionality, launch Docker Desktop and then open up Preferences from the Docker whale icon in your task/menu bar. Then select the Kubernetes tab, click Enable Kubernetes, and finally click the “Apply & Restart” button to make the required changes to the VM. The first time you do this, Docker will utilize the [`kubeadm`](https://kubernetes.io/docs/reference/setup-tools/kubeadm) command to set up the Kubernetes cluster.

**NOTE**

If you are interested in a bit more information about how the Docker Desktop-integrated Kubernetes is set up, Docker has a good [blog post](https://www.docker.com/blog/how-kubernetes-works-under-the-hood-with-docker-desktop) that covers some of these details.

This will create a new `kubectl` context called `docker-desktop` and should automatically switch you to this context.

You can confirm which context you are currently set to by running the following:

```
$ kubectl config current-context

docker-desktop
```

If you need to change the current context, you can do so like this:

```
$ kubectl config use-context docker-desktop --namespace=default

Switched to context "docker-desktop".
```

And finally, if you want to completely unset the current context, you can use this command:

```
$ kubectl config unset current-context

Property "current-context" unset.
```

Once this cluster is running, you can interact with it just like any other Kubernetes cluster via the `kubectl` command. Whenever you shut down Docker Desktop, this will also shut down the Kubernetes cluster.

If you want to completely disable this Kubernetes cluster, go back into the Preferences panel, select the Kubernetes tab, and un-check Enable Kubernetes.

### Kind

The final option that we are going to discuss here is `kind`, a very simple but useful tool that allows you to manage a Kubernetes cluster made up of one or more Linux containers running in Docker. The tool name, `kind`, is an acronym that means “Kubernetes in Docker” but also refers to the fact that object types in Kubernetes are identified in the API by a field called `Kind`.

**NOTE**

You will find that searching for this tool on the web can be a bit difficult, but you can always find the tool and documentation on its primary [website](https://kind.sigs.k8s.io/).

`kind` provides a nice middle ground between the simplistic Kubernetes cluster that is embedded into the Docker VM and the `minikube` VM, which can be overly complex at times. `kind` is distributed as a single binary and can be installed with your favorite package manager or by simply navigating to the [`kind` project releases page](https://github.com/kubernetes-sigs/kind/releases) and downloading the most recent release for your system. If you manually download the binary, make sure that you rename the binary to `kind`, copy it to a directory in your path, and then ensure that it has the correct permissions so that users can run it.

Once `kind` is installed, you can try to create your first cluster with it by running the following:

```
$ kind create cluster --name test

Creating cluster "test" …
 ✓ Ensuring node image (kindest/node:v1.25.3) 
 ✓ Preparing nodes 
 ✓ Writing configuration 
 ✓ Starting control-plane 
 ✓ Installing CNI 
 ✓ Installing StorageClass 
Set kubectl context to "kind-test"
You can now use your cluster with:

kubectl cluster-info --context kind-test

Thanks for using kind!  
```

By default, this command will spin up a single Docker container that represents a one-node Kubernetes cluster, using the most current stable Kubernetes release that `kind` currently supports.

`kind` has already set the Kubernetes current context to point at the cluster, so we can start running `kubectl` commands immediately:

```
$ kubectl cluster-info

Kubernetes control plane is running at https://127.0.0.1:56499
CoreDNS is running at
https://127.0.0.1:56499/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

You can see a redacted version of the information used by `kubectl` to connect to the Kubernetes server by running the following:

```
$ kubectl config view --minify
```

```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://127.0.0.1:56499
  name: kind-test
contexts:
- context:
    cluster: kind-test
    user: kind-test
  name: kind-test
current-context: kind-test
kind: Config
preferences: {}
users:
- name: kind-test
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
```

`kind` has some [advanced features](https://kind.sigs.k8s.io/docs/user/quick-start/#advanced) that can generally be controlled by passing in a configuration file with the `--config` argument when spinning up the cluster.

You may find some of the follwing features useful:

* Changing the version of Kubernetes that is used
* Spinning up multiple worker nodes
* Spinning up multiple control plane nodes for HA testing
* Mapping ports between Docker and the local host system
* Enabling and disabling [Kubernetes feature gates](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates)
* Exporting control plane component logs with `kind export logs`
* And more

**TIP**

One thing to remember when using `kind` is that Kubernetes is running inside one or more containers, which are potentially running inside a Linux VM when you are using something like Docker Desktop. This may mean that you need to set up some additional port forwarding when you spin up the cluster. This can be done using the `extraPortMappings` setting in the `kind` config.

At this point, you can go ahead and delete the cluster by running the following command:

```
$ kind delete cluster --name test

Deleting cluster "test" …
```

## Amazon ECS and Fargate

One of the most popular cloud providers is Amazon via their AWS offerings. Support for running containers natively has existed in [AWS Elastic Beanstalk](https://amzn.to/2wNa1rL) since mid-2014. But that service assigns only a single container to an Amazon instance, which means that it’s not ideal for short-lived or lightweight containers. Amazon Elastic Compute Cloud (Amazon EC2) itself is a great platform for hosting your own Docker environment, though, and because Docker is powerful, you don’t necessarily need much on top of your instances to make this a productive environment to work in. But Amazon has spent a lot of engineering time building a service that treats containers as first-class citizens: the Amazon Elastic Container Service (Amazon ECS). In the last few years, Amazon has built upon this support with products like the Elastic Kubernetes Services (EKS) and AWS Fargate.

**NOTE**

Fargate is simply a marketing label Amazon uses for the feature of ECS that makes it possible for AWS to automatically manage all the nodes in your container cluster so that you can focus on deploying your service.

The ECS is a set of tools that coordinates several AWS components. With ECS, you have a choice of whether or not you will run the Fargate tooling on top. If you do, then you don’t need to handle as much of the work. If you don’t, then in addition to the cluster nodes to handle your workload, you will also need to add one or more EC2 instances to the cluster running Docker and Amazon’s special ECS agent. If you run Fargate, then the cluster is automatically managed for you. In either case, you spin up the cluster and then push your containers into it.

The [Amazon ECS agent](https://github.com/aws/amazon-ecs-agent) we just mentioned works with the ECS service to coordinate your cluster and schedule containers to your hosts. You will only be directly exposed to this when you manage a traditional non-Fargate ECS cluster.

### Core AWS Setup

The rest of this section assumes that you have access to an AWS account and some familiarity with the service. You can learn about pricing and create a new account at [_https://aws.amazon.com/free_](https://aws.amazon.com/free). Amazon offers a free service tier, which may be enough for you to experiment with if you don’t already have a paid account. After you have your AWS account set up, you will need at least one administrative user, a key pair, an Amazon virtual private cloud (AWS VPC), and a default security group in your environment. If you do not already have these set up, follow the directions in the [Amazon documentation](https://amzn.to/2FcPDSL).

### IAM Role Setup

Amazon’s Identity and Access Management (Amazon IAM) roles are used to control what actions a user can take within your cloud environment. We need to make sure we can grant access to the right actions before moving on with the ECS. To work with the ECS, you must create a role called `ecsInstanceRole` that has the `AmazonEC2ContainerServiceRole` managed role attached to it. The easiest way to do this is by logging in to the [AWS console](https://console.aws.amazon.com/) and navigating to [Identity and Access Management](https://console.aws.amazon.com/iam/home):

**TIP**

Check to ensure that you don’t already have the proper role. If it already exists, then you should double-check that it is set up properly, as these directions have changed a bit over the years.

1. In the left sidebar, click Roles.
2. Then, click the “Create role” button.
3. Under AWS Service, select Elastic Container Service.
4. Under “Select your use case,” select Elastic Container Service.
5. Click Next: Permissions.
6. Click Next: Review.
7. In Role Name, type **`ecsInstanceRole`**.
8. Click “Create role.”

If you are interested in storing container configuration in an S3 object storage bucket, take a look at the Amazon ECS container agent configuration [documentation](https://amzn.to/2PNapOL).

### AWS CLI Setup

Amazon supplies command-line tools that make it easy to work with their API-driven infrastructure. You will need to install a very recent version of the AWS CLI tools. Amazon has [detailed documentation](https://amzn.to/1PCpPNA) that covers the installation of their tools, but the basic steps are as follows.

#### Installation

Here we’ll cover the native installation on a few different OSes, but be aware that you can also run the [AWS CLI via a Docker container](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-docker.html#cliv2-docker-install)! You can feel free to skip to the one you care about. If you’re curious or just like installation instructions, by all means, read them all!

macOS

In [Chapter 3](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch03.html#installing\_docker), we discussed installing Homebrew. If you previously did this, you can install the AWS CLI using the following commands:

```
$ brew update
$ brew install awscli
```

Windows

Amazon provides a standard MSI installer for Windows, which can be downloaded from Amazon S3 for your architecture:

* [32-bit Windows](https://s3.amazonaws.com/aws-cli/AWSCLI32.msi)
* [64-bit Windows](https://s3.amazonaws.com/aws-cli/AWSCLI64.msi)

Other

The Amazon CLI tools are written in Python. So on most platforms, you can install the tools with the Python `pip` package manager by running the following from a shell:

```
$ pip install awscli --upgrade --user
```

Some platforms won’t have `pip` installed by default. In that case, you can use the `easy_install` package manager, like this:

```
$ easy_install awscli
```

#### Configuration

Quickly verify that your AWS CLI version is at least 1.7.0 with the following command:

```
$ aws --version

aws-cli/1.14.50 Python/3.6.4 Darwin/17.3.0 botocore/1.9.3
```

To configure the AWS CLI tool, ensure that you have access to your AWS access key ID and AWS secret access key, and then run the `configure` command. You will be prompted for your authentication information and some preferred defaults:

```
$ aws configure

AWS Access Key ID [None]: EXAMPLEEXAMPLEEXAMPLE
AWS Secret Access Key [None]: ExaMPleKEy/7EXAMPL3/EXaMPLeEXAMPLEKEY
Default region name [None]: us-east-1
Default output format [None]: json
```

At this point, it’s a really good idea to test that the CLI tools are working correctly before proceeding. You can easily do that by running the following command to list the IAM users in your account:

```
$ aws iam list-users
```

Assuming everything went according to plan and you chose JSON as your default output format, you should get something like this:

```
{
    "Users": [
        {
            "Path": "/",
            "UserName": "administrator",
            "UserId": "ExmaPL3ExmaPL3ExmaPL3Ex",
            "Arn": "arn:aws:iam::936262807352:user/myuser",
            "CreateDate": "2021-04-08T17:22:23+00:00",
            "PasswordLastUsed": "2022-09-05T15:56:21+00:00"
        }
    ]
}
```

### Container Instances

The first thing you need to do after installing the required tools is to create at least a single cluster that your Docker hosts will register with when they are brought online.

**NOTE**

The default cluster is imaginatively named _default_. If you keep this name, you do not need to specify `--cluster-name` in many of the commands that follow.

The first thing you need to do is create a cluster in the container service. You will then launch your tasks in the cluster once it’s up and running. For these examples, you should start by creating a cluster called `fargate-testing`:

```
$ aws ecs create-cluster --cluster-name fargate-testing
```

```
{
    "cluster": {
        "clusterArn": "arn:aws:ecs:us-east-1:1…2:cluster/fargate-testing",
"clusterName": "fargate-testing",
        "status": "ACTIVE",
        "registeredContainerInstancesCount": 0,
        "runningTasksCount": 0,
        "pendingTasksCount": 0,
        "activeServicesCount": 0,
        "statistics": [],
        "tags": [],
        "settings": [
            {
                "name": "containerInsights",
                "value": "disabled"
            }
        ],
        "capacityProviders": [],
        "defaultCapacityProviderStrategy": []
    }
}
```

Before AWS Fargate was released, you were required to create AWS EC2 instances running `docker` and the `ecs-agent`, and add them to your cluster. You can still use this approach if you want (`EC2 launch type`), but Fargate makes it much easier to run a dynamic cluster that can scale fluidly with your workload.

### Tasks

Now that our container cluster is set up, we need to start putting it to work. To do this, we need to create at least one task definition. The Amazon ECS defines the term _task definition_ as a list of containers grouped together.

To create your first task definition, open up your favorite editor, copy in the following JSON, and then save it as _webgame-task.json_ in your current directory, as shown here:

```
{
  "containerDefinitions": [
    {
      "name": "web-game",
      "image": "spkane/quantum-game",
      "cpu": 0,
      "portMappings": [
        {
          "containerPort": 8080,
          "hostPort": 8080,
          "protocol": "tcp"
        }
      ],
      "essential": true,
      "environment": [],
      "mountPoints": [],
      "volumesFrom": []
    }
  ],
  "family": "fargate-game",
  "networkMode": "awsvpc",
  "volumes": [],
  "placementConstraints": [],
  "requiresCompatibilities": [
    "FARGATE"
  ],
  "cpu": "256",
  "memory": "512"
}
```

**TIP**

You can also check out these files and a few others by running the following:

```
git clone \
  https://github.com/bluewhalebook/\
docker-up-and-running-3rd-edition.git \
  --config core.autocrlf=input
```

The URL has been continued on the following line so that it fits in the margins. You may find that you need to reassemble the URL and remove the backslashes for the command to work properly.

In this task definition, we are saying that we want to create a task family called `fargate-game` running a single container called `web-game` that is based on the [_Quantum_ web game](https://github.com/stared/quantum-game). As you may have seen in an earlier chapter, this Docker image launches a browser-based puzzle game that uses real quantum mechanics.

**TIP**

Fargate limits some of the options that you can set in this configuration, including `networkMode` and the `cpu` and `memory` settings. You can find out more about the options in the task definition from the official [AWS documentation](https://amzn.to/2PkliGR).

In this task definition, we define some constraints on memory and CPU usage for the container, in addition to telling Amazon whether this container is essential to the task. The `essential` flag is useful when you have multiple containers defined in a task, and not all of them are required for the task to be successful. If `essential` is true and the container fails to start, then all the containers defined in the task will be killed and the task will be marked as failed. We can also use the task definition to define almost all of the typical variables and settings that would be included in a _Dockerfile_ or on the `docker container run` command line.

To upload this task definition to Amazon, you will need to run a command similar to what is shown here:

```
$ aws ecs register-task-definition --cli-input-json file://./webgame-task.json
```

```
{
    "taskDefinition": {
        "taskDefinitionArn": "arn:aws:ecs:…:task-definition/fargate-game:1",
        "containerDefinitions": [
            {
                "name": "web-game",
                "image": "spkane/quantum-game",
                "cpu": 0,
                "portMappings": [
                    {
                        "containerPort": 8080,
                        "hostPort": 8080,
                        "protocol": "tcp"
                    }
                ],
                "essential": true,
                "environment": [],
                "mountPoints": [],
                "volumesFrom": []
            }
        ],
        "family": "fargate-game",
        "networkMode": "awsvpc",
        "revision": 1,
        "volumes": [],
        "status": "ACTIVE",
        "requiresAttributes": [
            {
                "name": "com.amazonaws.ecs.capability.docker-remote-api.1.18"
            },
            {
                "name": "ecs.capability.task-eni"
            }
        ],
        "placementConstraints": [],
        "compatibilities": [
            "EC2",
            "FARGATE"
        ],
        "requiresCompatibilities": [
            "FARGATE"
        ],
        "cpu": "256",
        "memory": "512",
        "registeredAt": "2022-09-05T09:10:18.184000-07:00",
        "registeredBy": "arn:aws:iam::…:user/me"
    }
}
```

We can then list all of our task definitions by running the following:

```
$ aws ecs list-task-definitions
```

```
{
    "taskDefinitionArns": [
        "arn:aws:ecs:us-east-1:…:task-definition/fargate-game:1",
    ]
}
```

Now you are ready to create your first task in your cluster. You do so by running a command like the one shown next. The `count` argument in the command allows you to define how many copies of this task you want to be deployed into your cluster. For this job, one is enough.

You will need to modify the following command to reference a valid subnet ID and security-group ID from your AWS VPC. You should be able to find these in the [AWS console](https://console.aws.amazon.com/vpc/home) or by using the AWS CLI commands `aws ec2 describe-subnets` and `aws ec2 describe-security-groups`. You can also tell AWS to assign your tasks a public IP address by using a network configuration similar to this:

```
awsvpcConfiguration={subnets=[subnet-abcd1234],
                     securityGroups=[sg-abcd1234],
                     assignPublicIp=ENABLED}
```

Assigning a public IP address may be required if you are using public subnets:

```
$ aws ecs create-service --cluster fargate-testing --service-name \
    fargate-game-service --task-definition fargate-game:1 --desired-count 1 \
    --launch-type "FARGATE" --network-configuration \
    "awsvpcConfiguration={subnets=[subnet-abcd1234],\
    securityGroups=[sg-abcd1234]}"
```

```
{
    "service": {
        "serviceArn": "arn:aws:ecs:…:service/fargate-game-service",
        "serviceName": "fargate-game-service",
        "clusterArn": "arn:aws:ecs:…:cluster/fargate-testing",
        "loadBalancers": [],
        "serviceRegistries": [],
        "status": "ACTIVE",
        "desiredCount": 1,
        "runningCount": 0,
        "pendingCount": 0,
        "launchType": "FARGATE",
        "platformVersion": "LATEST",
        "platformFamily": "Linux",
        "taskDefinition": "arn:aws:ecs:…:task-definition/fargate-game:1",
        "deploymentConfiguration": {
            "deploymentCircuitBreaker": {
                "enable": false,
                "rollback": false
            },
            "maximumPercent": 200,
            "minimumHealthyPercent": 100
        },
        "deployments": [
            {
                "id": "ecs-svc/…",
                "status": "PRIMARY",
                "taskDefinition": "arn:aws:ecs:…definition/fargate-game:1",
                "desiredCount": 1,
                "pendingCount": 0,
                "runningCount": 0,
                "failedTasks": 0,
                "createdAt": "2022-09-05T09:14:51.653000-07:00",
                "updatedAt": "2022-09-05T09:14:51.653000-07:00",
                "launchType": "FARGATE",
                "platformVersion": "1.4.0",
                "platformFamily": "Linux",
                "networkConfiguration": {
…
                },
                "rolloutState": "IN_PROGRESS",
                "rolloutStateReason": "ECS deployment ecs-svc/… in progress."
            }
        ],
        "roleArn": "…aws-service-role/ecs.amazonaws.com/AWSServiceRoleForECS",
        "events": [],
        "createdAt": "2022-09-05T09:14:51.653000-07:00",
        "placementConstraints": [],
        "placementStrategy": [],
        "networkConfiguration": {
…
        },
        "schedulingStrategy": "REPLICA",
        "createdBy": "arn:aws:iam::…:user/me",
        "enableECSManagedTags": false,
        "propagateTags": "NONE",
        "enableExecuteCommand": false
    }
}
```

**TIP**

Fargate and the `awsvpc` network require that you have a service-linked role for ECS. In the preceding output, you should see a line that ends like this:

```
"role/aws-service-role/ecs.amazonaws.com/
AWSServiceRoleForECS"
```

Most of the time this will be autogenerated for you, but you can create it manually using the following command:

```
$ aws iam create-service-linked-role \
    --aws-service-name ecs.amazonaws.com
```

You can now list all of the services in your cluster with the following command:

```
$ aws ecs list-services --cluster fargate-testing
```

```
{
    "serviceArns": [
        "arn:aws:ecs:us-west-2:…:service/fargate-testing/fargate-game-service"
    ]
}
```

To retrieve all the details about your service, run the following:

```
$ aws ecs describe-services --cluster fargate-testing \
    --services fargate-game-service
```

```
{
    "services": [
        {
…
            "deployments": [
                {
                    "id": "ecs-svc/…",
                    "status": "PRIMARY",
                    "taskDefinition": "arn:…:task-definition/fargate-game:1",
                    "desiredCount": 1,
                    "pendingCount": 1,
                    "runningCount": 0,
                    "createdAt": "2022-09-05T09:14:51.653000-07:00",
                    "updatedAt": "2022-09-05T09:14:51.653000-07:00",
                    "launchType": "FARGATE",
                    "platformVersion": "1.4.0",
                    "platformFamily": "Linux",
                    "networkConfiguration": {
…
                    },
                    "rolloutState": "IN_PROGRESS",
                    "rolloutStateReason": "ECS deployment ecs-svc/…progress."
                }
            ],
            "roleArn": "…role/ecs.amazonaws.com/AWSServiceRoleForECS",
            "events": [
                {
                    "id": "83bd5c2eed5d4866bb7ec8c3c938666c",
                    "createdAt": "2022-09-05T09:14:54.950000-07:00",
                    "message": "(…game-service) has started 1 tasks: (…)."
                }
            ],
…
        }
    ],
    "failures": []
}
```

This output will tell you a lot about all the tasks in your service. In this case, we have a single task running at the moment.

**NOTE**

The `task-definition` value is a name followed by a number (`fargate-game:1`). The number is the revision. If you edit your task and re-register it with the `aws ecs register-task-definition` command, you will get a new revision, which means that you will want to reference that new revision in various commands, like `aws ecs update-service`. If you don’t change that number, you will continue to launch containers using the older JSON. This versioning makes it very easy to roll back changes and test new revisions without impacting all future instances.

If you want to see what individual tasks are running in your cluster, you can run the following:

```
$ aws ecs list-tasks --cluster fargate-testing
```

```
{
    "taskArns": [
        "arn:aws:ecs:…:task/fargate-testing/83bd5c2eed5d4866bb7ec8c3c938666c"
    ]
}
```

Since you only have a single task in your cluster at the moment, this list is very small.

To get more details about the individual task, you can run the following command after substituting the task ID with the correct one from your cluster:

```
$ aws ecs describe-tasks --cluster fargate-testing \
  --task 83bd5c2eed5d4866bb7ec8c3c938666c
```

```
{
    "tasks": [
        {
            "attachments": [
                {
…
                    "details": [
…
                        {
                            "name": "networkInterfaceId",
                            "value": "eni-00a40225208c9411a"
                        },
…
                        {
                            "name": "privateIPv4Address",
                            "value": "172.31.42.184"
                        }
                    ]
                }
            ],
            "attributes": [
…
            ],
            "availabilityZone": "us-west-2b",
            "clusterArn": "arn:aws:ecs:us-west-2:…:cluster/fargate-testing",
            "connectivity": "CONNECTED",
            "connectivityAt": "2022-09-05T09:23:46.929000-07:00",
            "containers": [
                {
                    "containerArn": "arn:…:container/fargate-testing/…",
                    "taskArn": "arn:…:task/fargate-testing/…",
                    "name": "web-game",
                    "image": "spkane/quantum-game",
                    "runtimeId": "83bd…998",
                    "lastStatus": "RUNNING",
                    "networkInterfaces": [
                        {
                            "attachmentId": "ddab…373a",
                            "privateIpv4Address": "172.31.42.184"
                        }
                    ],
                    "healthStatus": "UNKNOWN",
                    "cpu": "0"
                }
            ],
            "cpu": "256",
            "createdAt": "2022-09-05T09:23:42.700000-07:00",
            "desiredStatus": "RUNNING",
            "enableExecuteCommand": false,
            "group": "service:fargate-game-service",
            "healthStatus": "UNKNOWN",
            "lastStatus": "RUNNING",
            "launchType": "FARGATE",
            "memory": "512",
            "overrides": {
                "containerOverrides": [
                    {
                        "name": "web-game"
                    }
                ],
                "inferenceAcceleratorOverrides": []
            },
            "platformVersion": "1.4.0",
            "platformFamily": "Linux",
            "pullStartedAt": "2022-09-05T09:59:36.554000-07:00",
            "pullStoppedAt": "2022-09-05T09:59:46.361000-07:00",
            "startedAt": "2022-09-05T09:59:48.546000-07:00",
            "startedBy": "ecs-svc/…",
            "tags": [],
            "taskArn": "arn:aws:…:task/fargate-testing/83bd…666c",
            "taskDefinitionArn": "arn:aws:…:task-definition/fargate-game:1",
            "version": 4,
            "ephemeralStorage": {
                "sizeInGiB": 20
            }
        }
    ],
    "failures": []
}
```

If you notice that the `lastStatus` key is displaying a value of `PENDING`, this most likely means that your service is still starting up. You can describe the task again to ensure that it has completed transitioning into a `RUNNING` state. After verifying that the `lastStatus` key is set to `RUNNING`, you should be able to test your container.

**TIP**

Depending on the network setup, your task may not be able to download the image. If you see an error like this:

`"stoppedReason": "CannotPullContainerError: inspect image has been retried 5 time(s): failed to resolve ref \"docker.io/spkane/quantum-game:latest\": failed to do request: Head` [_`https://registry-1.docker.io/v2/spkane/quantum-game/manifests/latest`_](https://registry-1.docker.io/v2/spkane/quantum-game/manifests/latest)`: dial tcp 54.83.42.45:443: i/o timeout"`

then you should read through this [troubleshooting guide](https://oreil.ly/FYo9Z).[1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch10.html#idm46803129919408)

### Testing the Task

You will need a modern web browser installed on your system to connect to the container and test the web game.

In the previous output, you’ll notice that the `privateIPv4Address` for the example task was listed as `172.31.42.184`. Yours will be different.

**TIP**

If you need more information about the network setup for your task and the EC2 instance that it is running on, you can grab the `networkInterfaceId` from the `aws ecs describe-tasks` output and then append that to the `aws ec2 describe-network-interfaces --network-interface-ids` command to get everything you should need, including the `PublicIp` value if you configured your service for that.

Ensure that you are connected to a network that can reach either the public or private IP address of your host, then launch your web browser and navigate to port 8080 on that IP address.

In the example, this private URL would look like this:

```
http://172.31.42.184:8080/
```

If everything is working as expected, you should be greeted by the _Quantum Game_ puzzle board.

The official version of the game can be found at [_https://quantumgame.io_](https://quantumgame.io/).

**NOTE**

We completely understand if you get distracted at this point and stop reading for a few hours to try to solve some puzzles and learn a little bit of quantum mechanics at the same time. The book won’t notice! Put it down, play the puzzles, and pick it back up later.

### Stopping the Task

Right, so we have a running task. Now let’s take a look at stopping it. To do that, you need to know the task ID. One way to obtain this is by relisting all the tasks running in your cluster:

```
$ aws ecs list-tasks --cluster fargate-testing
```

```
{
    "taskArns": [
        "arn:aws:ecs:…:task/fargate-testing/83bd5c2eed5d4866bb7ec8c3c938666c"
    ]
}
```

You can also obtain it from the service information:

```
$ aws ecs describe-services --cluster fargate-testing \
    --services fargate-game-service
```

```
{
…
                {
                    "id": "6b7f…0384",
                    "createdAt": "2022-09-05T09:59:23.917000-07:00",
                    "message": "…: (task 83bd5c2eed5d4866bb7ec8c3c938666c)."
                }
…
}
```

Finally, we can stop the task by running the following command with the correct task ID:

```
$ aws ecs stop-task --cluster fargate-testing \
    --task 83bd5c2eed5d4866bb7ec8c3c938666c
```

```
{
        "desiredStatus": "STOPPED",
…
        "lastStatus": "RUNNING",
…
        "stopCode": "UserInitiated",
        "stoppedReason": "Task stopped by user",
        "stoppingAt": "2022-09-05T10:29:05.110000-07:00",
…
}
```

If you describe the task again using the same task ID, you should now see that the `lastStatus` key is set to `STOPPED`:

```
$ aws ecs describe-tasks --cluster fargate-testing \
    --task 83bd5c2eed5d4866bb7ec8c3c938666c
```

```
{
…
            "desiredStatus": "STOPPED",
…
            "lastStatus": "STOPPED",
…
}
```

Listing all the tasks in our cluster should return an empty set:

```
$ aws ecs list-tasks --cluster fargate-testing
```

```
{
    "taskArns": []
}
```

At this point, you could start creating more complicated tasks that tie multiple containers together and rely on the ECS and Fargate tooling to spin up hosts and deploy the tasks into your cluster as needed.

If you want to tear down the rest of the ECS environment, you can run the following commands:

```
$ aws ecs delete-service --cluster fargate-testing \
  --service fargate-game-service  --force
…

$ aws ecs delete-cluster --cluster fargate-testing
…
```

## Wrap-Up

In this chapter, we’ve certainly presented you with a lot of options! It’s unlikely that you’ll ever need to use all of these, since many of them overlap. However, each one has a unique perspective on exactly what a production system should look like and what problems are the most important to solve. After exploring all of these tools, you should have a pretty good idea of the wide range of options you can choose from to build your production Linux container environment.

Underlying all of these tools is Docker’s highly portable image format for Linux containers and its ability to abstract away so much of the underlying Linux system, which makes it easy to move your applications fluidly between your data center and as many cloud providers as you want. Now you just have to choose which approach will work best for you and your organization and then implement it.

In the meantime, let’s jump into the next chapter and explore some of the most technical topics in the Docker ecosystem, including security, networking, and storage.

[1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch10.html#idm46803129919408-marker) Full URL: [_https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task\_cannot\_pull\_image.html_](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task\_cannot\_pull\_image.html)
