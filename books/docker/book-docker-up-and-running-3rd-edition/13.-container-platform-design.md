# 13. Container Platform Design

## Chapter 13. Container Platform Design

When implementing any technology in production, you’ll often gain the most mileage by designing a resilient platform that can withstand the unexpected issues that will inevitably occur. Docker can be a powerful tool but requires attention to detail to get the whole platform right around it. As a technology that is going through very rapid growth, it is bound to produce frustrating bugs that crop up between the various components that make up your container platform.

If instead of simply deploying Docker into your existing environment, you take the time to build a well-designed container platform utilizing Docker as one of the core components, you can enjoy the many benefits of a container-based workflow while simultaneously protecting yourself from some of the sharper edges that can exist in such high-velocity projects.

Like all other technology, Docker doesn’t magically solve all your problems. To reach its true potential, organizations must make very conscious decisions about why and how to use it. For small projects, it is possible to use Docker in a simple manner; however, if you plan to support a large project that can scale with demand, it’s crucial that you design your applications and the platform very deliberately. This ensures that you can maximize the return on your investment in the technology. Taking the time to intentionally design your platform will also make it much easier to modify your production workflow over time. A well-designed container platform and deployment process will be as lightweight and straightforward as possible while still supporting the features required to meet all the technical and compliance requirements. A well-thought-out design will help ensure that your software is running on a dynamic foundation that can easily be upgraded as technology and company processes develop.

In this chapter, we will explore two open documents, [“The Twelve-Factor App”](https://12factor.net/) and [“The Reactive Manifesto”](https://www.reactivemanifesto.org/) (a companion document to [“The Reactive Principles”](https://www.reactiveprinciples.org/)), and discuss how they relate to Docker and building robust container platforms. Both documents contain a lot of ideas that should help guide you through the design and implementation of your container platform and ensure more resiliency and supportability across the board.

## The Twelve-Factor App

In November of 2011, well before the release of Docker, Heroku cofounder Adam Wiggins and his colleagues released an article called [“The Twelve-Factor App”](https://12factor.net/). This document describes a series of 12 practices, distilled from the experiences of the [Heroku](https://www.heroku.com/) engineers, for designing applications that will thrive and grow in a modern container-based SaaS environment.

Although not required, applications built with these 12 steps in mind are ideal candidates for the Docker workflow. Throughout this chapter, we will explore each of the following steps and explain why these practices can, in numerous ways, help improve your development cycle:

* Codebase
* Dependencies
* Config
* Backing services
* Build, release, run
* Processes
* Port binding
* Concurrency
* Disposability
* Development/production parity
* Logs
* Admin processes

### Codebase

_One codebase tracked in revision control._

Many instances of your application will be running at any given time, but they should all come from the same code repository. Every single Docker image for a given application should be built from a single source code repository that contains all the code required to build the Linux container. This ensures that the code can easily be rebuilt and that all third-party requirements are well-defined within the repository and will automatically be pulled in during a build.

What this means is that building your application shouldn’t require stitching together code from multiple source repositories. That is not to say that you can’t have a dependency on an artifact from another repo. But it does mean that there should be a clear mechanism for determining which pieces of code were shipped when you built your application. Docker’s ability to simplify dependency management is much less useful if building your application requires pulling down multiple source code repositories and stitching pieces together. It also is not very repeatable if you must know a magic incantation to get the build to work correctly.

A good test might be to give a new developer in your company a clean laptop and a paragraph of directions and then see if they can successfully build your application in under an hour. If they can’t, then the process probably needs to be refined and simplified.

### Dependencies

_Explicitly declare and isolate dependencies_.

Never rely on the belief that a dependency will be made available via some other avenue, like the operating system install. Any dependencies that your application requires should be well defined in the codebase and pulled in by the build process. This will help ensure that your application will run when deployed, without relying on libraries being installed by other people or processes. This is particularly important within a container since the container’s processes are isolated from the rest of the host operating system and will usually not have access to anything outside of the host’s kernel and the container image’s filesystem.

The _Dockerfile_ and language-dependent configuration files like Node’s _package.json_ or Ruby’s _Gemfile_ should define every nonexternal dependency required by your application. This ensures that your image will run correctly on any system to which it is deployed. Gone will be the days when you try to deploy and run your application in production only to find out that important libraries are missing or installed with the wrong version. This pattern has huge reliability and repeatability advantages and very positive ramifications for system security. If to fix a security issue, you update the OpenSSL or _libyaml_ libraries that your containerized application uses, then you can be assured that it will always be running with that version wherever you deploy that particular application.

It is also important to note that many Docker base images are larger than they need to be. Remember that your application process will be running on a shared kernel, and the only files that you need inside your image are the ones that the process will require to run. It’s good that base images are so readily available, but they can sometimes mask hidden dependencies. Although people often start with a minimal install of Alpine, Ubuntu, or Fedora, these images still contain a lot of operating system files and applications that your process almost certainly does not need, or possibly some files that your application is making use of that you aren’t consciously aware of, like compiling your application using the _musl_ system library in Alpine versus the _glibc_ system library in many other base images. You need to be fully aware of your dependencies, even when containerizing your application. It is also important to consider what support tools, if any, you are including in your images, as there can be a fine line between making things easier to debug and increasing the security attack surface of your application and environments.

A good way to shed light on what files are required inside an image is to compare a “small” base image with an image for a statically linked program written in a language like Go or C. These applications can be designed to run directly on the Linux kernel without any additional libraries or files.

To help drive this point home, it might be useful to review the exercises in [“Keeping Images Small”](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#small\_images), where we explored one of these ultra-light containers, `spkane/scratch-helloworld`, and then dived into the underlying filesystem a bit and compared it with the popular `alpine` base image.

In addition to being conscientious about how you manage the filesystem layers in your images, keeping your images stripped down to the bare necessities is another great way to keep everything streamlined and your `docker image pull` commands fast. Applications written with interpreted languages will require many more files because of the large runtimes and dependency graphs you often need to install, but you should try to keep as minimal a base layer as needed for your use case so that you can reason about your dependencies. Docker helps you package them up, but you still need to be in charge of reasoning about them.

### Config

_Store configuration in environment variables, not in files checked into the codebase._

This makes it simple to deploy the same codebase to different environments, like staging and production, without maintaining complicated configuration in code or rebuilding your container for each environment. This keeps your codebase much cleaner by keeping environment-specific information like database names and passwords out of your source code repository. More importantly, though, it means that you don’t bake deployment environment assumptions into the repository, and thus it is extremely easy to deploy your applications anywhere that it might be useful. You also want to be able to test the same image you will ship to production. You can’t do that if you have to build an image for each environment with all of its configuration already baked in.

As discussed in [Chapter 4](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#docker\_images), you can achieve this by launching `docker container run` commands that leverage the `-e` command-line argument. Using `-e APP_ENV=` _`production`_ tells Docker to set the environment variable `APP_ENV` to the value `production` within the newly launched container.

For a real-world example, let’s assume we pulled the image for the chat robot Hubot with the [Rocket.Chat](https://www.rocket.chat/) adapter installed. We’d issue something like the following command to get it running:

```
$ docker container run \
  --rm --name hubot -d \
  -e ENVIRONMENT="development" \
  -e ROCKETCHAT_URL='rocketchat:3000' \
  -e ROCKETCHAT_ROOM='general' \
  -e RESPOND_TO_DM=true \
  -e ROCKETCHAT_USER=bot \
  -e ROCKETCHAT_PASSWORD=bot \
  -e ROCKETCHAT_AUTH=password \
  -e BOT_NAME=bot \
  -e EXTERNAL_SCRIPTS=hubot-pugme,hubot-help \
  docker.io/rocketchat/hubot-rocketchat:latest
```

Here, we are passing a whole set of environment variables into the container when it is created. When the process is launched in the container, it will have access to these environment variables so that it can properly configure itself at runtime. These configuration items are now an external dependency that we can inject at runtime.

**NOTE**

There are many other ways to provide this data to a container, including using key/value stores like `etcd` and `consul`. Environment variables are simply a universal option that acts as a very good starting point for most projects. They are the easy path for container configuration because they are well supported by the platform and every programming language in common use. They also aid in the observability of your applications because the configuration can easily be inspected with `docker container inspect`.

In the case of a Node.js application like `hubot`, you could then write the following code to make decisions based on these environment variables:

```
switch(process.env.ENVIRONMENT){
        case 'development':
            console.log('[INFO] Running in development');

        case 'staging':
            console.log('[INFO] Running in staging');

        case 'production':
            console.log('[INFO] Running in production');

        default:
            console.log('[WARN] Environment value is unknown');
    }
```

**NOTE**

The exact method used to pass this configuration data into your container will vary depending on the specific tooling that you’ve chosen for your projects, but almost all of them will make it easy to ensure that every deployment contains the proper settings for that environment.

Keeping specific configuration information out of your source code makes it very easy to deploy the exact same container to multiple environments, with no changes and no sensitive information committed into your source code repository. Crucially, it supports testing your container images thoroughly before deploying to production by allowing the same image to be used in all environments.

```
$ docker container stop hubot
```

**TIP**

If you need a process for managing secrets that need to be provided to your containers, you might want to look into the [documentation](https://docs.docker.com/engine/swarm/secrets) for the `docker secret` command, which works with Docker Swarm mode, and HashiCorp’s [Vault](https://www.vaultproject.io/).

### Backing Services

_Treat backing services as attached resources._

Local databases are no more reliable than third-party services and should be treated as such. Applications should handle the loss of an attached resource gracefully. By implementing graceful degradation in your application and never assuming that any resource, including filesystem space, is available, you ensure that your application will continue to perform as many of its functions as it can, even when external resources are unavailable.

This isn’t something that Docker helps you with directly, and although it is always a good idea to write robust services, it is even more important when you are using containers. When using containers, you achieve high availability most often through horizontal scaling and rolling deployments, instead of relying on the live migration of long-running processes, like on traditional VMs. This means that specific instances of a service will often come and go over time, and your service should be able to handle this gracefully.

Additionally, because Linux containers have limited filesystem resources, you can’t simply rely on having some local storage available. You need to plan that into your application’s dependencies and handle it explicitly.

### Build, Release, Run

_Strictly separate build and run stages._

Build the code, release it with the proper configuration, and then deploy it. This ensures that you maintain control of the process and can perform any single step without triggering the whole workflow. By ensuring that each of these steps is self-contained in a distinct process, you can tighten the feedback loop and react more quickly to any problems within the deployment flow.

As you design your Docker workflow, you want to clearly separate each step in the deployment process. It is perfectly fine to have a single button that builds a container, tests it, and then deploys it, assuming that you trust your testing processes—but you don’t want to be forced to rebuild a container simply to deploy it to another environment.

Docker supports the 12-factor ideal well in this area because the image registry provides a clean handoff point between building an image and shipping it to production. If your build process generates images and pushes them to the registry, then deployment can simply be pulling the image down to servers and running it.

### Processes

_Execute the app as one or more stateless processes._

All shared data must be accessed via a stateful backing store so that application instances can easily be redeployed without losing any important session data. You don’t want to keep any critical state on disk in your ephemeral container or in the memory of one of its processes. Containerized applications should always be considered ephemeral. A truly dynamic container environment requires the ability to destroy and re-create containers at a moment’s notice. This flexibility helps enable the rapid deployment cycle and outage recovery demanded by modern, Agile workflows.

As much as possible, it is preferable to write applications that do not need to keep state longer than the time required to process and respond to a single request. This ensures that the impact of stopping any given container in your application pool is very minimal. When you must maintain state, the best approach is to use a remote datastore like Redis, PostgreSQL, Memcache, or even Amazon S3, depending on your resiliency needs.

### Port Binding

_Export services via port binding._

Your application needs to be addressable by a port specific to itself. Applications should bind directly to a port to expose the service and should not rely on an external daemon like `inetd` to handle that for them. You should be certain that when you’re talking to that port, you’re talking to your application. Most modern web platforms are quite capable of directly binding to a port and servicing their own requests.

To expose a port from your container, as discussed in [Chapter 4](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#docker\_images), you can launch `docker container run` commands that use the `--publish` command-line argument. Using `--publish mode=ingress,published=80,target=8080`, for example, would tell Docker to proxy the container’s port 8080 on the host’s port 80.

The statically linked Go Hello World container that we discussed in [“Keeping Images Small”](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch04.html#small\_images) is a great example of this, because the container contains nothing but our application to serve its content to a web browser. We did not need to include any additional web servers, which would require further configuration, introduce additional complexity, and increase the number of potential failure points in our system.

### Concurrency

_Scale out via the process model._

Design for concurrency and horizontal scaling within your applications. Increasing the resources of an existing instance can be difficult and hard to reverse. Adding and removing instances as scale fluctuates is much easier and helps maintain flexibility in the infrastructure. Launching another container on a new server is incredibly inexpensive compared to the effort and expense required to add resources to an underlying virtual or physical system. Designing for horizontal scaling allows the platform to react much faster to changes in resource requirements.

As an example, in [Chapter 10](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch10.html#containers\_scale), you saw how easily a service could be scaled using Docker Swarm mode by simply running a command like this:

```
$ docker service scale myservice=8
```

This is where tools like Docker Swarm mode, Mesos, and Kubernetes truly begin to shine. Once you have implemented a Docker cluster with a dynamic scheduler, it is very easy to add three more instances of a container to the cluster as load increases and then to be able to easily remove two instances of your application from the cluster as load starts to decrease again.

### Disposability

_Maximize robustness with fast startup and graceful shutdown._

Services should be designed to be ephemeral. We already talked a little bit about this when discussing external state with containers. Responding well to dynamic horizontal scaling, rolling deploys, and unexpected problems requires applications that can quickly and easily be started or shut down. Services should respond gracefully to a `SIGTERM` signal from the operating system and even handle hard failures confidently. Most importantly, we shouldn’t care if any given container for our application is up and running. As long as requests are being served, the developer should be freed of concerns about the health of any single component within the system. If an individual node is behaving poorly, turning it off or redeploying it should be an easy decision that doesn’t entail long planning sessions and concerns about the health of the rest of the cluster.

As discussed in [Chapter 7](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch07.html#debug\_docker), Docker sends standard Unix signals to containers when it is stopping or killing them; therefore, any containerized application can detect these signals and take the appropriate steps to shut down gracefully.

### Development/Production Parity

_Keep development, staging, and production as similar as possible._

The same processes and artifacts should be used to build, test, and deploy services into all environments. The same people should do the work in all environments, and the physical nature of the environments should be as similar as reasonably possible. Repeatability is incredibly important. Almost any issue discovered in production points to a failure in the process. Every area where production diverges from staging is an area where risk is being introduced into the system. These inconsistencies blind you to certain types of issues that could occur in your production environment until it is too late to proactively deal with them.

In many ways, this advice essentially repeats a few of the early recommendations. However, the specific point here is that any environment divergence introduces risks, and although these differences are common in many organizations, they are much less necessary in a containerized environment. Docker servers can normally be created so that they are identical in all of your environments, and environment-based configuration changes should typically impact only which endpoints your service connects to without specifically changing the application’s behavior.

### Logs

_Treat logs as event streams._

Services should not concern themselves with routing or storing logs. Instead, events should be streamed, unbuffered, to `STDOUT` and `STDERR` for handling by the hosting process. In development, `STDOUT` and `STDERR` can be easily viewed, whereas in staging and production, the streams can be routed to anything, including a central logging service. Different environments have different exceptions for log handling. This logic should never be hardcoded into the application. Streaming everything to `STDOUT` and `STDERR` enables the top-level process manager to handle the logs via whatever method is best for the environment, allowing the application developer to focus on core functionality.

In [Chapter 6](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch06.html#exploring\_docker), we discussed the `docker container logs` command, which collects the output from your container’s `STDOUT` and `STDERR` and records it as logs. If you write logs to random files within the container’s filesystem, you will not have easy access to them. It is also possible to configure Docker to send logs to a local or remote logging system using tools like `rsyslog`, `journald`, or `fluentd`.

If you use a process manager or initialization system on your servers, like `systemd` or `upstart`, it is usually very easy to direct all process output to `STDOUT` and `STDERR` and then have your process monitor capture them and send them to a remote logging host.

### Admin Processes

_Run admin/management tasks as one-off processes._

One-off administration tasks should be run via the same codebase and configuration that the application uses. This helps avoid synchronization problems and code/schema drift problems. Oftentimes, management tools exist as one-off scripts or live in a completely different codebase. It is much safer to build management tools within the application’s codebase and utilize the same libraries and functions to perform the required work. This can significantly improve the reliability of these tools by ensuring that they leverage the same code paths that the application relies on to perform its core functionality.

What this means is that you should never rely on random `cron`-like scripts to perform administrative and maintenance functions. Instead, include all of these scripts and functionality in your application codebase. Assuming that these don’t need to be run on every instance of your application, you can launch a special short-lived container, or use `docker container exec` with the existing container, whenever you need to run a maintenance job. This command can trigger the required job, report its status somewhere, and then exit.

### Twelve-Factor Wrap-Up

While “The Twelve-Factor App” wasn’t written as a Docker-specific manifesto, almost all of this advice can be applied to writing and deploying applications on a Docker platform. This is in part because the article heavily influenced Docker’s design, and in part because the manifesto itself codified many of the best practices promoted by modern software architects.

## The Reactive Manifesto

Riding alongside “The Twelve-Factor App,” another pertinent document was released in July of 2013 by Typesafe cofounder and CTO Jonas Bonér, entitled [“The Reactive Manifesto”](https://www.reactivemanifesto.org/). Jonas originally worked with a small group of contributors to solidify a manifesto that discusses how the expectations for application resiliency have evolved over the last few years and how applications should be engineered to react predictably to various forms of interaction, including events, users, load, and [failures](https://www.lightbend.com/blog/why-do-we-need-a-reactive-manifesto).

“The Reactive Manifesto” states that “reactive systems” are responsive, resilient, elastic, and message driven.

### Responsive

_The system responds in a timely manner if at all possible._

In general, this means that the application should respond to requests very quickly. Users simply don’t want to wait, and there is rarely a good reason to make them. If you have a containerized service that renders large PDF files, design it so that it immediately responds with a “job submitted” message so that users can go about their day, and then provide a message or banner that informs them when the job is finished and where they can download the resulting PDF.

### Resilient

_The system stays responsive in the face of failure_.

When your application fails for any reason, the situation will always be worse if it becomes unresponsive. It is much better to handle the failure gracefully and dynamically reduce the application’s functionality or even display a simple but clear problem message to the user while reporting the issue internally.

### Elastic

_The system stays responsive under varying workload._

With Docker, you achieve this by dynamically deploying and decommissioning containers as requirements and load fluctuate so that your application is always able to handle server requests quickly, without deploying a lot of underutilized resources.

### Message Driven

_Reactive systems rely on asynchronous message passing to establish a boundary between components that ensures loose coupling, isolation, and location transparency._

Although not directly addressed by Docker, the idea here is that there are times when an application can become busy or unavailable. If you utilize asynchronous message passing between your services, you can help ensure that your services will not lose requests and that they will be processed as soon as possible.

## Wrap-Up

All four of the design features in “The Reactive Manifesto” require application developers to design graceful degradation and define a clear separation of responsibilities in their applications. By treating all dependencies as properly designed, attached resources, dynamic container environments allow you to easily maintain _N_+2 status across your application stack, reliably scale individual services in your environment, and quickly replace unhealthy nodes.

A service is only as reliable as its least reliable dependency, so it is vital to incorporate these ideas into every component of your platform.

The core ideas in “The Reactive Manifesto” merge very nicely with “The Twelve-Factor App” and the Docker workflow. These documents successfully summarize many of the most important discussions about the way you need to think and work if you want to be successful in meeting new expectations in the industry. The Docker workflow provides a practical way to implement many of these ideas in any organization in a completely approachable manner.
