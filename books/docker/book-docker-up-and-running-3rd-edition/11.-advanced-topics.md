# 11. Advanced Topics

## Chapter 11. Advanced Topics

In this chapter, we’ll do a quick pass through some of the more advanced topics. We’re going to assume that you have a pretty good hold on Docker by now and that you’ve already got it in production or you’re at least a regular user. We’ll talk about how containers work in detail and about some of the aspects of Docker security, Docker networking, Docker plug-ins, swappable runtimes, and other advanced configurations.

Some of this chapter covers configurable changes you can make to your Docker installation. These can be useful, but Docker has good defaults, so as with most software, you should stick to the defaults on your operating system unless you have a good reason to change them and have educated yourself on what those changes mean to you. Getting your installation right for your environment will likely involve some trial and error, tuning, and adjustment over time. However, changing settings from their defaults before understanding them well is not recommended.

## Containers in Detail

Though we usually talk about Linux containers as a single entity, they are actually implemented through several separate mechanisms built into the Linux kernel that all work together: control groups (cgroups), namespaces, Secure Computing Mode (`seccomp`), and SELinux or AppArmor, all of which serve to _contain_ the process. cgroups provide for resource limits, namespaces allow for processes to use identically named resources and isolate them from one another’s view of the system, Secure Computing Mode limits which system calls a process can use, and SELinux or AppArmor provides additional strong security isolation for processes. So, to start, what do cgroups and namespaces do for you?

Before we launch into detail, an analogy might help you understand how each of these subsystems plays into the way that containers work. Imagine that the typical computer is like a large open warehouse, full of workers (processes). The warehouse is full of space and resources, but it is very easy for the workers to get in one another’s way, and most of the resources are simply used by whomever gets them first.

When you are running Docker and using Linux containers for your workloads, it is like that warehouse has been converted into an office building, where each worker now has their own individual office. Each office has all the normal things that the workers need to accomplish their jobs, and in general, they can now work without worrying much about what other people (processes) are doing.

Namespaces make up the walls of the office and ensure that processes cannot interact with neighboring processes in any way that they are not specifically allowed to. Control groups are a bit like paying rent to receive utilities. When the process is first spun up, it is assigned time on the CPU and storage subsystem that it will be allowed each cycle, in addition to the amount of memory that it will be allowed to use at any moment. This helps ensure that the workers (processes) have the resources they need, without allowing them to use resources or space reserved for others. Imagine the worst kind of noisy neighbors, and you can suddenly truly appreciate good, solid barriers between offices. Finally, Secure Computing Mode, SELinux, and AppArmor are a bit like office security, ensuring that even if something unexpected or untoward happens, it is unlikely to cause much more than the headache of filling out paperwork and filing an incident report.

### cgroups

Traditional distributed system design dictates running each intensive task on its own virtual server. So, for example, you don’t run your applications on the database server because they have competing resource demands, and their resource usage could grow unbounded and begin to dominate the server, starving the database of performance.

On real hardware systems, this could be quite expensive, so solutions like virtual servers are very appealing, in part because you can share expensive hardware between competing applications, and the virtualization layer will handle your resource partitioning. But while it saves money, this is still a fairly expensive approach if you don’t need all the other separation provided by virtualization, because running multiple kernels introduces a reasonable overhead on the applications. Maintaining VMs is also not the cheapest solution. All the same, cloud computing has shown that it’s immensely powerful and, with the right tooling, incredibly effective.

But if the only kind of isolation you needed was resource partitioning, wouldn’t it be great if you could get that on the same kernel without running another operating system instance? For many years, you could assign a “niceness” value to a process, and it would give the scheduler hints about how you wanted this process to be treated in relation to the others. But it wasn’t possible to impose hard limits like those that you get with VMs. And niceness is not at all fine-grained: you can’t give something more I/O and less CPU than other processes. This fine-grained control, of course, is one of the promises of Linux containers, and the mechanism that they use to provide that functionality is cgroups, which predate Docker and were invented to solve just this problem.

_Control groups_ allow you to set limits on resources for processes and their children. This is the mechanism that the Linux kernel uses to control limits on memory, swap, CPU, storage, and network I/O resources. cgroups are built into the kernel and originally shipped in 2007 in Linux 2.6.24. The official [kernel documentation](https://www.kernel.org/doc/Documentation/cgroup-v2.txt) defines them as “a mechanism to organize processes hierarchically and distribute system resources along the hierarchy in a controlled and configurable manner.” It’s important to note that this setting applies to a process and all of the children that descend from it. That’s exactly how containers are structured.

**NOTE**

It is worth mentioning that there have been at least two major releases of Linux control groups: [v1](https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt) and [v2](https://www.kernel.org/doc/Documentation/cgroup-v2.txt). Make sure that you know which version is being used in production so that you can leverage all the abilities that it provides.

Every Linux container is assigned a cgroup that is unique to that container. All of the processes in the container will be in the same group. This means that it’s easy to control resources for each container as a whole without worrying about what might be running. If a container is redeployed with new processes added, you can have Docker assign the same policy and it will apply to the whole container and all the process containers within it.

We talked previously about the cgroups hooks exposed by Docker via its API. That interface allows you to control memory, swap, and disk usage. But there are lots of other things that you can manage with cgroups, including tagging network packets from a container so that you can use those tags to prioritize traffic. You might find that in your environment you need to use some of these levers to keep your containers under control, and there are a few ways you can go about doing that. By their very nature, cgroups need to do a lot of accounting of resources used by each group. That means that when you’re using them, the kernel has a lot of interesting statistics about how much CPU, RAM, disk I/O, and so on your processes are using. So Docker uses cgroups not just to limit resources but also to report on them. These are many of the metrics you see, for example, in the output of `docker` `container stats`.

#### The /sys filesystem

The primary way to control cgroups in a fine-grained manner, even if you configured them with Docker, is to manage them yourself. This is the most powerful method because changes don’t just happen at container creation time—they can be done on the fly.

On systems with `systemd`, there are command-line tools like `systemctl` that you can use to do this. But since cgroups are built into the kernel, the method that works everywhere is to talk to the kernel directly via the _/sys_ filesystem. If you’re not familiar with _/sys_, it’s a filesystem that directly exposes several kernel settings and outputs. You can use it with simple command-line tools to tell the kernel how you would like it to behave.

This method of configuring cgroups controls for containers only works directly on the Docker server, so it is not available remotely via any API. If you use this method, you’ll need to figure out how to script this for your environment.

**WARNING**

Changing cgroups values yourself, outside of any Docker configuration, breaks some of the repeatability of a Docker deployment. Unless you implement changes in your deployment process, settings will revert to their defaults when containers are replaced. Some schedulers take care of this for you, so if you run one in production, you might check the documentation to see how to best apply these changes repeatably.

Let’s use an example of changing the CPU cgroups settings for a container we have just started up. We need to get the long ID of the container, and then we need to find it in the _/sys_ filesystem. Here’s what that looks like:

```
$ docker container run -d spkane/train-os \
  stress -v --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout 360s

dcbb…8e86f1dc0a91e7675d3c93895cb6a6d83371e25b7f0bd62803ed8e86
```

Here, we’ve had `docker container run` give us the long ID in the output, and the ID we want is `dcbb…8e86f1dc0a91e7675d3c93895cb6a6d83371e25b7f0bd62803ed8e86`. You can see why Docker normally truncates this.

**NOTE**

In the examples, we may need to truncate the ID to make it fit into the constraints of a standard page. But remember that you will need to use the long ID!

Now that we have the ID, we can find our container’s cgroup in the _/sys_ filesystem. _/sys_ is laid out so that each type of setting is grouped into a module, and that module might be exposed at a different place in the _/sys_ filesystem. So when we look at CPU settings, we won’t see `blkio` settings, for example. You might take a look around in _/sys_ to see what else is there. But for now we’re interested in the CPU controller, so let’s inspect what that gives us. You need `root` access on the system to do this because you’re manipulating kernel settings.

**TIP**

Remember our `nsenter` trick we originally discussed in [Chapter 3](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch03.html#installing\_docker). You can run `docker container run --rm -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh` to get access to the Docker host, even if you can’t SSH into the server.

```
$ ls /sys/fs/cgroup/docker/dcbb…8e86

cgroup.controllers        cpuset.cpus.partition     memory.high
cgroup.events             cpuset.mems               memory.low
cgroup.freeze             cpuset.mems.effective     memory.max
cgroup.max.depth          hugetlb.2MB.current       memory.min
cgroup.max.descendants    hugetlb.2MB.events        memory.oom.group
cgroup.procs              hugetlb.2MB.events.local  memory.stat
cgroup.stat               hugetlb.2MB.max           memory.swap.current
cgroup.subtree_control    hugetlb.2MB.rsvd.current  memory.swap.events
cgroup.threads            hugetlb.2MB.rsvd.max      memory.swap.high
cgroup.type               io.bfq.weight             memory.swap.max
cpu.max                   io.latency                pids.current
cpu.stat                  io.max                    pids.events
cpu.weight                io.stat                   pids.max
cpu.weight.nice           memory.current            rdma.current
cpuset.cpus               memory.events             rdma.max
cpuset.cpus.effective     memory.events.local
```

**NOTE**

The exact path here may change a bit depending on the Linux distribution your Docker server is running on and what the hash of your container is.

You can see that under cgroups, there is a _docker_ directory that contains all of the Linux containers that are running on this host. You can’t set cgroups for things that aren’t running, because they apply only to running processes. This is an important point that you should consider. Docker takes care of reapplying cgroup settings for you when you start and stop containers. Without that mechanism, you are somewhat on your own.

Let’s go ahead and inspect the [CPU weight](https://docs.kernel.org/admin-guide/cgroup-v2.html#cpu-interface-files) for this container. Remember that we explored setting some of these CPU values in [Chapter 5](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch05.html#docker\_containers) via the `--cpus` command-line argument to `docker container run`. But for a normal container where no settings were passed, this setting is the default:

```
$ cat /sys/fs/cgroup/docker/dcbb…8e86/cpu.weight
100
```

`100` CPU weight means we are not limited at all. Let’s tell the kernel that this container should be limited to half that:

```
$ echo 50 > /sys/fs/cgroup/docker/dcbb…8e86/cpu.weight
$ cat /sys/fs/cgroup/docker/dcbb…8e86/cpu.weight
50
```

**WARNING**

In production, you should not use this method to adjust cgroups on the fly, but we are demonstrating it here so that you understand the underlying mechanics that make all of this work. Take a look at [`docker container update`](https://dockr.ly/2PPC4P1) if you’d like to adjust these on a running container. You might also find the [`--cgroup-parent`](https://dockr.ly/2PTLaKK) option to `docker container run` interesting.

There you have it. We’ve changed the container’s settings on the fly. This method is very powerful because it allows you to set any cgroups setting for the container. But as we mentioned earlier, it’s entirely ephemeral. When the container is stopped and restarted, the setting reverts to the default:

```
$ docker container stop dcbb…8e86
dcbb…8e86

$ cat /sys/fs/cgroup/docker/dcbb…8e86/cpu.weight
cat: /sys/fs/…/cpu.weight: No such file or directory
```

You can see that the directory path doesn’t even exist anymore now that the container is stopped. And when we start it back up, the directory comes back but the setting is back to `100`:

```
$ docker container start dcbb…8e86
dcbb…8e86

$ cat /sys/fs/cgroup/docker/dcbb…8e86/cpu.weight
100
```

If you were to change these kinds of settings in a production system via the _/sys_ filesystem directly, you’d want to manage that directly. A daemon that watches the `docker system events` stream and changes settings at container startup, for example, is a possibility.

**NOTE**

It is possible to create custom cgroups outside of Docker and then attach a new container to that cgroup using the `--cgroup-parent` argument to `docker container create`. This mechanism is also used by schedulers that run multiple containers inside the same cgroup (e.g., Kubernetes pods).

### Namespaces

Inside each container, you see a filesystem, network interfaces, disks, and other resources that all appear to be unique to the container despite sharing the kernel with all the other processes on the system. The primary network interface on the actual machine, for example, is a single shared resource. But inside your container, it will look like it has an entire network interface to itself. This is a really useful abstraction: it’s what makes your container feel like a machine all by itself. The way this is implemented in the kernel is with Linux namespaces. Namespaces take a traditionally global resource and present the container with its own unique and unshared version of that resource.

**NOTE**

Namespaces cannot be explored on the filesystem quite as easily as cgroups, but most of the details can be found under the _/proc/\*/ns/\*_ and _/proc/\*/task/\*/ns/\*_ hierarchies. In newer Linux releases, the `lsns` command can also be quite useful.

Rather than just having a single namespace, however, by default containers have a namespace on each of the resources that are currently namespaced in the kernel: mount, UTS, IPC, PID, network, and user namespaces, in addition to the partially implemented time namespace. Essentially, when you talk about a container, you’re talking about several different namespaces that Docker sets up on your behalf. So what do they all do?

Mount namespaces

Linux uses these primarily to make your container look like it has its own entire filesystem. If you’ve ever used a `chroot` jail, this is its more robust relative. It looks a lot like a `chroot` jail but goes all the way down to the deepest levels of the kernel so that even `mount` and `unmount` system calls are namespaced. If you use `docker container exec` or `nsenter`, which we will discuss later in this chapter, to get into a container, you’ll see a filesystem rooted on _/_. But we know that this isn’t the actual root partition of the system. It’s the mount namespace that makes that possible.

UTS namespaces

Named for the kernel structure they namespace, UTS (Unix Time Sharing System) namespaces give your container its own hostname and domain name. This is also used by older systems like NIS to identify which domain a host belongs to. When you enter a container and see a hostname that is not the same as the machine on which it runs, it’s this namespace that makes that happen.

**TIP**

To have a container use its host’s UTS namespace, you can specify the `--uts=host` option when launching the container with `docker container run`. There are similar commands for sharing the other namespaces as well.

IPC namespaces

These isolate your container’s System V IPC and POSIX message queue systems from those of the host. Some IPC mechanisms use filesystem resources like named pipes, and those are covered by the mount namespace. The IPC namespace covers things like shared memory and semaphores that aren’t filesystem resources but that really should not cross the container wall.

PID namespaces

We have already shown that you can see all of the processes in containers in the Linux `ps` output on the host Linux server. But inside the container, processes have a different PID. This is the PID namespace in action. A process has a unique PID in each namespace to which it belongs. If you look in _/proc_ inside a container, or run `ps`, you will only see the processes inside the container’s PID namespace.

Network namespaces

This is what allows your container to have its own network devices, ports, and so on. When you run `docker container ls` and see the bound ports for your container, you are seeing ports from both namespaces. Inside the container, your `nginx` might be bound to port 80, but that’s on the namespaced network interface. This namespace makes it possible to have what seems to be a completely separate network stack for your container.

User namespaces

These provide isolation between the user and group IDs inside a container and those on the Linux host. Earlier, when we looked at `ps` output outside and then inside the container, we saw different user IDs; this is how that happened. A new user inside a container is not a new user on the Linux host’s main namespace, and vice versa. There are some subtleties here, though. For example, UID 0 (`root`) in a user namespace is not the same thing as UID 0 on the host, although running as `root` inside the container does increase the risk of potential security exploits. There are concerns about security leakage, which we’ll talk about in a bit, and this is why things like rootless containers are growing in popularity.

Cgroup namespaces

This namespace was introduced in Linux kernel 4.6 in 2016 and is intended to hide the identity of the cgroup of which the process is a member. A process checking which cgroup any process is part of would see a path that is relative to the cgroup set at creation time, hiding its true cgroup position and identity.

Time namespaces

Time has historically not been namespaced since it is so integral to the Linux kernel, and providing full namespacing would be very complex. However, with the release of Linux kernel 5.6 in 2020, support was added for a [time namespace](https://man7.org/linux/man-pages/man7/time\_namespaces.7.html) that allows containers to have their own unique clock offsets.

**NOTE**

At the time of this writing, Docker still does not have direct support for setting the time offset, but like everything else, it can be set directly, if required.

So by combining all of these namespaces, Linux can provide the visual and, in many cases, the functional isolation that makes a container look like a VM even though it’s running on the same kernel. Let’s explore what some of the namespacing that we just described looks like in more detail.

**NOTE**

There is a lot of ongoing work trying to make containers more secure. The community is actively looking into ways to improve support for [rootless containers](https://rootlesscontaine.rs/), which enables regular users to create, run, and manage containers locally without needing special privileges. In Docker, this can now be achieved via [rootless mode](https://docs.docker.com/engine/security/rootless). New container runtimes like [Google gVisor](https://github.com/google/gvisor) are also trying to explore better ways to create much more secure container sandboxes without losing most of the advantages of containerized workflows.

#### Exploring namespaces

One of the easiest namespaces to demonstrate is UTS, so let’s use `docker container exec` to get a shell in a container and take a look. From within the Docker server, run the following:

```
$ hostname

docker-desktop
```

**TIP**

Again, remember that you can use the `docker container run --rm -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh` command that we originally discussed in [Chapter 3](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch03.html#installing\_docker) to get access to the Docker host, even if you can’t SSH into the server.

And then on your local system, run the following:

```
$ docker container run -ti --rm ubuntu \
    bash -c 'echo "Container hostname: $(hostname)"'

Container hostname: 4cdb66d4495b
```

That `docker container run` command line gets us an interactive session (`-ti`) and then executes the `hostname` command via `/bin/bash` inside the container. Since the `hostname` command is run inside the container’s namespace, we get back the short container ID, which is used as the hostname by default. This is a pretty simple example, but it should clearly show that we’re not in the same namespace as the host.

Another example that’s easy to understand and demonstrate involves PID namespaces. Let’s create a new container:

```
$ docker container run -d --rm --name pstest spkane/train-os sleep 240
6e005f895e259ed03c4386b5aeb03e0a50368cc173078007b6d1beaa8cd7dded

$ docker container exec -ti pstest ps -ef

UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 15:33 ?        00:00:00 sleep 240
root        13     0  0 15:33 pts/0    00:00:00 ps -ef
```

And now let’s get Docker to show us the process IDs from the host’s perspective:

```
$ docker container top pstest

UID   PID    PPID   C  STIME  TTY  TIME      CMD
root  31396  31370  0  15:33 ?     00:00:00  sleep 240
```

What we can see here is that from inside our container, the original command run by Docker is `sleep 240`, and it has been assigned PID `1` inside the container. You might recall that this is the PID normally used by the `init` process on Unix systems. In this case, the `sleep 240` command that we started the container with is the first process, so it gets PID `1`. But in the Docker server’s main namespace, we can see that the PID there is not `1` but `31396`, and it’s a child of process ID `31370`.

If you are curious, you can run a command like this to determine what PID `31370` is:

```
$ docker container run --pid=host ubuntu ps -p 31370
PID    TTY  TIME      CMD
31370  ?    00:00:00  containerd-shim
```

Now we can go ahead and remove the container we started in the last example by running the following:

```
 $ docker container rm -f pstest
```

The other namespaces work in essentially the same manner, and you probably get the idea by now. It’s worth pointing out here that when we were first working with `nsenter` back in [Chapter 3](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch03.html#installing\_docker), we had to pass what appeared to be some pretty arcane arguments to the command when we ran it to enter a container from the Docker server. Let’s go ahead and look at the `nsenter` portion of the command `docker container run --rm -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh`.

It turns out that `nsenter -t 1 -m -u -n -i sh` is exactly the same as `nsenter --target 1 --mount --uts --net -ipc sh`. So this command really just says, look at PID `1` and then open up a shell in the same `mount`, `uts`, `net`, and `ipc` namespaces of that process.

Now that we’ve explained namespaces in detail, this probably makes a lot more sense to you. It can also be educational to use `nsenter` to try entering different sets of namespaces in a throwaway container to see what you get and simply explore how all of this works in some more detail.

When it comes down to it, namespaces are the primary things that make a container look like a container. Combine them with cgroups, and you have reasonably robust isolation between processes on the same kernel.

## Security

We’ve spent a good bit of space now talking about how Docker provides containment for applications, allows you to constrain resource utilization, and uses namespaces to give the container a unique view of the world. We have also briefly mentioned the need for technologies like Secure Computing Mode, SELinux, and AppArmor. One of the advantages of containers is the ability to replace VMs in several use cases. So let’s take a look at what isolation we get by default and what we don’t.

You are undoubtedly aware by now that the isolation you get from a container is not as strong as that from a VM. We’ve been reinforcing the idea from the start of this book that containers are just processes running on the Linux server. Despite the isolation provided by namespaces, containers are not as secure as you might imagine, especially if you are still mentally comparing them to lightweight VMs.

One of the big boosts in performance for containers, and one of the things that makes them lightweight, is that they share the kernel of the Linux server. This is also the source of the greatest security concern around Linux containers. The main reason for this concern is that not everything in the kernel is namespaced. We have talked about all of the namespaces that exist and how the container’s view of the world is constrained by the namespaces it runs in. However, there are still lots of places in the kernel where no real isolation exists, and namespaces constrain the container only if it does not have the power to tell the kernel to give it access to a different namespace.

Containerized applications are more secure than noncontainerized applications because cgroups and standard namespaces provide some important isolation from the host’s core resources. But you should not think of containers as a substitute for good security practices. If you think about how you would run an application on a production system, that is really how you should run all your containers. If your application would traditionally run as a nonprivileged user on a server, then it should be run in the same manner inside the container. It is very easy to tell Docker to run your container processes as a nonprivileged user, and in almost all cases, this is what you should be doing.

**TIP**

The `--userns-remap` argument to the `dockerd` command and rootless mode both make it possible to force all containers to run within a user and group context that is unprivileged on the host system. These approaches help protect the host from many potential security exploits.

For more information about `userns-remap`, read through the official [feature](https://dockr.ly/2BYfWze) and [Docker daemon](https://dockr.ly/2LE9gG2) documentation.

You can learn more about rootless mode in the section [“Rootless Mode”](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch11.html#rootless\_mode).

Let’s look at some common security risks and controls.

### UID 0

The first and most overarching security risk in a container is that, unless you are using rootless mode or the `userns-remap` functionality in the Docker daemon, the `root` user in the container is actually the `root` user on the system. There are extra constraints on `root` in a container, and namespaces do a good job of isolating `root` in the container from the most dangerous parts of the _/proc_ and _/sys_ filesystems. But if you are UID 0, you have `root` access, so if you somehow get access to protected resources on a file mount or outside of your namespace, then the kernel will treat you as `root` and therefore give you access to the resource. Unless otherwise configured, Docker starts all services in containers as `root`, which means you are responsible for managing privileges in your applications just like if you are on any standard Linux system. Let’s explore some of the limits on `root` access and look at some obvious holes. This is not intended to be an exhaustive statement on container security but rather an attempt to give you a healthy understanding of some of the classes of security risks.

First, let’s fire up a container and get a `bash` shell using the public Ubuntu image shown in the following code. Then we’ll see what kinds of access we have, after installing some tools we want to run:

```
$ docker container run --rm -ti ubuntu /bin/bash

root@808a2b8426d1:/# apt-get update
…
root@808a2b8426d1:/# apt-get install -y kmod
…
root@808a2b8426d1:/# lsmod
Module                             Size  Used by
xfrm_user                         36864  1
xfrm_algo                         16384  1 xfrm_user
shiftfs                           28672  0
grpcfuse                          16384  0
vmw_vsock_virtio_transport        16384  2
vmw_vsock_virtio_transport_common 28672  1 vmw_vsock_virtio_transport
vsock                             36864  9 vmw_vsock_virtio_transport_common…
```

In Docker Desktop, you may only see a few modules in the list, but on a normal Linux system, this list can be very long. Using `lsmod`, we’ve just asked the kernel to tell us what modules are loaded. It is not that surprising that we get this list from inside our container, since a normal user can always do this. If you run this listing on the Docker server itself, it will be identical, which reinforces the fact that the container is talking to the same Linux kernel that is running on the server. So we can see the kernel modules; what happens if we try to unload the `floppy` module?

```
root@808a2b8426d1:/# rmmod shiftfs

rmmod: ERROR: ../libkmod/libkmod-module.c:799 kmod_module_remove_module() …
rmmod: ERROR: could not remove module shiftfs: Operation not permitted

root@808a2b8426d1:/# exit
```

That’s the same error message we would get if we were a nonprivileged user trying to tell the kernel to remove a module. This should give you a good sense that the kernel is doing its best to prevent us from doing things we shouldn’t. And because we’re in a limited namespace, we can’t get the kernel to give us access to the top-level namespace either. We are essentially relying on the hope that there are no bugs in the kernel that allow us to escalate our privileges inside the container. Because if we do manage to do that, we are `root`, which means that we will be able to make changes if the kernel allows us to.

We can contrive a simple example of how things can go wrong by starting a `bash` shell in a container that has had the Docker server’s _/etc_ bind-mounted into the container’s namespace. Keep in mind that anyone who can start a container on your Docker server can do what we’re about to do any time they like because you can’t configure Docker to prevent it, so you must instead rely on external tools like SELinux to avoid exploits like this.

**NOTE**

This example assumes that you are running the `docker` CLI on a Linux system, which has an _/etc/shadow_ file. This file will not exist on Windows or macOS hosts running something like Docker Desktop.

```
$ docker container run --rm -it -v /etc:/host_etc ubuntu /bin/bash

root@e674eb96bb74:/# more /host_etc/shadow
root:!:16230:0:99999:7:::
daemon:*:16230:0:99999:7:::
bin:*:16230:0:99999:7:::
sys:*:16230:0:99999:7:::
…
irc:*:16230:0:99999:7:::
nobody:*:16230:0:99999:7:::
libuuid:!:16230:0:99999:7:::
syslog:*:16230:0:99999:7:::
messagebus:*:16230:0:99999:7:::
kmatthias:$1$aTAYQT.j$3xamPL3dHGow4ITBdRh1:16230:0:99999:7:::
sshd:*:16230:0:99999:7:::
lxc-dnsmasq:!:16458:0:99999:7:::

root@e674eb96bb74:/# exit
```

Here we’ve used the `-v` switch to tell Docker to mount a host path into the container. The one we’ve chosen is _/etc_, which is a very dangerous thing to do. But it serves to prove a point: we are `root` in the container, and `root` has file permissions in this path. So we can look at the _/etc/shadow_ file on the Linux server, which contains the encrypted passwords for all the users. There are plenty of other things you could do here, but the point is that by default you’re only partly constrained.

**WARNING**

It is a bad idea to run your container processes with UID 0. This is because any exploit that allows the process to somehow escape its namespaces will expose your host system to a fully privileged process. You should always run your standard containers with a nonprivileged UID.

The easiest way to deal with the potential problems surrounding the use of UID 0 inside containers is to always tell Docker to use a different UID for your container.

You can do this by passing the `-u` argument to `docker container run`. In the next example, we run the `whoami` command to show that we are `root` by default and that we can read the _/etc/shadow_ file that is inside this container:

```
$ docker container run --rm spkane/train-os:latest whoami
root

$ docker container run --rm spkane/train-os:latest cat /etc/shadow
root:!locked::0:99999:7:::
bin:*:18656:0:99999:7:::
daemon:*:18656:0:99999:7:::
adm:*:18656:0:99999:7:::
lp:*:18656:0:99999:7:::
…
```

In this example, when you add `-u 500`, you will see that we become a new, unprivileged user and can no longer read the same _/etc/shadow_ file:

```
$ docker container run --rm -u 500 spkane/train-os:latest whoami
user500

$ docker container run --rm -u 500 spkane/train-os:latest cat /etc/shadow
cat: /etc/shadow: Permission denied
```

Another highly recommended approach is to add the `USER` directive to your _Dockerfile_s so that containers created from them will launch using a nonprivileged user by default:

```
FROM fedora:34
RUN useradd -u 500 -m myuser
USER 500:500
CMD ["whoami"]
```

If you create this _Dockerfile_, and then build and run it, you will see that `whoami` returns `myuser` instead of `root`:

```
$ docker image build -t user-test .

[+] Building 0.5s (6/6) FINISHED
 => [internal] load build definition from Dockerfile                      0.0s
 => => transferring dockerfile: 36B                                       0.0s
 => [internal] load .dockerignore                                         0.0s
 => => transferring context: 2B                                           0.0s
 => [internal] load metadata for docker.io/library/fedora:34              0.4s
 => [1/2] FROM docker.io/library/fedora:34@sha256:321d…2697               0.0s
 => CACHED [2/2] RUN useradd -u 500 -m myuser                             0.0s
 => exporting to image                                                    0.0s
 => => exporting layers                                                   0.0s
 => => writing image sha256:4727…30d5                                     0.0s
 => => naming to docker.io/library/user-test                              0.0s

$ docker container run --rm user-test
myuser
```

### Rootless Mode

One of the primary security challenges with containers is that they often require some root-privileged processes to launch and manage them. Even when you use the `--userns-remap` feature of the Docker daemon, the daemon itself still runs as a privileged process, even though the containers that it launches will not.

With [rootless mode](https://docs.docker.com/engine/security/rootless), it is possible to run the daemon and all containers without root privileges, which can do a great deal to improve the security of the underlying system.

Rootless mode requires a Linux system, and Docker recommends Ubuntu, so let’s run through an example using a new Ubuntu 22.04 system.

**NOTE**

These steps assume that you are logging in a regular unprivileged user and that you already have [Docker Engine installed](https://docs.docker.com/engine/install/ubuntu).

The first thing we need to do is make sure that `dbus-user-session` and `uidmap` are installed. If `dbus-user-session` isn’t already installed, then we need to log out and log back in after running the following command:

```
$ sudo apt-get install -y dbus-user-session uidmap
…
dbus-user-session is already the newest version (1.12.20-2ubuntu4).
…
Setting up uidmap (1:4.8.1-2ubuntu2) …
…
```

Although, it is not strictly required, if the system-wide Docker daemon is set up to run, it is a very good idea to disable it and then reboot:

```
$ sudo systemctl disable --now docker.service docker.socket

Synchronizing state of docker.service with SysV service script with
  /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install disable docker
Removed /etc/systemd/system/sockets.target.wants/docker.socket.
Removed /etc/systemd/system/multi-user.target.wants/docker.service.

$ sudo shutdown -r now
```

Once the system is back up, you can SSH back into the server as a regular user and confirm that _/var/run/docker.sock_ is no longer on the system:

```
$ ls /var/run/docker.sock
ls: cannot access '/var/run/docker.sock': No such file or directory
```

The next step is to run the rootless mode installation script, which is installed in _/usr/bin_ by the Docker installer:

```
$ dockerd-rootless-setuptool.sh install

[INFO] Creating /home/me/.config/systemd/user/docker.service
[INFO] starting systemd service docker.service
+ systemctl --user start docker.service
+ sleep 3
+ systemctl --user --no-pager --full status docker.service
● docker.service - Docker Application Container Engine (Rootless)
     Loaded: loaded (/home/me/.config/systemd/user/docker.service; …)
…
+ DOCKER_HOST=unix:///run/user/1000/docker.sock /usr/bin/docker version
Client: Docker Engine - Community
 Version:           20.10.18
…
Server: Docker Engine - Community
 Engine:
  Version:          20.10.18
…
+ systemctl --user enable docker.service
Created symlink /home/me/.config/systemd/user/default.target.wants/
  docker.service → /home/me/.config/systemd/user/docker.service.
[INFO] Installed docker.service successfully.

[INFO] To control docker.service, run:
         `systemctl --user (start|stop|restart) docker.service`
[INFO] To run docker.service on system startup, run:
         `sudo loginctl enable-linger me`

[INFO] Creating CLI context "rootless"
Successfully created context "rootless"

[INFO] Make sure the following environment variables are set
       (or add them to ~/.bashrc):
export PATH=/usr/bin:$PATH
export DOCKER_HOST=unix:///run/user/1000/docker.sock
```

**NOTE**

The `` UID` `` in the `` DOCKER_HOST` `` variable here should match the UID of the user who ran the script. In this case, the `UID` is `1000`.

This script ran a few checks to ensure that our system was ready and then installed and started a user-scoped `systemd` service file into `${HOME}/.config/systemd/user/docker.service`. Each and every user on the system could do the same thing, if desired.

The user Docker daemon can be controlled, like most `systemd` services. A few basic examples are shown here:

```
$ systemctl --user restart docker.service
$ systemctl --user stop docker.service
$ systemctl --user start docker.service
```

To allow the user Docker daemon to run when the user is not logged in, the user needs to use `sudo` to enable a `systemd` feature called `linger`, and then you can also enable the Docker daemon to start whenever the system boots up:

```
$ sudo loginctl enable-linger $(whoami)
$ systemctl --user enable docker
```

This would be a good time to go ahead and add those environment variables to our shell startup files, but at a minimum we need to make sure both of these environment variables are set in our current terminal:

```
$ export PATH=/usr/bin:$PATH
$ export DOCKER_HOST=unix:///run/user/1000/docker.sock
```

We can easily run a standard container:

```
$ docker container run --rm hello-world

Hello from Docker!
This message shows that your installation appears to be working correctly.
…
For more examples and ideas, visit:
 https://docs.docker.com/get-started/
```

However, you will notice that some of the more privileged containers that we have used in earlier sections will not work in this environment:

```
$ docker container run --rm -it --privileged --pid=host debian nsenter \
    -t 1 -m -u -n -i sh

docker: Error response from daemon: failed to create shim task: OCI runtime
create failed: runc create failed: unable to start container process: error
during container init: error mounting "proc" to rootfs at "/proc":
mount proc:/proc (via /proc/self/fd/7), flags: 0xe:
operation not permitted: unknown.
```

And this is because, in rootless mode, the container cannot have more privileges than the user who is running the container, even though, on the surface, the container appears to still have full `root` privileges:

```
$ docker container run --rm spkane/train-os:latest whoami
root
```

Let’s explore this just a little bit more by launching a small container that is running `sleep 480s`:

```
$ docker container run -d --rm --name sleep spkane/train-os:latest sleep 480s
1f8ccec0a834537da20c6e07423f9217efe34c0eac94f0b0e178fb97612341ef
```

If we look at the processes inside the container, we see that they all appear to be running with the user `root`:

```
$ docker container exec sleep ps auxwww
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root           1  0.1  0.0   2400   824 ?        Ss   17:51   0:00 sleep 480s
root           7  0.0  0.0   7780  3316 ?        Rs   17:51   0:00 ps auxwww
```

However, if we look at the processes on the Linux system, we see that the `sleep` command is actually being run by the local user, named `me`, and not by `root` at all:

```
$ ps auxwww | grep sleep
me   3509 0.0 0.0  2400  824 ?     Ss 10:51 0:00 sleep 480s
me   3569 0.0 0.0 17732 2360 pts/0 S+ 10:51 0:00 grep --color=auto sleep
```

The `root` user inside a rootless container is actually mapped to the user themself. The container processes cannot use any privileges that the user running the daemon does not already have, and because of this, they are a very safe way to allow users on a multiuser system to run containers without granting any of them elevated privileges on the system.

**TIP**

There are directions to [uninstall rootless mode](https://docs.docker.com/engine/security/rootless/#uninstall) on the Docker website.

### Privileged Containers

There are times when you need your container to have special [kernel capabilities](https://man7.org/linux/man-pages/man7/capabilities.7.html) that would normally be denied to the container. These could include mounting a USB drive, modifying the network configuration, or creating a new Unix device.

In the following code, we try to change the MAC address of our container:

```
$ docker container run --rm -ti spkane/train-os /bin/bash

[root@280d4dc16407 /]# ip link ls
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode …
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN mode DEFAULT …
    link/ipip 0.0.0.0 brd 0.0.0.0
3: ip6tnl0@NONE: <NOARP> mtu 1452 qdisc noop state DOWN mode DEFAULT …
    link/tunnel6 :: brd :: permaddr 12b5:6f1b:a7e9::
22: eth0@if23: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue …
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0

[root@fc4589fb8778 /]# ip link set eth0 address 02:0a:03:0b:04:0c
RTNETLINK answers: Operation not permitted

[root@280d4dc16407 /]# exit
```

As you can see, it doesn’t work. This is because the underlying Linux kernel blocks the nonprivileged container from doing this, which is exactly what we’d normally want. However, assuming that we need this functionality for our container to work as intended, the easiest way to significantly expand a container’s privileges is by launching it with the `--privileged=true` argument.

**WARNING**

We don’t recommend running the `ip link set eth0 address` command in the next example, since this will change the MAC address on the container’s network interface. We show it to help you understand the mechanism. Try it at your own risk.

```
$ docker container run -ti --rm --privileged=true spkane/train-os /bin/bash

[root@853e0ef5dd63 /]# ip link ls
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode …
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN mode DEFAULT …
    link/ipip 0.0.0.0 brd 0.0.0.0
3: ip6tnl0@NONE: <NOARP> mtu 1452 qdisc noop state DOWN mode DEFAULT …
    link/tunnel6 :: brd :: permaddr 12b5:6f1b:a7e9::
22: eth0@if23: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue …
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0

[root@853e0ef5dd63 /]# ip link set eth0 address 02:0a:03:0b:04:0c

[root@853e0ef5dd63 /]#  ip link show eth0
26: eth0@if27: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue …
    link/ether 02:0a:03:0b:04:0c brd ff:ff:ff:ff:ff:ff link-netnsid 0

[root@853e0ef5dd63 /]# exit
```

In the preceding output, you will notice that we no longer get the error, and the `link/ether` entry for `eth0` has been changed.

The problem with using the `--privileged=true` argument is that you are giving your container very broad privileges, and in most cases, you likely need only one or two kernel capabilities to get the job done.

If we explore our privileged container some more, we will discover that we have capabilities that have nothing to do with changing the MAC address. We can even do things that could cause issues with both Docker and the host system. In the following code, we are going to mount a disk partition from the underlying host system, list all of the underlying Docker-based Linux containers on the system, and explore some of their critical files:

```
$ docker container run -ti --rm --privileged=true spkane/train-os /bin/bash

[root@664a896983d7 /]# mount /dev/vda1 /mnt && \
                         ls -F /mnt/docker/containers | \
                         head -n 10

047df420f6d1f227a26667f83e477f608298c25b0cdad2e149a781587aae5e11/
0888b9f97b1ecc4261f637404e0adcc8ef0c8df291b87c9160426e42dc9b5dea/
174ea3ec35cd3a576bed6f475b477b1a474d897ece15acfc46e61685abb3101d/
1eddad26ee64c4b29eb164b71d56d680739922b3538dc8aa6c6966fce61125b0/
22b2aa38a687f423522dd174fdd85d578eb21c9c8ec154a0f9b8411d08f6fd4b/
23879e3b9cd6a42a1e09dc8e96912ad66e80ec09949c744d1177a911322e7462/
266fe7da627d2e8ec5429140487e984c8d5d36a26bb3cc36a88295e38216e8a7/
2cb6223e115c12ae729d968db0d2f29a934b4724f0c9536e377e0dbd566f1102/
306f00e86122b69eeba9323415532a12f88360a1661f445fc7d64c07249eb0ce/
333b85236409f873d07cd47f62ec1a987df59f688a201df744f40f98b7e4ef2c/

[root@664a896983d7 /]# ls -F /mnt/docker/containers/047d…5e11/

047df420f6d1f227a26667f83e477f608298c25b0cdad2e149a781587aae5e11-json.log
checkpoints/
config.v2.json
hostconfig.json
hostname
hosts
mounts/
resolv.conf
resolv.conf.hash

[root@664a896983d7 /]# cat /mnt/docker/containers/047d…5e11/047…e11-json.log
```

```
{"log":"047df420f6d1\r\n","stream":"stdout","time":"2022-09-14T15:18:29.…"}
…
```

```
[root@664a896983d7 /]# exit
```

**WARNING**

Do not change or delete any of these files. It could have an unpredictable impact on the containers or the underlying Linux system.

So, as we’ve seen, people can run commands and get access to things that they shouldn’t from a fully privileged container.

To change the MAC address, the only kernel capability we need is `CAP_NET_ADMIN`. Instead of giving our container the full set of privileges, we can give it this one privilege by launching our Linux container with the `--cap-add` argument, as shown here:

```
$ docker container run -ti --rm --cap-add=NET_ADMIN spkane/train-os /bin/bash

[root@087c02a3c6e7 /]# ip link show eth0
36: eth0@if37: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue …
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0

[root@087c02a3c6e7 /]# ip link set eth0 address 02:0a:03:0b:04:0c

[root@087c02a3c6e7 /]# ip link show eth0
36: eth0@if37: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue …
    link/ether 02:0a:03:0b:04:0c brd ff:ff:ff:ff:ff:ff link-netnsid 0

[root@087c02a3c6e7 /]# exit
```

You should also notice that although we can change the MAC address, we can no longer use the `mount` command inside our container:

```
$ docker container run -ti --rm --cap-add=NET_ADMIN spkane/train-os /bin/bash

[root@b84a06ddaa0d /]# mount /dev/vda1 /mnt
mount: /mnt: permission denied.

[root@b84a06ddaa0d /]# exit
```

It is also possible to remove specific capabilities from a container. Imagine for a moment that your security team requires that `tcpdump` be disabled in all containers, and when you test some of your containers, you find that `tcpdump` is installed and can easily be run:

```
$ docker container run -ti --rm spkane/train-os:latest tcpdump -i eth0

dropped privs to tcpdump
tcpdump: verbose output suppressed, use -v[v]… for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), snapshot length 262144 bytes
15:40:49.847446 IP6 fe80::23:6cff:fed6:424f > ff02::16: HBH ICMP6, …
15:40:49.913977 ARP, Request who-has _gateway tell 5614703ffee2, length 28
15:40:49.914048 ARP, Request who-has _gateway tell 5614703ffee2, length 28
15:40:49.914051 ARP, Reply _gateway is-at 02:49:9b:d9:49:4e (oui Unknown), …
15:40:49.914053 IP 5642703bbff2.45432 > 192.168.75.8.domain: 44649+ PTR? …
…
```

You could remove `tcpdump` from your images, but there is very little preventing someone from reinstalling it. The most effective way to solve this problem is to determine what capability `tcpdump` needs to operate and remove that from the container. In this case, you can do so by adding `--cap-drop=NET_RAW` to your `docker container run` command:

```
$ docker container run -ti --rm --cap-drop=NET_RAW spkane/train-os:latest \
  tcpdump -i eth0

tcpdump: eth0: You don't have permission to capture on that device
(socket: Operation not permitted)
```

By using both the `--cap-add` and `--cap-drop` arguments to `docker container run`, you can finely control your container’s [Linux kernel capabilities](https://man7.org/linux/man-pages/man7/capabilities.7.html).

**NOTE**

Be aware that in addition to providing access to system calls, there are actually some other things that enabling a specific Linux capability can provide. This might include visibility of all the devices on the system or the ability to change the time on the system.

### Secure Computing Mode

When Linux kernel version 2.6.12 was released in 2005, it included a new security feature called Secure Computing Mode, or `seccomp` for short. This feature enables a process to make a one-way transition into a special state, where it will only be allowed to make the system calls `exit()`, `sigreturn()`, and `read()` or `write()` to already-open file descriptors.

An extension to `seccomp`, called `seccomp-bpf`, utilizes the Linux version of [Berkeley Packet Filter (BPF)](https://www.kernel.org/doc/Documentation/networking/filter.txt) rules to allow you to create a policy that will provide an explicit list of system calls that a process can utilize while running under Secure Computing Mode. The Docker support for Secure Computing Mode utilizes `seccomp-bpf` so that users can create profiles that give them very fine-grained control of which kernel system calls their containerized processes are allowed to make.

**NOTE**

By default, all containers use Secure Computing Mode and have the default profile attached to them. You can [read more about Secure Computing Mode](https://docs.docker.com/engine/security/seccomp) and which system calls the default profile blocks in the documentation. You can also examine the [default policy’s JSON file](https://github.com/moby/moby/blob/master/profiles/seccomp/default.json) to see what a policy looks like and understand exactly what it defines.

To see how you could use this, let’s use the program `strace` to trace the system calls that a process is making when we try to unmount a filesystem with the `umount` command.

**WARNING**

These examples are here to prove a point, but you obviously shouldn’t be unmounting filesystems out of your container without knowing exactly what is going to happen.

```
$ docker container run -ti --rm spkane/train-os:latest umount /sys/fs/cgroup
umount: /sys/fs/cgroup: must be superuser to unmount.

$ docker container run -ti --rm spkane/train-os:latest \
  strace umount /sys/fs/cgroup

execve("/usr/bin/umount", ["umount", "/sys/fs/cgroup"], 0x7fff902ddbe8 …
…
umount2("/sys/fs/cgroup", 0)            = -1 EPERM (Operation not permitted)
write(2, "umount: ", 8umount: )                 = 8
write(2, "/sys/fs/cgroup: must be superuse"…,
      45/sys/fs/cgroup: must be superuser to unmount.) = 45
write(2, "\n", 1
)                       = 1
dup(1)                                  = 3
close(3)                                = 0
dup(2)                                  = 3
close(3)                                = 0
exit_group(32)                          = ?
+++ exited with 32 +++
```

We already know that mount-related commands do not work in a container with standard permissions, and `strace` makes it clear that the system returns an “Operation not permitted” error message when the `umount` command tries to use the `umount2` system call.

You could potentially fix this by giving your container the `SYS_ADMIN` capability, like this:

```
$ docker container run -ti --rm --cap-add=SYS_ADMIN spkane/train-os:latest \
    strace umount /sys/fs/cgroup

execve("/usr/bin/umount", ["umount", "/sys/fs/cgroup"], 0x7ffd3e4452b8 …
…
umount2("/sys/fs/cgroup", 0)            = 0
dup(1)                                  = 3
close(3)                                = 0
dup(2)                                  = 3
close(3)                                = 0
exit_group(0)                           = ?
+++ exited with 0 +++
```

However, remember that using `--cap-add=SYS_ADMIN` will make it possible for us to do many other things, including mounting system partitions using a command like this:

```
$ docker container run -ti --rm --cap-add=SYS_ADMIN spkane/train-os:latest \
  mount /dev/vda1 /mnt
```

You can solve this problem with a more focused approach by using a `seccomp` profile. Unlike `seccomp`, `--cap-add` will enable a whole set of system calls and some additional privileges, and you almost certainly don’t need them all. `CAP_SYS_ADMIN` is particularly powerful and provides way more privileges than any one capability should. With a `seccomp` profile, however, you can be very specific about exactly what system calls you want to be enabled or disabled.

If we take a look at the default `seccomp` profile, we’ll see something like this:

```
{
    "defaultAction": "SCMP_ACT_ERRNO",
    "defaultErrnoRet": 1,
    "archMap": [
        {
            "architecture": "SCMP_ARCH_X86_64",
            "subArchitectures": [
                "SCMP_ARCH_X86",
                "SCMP_ARCH_X32"
            ]
        },
…
    ],
    "syscalls": [
        {
            "names": [
                "accept",
                "accept4",
                "access",
                "adjtimex",
…
                "waitid",
                "waitpid",
                "write",
                "writev"
            ],
            "action": "SCMP_ACT_ALLOW"
        },
        {
            "names": [
                "bpf",
                "clone",
…
                "umount2",
                "unshare"
            ],
            "action": "SCMP_ACT_ALLOW",
            "includes": {
                "caps": [
                    "CAP_SYS_ADMIN"
                ]
            }
        },
…
    ]
}
```

This JSON file provides a list of supported architectures, a default ruleset, and groups of system calls that fall within the scope of each capability. In this case, the default action is `SCMP_ACT_ERRNO` and will generate an error if an unspecified call is attempted.

If you examine the default profile in detail, you’ll notice that `CAP_SYS_ADMIN` controls access to 37 system calls, a huge number that is even larger than the 4-6 system calls included in most other capabilities.

In the current use case, we actually need some of the special functionality provided by `CAP_SYS_ADMIN`, but we do not need all of those system calls. To ensure that we are adding only the one additional system call that we need, we can create our own Secure Computing Mode policy, based on the default policy that Docker provides.

First, pull down the default policy and make a copy of it:

```
$ wget https://raw.githubusercontent.com/moby/moby/master/\
profiles/seccomp/default.json

$ cp default.json umount2.json
```

**NOTE**

The URL has been continued on the following line so that it fits in the margins. You may find that you need to reassemble the URL and remove the backslashes for the command to work properly in your environment.

Then edit the file and remove a bunch of the system calls that `CAP_SYS_ADMIN` normally provides. In this case, we actually need to retain two system calls to ensure that both `strace` and `umount` work correctly.

The section of the file that we are targeting ends with this JSON block:

```
            "includes": {
                "caps": [
                    "CAP_SYS_ADMIN"
                ]
            }
```

This `diff` shows the exact changes that need to be made in this use case:

```
$ diff -u -U5 default.json umount2.json
```

```
diff -u -U5 default.json umount2.json
--- default.json        2022-09-25 13:23:57.000000000 -0700
+++ umount2.json        2022-09-25 13:38:31.000000000 -0700
@@ -575,34 +575,12 @@
                                ]
                        }
                },
                {
                        "names": [
-                               "bpf",
                                "clone",
-                               "clone3",
-                               "fanotify_init",
-                               "fsconfig",
-                               "fsmount",
-                               "fsopen",
-                               "fspick",
-                               "lookup_dcookie",
-                               "mount",
-                               "mount_setattr",
-                               "move_mount",
-                               "name_to_handle_at",
-                               "open_tree",
-                               "perf_event_open",
-                               "quotactl",
-                               "quotactl_fd",
-                               "setdomainname",
-                               "sethostname",
-                               "setns",
-                               "syslog",
-                               "umount",
-                               "umount2",
-                               "unshare"
+                               "umount2"
                        ],
                        "action": "SCMP_ACT_ALLOW",
                        "includes": {
                                "caps": [
                                        "CAP_SYS_ADMIN"
```

You are now ready to test your new finely tuned `seccomp` profile to ensure that it can run `umount` but cannot run `mount`:

```
$ docker container run -ti --rm --security-opt seccomp=umount2.json \
  --cap-add=SYS_ADMIN spkane/train-os:latest /bin/bash

[root@15b8a26b6cfe /]# strace umount /sys/fs/cgroup
execve("/usr/bin/umount", ["umount", "/sys/fs/cgroup"], 0x7ffece9ebc38 …
close(3)                                = 0
exit_group(0)                           = ?
+++ exited with 0 +++

[root@15b8a26b6cfe /]# mount /dev/vda1 /mnt
mount: /mnt: permission denied.

[root@15b8a26b6cfe /]# exit
```

If everything went according to plan, your `strace` of the `umount` program should have run perfectly and the `mount` command should have been blocked. In the real world, it would be much safer to consider redesigning your applications so that they do not need these special privileges, but when it cannot be avoided, you should be able to use these tools to help ensure that your containers remain as secure as possible while still doing their jobs.

**WARNING**

You could completely disable the default Secure Computing Mode profile by setting `--security-opt seccomp=unconfined`; however, running a container unconfined is a very bad idea in general and is probably only useful when you are trying to figure out exactly what system calls you may need to define in your profile.

The strength of Secure Computing Mode is that it allows users to be much more selective about what a container can and can’t do with the underlying Linux kernel. Custom profiles are not required for most containers, but they are an incredibly handy tool when you need to carefully craft a powerful container and ensure that you maintain the overall security of the system.

### SELinux and AppArmor

Earlier, we talked about how containers primarily leverage cgroups and namespaces for their functionality. [SELinux](https://www.redhat.com/en/topics/linux/what-is-selinux) and [AppArmor](https://apparmor.net/) are security layers in the Linux ecosystem that can be used to increase the security of containers even further. In this section, we are going to discuss these two systems a bit. SELinux and AppArmor allow you to apply security controls that extend beyond those normally supported by Unix systems. SELinux originated in the US National Security Agency, was strongly adopted by Red Hat, and supports very fine-grained control. AppArmor is an effort to achieve many of the same goals while being a bit more user-friendly than SELinux.

By default, Docker ships with reasonable profiles enabled on platforms that support either of these systems. You can further configure these profiles to enable or prevent all sorts of features, and if you’re running Docker in production, you should do a risk analysis to determine if there are additional considerations that you should be aware of. We’ll give a quick outline of the benefits you are getting from these systems.

Both systems provide _mandatory access control_, a class of security system where a systemwide security policy grants users (or “initiators”) access to a resource (or “target”). This allows you to prevent anyone, including `root`, from accessing a part of the system that they should not have access to. You can apply the policy to a whole container so that all processes are constrained. Many chapters would be required to provide a clear and detailed overview of how to configure these systems. The default profiles are performing tasks like blocking access to parts of the _/proc_ and _/sys_ filesystems that would be dangerous to expose in the container, even though they show up in the container’s namespace. The default profiles also provide more narrowly scoped mount access to prevent containers from getting hold of mount points they should not see.

If you are considering using Linux containers in production, it is worth seriously considering going through the effort to enable AppArmor or SELinux on these systems. For the most part, both systems are reasonably equivalent. But in the Docker context, one notable limitation of SELinux is that it only works fully on systems that support filesystem metadata, which means that it won’t work with all Docker storage drivers. AppArmor, on the other hand, does not use filesystem metadata and therefore works on all of the Docker backends. Which one you use is somewhat distribution-centric, so you may be forced to choose a filesystem backend that also supports the security system that you use.

### The Docker Daemon

From a security standpoint, the Docker daemon and its components are the only completely new risk you are introducing to your infrastructure. Your containerized applications are not any less secure and are, at least, a little more secure than they would be if deployed outside of containers. But without the containers, you would not be running `dockerd`, the Docker daemon. You can run Docker such that it doesn’t expose any ports on the network. This is highly recommended and the default for most Docker installations.

The default configuration for Docker, on most distributions, leaves Docker isolated from the network with only a local Unix socket exposed. Since you cannot remotely administer Docker when it is set up this way, it is not uncommon to see people simply add the nonencrypted port 2375 to the configuration. This may be great for getting started with Docker, but it is not what you should do in any environment where you care about the security of your systems. You should not open Docker up to the outside world at all unless you have a very good reason to. If you do, you should also commit to properly securing it. Most scheduler systems run their services on each node and expect to talk to Docker over the Unix domain socket instead of over a network port.

If you do need to expose the daemon to the network, you can do a few things to tighten Docker down in a way that makes sense in most production environments. But no matter what you do, you are relying on the Docker daemon itself to be resilient against threats like buffer overflows and race conditions, two of the more common classes of security vulnerabilities. This is true of any network service. The risk is a lot higher with the Docker daemon because it is normally run as `root`, it can run anything on your system, and it has no integrated role-based access controls.

The basics of locking Docker down are common with many other network daemons: encrypt your traffic and authenticate users. The first is reasonably easy to set up on Docker; the second is not as easy. If you have SSL certificates you can use for protecting HTTP traffic to your hosts, such as a wildcard certificate for your domain, you can turn on TLS support to encrypt all of the traffic to your Docker servers, using port 2376. This is a good first step. The [Docker documentation](https://docs.docker.com/engine/security/protect-access) will walk you through doing this.

Authenticating users is more complicated. Docker does not provide any kind of fine-grained authorization: you either have access or you don’t. But the authentication control it does provide—signed certificates—is reasonably strong. Unfortunately, this also means that you don’t get a cheap step from no authentication to some authentication without also having to set up a certificate authority in most cases. If your organization already has one, then you are in luck. Certificate management needs to be implemented carefully in any organization, both to keep certificates secure and to distribute them efficiently. So, given that, here are the basic steps:

1. Set up a method of generating and signing certificates.
2. Generate certificates for the server and clients.
3. Configure Docker to require certificates with `--tlsverify`.

Detailed instructions on getting a server and client set up, as well as a simple certificate authority, are included in the [Docker documentation](https://docs.docker.com/engine/security/protect-access).

**WARNING**

Because it’s a daemon that almost always runs with privilege, and because it has direct control of your applications, it is a bad idea to expose Docker directly on the internet. If you need to talk to your Docker hosts from outside your network, consider something like a VPN or an SSH tunnel to a secure jump host.

## Advanced Configuration

Docker has a very clean external interface, and on the surface, it looks pretty monolithic. But there are actually a lot of things going on behind the scenes that are configurable, and the logging backends we described in [“Logging”](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch06.html#docker\_logs) are a good example. You can also do things like change out the storage backend for container images for the whole daemon, use a completely different runtime, or configure individual containers to run on a different network configuration. Those are powerful switches, and you’ll want to know what they do before turning them on. First, we’ll talk about the network configuration, then we’ll cover the storage backends, and finally, we’ll try out a completely different container runtime to replace the default `runc` supplied with Docker.

### Networking

Early on, we described the layers of networking between a Linux container and the real, live network. Let’s take a closer look at how that works. Docker supports a rich set of network configurations, but let’s start with the default setup. [Figure 11-1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch11.html#figure11-1) shows a drawing of a typical Docker server, where three containers are running on their private network, shown on the right. One of them has a public port (TCP port 10520) that is exposed on the Docker server. We’ll track how an inbound request gets to the Linux container and also how a Linux container can make an outbound connection to the external network.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098131814/files/assets/dur3_1101.png" alt="The network on a typical Docker server" height="289" width="600"><figcaption></figcaption></figure>

**Figure 11-1. The network on a typical Docker server**

If we have a client somewhere on the network that wants to talk to the `nginx` server running on TCP port 80 inside Container 1, the request will come into the `eth0` interface on the Docker server. Because Docker knows this is a public port, it has spun up an instance of `docker-proxy` to listen on port 10520. So our request is passed to the `docker-proxy` process, which then forwards the request to the correct container address and port on the private network. Return traffic from the request flows through the same route.

Outbound traffic from the container follows a different route in which the `docker-proxy` is not involved at all. In this case, Container 3 wants to contact a server on the public internet. It has an address on the private network of 172.16.23.1, and its default route is the `docker0` interface 172.16.23.7. So it sends the traffic there. The Docker server now sees that this traffic is outbound and that it has traffic forwarding enabled. And since the virtual network is private, it wants to send the traffic from its public address instead. So the request is passed through the kernel’s network address translation (NAT) layer and put onto the external network via the `eth0` interface on the server. Return traffic passes through the same route. The NAT is one-way, so containers on the virtual network will see real network addresses in response packets.

You’ve probably noticed that it’s not a simple configuration. It’s a fair amount of complexity, but it makes Docker seem pretty transparent. It also contributes to the security posture of the Docker stack because the containers are namespaced into individual network namespaces, are on individual private networks, and don’t have access to things like the main system’s DBus (Desktop Bus) or iptables.

Let’s examine what’s happening at a more detailed level. The interfaces that show up in `ifconfig` or `ip addr show` in the Linux container are actually virtual Ethernet interfaces on the Docker server’s kernel. They are then mapped into the container’s network namespace and given the names that you see inside the container. Let’s take a look at what we might see when running `ip addr show` on a Docker server. We’ll shorten the output a little for clarity and spaces, as shown here:

```
$ ip addr show

1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group …
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 brd 127.255.255.255 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state …
    link/ether 02:50:00:00:00:01 brd ff:ff:ff:ff:ff:ff
    inet 172.16.168.178/24 brd 192.168.65.255 scope global dynamic …
       valid_lft 4908sec preferred_lft 3468sec
    inet6 fe80::50:ff:fe00:1/64 scope link
       valid_lft forever preferred_lft forever
…
7: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue …
    link/ether 02:42:9c:d2:89:4f brd ff:ff:ff:ff:ff:ff
    inet 172.17.42.1/16brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:9cff:fed2:894f/64 scope link
       valid_lft forever preferred_lft forever
…
185: veth772de2a@if184: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc …
    link/ether 9a:a9:24:b7:5a:31 brd ff:ff:ff:ff:ff:ff link-netnsid 1
    inet6 fe80::98a9:24ff:feb7:5a31/64 scope link
       valid_lft forever preferred_lft forever
```

What this tells us is that we have the normal loopback interface, our real Ethernet interface `eth0`, and then the Docker bridge interface, `docker0`, that we described earlier. This is where all the traffic from the Linux containers is picked up to be routed outside the virtual network. The surprising thing in this output is the `veth772de2a` interface. When Docker creates a container, it creates two virtual interfaces, one of which sits on the server side and is attached to the `docker0` bridge, and one that is attached to the container’s namespace. What we’re seeing here is the server-side interface. Did you notice how it doesn’t show up as having an IP address assigned to it? That’s because this interface is just joined to the bridge. This interface will have a different name in the container’s namespace as well.

As with so many pieces of Docker, you can replace the proxy with a different implementation. To do so, you would use the `--userland-proxy-path=<path>` setting, but there are probably not that many good reasons to do this unless you have a very specialized network. However, the `--userland-proxy=false` flag to `dockerd` will completely disable the `userland-proxy` and instead rely on hairpin [NAT](https://www.geeksforgeeks.org/network-address-translation-nat) functionality to route traffic between local containers. If you need higher-throughput services, this might be right for you.

**NOTE**

A hairpin NAT is typically used to describe services inside a NATed network that address one another with their public IP addresses. This causes traffic from the source service to route out to the internet, hit the external interface for the NAT router, and then get routed back into the original network to the destination service. The traffic is shaped like the letter U or a standard hairpin.

#### Host networking

As we’ve noted, there is a lot of complexity involved in the default implementation. You can, however, run a container without the whole networking configuration that Docker puts in place for you. And the `docker-proxy` can also limit the throughput for very high-volume data services by requiring all the network traffic to pass through the `docker-proxy` process before being received by the container. So what does it look like if we turn off the Docker network layer? Since the beginning, Docker has let you do this on a per-container basis with the `--net=host` command-line switch. There are times, like when you want to run high-throughput applications, when you might want to do this. But you lose some of Docker’s flexibility when you do. Let’s examine how this mechanism works.

**WARNING**

Like others we discuss in this chapter, this is not a setting you should take lightly. It has operational and security implications that might be outside your tolerance level. It can be the right thing to do, but you should understand the consequences.

Let’s start a container with `--net=host` and see what happens:

```
$ docker container run --rm -it --net=host spkane/train-os bash

[root@docker-desktop /]# docker container run --rm -it --net=host \
                         spkane/train-os ip addr show

1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group
                              default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 brd 127.255.255.255 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast
                                           state UP group default qlen 1000
    link/ether 02:50:00:00:00:01 brd ff:ff:ff:ff:ff:ff
    inet 192.168.65.3/24 brd 192.168.65.255 scope global dynamic
                                            noprefixroute eth0
       valid_lft 4282sec preferred_lft 2842sec
    inet6 fe80::50:ff:fe00:1/64 scope link
       valid_lft forever preferred_lft forever
…
7: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue
                                                state DOWN group default
    link/ether 02:42:9c:d2:89:4f brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:9cff:fed2:894f/64 scope link
       valid_lft forever preferred_lft forever
8: br-340323d07310: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc
                                          noqueue state DOWN group default
    link/ether 02:42:56:24:42:b8 brd ff:ff:ff:ff:ff:ff
    inet 172.22.0.1/16 brd 172.22.255.255 scope global br-340323d07310
       valid_lft forever preferred_lft forever
11: br-01f7537b9475: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc
                                           noqueue state DOWN group default
    link/ether 02:42:ed:14:67:61 brd ff:ff:ff:ff:ff:ff
    inet 172.18.0.1/16 brd 172.18.255.255 scope global br-01f7537b9475
       valid_lft forever preferred_lft forever
    inet6 fc00:f853:ccd:e793::1/64 scope global
       valid_lft forever preferred_lft forever
    inet6 fe80::42:edff:fe14:6761/64 scope link
       valid_lft forever preferred_lft forever
    inet6 fe80::1/64 scope link
       valid_lft forever preferred_lft forever
```

That should look pretty familiar. That’s because when we run a container with the host networking option, the container is running in both the host server’s network and UTS namespaces. Our server’s hostname is `docker-desktop`, and from the shell prompt, we can tell that our container has the same hostname:

```
[root@docker-desktop /]# hostname
docker-desktop
```

If we run the `mount` command to see what’s mounted, though, we see that Docker is still maintaining our _/etc/resolv.conf_, _/etc/hosts_, and _/etc/hostname_ directories. And as expected, the _/etc/hostname_ directory simply contains the server’s hostname:

```
[root@docker-desktop /]# mount

overlay on / type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/…)
…
/dev/vda1 on /etc/resolv.conf type ext4 (rw,relatime)
/dev/vda1 on /etc/hostname type ext4 (rw,relatime)
/dev/vda1 on /etc/hosts type ext4 (rw,relatime)
…

[root@docker-desktop /]# cat /etc/hostname
docker-desktop
```

Just to prove that we can see all the normal networking on the Docker server, let’s look at the output from `ss` to see if we can see the sockets that Docker is utilizing:

```
root@852d18f5c38d:/# ss | grep docker

u_str  ESTAB  0  0  /run/guest-services/docker.sock  18086  * 16860
…
u_str  ESTAB  0  0  /var/run/docker.sock             21430  * 21942
```

**NOTE**

If the Docker daemon was listening on a TCP port, like 2375, you could have looked for that as well. Feel free to look for another TCP port on your server port that you know is in use.

If you search for `docker` in the output of a normal container within its own namespace, you will notice that you get no results:

```
$ docker container run --rm -it spkane/train-os bash -c "ss | grep docker"
```

So we are indeed in the server’s network namespace. What all of this means is that if we were to launch a high-throughput network service, we could expect network performance from it that is essentially native. But it also means we could try to bind to ports that would collide with those on the server, so if you do this, you should be careful about how you allocate port assignments.

#### Configuring networks

There is more to networking than just the default network or host networking, however. The `docker network` command lets you create multiple networks backed by different drivers. It also allows you to view and manipulate the Docker network layers and how they are attached to containers that are running on the system.

Listing the networks available from Docker’s perspective is easily accomplished with the following command:

```
$ docker network ls

NETWORK ID      NAME      DRIVER    SCOPE
5840a6c23373    bridge    bridge    local
1c22b4582189    host      host      local
c128bfdbe003    none      null      local
```

You can then find out more details about any individual network by using the `docker network inspect` command along with the network ID:

```
$ docker network inspect 5840a6c23373
```

```
[
    {
        "Name": "bridge",
        "Id": "5840…fc94",
        "Created": "2022-09-23T01:21:55.697907958Z",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": [
                {
                    "Subnet": "172.17.0.0/16",
                    "Gateway": "172.17.0.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {},
        "Options": {
            "com.docker.network.bridge.default_bridge": "true",
            "com.docker.network.bridge.enable_icc": "true",
            "com.docker.network.bridge.enable_ip_masquerade": "true",
            "com.docker.network.bridge.host_binding_ipv4": "0.0.0.0",
            "com.docker.network.bridge.name": "docker0",
            "com.docker.network.driver.mtu": "1500"
        },
        "Labels": {}
    }
]
```

Docker networks can be created and removed, as well as attached and detached from individual containers, with the `network` subcommand.

So far, we’ve set up a bridged network, no Docker network, and a bridged network with hairpin NAT. There are a few other drivers that you can use to create different topologies using Docker as well, with the `overlay` and `macvlan` drivers being the most common. Let’s take a brief look at what these can do for you:

`overlay`

This driver is used in Swarm mode to generate a network overlay between the Docker hosts, creating a private network between all the containers that run on top of the real network. This is useful for Swarm but not in scope for general use with non-Swarm containers.

`macvlan`

This driver creates a real MAC address for each of your containers and then exposes them on the network via the interface of your choice. This requires that you switch gears to support more than one MAC address per physical port on the switch. The result is that all the containers appear directly on the underlying network. When you’re moving from a legacy system to a container-native one, this can be a really useful step. There are drawbacks here, such as making it harder when debugging to identify which host the traffic is really coming from, overflowing the MAC tables in your network switches, excessive ARPing by container hosts, and other underlying network issues. For this reason, the `macvlan` driver is not recommended unless you have a good understanding of your underlying network and can manage it effectively.

There are a few sets of configurations that are possible here, but the basic setup is easy to configure:

```
$ docker network create -d macvlan \
    --subnet=172.16.16.0/24 \
    --gateway=172.16.16.1  \
    -o parent=eth0 ourvlan

$ docker network ls
NETWORK ID     NAME            DRIVER    SCOPE
5840a6c23373   bridge          bridge    local
1c22b4582189   host            host      local
c128bfdbe003   none            null      local
8218c0ecc9e2   ourvlan         macvlan   local

$ docker network rm 8218c0ecc9e2
```

**TIP**

You can prevent Docker from allocating specific addresses by specifying them as named auxiliary addresses, `--aux-address="my-router=172.16.16.129"`.

There is a lot more you can configure with the Docker network layer. However, the defaults, host networking, and userland proxyless mode are the ones that you’re most likely to use or encounter in the wild. Some of the other options you can configure include the container’s DNS nameservers, resolver options, and default gateways, among other things. The networking section of the [Docker documentation](https://docs.docker.com/network) gives an overview of how to do some of this configuration.

**NOTE**

For advanced network configuration of Docker, check out [Weave](https://github.com/weaveworks/weave)—a well-supported overlay network tool for spanning containers across multiple Docker hosts, similar to the `overlay` driver but much more configurable and without the Swarm requirement. Another offering is [Project Calico](https://www.tigera.io/project-calico). If you’re running Kubernetes, which has its own networking configuration, you might also want to familiarize yourself with the [Container Network Interface (CNI)](https://www.cni.dev/) and then look at [Cilium](https://cilium.io/), which provides robust eBPF-based networking for containers.

## Storage

Backing all of the images and containers on your Docker server is a storage backend that handles reading and writing all of that data. Docker has some strenuous requirements on its storage backend: it has to support layering, the mechanism by which Docker tracks changes and reduces both how much disk a container occupies and how much is shipped over the wire to deploy new images. Using a copy-on-write strategy, Docker can start up a new container from an existing image without having to copy the whole image. The storage backend supports that. The storage backend is what makes it possible to export images as groups of changes in layers and also lets you save the state of a running container. In most cases, you need the kernel’s help in doing this efficiently. That’s because the filesystem view in your container is generally a union of all of the layers below it, which are not actually copied into your container. Instead, they are made visible to your container, and only when you make changes does anything get written to your container’s filesystem. One place this layering mechanism is exposed to you is when you upload or download a new image from a registry like Docker Hub. The Docker daemon will push or pull each layer separately, and if some of the layers are the same as others it has already stored, it will use the cached layer instead. In the case of a push to a registry, it will sometimes even tell you which image they are mounted from.

Docker relies on an array of possible kernel drivers to handle the layering. The Docker codebase contains code that can handle interacting with many of these backends, and you can configure the decision about which to use on daemon restart. So let’s look at what is available and some of the pluses and minuses of each.

Various backends have different limitations that may or may not make them your best option. In some cases, your choices of which backend to use are limited by what your distribution of Linux supports. Using the drivers that are built into the kernel shipped with your distribution will always be the easiest approach. It’s generally best to stay close to the well-tested path. We’ve seen all manner of oddities from various backends since Docker’s release. And, as usual, the common case is always the best-supported one. Different backends also report different statistics through the Docker Remote API (_/info_ endpoint). This can be very useful for monitoring your Docker systems. However, not all backends are created equal, so let’s see how they differ:

_Overlay_

[Overlay](https://www.kernel.org/doc/html/latest/filesystems/overlayfs.html) (formerly OverlayFS) is a union filesystem where multiple layers are mounted together so that they appear as a single filesystem. The Overlay filesystem is the most recommended choice for Docker storage these days and works on most major distributions. If you are running on a Linux kernel older than 4.0 (or 3.10.0-693 for RHEL), then you won’t be able to take advantage of this backend. The reliability and performance are good enough that it might be worth updating your OS for Docker hosts to support it, even if your company standard is an older distribution. The Overlay filesystem is part of the mainline Linux kernel and has become increasingly stable over time. Being in the mainline means that long-term support is virtually guaranteed, which is another nice advantage. Docker supports two versions of the Overlay backend, `overlay` and `overlay2`. As you might expect, you are strongly advised to use `overlay2` as it is faster, more efficient with inode usage, and more robust.

**NOTE**

The Docker community is frequently improving support for a variety of filesystem backends. For more details about the supported filesystems, take a look at the [official documentation](https://docs.docker.com/storage/storagedriver).

_AuFS_

Although at the time of this writing it is no longer recommended, `aufs` is the original backend for Docker. [AuFS (Advanced multilayered unification filesystem)](https://aufs.sourceforge.net/) is a union filesystem driver with reasonable support on various popular Linux distributions. It was never accepted into the mainline kernel, however, and this has limited its availability on various distributions. It is not supported on recent versions of Red Hat or Fedora, for example. It is not shipped in the standard Ubuntu distribution but is in the Ubuntu `linux-image-extra` package.

Its status as a second-class citizen in the kernel has led to the development of many of the other backends now available. If you are running an older distribution that supports AuFS, you might consider it, but you should upgrade to a kernel version that natively supports Overlay or Btrfs, which is discussed next.

_Btrfs_

[B-Tree File System (Btrfs)](https://btrfs.wiki.kernel.org/index.php/Main\_Page) is fundamentally a copy-on-write filesystem, which means it’s a pretty good fit for the Docker image model. Like `aufs` and unlike `devicemapper`, Docker is using the backend in the way it was intended. That means it’s both pretty stable in production and also a good performer. It scales reasonably to thousands of containers on the same system. A drawback for Red Hat–based systems is that Btrfs does not support SELinux. If you can use the `btrfs` backend, it is worth exploring another option, after the `overlay2` driver. One popular way to run `btrfs` backends for Linux containers without having to give over a whole volume to this filesystem is to make a Btrfs filesystem in a file and loopback-mount it with something like `mount -o loop file.btrs /mnt`. Using this method, you could build a 50 GB Linux container storage filesystem even on cloud-based systems without having to give over all your precious local storage to Btrfs.

_Device Mapper_

Originally written by Red Hat to support their distributions, which lacked AuFS in Docker’s early days, Device Mapper became the default backend on all Red Hat–based distributions of Linux. Depending on the version of Red Hat Linux that you are using, this may be your only option. Device Mapper itself has been built into the Linux kernel for ages and is very stable. The way the Docker daemon uses it is a bit unconventional, though, and in the past, this backend was not that stable. This checkered past means that we recommend picking a different backend when possible. If your distribution supports only the `devicemapper` driver, then you will likely be fine. But it’s worth considering using `overlay2` or `btrfs`. By default, `devicemapper` utilizes the `loop-lvm` mode, which has zero configuration and is very slow and generally only useful for development. If you decide to use the `devicemapper` driver, you must make sure it is configured to use `direct-lvm` mode for all nondevelopment environments.

**NOTE**

You can find out more about using the various `devicemapper` modes with Docker in the [official documentation](https://docs.docker.com/storage/storagedriver/device-mapper-driver). A 2014 [blog article](https://developers.redhat.com/blog/2014/09/30/overview-storage-scalability-docker) also provides some interesting history about the various Docker storage backends.

_VFS_

Of the supported drivers, the Virtual File System (`vfs`) driver is the simplest, and slowest, to start up. It doesn’t actually support copy-on-write. Instead, it makes a new directory and copies over all of the existing data. It was originally intended for use in tests and for mounting host volumes. The `vfs` driver is very slow to create new containers, but runtime performance is native, which is a real benefit. Its mechanism is very simple, which means there is less to go wrong. Docker, Inc., does not recommend it for production use, so proceed with caution if you think it’s the right solution for your production environment.

_ZFS_

ZFS, which was created by Sun Microsystems, is the most advanced open source filesystem available on Linux. Due to licensing restrictions, it does not ship in mainline Linux. However, the [ZFS on Linux project](https://zfsonlinux.org/) has made it pretty easy to install. Docker can then run on top of the ZFS filesystem and use its advanced copy-on-write facilities to implement layering. Given that ZFS is not in the mainline kernel and not available off the shelf in the major commercial distributions, going this route requires some extended effort. However, if you are already running ZFS in production, this may be your very best option.

**WARNING**

Storage backends can have a big impact on the performance of your containers. And if you swap the backend on your Docker server, all of your existing images will disappear. They are not gone, but they will not be visible until you switch the driver back. Caution is advised.

You can use `docker system info` to see which storage backend your system is running:

```
$ docker system info
```

```
…
 Storage Driver: overlay2
  Backing Filesystem: extfs
  Supports d_type: true
  Native Overlay Diff: true
  userxattr: false
…
```

As you can see, Docker will also tell you what the underlying or “backing” filesystem is if there is one. Since we’re running `overlay2` here, we can see it’s backed by an `ext` filesystem. In some cases, like with `devicemapper` on raw partitions or with `btrfs`, there won’t be a different underlying filesystem.

Storage backends can be swapped via the `daemon-json` configuration file or via command-line arguments to `dockerd` on startup. If we wanted to switch our Ubuntu system from `aufs` to `devicemapper`, we could do so like this:

```
$ dockerd --storage-driver=devicemapper
```

That will work on pretty much any Linux system that can support Docker because `devicemapper` is almost always present. The same is true for `overlay2` on modern Linux kernels. However, you will need to have the actual underlying dependencies in place for the other drivers. For example, without `aufs` in the kernel—​usually via a kernel module—​Docker will not start up with `aufs` set as the storage driver, and the same is true for Btrfs or ZFS.

Getting the appropriate storage driver for your systems and deployment needs is one of the more important technical points to get right when you’re taking Docker to production. Be conservative: make sure the path you choose is well supported in your kernel and distribution. Historically, this was a pain point, but most of the drivers have reached reasonable maturity. Remain cautious for any newly appearing backends, however, as this space continues to change. Getting new backend drivers to work reliably for production systems takes quite some time, in our experience.

## nsenter

`nsenter`, which is short for “namespace enter,” allows you to enter any Linux namespace and is part of the core `util-linux` package from [kernel.org](https://mirrors.edge.kernel.org/pub/linux/utils/util-linux). Using `nsenter`, we can get into a Linux container from the server itself, even in situations where the `dockerd` server is not responding and we can’t use `docker container exec`. It can also be used to manipulate things in a container as `root` on the server that would otherwise be prevented by `docker container exec`. This can be truly useful when you are debugging. Most of the time, `docker container exec` is all you need, but you should have `nsenter` in your tool belt.

Most Linux distributions ship with a new-enough `util-linux` package that it will contain `nsenter`. If you are on a distribution that does not have it, the easiest way to get hold of `nsenter` is to install it via the third-party [Linux container](https://github.com/jpetazzo/nsenter).

This container works by pulling a Docker image from the Docker Hub registry and then running a Linux container that will install the `nsenter` command-line tool into _/usr/local/bin_. This might seem strange at first, but it’s a clever way to allow you to install `nsenter` to any Docker server remotely using nothing more than the `docker` command.

Unlike `docker container exec`, which can be run remotely, `nsenter` requires that you run it on the server itself, directly or via a container. For our purposes, we’ll use a specially crafted container to run `nsenter`. As with the `docker container exec` example, we need to have a container running:

```
$ docker container run -d --rm  ubuntu:22.04 sleep 600
fd521174d66dc32650d165e0ce7dd97255c7b3624c34cb1d119d955284382ddf
```

`docker container exec` is pretty simple, but `nsenter` is a little inconvenient to use. It needs to have the PID of the actual top-level process in your container, which is not obvious to find. Let’s go ahead and run `nsenter` by hand so you can see what’s going on.

First, we need to find out the ID of the running container, because `nsenter` needs to know that to access it. We can easily get this using `docker container ls`:

```
$ docker container ls

CONTAINER ID  IMAGE          COMMAND      …  NAMES
fd521174d66d   ubuntu:22.04  "sleep 1000" …  angry_albattani
```

The ID we want is that first field, `fd521174d66d`. With that, we can now find the PID we need, like this:

```
$ docker container inspect --format \{{.State.Pid\}} fd521174d66d
2721
```

**TIP**

You can also get the real PIDs of the processes in your container by running the command `docker container top`, followed by the container ID. In our example, this would look like the following:

```
$ docker container top fd521174d66d

UID   PID   PPID  C  STIME  TTY  TIME      CMD
root  2721  2696  0  20:37  ?    00:00:00  sleep 600
```

Make sure to update the `--target` argument in the following command with the process ID that you got from the previous command, then go ahead and invoke `nsenter`:

```
$ docker container run --rm -it --privileged --pid=host debian \
    nsenter --target 2721 --all

# ps -ef

UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 20:37 ?        00:00:00 sleep 600
root        11     0  0 20:51 ?        00:00:00 -sh
root        15    11  0 20:51 ?        00:00:00 ps -ef
# exit
```

If the result looks a lot like `docker container exec`, that’s because it does almost the same thing under the hood!

The command-line argument `--all` is telling `nsenter` that we want to enter all of the namespaces used by the process specified with `--target`.

### Debugging Shell-less Containers

If you want to troubleshoot a container that does not have a Unix shell, then things get a little trickier, but it is still possible. For this example, we can run a container that has a single executable in it:

```
$ docker container run --rm -d --name outyet-small \
    --publish mode=ingress,published=8090,target=8080 \
    spkane/outyet:1.9.4-small
4f6de24d4c9c794c884afa758ef5b33ea38c01f8ec9314dcddd9fadc25c1a443
```

Let’s take a quick look at the processes that are running in this container:

```
$ docker container top outyet-small

UID  PID   PPID  C STIME TTY TIME     CMD
root 61033 61008 0 22:43 ?   00:00:00 /outyet -version 1.9.4 -poll 600s …
```

If you try to launch a Unix shell in the container, you will get an error:

```
$ docker container exec -it outyet-small /bin/sh

OCI runtime exec failed: exec failed: unable to start container process: exec:
  "/bin/sh": stat /bin/sh: no such file or directory: unknown
```

We can then launch a second container that includes a shell and some other useful tools in a way that the new container can see the processes in the first container, is using the same network stack as the first container, and has some extra privileges which will be helpful for our debugging:

```
$ docker container run --rm -it --pid=container:outyet-small \
  --net=container:outyet-small --cap-add sys_ptrace \
  --cap-add sys_admin spkane/train-os /bin/sh

sh-5.1#
```

If you type `ls` in this container, you will see in the filesystem the `spkane/train-os` image, which contains `/bin/sh` and all of our debugging tools, but it does not contain any of the files from our `outyet-small` container:

```
sh-5.1# ls

bin   dev  home  lib64       media  opt   root  sbin  sys  usr
boot  etc  lib   lost+found  mnt    proc  run   srv   tmp  var
```

However, if you type `ps -ef`, you will notice that you see all of the processes from the original container. This is because we told Docker to attach to use the namespace from the `outyet-small` container by passing in `--pid=container:outyet-small`:

```
sh-5.1# ps -ef

UID  PID PPID C STIME TTY   TIME     CMD
root   1    0 0 22:43 ?     00:00:00 /outyet -version 1.9.4 -poll 600s …
root  29    0 0 22:47 pts/0 00:00:00 /bin/sh
root  36   29 0 22:49 pts/0 00:00:00 ps -ef
```

And because we are using the same network stack, you can even `curl` the port that the `outyet` service from the first container is bound to:

```
sh-5.1# curl localhost:8080
```

```
<!DOCTYPE html><html><body><center>
  <h2>Is Go 1.9.4 out yet?</h2>
  <h1>

    <a href="https://go.googlesource.com/go/&#43;/go1.9.4">YES!</a>

  </h1>
  <p>Hostname: 155914f7c6cd</p>
</center></body></html>
```

At this point, you could use `strace` or whatever else you wanted to debug your application, and then finally `exit` the new debug container, leaving your original container still running on the server.

**WARNING**

If you run `strace`, you will need to type Ctrl-C to exit the `strace` process.

```
sh-5.1# strace -p 1

strace: Process 1 attached
futex(0x963698, FUTEX_WAIT, 0, NULL^Cstrace: Process 1 detached
 <detached …>

sh-5.1# exit
exit
```

You’ll notice that we could not see the filesystem in this use case. If you need to view or copy files from the container, you can make use of the `docker container export` command to retrieve a tarball of the container’s filesystem:

```
$ docker container export outyet-small -o export.tar
```

You can then use `tar` to view or extract the files:

```
$ tar -tvf export.tar

-rwxr-xr-x  0 0   0         0 Jul 17 16:04 .dockerenv
drwxr-xr-x  0 0   0         0 Jul 17 16:04 dev/
-rwxr-xr-x  0 0   0         0 Jul 17 16:04 dev/console
drwxr-xr-x  0 0   0         0 Jul 17 16:04 dev/pts/
drwxr-xr-x  0 0   0         0 Jul 17 16:04 dev/shm/
drwxr-xr-x  0 0   0         0 Jul 17 16:04 etc/
-rwxr-xr-x  0 0   0         0 Jul 17 16:04 etc/hostname
-rwxr-xr-x  0 0   0         0 Jul 17 16:04 etc/hosts
lrwxrwxrwx  0 0   0         0 Jul 17 16:04 etc/mtab -> /proc/mounts
-rwxr-xr-x  0 0   0         0 Jul 17 16:04 etc/resolv.conf
drwxr-xr-x  0 0   0         0 Apr 24  2021 etc/ssl/
drwxr-xr-x  0 0   0         0 Apr 24  2021 etc/ssl/certs/
-rw-r--r--  0 0   0    261407 Mar 13  2018 etc/ssl/certs/ca-certificates.crt
-rwxr-xr-x  0 0   0   5640640 Apr 24  2021 outyet
drwxr-xr-x  0 0   0         0 Jul 17 16:04 proc/
drwxr-xr-x  0 0   0         0 Jul 17 16:04 sys/
```

When you are finished, go ahead and delete `export.tar`, and then stop the `outyet-small` container with `docker container stop outyet-small`.

**NOTE**

You can explore the container’s filesystem from the Docker server by navigating directly to where the filesystem resides on the server’s storage system. This will typically look something like _/var/lib/docker/overlay/fd5…_ but will vary based on the Docker setup, storage backend, and container hash. You can determine your Docker root directory by running `docker system info`.

## The Structure of Docker

What we think of as Docker is made of five major server-side components that present a common front via the API. These parts are `dockerd`, `containerd`, `runc`, `containerd-shim-runc-v2`, and the `docker-proxy` we described in [“Networking”](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch11.html#docker\_net). We’ve spent a lot of time interacting with `dockerd` and the API it presents. It is, in fact, responsible for orchestrating the whole set of components that make up Docker. But when it starts a container, Docker relies on `containerd` to handle instantiating the container. All of this used to be handled in the `dockerd` process itself, but there were several shortcomings to that design:

* `dockerd` had a huge number of jobs.
* A monolithic runtime prevented any of the components from being swapped out easily.
* `dockerd` had to supervise the lifecycle of the containers themselves, and it couldn’t be restarted or upgraded without losing all the running containers.

Another major motivation for `containerd` was that, as we’ve just shown, containers are not just a single abstraction. On the Linux platform, they are processes involving namespaces, cgroups, and security rules in AppArmor or SELinux. But Docker also runs on Windows and may even work on other platforms in the future. The idea of `containerd` is to present a standard layer to the outside world where, regardless of implementation, developers can think about the higher-level concepts of containers, tasks, and snapshots rather than worry about specific Linux system calls. This simplifies the Docker daemon a lot and enables platforms like Kubernetes to integrate directly into `containerd` rather than using the Docker API. Kubernetes relied on a Docker shim for many years, but nowadays it uses `containerd` directly.

Let’s take a look at the components (shown in [Figure 11-2](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch11.html#figure11-2)) and see what each of them does:

`dockerd`

One per server. Serves the API, builds container images, and does high-level network management, including volumes, logging, statistics reporting, and more.

`docker-proxy`

One per port forwarding rule. Each instance handles the forwarding of the defined protocol traffic (TCP/UDP) from the defined host IP and port to the defined container IP and port.

`containerd`

One per server. Manages the lifecycle, execution, copy-on-write filesystem, and low-level networking drivers.

`containerd-shim-runc-v2`

One per container. Handles file descriptors passed to the container (e.g., `stdin`/`out`) and reports exit status.

`runc`

Constructs the container and executes it, gathers statistics, and reports events on the lifecycle.

<figure><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098131814/files/assets/dur3_1102.png" alt="Structure of Docker" height="339" width="600"><figcaption></figcaption></figure>

**Figure 11-2. Structure of Docker**

`dockerd` and `containerd` speak to each other over a socket, usually a Unix socket, using a [gRPC API](https://grpc.io/). `dockerd` is the client in this case, and `containerd` is the server! `runc` is a CLI tool that reads configuration from JSON on disk and is executed by `containerd`.

When we start a new container, `dockerd` will handle making sure that the image is present or will pull it from the repository specified in the image name. (In the future, this responsibility may shift to `containerd`, which already supports image pulls.) The Docker daemon also does most of the rest of the setup around the container, like launching `docker-proxy` to set up port forwarding. It then talks to `containerd` and asks it to run the container. `containerd` will take the image and apply the container configuration passed in from `dockerd` to generate an [OCI bundle](https://www.opencontainers.org/) that `runc` can execute.[1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch11.html#idm46803124791184) It will then execute `containerd-shim-runc-v2` to start the container. This will in turn execute `runc` to construct and start the container. However, `runc` will not stay running, and the `containerd-shim-runc-v2` will be the actual parent process of the new container process.

If we launch a container and then look at the output of `ps axlf` on the Docker server, we can see the parent/child relationship between the various processes. PID 1 is `/sbin/init` and is the parent process for `containerd`, `dockerd`, and the `containerd-shim-runc-v2`.

**NOTE**

Docker Desktop’s VM contains minimal versions of most Linux tools, and some of these commands may not produce the same output that you will get if you use a standard Linux server as the Docker daemon host.

```
$ docker container run --rm -d \
  --publish mode=ingress,published=8080,target=80 \
  --name nginx-test --rm nginx:latest
08b5cffed7baaf32b3af50498f7e5c5fa7ed35e094fa6045c205a88746fe53dd

$ ps axlf
… PID  PPID COMMAND
…
… 5171 1    /usr/bin/containerd
… 5288 1    /usr/bin/dockerd -H fd:// --containerd=/run/cont…/containerd.…
… 5784 5288 \_ /usr/bin/docker-proxy -proto tcp -host-ip … -host-port 8080
… 5791 5288 \_ /usr/bin/docker-proxy -proto tcp -host-ip :: -host-port …
… 5807 1    /usr/bin/containerd-shim-runc-v2 -namespace moby -id …
… 5829 5807  \_ nginx: master process nginx -g daemon off;
… 5880 5829      \_ nginx: worker process
… 5881 5829      \_ nginx: worker process
… 5882 5829      \_ nginx: worker process
… 5883 5829      \_ nginx: worker process
…
```

So what happened to `runc`? Its job is to construct the container and start it running, then it leaves and its children are inherited by its parent, the `containerd-shim-runc-v2`. This leaves the minimal amount of code in memory necessary to manage the file descriptors and exit status for `containerd`.

To help you understand what’s going on here, let’s take a deeper look at what happens when we start a container. We’ll just reuse the `nginx` container that we already have running for this since it’s very lightweight and the container stays running when backgrounded:

```
$ docker container ls

CONTAINER ID IMAGE        COMMAND        … PORTS                  NAMES
08b5cffed7ba nginx:latest "/docker-ent…" … 0.0.0.0:8080->80/tcp … nginx-test
```

Let’s use the `runc` runtime CLI tool to take a look at its view of the system. We could see a similar view from `ctr`, the CLI client for `containerd`, but `runc` is nicer to work with, and it’s at the lowest level:

```
$ sudo runc --root /run/docker/runtime-runc/moby list

ID         PID   …  BUNDLE                                          … OWNER
08b5…53dd  5829  …  …/io.containerd.runtime.v2.task/moby/08b5…53dd  … root
```

We normally need root privileges to run this command. Unlike with the Docker CLI, we can’t rely on the Docker daemon’s permissions to let us access lower-level functionality. With `runc` we need direct access to these privileges. What we can see in the output from `runc` is our container! This is the actual OCI runtime bundle that represents our container, with which it shares an ID. Notice that it also gives us the PID of the container; that’s the PID on the host of the application running inside the container:

```
$ ps -edaf | grep 5829

root      5829  5807  …  nginx: master process nginx -g daemon off;
systemd+  5880  5829  …  nginx: worker process
systemd+  5881  5829  …  nginx: worker process
systemd+  5882  5829  …  nginx: worker process
systemd+  5883  5829  …  nginx: worker process
```

If we look in the bundle, we’ll see a set of named pipes for our container:

```
$ sudo ls -la /run/docker/containerd/08b5…53dd

total 0
drwxr-xr-x 2 root root 80 Oct  1 08:49 .
drwxr-xr-x 3 root root 60 Oct  1 08:49 ..
prwx------ 1 root root  0 Oct  1 08:49 init-stderr
prwx------ 1 root root  0 Oct  1 08:49 init-stdout
```

You can find a lot of additional files related to your container underneath _/run/containerd/io.containerd.runtime.v2.task/moby_:

```
$ sudo ls -la /run/containerd/io.containerd.runtime.v2.task/moby/08b5…53dd/

total 32
drwx------ 3 root root  240 Oct  1 08:49 .
drwx--x--x 3 root root   60 Oct  1 08:49 ..
-rw-r--r-- 1 root root   89 Oct  1 08:49 address
-rw-r--r-- 1 root root 9198 Oct  1 08:49 config.json
-rw-r--r-- 1 root root    4 Oct  1 08:49 init.pid
prwx------ 1 root root    0 Oct  1 08:49 log
-rw-r--r-- 1 root root    0 Oct  1 08:49 log.json
-rw------- 1 root root   82 Oct  1 08:49 options.json
drwx--x--x 2 root root   40 Oct  1 08:49 rootfs
-rw------- 1 root root    4 Oct  1 08:49 runtime
-rw------- 1 root root   32 Oct  1 08:49 shim-binary-path
lrwxrwxrwx 1 root root  119 Oct  1 08:49 work -> /var/lib/containerd/io…
```

The _config.json_ file is a very verbose equivalent of what Docker shows in `docker container inspect`. We are not going to reproduce it here due to size, but we encourage you to dig around and see what’s in the config. You may, for example, note all the entries for the [“Secure Computing Mode”](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch11.html#seccomp) that are present in it.

If you want to explore `runc` some more, you can experiment with the CLI tool. Most of this is already available in Docker, usually on a higher and more useful level than the one available in `runc`. But it can be useful to explore so that you can better understand how containers and the Docker stack are put together. It’s also interesting to watch the events that `runc` reports about a running container. We can hook into those with the `runc events` command. During the normal operations of a running container, there is not a lot of activity in the events stream. But `runc` regularly reports runtime statistics, which we can see in JSON format:

```
$ sudo runc --root /run/docker/runtime-runc/moby events 08b5…53dd
```

```
{"type":"stats","id":"08b5…53dd","data":{"cpu":{"usage":{"…"}}}}
```

To conserve space, we have removed much of the output from the previous command, but this might look familiar to you now that we’ve spent some time looking at `docker container stats`. Guess where Docker gets those statistics by default. That’s right, `runc`.

At this point, you can go ahead and stop the example container by running `docker container stop nginx-test`.

## Swapping Runtimes

As we mentioned in [Chapter 2](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch02.html#docker\_glance), there are a few other native OCI-compliant runtimes that can be substituted in place of `runc`. As an example, there is [crun](https://github.com/containers/crun), which describes itself as “a fast and low-memory footprint OCI Container Runtime fully written in C.” Some other alternative native runtimes, like `railcar` and `rkt`, have been deprecated and largely abandoned. In the next section, we’ll talk about a sandboxed runtime from Google, called [gVisor](https://gvisor.dev/), which provides a user space runtime for untrusted code.

**TIP**

[Kata Containers](https://github.com/kata-containers) is a very interesting open source project that provides a runtime capable of using VMs as an isolation layer for containers. At the time of this writing, version 3 of Kata works with Kubernetes but does not work with Docker. The Kata developers [are working with the Docker developers](https://github.com/kata-containers/kata-containers/issues/5321) to try and improve this situation and create better documentation. This may be resolved when Docker 22.06 is publicly released.

### gVisor

In mid-2018, Google released gVisor, which is a completely new take on a runtime. It’s OCI compliant and can therefore also be used with Docker. However, gVisor also runs in user space and isolates the application by implementing system calls there rather than relying on Kernel isolation mechanisms. It doesn’t redirect the calls to the kernel; rather, it implements them itself using kernel calls. The most obvious win from this approach is security isolation since gVisor itself is running in user space and thus is isolated from the kernel. Any security issues are still trapped in user space, and all of the kernel security controls we’ve mentioned still apply. The downside is that it typically performs worse than Kernel or VM-based solutions.

If you have processes that do not require massive scaling but do require highly secure isolation, gVisor may be an ideal solution for you. A common use case for gVisor is when your containers will be running code provided by your end users and you cannot guarantee that the code is benign. Let’s run a quick demo so you can see how gVisor works.

Installation is covered in the [gVisor documentation](https://gvisor.dev/docs/user\_guide/quick\_start/docker). It is written in Go and is delivered as a single executable with no packages required. Once it’s installed, you can start containers with the `runsc` runtime. To demonstrate the different isolation levels offered by gVisor, we’ll run a shell using it and compare that to one using a standard container.

First, let’s start a shell on gVisor and look around a bit:

```
$ docker container run --rm --runtime=runsc -it alpine /bin/sh
```

That will drop us into a shell running in an Alpine Linux container. One very revealing difference is apparent when you look at the output of the `mount` command:

```
$ docker container run --rm --runtime=runsc -it alpine /bin/sh -c "mount"

none on / type 9p (rw,trans=fd,rfdno=4,wfdno=4,aname=/,…)
none on /dev type tmpfs (rw,mode=0755)
none on /sys type sysfs (ro,noexec,dentry_cache_limit=1000)
none on /proc type proc (rw,noexec,dentry_cache_limit=1000)
none on /dev/pts type devpts (rw,noexec)
none on /dev/shm type tmpfs (rw,noexec,mode=1777,size=67108864)
none on /etc/hosts type 9p (rw,trans=fd,rfdno=7,wfdno=7,…)
none on /etc/hostname type 9p (rw,trans=fd,rfdno=6,wfdno=6,…)
none on /etc/resolv.conf type 9p (rw,trans=fd,rfdno=5,wfdno=5,…)
none on /tmp type tmpfs (rw,mode=01777)
```

There is not very much in there! Compare that with the output from a traditional container launched with `runc`:

```
$ docker container run --rm -it alpine /bin/sh -c "mount"

overlay on / type overlay (rw,relatime,…)
proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
tmpfs on /dev type tmpfs (rw,nosuid,size=65536k,mode=755,inode64)
devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,…)
sysfs on /sys type sysfs (ro,nosuid,nodev,noexec,relatime)
cgroup on /sys/fs/cgroup type cgroup2 (ro,nosuid,nodev,noexec,relatime)
mqueue on /dev/mqueue type mqueue (rw,nosuid,nodev,noexec,relatime)
shm on /dev/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,…)
/dev/sda3 on /etc/resolv.conf type ext4 (rw,relatime,errors=remount-ro)
…
devpts on /dev/console type devpts (rw,nosuid,noexec,relatime,gid=5,…)
proc on /proc/bus type proc (ro,nosuid,nodev,noexec,relatime)
…
tmpfs on /proc/asound type tmpfs (ro,relatime,inode64)
…
```

This output was 24 lines long, so we truncated it a lot. It should be pretty clear that there is a lot of system detail here. That detail represents the kernel footprint exposed to the container in one way or another. The contrast with the very short output from gVisor should give you an idea of the differing level of isolation. We won’t spend a lot more time on it, but it’s also worth looking at the output of `ip addr show` as well. On gVisor:

```
$ docker container run --rm --runtime=runsc alpine ip addr show

1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65522
    link/loopback 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff
    inet 127.0.0.1/8 scope global dynamic
2: eth0: <UP,LOWER_UP> mtu 1500
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.2/16 scope global dynamic
```

And in a normal Linux container:

```
$ docker container run --rm alpine ip addr show

1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
44: eth0@if45: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc
                                                        noqueue state UP
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever
```

Even the Linux _/proc_ filesystem exposes a lot less in the gVisor container:

```
$ docker container run --rm --runtime=runsc alpine ls -C /proc

1               filesystems     net             sys
cgroups         loadavg         self            thread-self
cmdline         meminfo         sentry-meminfo  uptime
cpuinfo         mounts          stat            version
```

Once more comparing this to a normal Linux container:

```
$ docker container run --rm alpine ls -C /proc

1                  fb                 mdstat             stat
acpi               filesystems        meminfo            swaps
asound             fs                 misc               sys
bootconfig         interrupts         modules            sysrq-trigger
buddyinfo          iomem              mounts             sysvipc
bus                ioports            mpt                thread-self
cgroups            irq                mtd                timer_list
cmdline            kallsyms           mtrr               tty
consoles           kcore              net                uptime
cpuinfo            key-users          pagetypeinfo       version
crypto             keys               partitions         version_signature
devices            kmsg               pressure           vmallocinfo
diskstats          kpagecgroup        schedstat          vmstat
dma                kpagecount         scsi               zoneinfo
driver             kpageflags         self
dynamic_debug      loadavg            slabinfo
execdomains        locks              softirqs
```

Aside from being more isolated, the experience inside the gVisor container is interesting because it looks a lot more like what you might expect to see in an isolated environment. Sandboxed runtimes like gVisor provide a lot of potential for securely running untrusted workloads by providing a much stronger barrier between the application and the underlying kernel.

## Wrap-Up

That’s a quick tour of some of the more advanced concepts of Docker. Hopefully, it has expanded your knowledge of what is happening behind the scenes and has opened up some avenues for you to continue your exploration. As you build and maintain a production platform, this background should provide you with a broad enough perspective of Docker to know where to start when you need to customize the system.

[1](https://learning.oreilly.com/library/view/docker-up/9781098131814/ch11.html#idm46803124791184-marker) To quote the OCI website: “The Open Container Initiative (OCI) is a lightweight, open governance structure (project), formed under the auspices of the Linux Foundation, for the express purpose of creating open industry standards around container formats and runtime. The OCI was launched on June 22nd, 2015 by Docker, CoreOS and other leaders in the container industry.”
