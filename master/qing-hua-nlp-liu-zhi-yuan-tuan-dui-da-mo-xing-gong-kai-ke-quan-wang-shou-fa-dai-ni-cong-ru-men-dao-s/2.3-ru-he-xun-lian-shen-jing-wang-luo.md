---
description: >-
  https://www.bilibili.com/video/BV1UG411p7zv?p=13&vd_source=9772a60c44b4a881b99fd8ac9a574793
---

# 2.3 如何训练神经网络

## Training Objective

<figure><img src="../../.gitbook/assets/image (161).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image (162).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image (163).png" alt=""><figcaption></figcaption></figure>

## Stochastic Gradient Descent

<figure><img src="../../.gitbook/assets/image (164).png" alt=""><figcaption></figcaption></figure>

## Gradients

<figure><img src="../../.gitbook/assets/image (165).png" alt=""><figcaption></figcaption></figure>

## Chain Rule for Jacobians

<figure><img src="../../.gitbook/assets/image (166).png" alt=""><figcaption></figcaption></figure>

## Computational Graphs

<figure><img src="../../.gitbook/assets/image (167).png" alt=""><figcaption></figcaption></figure>

## Backpropagation

* Compute gradients algorithmically
* Used by deep learning frameworks

<figure><img src="../../.gitbook/assets/image (168).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image (169).png" alt=""><figcaption></figcaption></figure>

## Summary

* Forward pass: compute results of operation and save intermediate values
* Backpropagation: recursively apply the chain rule along computational graph to compute gradients
  * \[downstream gradient] = \[upstream gradient] x \[local gradient]

