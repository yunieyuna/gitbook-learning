---
description: >-
  https://www.bilibili.com/video/BV1UG411p7zv?p=13&vd_source=9772a60c44b4a881b99fd8ac9a574793
---

# 2.3 如何训练神经网络

## Training Objective

<figure><img src="../../.gitbook/assets/image (183).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image (184).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image (185).png" alt=""><figcaption></figcaption></figure>

## Stochastic Gradient Descent

<figure><img src="../../.gitbook/assets/image (186).png" alt=""><figcaption></figcaption></figure>

## Gradients

<figure><img src="../../.gitbook/assets/image (187).png" alt=""><figcaption></figcaption></figure>

## Chain Rule for Jacobians

<figure><img src="../../.gitbook/assets/image (188).png" alt=""><figcaption></figcaption></figure>

## Computational Graphs

<figure><img src="../../.gitbook/assets/image (189).png" alt=""><figcaption></figcaption></figure>

## Backpropagation

* Compute gradients algorithmically
* Used by deep learning frameworks

<figure><img src="../../.gitbook/assets/image (190).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image (191).png" alt=""><figcaption></figcaption></figure>

## Summary

* Forward pass: compute results of operation and save intermediate values
* Backpropagation: recursively apply the chain rule along computational graph to compute gradients
  * \[downstream gradient] = \[upstream gradient] x \[local gradient]

